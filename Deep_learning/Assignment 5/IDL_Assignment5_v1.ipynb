{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2wtSADyaQ3SF"
      },
      "source": [
        "# **IDL Assignment 5 - Language Modeling & Recurrent Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b3fVxBd5HocV"
      },
      "source": [
        "## **Assigning Tensorflow version and importing the libraries required for the tasks**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "colab_type": "code",
        "id": "3g1v3YHPFgiV",
        "outputId": "0aae1083-aa88-486c-97ce-3578311ca204",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VIrcPHFKmI_b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "os.getcwd()\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/IDL /IDL Assignments/Assignment helper files\") \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CtZrgAErV832"
      },
      "source": [
        "## Preprocess the text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "zoHuC3T6UXaZ",
        "outputId": "f50bfe51-9428-4a5c-bffe-1ec0c6b08a9f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-05-22 14:45:22.652495: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Split input into 22981 sequences...\n",
            "Serialized 100 sequences...\n",
            "Serialized 200 sequences...\n",
            "Serialized 300 sequences...\n",
            "Serialized 400 sequences...\n",
            "Serialized 500 sequences...\n",
            "Serialized 600 sequences...\n",
            "Serialized 700 sequences...\n",
            "Serialized 800 sequences...\n",
            "Serialized 900 sequences...\n",
            "Serialized 1000 sequences...\n",
            "Serialized 1100 sequences...\n",
            "Serialized 1200 sequences...\n",
            "Serialized 1300 sequences...\n",
            "Serialized 1400 sequences...\n",
            "Serialized 1500 sequences...\n",
            "Serialized 1600 sequences...\n",
            "Serialized 1700 sequences...\n",
            "Serialized 1800 sequences...\n",
            "Serialized 1900 sequences...\n",
            "Serialized 2000 sequences...\n",
            "Serialized 2100 sequences...\n",
            "Serialized 2200 sequences...\n",
            "Serialized 2300 sequences...\n",
            "Serialized 2400 sequences...\n",
            "Serialized 2500 sequences...\n",
            "Serialized 2600 sequences...\n",
            "Serialized 2700 sequences...\n",
            "Serialized 2800 sequences...\n",
            "Serialized 2900 sequences...\n",
            "Serialized 3000 sequences...\n",
            "Serialized 3100 sequences...\n",
            "Serialized 3200 sequences...\n",
            "Serialized 3300 sequences...\n",
            "Serialized 3400 sequences...\n",
            "Serialized 3500 sequences...\n",
            "Serialized 3600 sequences...\n",
            "Serialized 3700 sequences...\n",
            "Serialized 3800 sequences...\n",
            "Serialized 3900 sequences...\n",
            "Serialized 4000 sequences...\n",
            "Serialized 4100 sequences...\n",
            "Serialized 4200 sequences...\n",
            "Serialized 4300 sequences...\n",
            "Serialized 4400 sequences...\n",
            "Serialized 4500 sequences...\n",
            "Serialized 4600 sequences...\n",
            "Serialized 4700 sequences...\n",
            "Serialized 4800 sequences...\n",
            "Serialized 4900 sequences...\n",
            "Serialized 5000 sequences...\n",
            "Serialized 5100 sequences...\n",
            "Serialized 5200 sequences...\n",
            "Serialized 5300 sequences...\n",
            "Serialized 5400 sequences...\n",
            "Serialized 5500 sequences...\n",
            "Serialized 5600 sequences...\n",
            "Serialized 5700 sequences...\n",
            "Serialized 5800 sequences...\n",
            "Serialized 5900 sequences...\n",
            "Serialized 6000 sequences...\n",
            "Serialized 6100 sequences...\n",
            "Serialized 6200 sequences...\n",
            "Serialized 6300 sequences...\n",
            "Serialized 6400 sequences...\n",
            "Serialized 6500 sequences...\n",
            "Serialized 6600 sequences...\n",
            "Serialized 6700 sequences...\n",
            "Serialized 6800 sequences...\n",
            "Serialized 6900 sequences...\n",
            "Serialized 7000 sequences...\n",
            "Serialized 7100 sequences...\n",
            "Serialized 7200 sequences...\n",
            "Serialized 7300 sequences...\n",
            "Serialized 7400 sequences...\n",
            "Serialized 7500 sequences...\n",
            "Serialized 7600 sequences...\n",
            "Serialized 7700 sequences...\n",
            "Serialized 7800 sequences...\n",
            "Serialized 7900 sequences...\n",
            "Serialized 8000 sequences...\n",
            "Serialized 8100 sequences...\n",
            "Serialized 8200 sequences...\n",
            "Serialized 8300 sequences...\n",
            "Serialized 8400 sequences...\n",
            "Serialized 8500 sequences...\n",
            "Serialized 8600 sequences...\n",
            "Serialized 8700 sequences...\n",
            "Serialized 8800 sequences...\n",
            "Serialized 8900 sequences...\n",
            "Serialized 9000 sequences...\n",
            "Serialized 9100 sequences...\n",
            "Serialized 9200 sequences...\n",
            "Serialized 9300 sequences...\n",
            "Serialized 9400 sequences...\n",
            "Serialized 9500 sequences...\n",
            "Serialized 9600 sequences...\n",
            "Serialized 9700 sequences...\n",
            "Serialized 9800 sequences...\n",
            "Serialized 9900 sequences...\n",
            "Serialized 10000 sequences...\n",
            "Serialized 10100 sequences...\n",
            "Serialized 10200 sequences...\n",
            "Serialized 10300 sequences...\n",
            "Serialized 10400 sequences...\n",
            "Serialized 10500 sequences...\n",
            "Serialized 10600 sequences...\n",
            "Serialized 10700 sequences...\n",
            "Serialized 10800 sequences...\n",
            "Serialized 10900 sequences...\n",
            "Serialized 11000 sequences...\n",
            "Serialized 11100 sequences...\n",
            "Serialized 11200 sequences...\n",
            "Serialized 11300 sequences...\n",
            "Serialized 11400 sequences...\n",
            "Serialized 11500 sequences...\n",
            "Serialized 11600 sequences...\n",
            "Serialized 11700 sequences...\n",
            "Serialized 11800 sequences...\n",
            "Serialized 11900 sequences...\n",
            "Serialized 12000 sequences...\n",
            "Serialized 12100 sequences...\n",
            "Serialized 12200 sequences...\n",
            "Serialized 12300 sequences...\n",
            "Serialized 12400 sequences...\n",
            "Serialized 12500 sequences...\n",
            "Serialized 12600 sequences...\n",
            "Serialized 12700 sequences...\n",
            "Serialized 12800 sequences...\n",
            "Serialized 12900 sequences...\n",
            "Serialized 13000 sequences...\n",
            "Serialized 13100 sequences...\n",
            "Serialized 13200 sequences...\n",
            "Serialized 13300 sequences...\n",
            "Serialized 13400 sequences...\n",
            "Serialized 13500 sequences...\n",
            "Serialized 13600 sequences...\n",
            "Serialized 13700 sequences...\n",
            "Serialized 13800 sequences...\n",
            "Serialized 13900 sequences...\n",
            "Serialized 14000 sequences...\n",
            "Serialized 14100 sequences...\n",
            "Serialized 14200 sequences...\n",
            "Serialized 14300 sequences...\n",
            "Serialized 14400 sequences...\n",
            "Serialized 14500 sequences...\n",
            "Serialized 14600 sequences...\n",
            "Serialized 14700 sequences...\n",
            "Serialized 14800 sequences...\n",
            "Serialized 14900 sequences...\n",
            "Serialized 15000 sequences...\n",
            "Serialized 15100 sequences...\n",
            "Serialized 15200 sequences...\n",
            "Serialized 15300 sequences...\n",
            "Serialized 15400 sequences...\n",
            "Serialized 15500 sequences...\n",
            "Serialized 15600 sequences...\n",
            "Serialized 15700 sequences...\n",
            "Serialized 15800 sequences...\n",
            "Serialized 15900 sequences...\n",
            "Serialized 16000 sequences...\n",
            "Serialized 16100 sequences...\n",
            "Serialized 16200 sequences...\n",
            "Serialized 16300 sequences...\n",
            "Serialized 16400 sequences...\n",
            "Serialized 16500 sequences...\n",
            "Serialized 16600 sequences...\n",
            "Serialized 16700 sequences...\n",
            "Serialized 16800 sequences...\n",
            "Serialized 16900 sequences...\n",
            "Serialized 17000 sequences...\n",
            "Serialized 17100 sequences...\n",
            "Serialized 17200 sequences...\n",
            "Serialized 17300 sequences...\n",
            "Serialized 17400 sequences...\n",
            "Serialized 17500 sequences...\n",
            "Serialized 17600 sequences...\n",
            "Serialized 17700 sequences...\n",
            "Serialized 17800 sequences...\n",
            "Serialized 17900 sequences...\n",
            "Serialized 18000 sequences...\n",
            "Serialized 18100 sequences...\n",
            "Serialized 18200 sequences...\n",
            "Serialized 18300 sequences...\n",
            "Serialized 18400 sequences...\n",
            "Serialized 18500 sequences...\n",
            "Serialized 18600 sequences...\n",
            "Serialized 18700 sequences...\n",
            "Serialized 18800 sequences...\n",
            "Serialized 18900 sequences...\n",
            "Serialized 19000 sequences...\n",
            "Serialized 19100 sequences...\n",
            "Serialized 19200 sequences...\n",
            "Serialized 19300 sequences...\n",
            "Serialized 19400 sequences...\n",
            "Serialized 19500 sequences...\n",
            "Serialized 19600 sequences...\n",
            "Serialized 19700 sequences...\n",
            "Serialized 19800 sequences...\n",
            "Serialized 19900 sequences...\n",
            "Serialized 20000 sequences...\n",
            "Serialized 20100 sequences...\n",
            "Serialized 20200 sequences...\n",
            "Serialized 20300 sequences...\n",
            "Serialized 20400 sequences...\n",
            "Serialized 20500 sequences...\n",
            "Serialized 20600 sequences...\n",
            "Serialized 20700 sequences...\n",
            "Serialized 20800 sequences...\n",
            "Serialized 20900 sequences...\n",
            "Serialized 21000 sequences...\n",
            "Serialized 21100 sequences...\n",
            "Serialized 21200 sequences...\n",
            "Serialized 21300 sequences...\n",
            "Serialized 21400 sequences...\n",
            "Serialized 21500 sequences...\n",
            "Serialized 21600 sequences...\n",
            "Serialized 21700 sequences...\n",
            "Serialized 21800 sequences...\n",
            "Serialized 21900 sequences...\n",
            "Serialized 22000 sequences...\n",
            "Serialized 22100 sequences...\n",
            "Serialized 22200 sequences...\n",
            "Serialized 22300 sequences...\n",
            "Serialized 22400 sequences...\n",
            "Serialized 22500 sequences...\n",
            "Serialized 22600 sequences...\n",
            "Serialized 22700 sequences...\n",
            "Serialized 22800 sequences...\n",
            "Serialized 22900 sequences...\n"
          ]
        }
      ],
      "source": [
        "!python prepare_data.py shakespeare_input.txt skp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TuThM3SqW-zX"
      },
      "source": [
        "**total serialized seq is 22981**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SlitZe4kwCE7"
      },
      "source": [
        "**Loading the data from skp.tfrecords and skp_vocab**\n",
        "\n",
        "The files mentioned are the output obtained after running the program *prepare_data.py* for the Shakespeare data. These files are loaded as data using tf.data and create a vocabulary dictionary \n",
        "\n",
        "**Note:** The vocab contains elements as dict with (key,val) as (character, index). Reverse mapping is done and stored as ind_to_ch which has (key,val) as (index,character)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "colab_type": "code",
        "id": "qZ9hhvAKWLbQ",
        "outputId": "cff2b724-fa5f-43a8-c178-9cf5e58a8371",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'-': 1, '.': 2, 'u': 3, 'o': 4, 'Z': 5, 'U': 6, 'i': 7, ';': 8, '[': 9, 'y': 10, '?': 11, 'n': 12, 'z': 13, ',': 14, 'S': 15, 't': 16, 'N': 17, ':': 18, 'R': 19, '$': 20, 'e': 21, 'J': 22, 'q': 23, 'L': 24, 'l': 25, 'W': 26, 'w': 27, 'F': 28, 'V': 29, 'E': 30, 'Q': 31, 'I': 32, '!': 33, 'G': 34, 'a': 35, 'd': 36, 'v': 37, 'b': 38, 'O': 39, 'K': 40, '3': 41, 'T': 42, 'm': 43, 'Y': 44, ']': 45, ' ': 46, 'g': 47, 'r': 48, 'P': 49, 'k': 50, \"'\": 51, 'H': 52, 'A': 53, 'C': 54, 'B': 55, 'x': 56, 'p': 57, 'D': 58, 'h': 59, '&': 60, 'M': 61, 'X': 62, '\\n': 63, 'f': 64, 'c': 65, 'j': 66, 's': 67, '<S>': 0}\n",
            "68\n",
            "Indices to char\n",
            "{1: '-', 2: '.', 3: 'u', 4: 'o', 5: 'Z', 6: 'U', 7: 'i', 8: ';', 9: '[', 10: 'y', 11: '?', 12: 'n', 13: 'z', 14: ',', 15: 'S', 16: 't', 17: 'N', 18: ':', 19: 'R', 20: '$', 21: 'e', 22: 'J', 23: 'q', 24: 'L', 25: 'l', 26: 'W', 27: 'w', 28: 'F', 29: 'V', 30: 'E', 31: 'Q', 32: 'I', 33: '!', 34: 'G', 35: 'a', 36: 'd', 37: 'v', 38: 'b', 39: 'O', 40: 'K', 41: '3', 42: 'T', 43: 'm', 44: 'Y', 45: ']', 46: ' ', 47: 'g', 48: 'r', 49: 'P', 50: 'k', 51: \"'\", 52: 'H', 53: 'A', 54: 'C', 55: 'B', 56: 'x', 57: 'p', 58: 'D', 59: 'h', 60: '&', 61: 'M', 62: 'X', 63: '\\n', 64: 'f', 65: 'c', 66: 'j', 67: 's', 0: '<S>'}\n"
          ]
        }
      ],
      "source": [
        "from prepare_data import parse_seq\n",
        "import pickle\n",
        "\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data = tf.data.TFRecordDataset(\"skp.tfrecords\")\n",
        "\n",
        "# this maps a parser function that properly interprets the bytes over the dataset\n",
        "# (with fixed sequence length 200)\n",
        "# if you change the sequence length in preprocessing you also need to change it here\n",
        "data = data.map(lambda x: parse_seq(x, 200))\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab = pickle.load(open(\"skp_vocab\", mode=\"rb\"))\n",
        "vocab_size = len(vocab)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "\n",
        "print(vocab)\n",
        "print(vocab_size)\n",
        "\n",
        "print(\"Indices to char\")\n",
        "print(ind_to_ch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "lNQDmDUfTx2Z",
        "outputId": "5b0d0147-1631-475e-a833-478727c8699d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<MapDataset shapes: (200,), types: tf.int32>"
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rFRLy9TYEeKQ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## Declare the sizes of batch, shuffle and repeat\n",
        "\n",
        "SHUFFLE_SIZE = 1000\n",
        "BATCH_SIZE = 128\n",
        "REPEAT_TIMES = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "H5JAavDtD_cu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def batch_shuffle_repeat(data):\n",
        "\n",
        "\n",
        "    data = data.shuffle(SHUFFLE_SIZE)\n",
        "    data = data.padded_batch(BATCH_SIZE, padded_shapes=None,drop_remainder=False)   \n",
        "    data = data.repeat(REPEAT_TIMES)\n",
        "\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wyyEwmin_pDP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "dataset = batch_shuffle_repeat(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "colab_type": "code",
        "id": "SpGmPlMCpGiX",
        "outputId": "68b9842c-e220-4939-ec9e-ec6d3760b704",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([128 200], shape=(2,), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([128, 200], dtype=int32)>\n",
            "tf.Tensor(\n",
            "[[ 0 26 59 ... 67 16 46]\n",
            " [ 0 16 46 ... 48 35 16]\n",
            " [ 0 21 46 ...  7 67 46]\n",
            " ...\n",
            " [ 0 25  7 ... 16 46 67]\n",
            " [ 0 63 32 ... 46 16 48]\n",
            " [ 0 46 59 ... 67 16  4]], shape=(128, 200), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for x in dataset.take(1):\n",
        "  print(tf.shape(x))\n",
        "  print(type(x))\n",
        "  print(repr(tf.shape(x)))\n",
        "  print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "E1VmdC2twpp6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "n_h = 512\n",
        "\n",
        "## w_xh is input to hidden weight --> known as U from the literature\n",
        "## w_hh is hidden to hidden weights --> known as W from the literature\n",
        "## w_ho is hidden to output weights --> known as V from the literature\n",
        "## b_h and b_o are the biases at the hidden layer and output layer\n",
        "\n",
        "\n",
        "w_xh = tf.Variable(tf.initializers.glorot_uniform()([vocab_size,n_h]))\n",
        "\n",
        "w_hh = tf.Variable(tf.initializers.glorot_uniform()([n_h,n_h]))\n",
        "b_h = tf.Variable(tf.zeros([n_h]))\n",
        "\n",
        "w_ho = tf.Variable(tf.initializers.glorot_uniform()([n_h,vocab_size]))\n",
        "b_o = tf.Variable(tf.zeros([vocab_size]))\n",
        "\n",
        "variables = [w_xh,w_hh,b_h,w_ho,b_o]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "E_Vqww0cNDBW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "opt = tf.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def rnn_sequence(batch_data):\n",
        "    with tf.GradientTape() as tape:\n",
        "        h_t = tf.zeros([tf.shape(batch_data)[0],n_h])\n",
        "        loss = tf.TensorArray(tf.float32,size=tf.shape(batch_data)[1]-1)\n",
        "\n",
        "        for timestep in tf.range(tf.shape(batch_data)[1]-1):\n",
        "            x_t = tf.one_hot(batch_data[:,timestep],vocab_size)\n",
        "            h_t = tf.nn.tanh(tf.matmul(x_t,w_xh) + tf.matmul(h_t,w_hh) + b_h)\n",
        "            logits = tf.matmul(h_t,w_ho) + b_o\n",
        "\n",
        "            local_loss = loss_fn(batch_data[:,timestep+1],logits)\n",
        "\n",
        "            loss = loss.write(timestep, local_loss)\n",
        "        loss = loss.stack()\n",
        "\n",
        "        batch_loss = tf.reduce_mean(loss)\n",
        "        \n",
        "    \n",
        "    grads = tape.gradient(batch_loss, variables)\n",
        "    opt.apply_gradients(zip(grads, variables))\n",
        "\n",
        "    return batch_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wa3BPxZo22jY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "##### DONT RUN THIS BLOCK .. THIS IS JUST FOR TRIAL PURPOSES\n",
        "\n",
        "\n",
        "###############################################################################################################################################\n",
        "opt = tf.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "\"\"\"\n",
        "        tf.print(tf.shape(batch_data))\n",
        "        tf.print(type(batch_data))\n",
        "        tf.print(repr(tf.shape(batch_data)))\n",
        "        tf.print(batch_data)\n",
        "\n",
        "        for i in tf.range(tf.shape(batch_data)[1]-1):\n",
        "          tf.print(i)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def rnn_try(batch_data):\n",
        "    with tf.GradientTape() as tape:\n",
        "        h_t = tf.zeros([tf.shape(batch_data)[0],n_h])\n",
        "        loss = tf.TensorArray(tf.float32,size=tf.shape(batch_data)[1]-1)\n",
        "        \n",
        "\n",
        "        for timestep in tf.range(tf.shape(batch_data)[1]-1):\n",
        "            tf.print(\"=\"*100)\n",
        "            tf.print(timestep)\n",
        "\n",
        "            x_t = tf.one_hot(batch_data[:,timestep],vocab_size)\n",
        "            tf.print(tf.shape(x_t))\n",
        "            tf.print(type(x_t))\n",
        "            tf.print(repr(tf.shape(x_t)))\n",
        "\n",
        "\n",
        "\n",
        "            h_t = tf.nn.tanh(tf.matmul(x_t,w_xh) + tf.matmul(h_t,w_hh) + b_h)\n",
        "            tf.print(tf.shape(h_t))\n",
        "            tf.print(type(h_t))\n",
        "            tf.print(repr(tf.shape(h_t)))\n",
        "\n",
        "\n",
        "            logits = tf.matmul(h_t,w_ho) + b_o\n",
        "            tf.print(tf.shape(logits))\n",
        "            tf.print(type(logits))\n",
        "            tf.print(repr(tf.shape(logits)))\n",
        "\n",
        "\n",
        "            local_loss = loss_fn(batch_data[:,timestep+1],logits)\n",
        "            tf.print(tf.shape(local_loss))\n",
        "            tf.print(type(local_loss))\n",
        "            tf.print(repr(tf.shape(local_loss)))\n",
        "\n",
        "\n",
        "            loss = loss.write(timestep, local_loss)\n",
        "\n",
        "        loss = loss.stack()\n",
        "        tf.print(tf.shape(loss))\n",
        "        tf.print(type(loss))\n",
        "        tf.print(repr(tf.shape(loss)))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "BKsNkX8luxv8",
        "outputId": "39c76cf5-624f-482e-f027-a199f3fbb6c0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch Number: 1 Loss: 4.268254280090332 Time taken: 3.021419048309326\n",
            "Batch Number: 2 Loss: 4.130809307098389 Time taken: 0.3425624370574951\n",
            "Batch Number: 3 Loss: 3.7822329998016357 Time taken: 0.3167083263397217\n",
            "Batch Number: 4 Loss: 3.567951202392578 Time taken: 0.32003140449523926\n",
            "Batch Number: 5 Loss: 3.523487091064453 Time taken: 0.32482409477233887\n",
            "Batch Number: 6 Loss: 3.432318687438965 Time taken: 0.3274343013763428\n",
            "Batch Number: 7 Loss: 3.3816611766815186 Time taken: 0.3256051540374756\n",
            "Batch Number: 8 Loss: 3.381779432296753 Time taken: 0.32646989822387695\n",
            "Batch Number: 9 Loss: 3.3538711071014404 Time taken: 0.3188135623931885\n",
            "Batch Number: 10 Loss: 3.3600902557373047 Time taken: 0.32323265075683594\n",
            "Batch Number: 11 Loss: 3.341378927230835 Time taken: 0.32181215286254883\n",
            "Batch Number: 12 Loss: 3.309678554534912 Time taken: 0.31246161460876465\n",
            "Batch Number: 13 Loss: 3.2677252292633057 Time taken: 0.32370471954345703\n",
            "Batch Number: 14 Loss: 3.300065040588379 Time taken: 0.33074331283569336\n",
            "Batch Number: 15 Loss: 3.261983633041382 Time taken: 0.3331339359283447\n",
            "Batch Number: 16 Loss: 3.2952914237976074 Time taken: 0.34465956687927246\n",
            "Batch Number: 17 Loss: 3.4505774974823 Time taken: 0.32857179641723633\n",
            "Batch Number: 18 Loss: 3.2640368938446045 Time taken: 0.30307435989379883\n",
            "Batch Number: 19 Loss: 3.2373578548431396 Time taken: 0.3066067695617676\n",
            "Batch Number: 20 Loss: 3.254650592803955 Time taken: 0.308746337890625\n",
            "Batch Number: 21 Loss: 3.223696708679199 Time taken: 0.3243069648742676\n",
            "Batch Number: 22 Loss: 3.2269649505615234 Time taken: 0.3156254291534424\n",
            "Batch Number: 23 Loss: 4.011166572570801 Time taken: 0.30871081352233887\n",
            "Batch Number: 24 Loss: 3.332979202270508 Time taken: 0.33670783042907715\n",
            "Batch Number: 25 Loss: 3.359930992126465 Time taken: 0.3439958095550537\n",
            "Batch Number: 26 Loss: 3.388249635696411 Time taken: 0.31528353691101074\n",
            "Batch Number: 27 Loss: 3.307121515274048 Time taken: 0.3275878429412842\n",
            "Batch Number: 28 Loss: 3.290212631225586 Time taken: 0.31183385848999023\n",
            "Batch Number: 29 Loss: 3.300304889678955 Time taken: 0.31099581718444824\n",
            "Batch Number: 30 Loss: 3.291822910308838 Time taken: 0.3320963382720947\n",
            "Batch Number: 31 Loss: 3.274386167526245 Time taken: 0.3251767158508301\n",
            "Batch Number: 32 Loss: 3.2732791900634766 Time taken: 0.32085084915161133\n",
            "Batch Number: 33 Loss: 3.280177116394043 Time taken: 0.32683539390563965\n",
            "Batch Number: 34 Loss: 3.263791799545288 Time taken: 0.31087660789489746\n",
            "Batch Number: 35 Loss: 3.2536282539367676 Time taken: 0.31555724143981934\n",
            "Batch Number: 36 Loss: 3.294027805328369 Time taken: 0.32353973388671875\n",
            "Batch Number: 37 Loss: 3.2805724143981934 Time taken: 0.31676435470581055\n",
            "Batch Number: 38 Loss: 3.2561235427856445 Time taken: 0.31964874267578125\n",
            "Batch Number: 39 Loss: 3.270775318145752 Time taken: 0.3191838264465332\n",
            "Batch Number: 40 Loss: 3.249171018600464 Time taken: 0.31188368797302246\n",
            "Batch Number: 41 Loss: 3.2558865547180176 Time taken: 0.3238654136657715\n",
            "Batch Number: 42 Loss: 3.2454428672790527 Time taken: 0.3268435001373291\n",
            "Batch Number: 43 Loss: 3.238950252532959 Time taken: 0.3203907012939453\n",
            "Batch Number: 44 Loss: 3.241760492324829 Time taken: 0.335263729095459\n",
            "Batch Number: 45 Loss: 3.211245536804199 Time taken: 0.3196754455566406\n",
            "Batch Number: 46 Loss: 3.2255938053131104 Time taken: 0.3236429691314697\n",
            "Batch Number: 47 Loss: 3.239058494567871 Time taken: 0.32482075691223145\n",
            "Batch Number: 48 Loss: 3.221698760986328 Time taken: 0.34499144554138184\n",
            "Batch Number: 49 Loss: 3.2137341499328613 Time taken: 0.3420374393463135\n",
            "Batch Number: 50 Loss: 3.2112691402435303 Time taken: 0.3227729797363281\n",
            "Batch Number: 51 Loss: 3.2266738414764404 Time taken: 0.30898237228393555\n",
            "Batch Number: 52 Loss: 3.232412338256836 Time taken: 0.3368191719055176\n",
            "Batch Number: 53 Loss: 3.234546184539795 Time taken: 0.32093286514282227\n",
            "Batch Number: 54 Loss: 3.2467663288116455 Time taken: 0.3179004192352295\n",
            "Batch Number: 55 Loss: 3.1866767406463623 Time taken: 0.3225085735321045\n",
            "Batch Number: 56 Loss: 3.188178777694702 Time taken: 0.32306456565856934\n",
            "Batch Number: 57 Loss: 3.1759161949157715 Time taken: 0.310955286026001\n",
            "Batch Number: 58 Loss: 3.1864969730377197 Time taken: 0.3319966793060303\n",
            "Batch Number: 59 Loss: 3.1713814735412598 Time taken: 0.31986379623413086\n",
            "Batch Number: 60 Loss: 3.1512057781219482 Time taken: 0.3097052574157715\n",
            "Batch Number: 61 Loss: 3.139551877975464 Time taken: 0.3273332118988037\n",
            "Batch Number: 62 Loss: 3.1530792713165283 Time taken: 0.3131871223449707\n",
            "Batch Number: 63 Loss: 3.129718065261841 Time taken: 0.3112223148345947\n",
            "Batch Number: 64 Loss: 3.133222818374634 Time taken: 0.31844258308410645\n",
            "Batch Number: 65 Loss: 3.111096143722534 Time taken: 0.33341288566589355\n",
            "Batch Number: 66 Loss: 3.129406213760376 Time taken: 0.31258392333984375\n",
            "Batch Number: 67 Loss: 3.102487802505493 Time taken: 0.3189969062805176\n",
            "Batch Number: 68 Loss: 3.1071527004241943 Time taken: 0.31615400314331055\n",
            "Batch Number: 69 Loss: 3.083740234375 Time taken: 0.33209919929504395\n",
            "Batch Number: 70 Loss: 3.068875551223755 Time taken: 0.3114497661590576\n",
            "Batch Number: 71 Loss: 3.062405586242676 Time taken: 0.3057889938354492\n",
            "Batch Number: 72 Loss: 3.069016695022583 Time taken: 0.3274853229522705\n",
            "Batch Number: 73 Loss: 3.0485799312591553 Time taken: 0.31572604179382324\n",
            "Batch Number: 74 Loss: 3.0610909461975098 Time taken: 0.3236408233642578\n",
            "Batch Number: 75 Loss: 3.0580990314483643 Time taken: 0.32536864280700684\n",
            "Batch Number: 76 Loss: 3.0418429374694824 Time taken: 0.32776713371276855\n",
            "Batch Number: 77 Loss: 3.042938470840454 Time taken: 0.32309651374816895\n",
            "Batch Number: 78 Loss: 3.026989221572876 Time taken: 0.3288443088531494\n",
            "Batch Number: 79 Loss: 3.0026297569274902 Time taken: 0.31527018547058105\n",
            "Batch Number: 80 Loss: 3.040262460708618 Time taken: 0.34835028648376465\n",
            "Batch Number: 81 Loss: 3.0016207695007324 Time taken: 0.32961225509643555\n",
            "Batch Number: 82 Loss: 2.987293243408203 Time taken: 0.31722211837768555\n",
            "Batch Number: 83 Loss: 2.9835262298583984 Time taken: 0.32241344451904297\n",
            "Batch Number: 84 Loss: 2.988301992416382 Time taken: 0.3364858627319336\n",
            "Batch Number: 85 Loss: 2.9767894744873047 Time taken: 0.3177683353424072\n",
            "Batch Number: 86 Loss: 2.96518874168396 Time taken: 0.33347296714782715\n",
            "Batch Number: 87 Loss: 2.9769906997680664 Time taken: 0.32482028007507324\n",
            "Batch Number: 88 Loss: 2.961913824081421 Time taken: 0.3013451099395752\n",
            "Batch Number: 89 Loss: 2.9222211837768555 Time taken: 0.3219635486602783\n",
            "Batch Number: 90 Loss: 2.92995023727417 Time taken: 0.31346678733825684\n",
            "Batch Number: 91 Loss: 2.933845281600952 Time taken: 0.31384706497192383\n",
            "Batch Number: 92 Loss: 2.925604820251465 Time taken: 0.3190572261810303\n",
            "Batch Number: 93 Loss: 2.93046236038208 Time taken: 0.3337843418121338\n",
            "Batch Number: 94 Loss: 2.923964500427246 Time taken: 0.31932902336120605\n",
            "Batch Number: 95 Loss: 2.927259922027588 Time taken: 0.3247108459472656\n",
            "Batch Number: 96 Loss: 2.8991925716400146 Time taken: 0.3242206573486328\n",
            "Batch Number: 97 Loss: 2.899646282196045 Time taken: 0.3174147605895996\n",
            "Batch Number: 98 Loss: 2.8825738430023193 Time taken: 0.31783604621887207\n",
            "Batch Number: 99 Loss: 2.857243061065674 Time taken: 0.3214855194091797\n",
            "Batch Number: 100 Loss: 2.878392457962036 Time taken: 0.32376885414123535\n",
            "Batch Number: 101 Loss: 2.8547587394714355 Time taken: 0.33036136627197266\n",
            "Batch Number: 102 Loss: 2.8561580181121826 Time taken: 0.32021164894104004\n",
            "Batch Number: 103 Loss: 2.8310415744781494 Time taken: 0.31899452209472656\n",
            "Batch Number: 104 Loss: 2.8510043621063232 Time taken: 0.31751394271850586\n",
            "Batch Number: 105 Loss: 2.830500364303589 Time taken: 0.3232390880584717\n",
            "Batch Number: 106 Loss: 2.8024420738220215 Time taken: 0.33397769927978516\n",
            "Batch Number: 107 Loss: 2.7987871170043945 Time taken: 0.31677675247192383\n",
            "Batch Number: 108 Loss: 2.791799306869507 Time taken: 0.32115888595581055\n",
            "Batch Number: 109 Loss: 2.79172420501709 Time taken: 0.319044828414917\n",
            "Batch Number: 110 Loss: 2.779205083847046 Time taken: 0.31627964973449707\n",
            "Batch Number: 111 Loss: 2.801881790161133 Time taken: 0.3229024410247803\n",
            "Batch Number: 112 Loss: 2.757444381713867 Time taken: 0.34647130966186523\n",
            "Batch Number: 113 Loss: 2.765316963195801 Time taken: 0.31650853157043457\n",
            "Batch Number: 114 Loss: 2.7538375854492188 Time taken: 0.3318765163421631\n",
            "Batch Number: 115 Loss: 2.760464906692505 Time taken: 0.3181123733520508\n",
            "Batch Number: 116 Loss: 2.7527077198028564 Time taken: 0.3015275001525879\n",
            "Batch Number: 117 Loss: 2.726651191711426 Time taken: 0.3367033004760742\n",
            "Batch Number: 118 Loss: 2.728538990020752 Time taken: 0.32692956924438477\n",
            "Batch Number: 119 Loss: 2.7251248359680176 Time taken: 0.3159770965576172\n",
            "Batch Number: 120 Loss: 2.732860565185547 Time taken: 0.33211636543273926\n",
            "Batch Number: 121 Loss: 2.7119333744049072 Time taken: 0.32660746574401855\n",
            "Batch Number: 122 Loss: 2.7227118015289307 Time taken: 0.31609439849853516\n",
            "Batch Number: 123 Loss: 2.6868338584899902 Time taken: 0.3166391849517822\n",
            "Batch Number: 124 Loss: 2.706207036972046 Time taken: 0.32739734649658203\n",
            "Batch Number: 125 Loss: 2.693910837173462 Time taken: 0.31545114517211914\n",
            "Batch Number: 126 Loss: 2.6952998638153076 Time taken: 0.31970691680908203\n",
            "Batch Number: 127 Loss: 2.6952407360076904 Time taken: 0.3270261287689209\n",
            "Batch Number: 128 Loss: 2.6554574966430664 Time taken: 0.3145015239715576\n",
            "Batch Number: 129 Loss: 2.6687986850738525 Time taken: 0.31588220596313477\n",
            "Batch Number: 130 Loss: 2.6517045497894287 Time taken: 0.32165980339050293\n",
            "Batch Number: 131 Loss: 2.665405511856079 Time taken: 0.31394076347351074\n",
            "Batch Number: 132 Loss: 2.6542587280273438 Time taken: 0.3157768249511719\n",
            "Batch Number: 133 Loss: 2.6710634231567383 Time taken: 0.3332512378692627\n",
            "Batch Number: 134 Loss: 2.655245780944824 Time taken: 0.3136417865753174\n",
            "Batch Number: 135 Loss: 2.6564040184020996 Time taken: 0.32006335258483887\n",
            "Batch Number: 136 Loss: 2.645167589187622 Time taken: 0.3257608413696289\n",
            "Batch Number: 137 Loss: 2.6227941513061523 Time taken: 0.3326437473297119\n",
            "Batch Number: 138 Loss: 2.618886947631836 Time taken: 0.3156003952026367\n",
            "Batch Number: 139 Loss: 2.6190567016601562 Time taken: 0.3231852054595947\n",
            "Batch Number: 140 Loss: 2.5966413021087646 Time taken: 0.3228018283843994\n",
            "Batch Number: 141 Loss: 2.6317126750946045 Time taken: 0.3184537887573242\n",
            "Batch Number: 142 Loss: 2.599140167236328 Time taken: 0.32643771171569824\n",
            "Batch Number: 143 Loss: 2.6057162284851074 Time taken: 0.321044921875\n",
            "Batch Number: 144 Loss: 2.6090188026428223 Time taken: 0.333629846572876\n",
            "Batch Number: 145 Loss: 2.598132610321045 Time taken: 0.3316526412963867\n",
            "Batch Number: 146 Loss: 2.5898451805114746 Time taken: 0.3235325813293457\n",
            "Batch Number: 147 Loss: 2.579584836959839 Time taken: 0.3133046627044678\n",
            "Batch Number: 148 Loss: 2.5800952911376953 Time taken: 0.3205106258392334\n",
            "Batch Number: 149 Loss: 2.5890660285949707 Time taken: 0.3217732906341553\n",
            "Batch Number: 150 Loss: 2.5646374225616455 Time taken: 0.3211400508880615\n",
            "Batch Number: 151 Loss: 2.5584633350372314 Time taken: 0.3186309337615967\n",
            "Batch Number: 152 Loss: 2.5672571659088135 Time taken: 0.3185465335845947\n",
            "Batch Number: 153 Loss: 2.5467045307159424 Time taken: 0.3207681179046631\n",
            "Batch Number: 154 Loss: 2.5405664443969727 Time taken: 0.3348250389099121\n",
            "Batch Number: 155 Loss: 2.5522167682647705 Time taken: 0.32300853729248047\n",
            "Batch Number: 156 Loss: 2.5350804328918457 Time taken: 0.312762975692749\n",
            "Batch Number: 157 Loss: 2.5479912757873535 Time taken: 0.32346343994140625\n",
            "Batch Number: 158 Loss: 2.5411694049835205 Time taken: 0.32456398010253906\n",
            "Batch Number: 159 Loss: 2.546084403991699 Time taken: 0.3145174980163574\n",
            "Batch Number: 160 Loss: 2.5391621589660645 Time taken: 0.3131897449493408\n",
            "Batch Number: 161 Loss: 2.537611484527588 Time taken: 0.3332233428955078\n",
            "Batch Number: 162 Loss: 2.5321879386901855 Time taken: 0.3247404098510742\n",
            "Batch Number: 163 Loss: 2.5191705226898193 Time taken: 0.30695629119873047\n",
            "Batch Number: 164 Loss: 2.5262577533721924 Time taken: 0.33368730545043945\n",
            "Batch Number: 165 Loss: 2.521939277648926 Time taken: 0.30232858657836914\n",
            "Batch Number: 166 Loss: 2.5304114818573 Time taken: 0.31281280517578125\n",
            "Batch Number: 167 Loss: 2.627657413482666 Time taken: 0.3317568302154541\n",
            "Batch Number: 168 Loss: 2.6215016841888428 Time taken: 0.3035714626312256\n",
            "Batch Number: 169 Loss: 2.531370162963867 Time taken: 0.31844329833984375\n",
            "Batch Number: 170 Loss: 2.524653196334839 Time taken: 0.31697702407836914\n",
            "Batch Number: 171 Loss: 2.527343988418579 Time taken: 0.31896042823791504\n",
            "Batch Number: 172 Loss: 2.529357671737671 Time taken: 0.31934571266174316\n",
            "Batch Number: 173 Loss: 2.5086376667022705 Time taken: 0.31386518478393555\n",
            "Batch Number: 174 Loss: 2.52960205078125 Time taken: 0.32875537872314453\n",
            "Batch Number: 175 Loss: 2.5171544551849365 Time taken: 0.3091390132904053\n",
            "Batch Number: 176 Loss: 2.4918975830078125 Time taken: 0.32766079902648926\n",
            "Batch Number: 177 Loss: 2.4984018802642822 Time taken: 0.35121703147888184\n",
            "Batch Number: 178 Loss: 2.4858100414276123 Time taken: 0.3171706199645996\n",
            "Batch Number: 179 Loss: 2.483090877532959 Time taken: 0.3194568157196045\n",
            "Batch Number: 180 Loss: 2.465827465057373 Time taken: 0.8264577388763428\n",
            "Batch Number: 181 Loss: 2.4588401317596436 Time taken: 0.3296961784362793\n",
            "Batch Number: 182 Loss: 2.4744441509246826 Time taken: 0.31066417694091797\n",
            "Batch Number: 183 Loss: 2.453294515609741 Time taken: 0.320789098739624\n",
            "Batch Number: 184 Loss: 2.451192855834961 Time taken: 0.32598376274108887\n",
            "Batch Number: 185 Loss: 2.4636013507843018 Time taken: 0.31014299392700195\n",
            "Batch Number: 186 Loss: 2.4546022415161133 Time taken: 0.3146369457244873\n",
            "Batch Number: 187 Loss: 2.451331615447998 Time taken: 0.3315248489379883\n",
            "Batch Number: 188 Loss: 2.452906370162964 Time taken: 0.3318490982055664\n",
            "Batch Number: 189 Loss: 2.451073408126831 Time taken: 0.3293263912200928\n",
            "Batch Number: 190 Loss: 2.455615520477295 Time taken: 0.3247387409210205\n",
            "Batch Number: 191 Loss: 2.452692747116089 Time taken: 0.3221602439880371\n",
            "Batch Number: 192 Loss: 2.443321704864502 Time taken: 0.3209714889526367\n",
            "Batch Number: 193 Loss: 2.4588160514831543 Time taken: 0.3176536560058594\n",
            "Batch Number: 194 Loss: 2.4150474071502686 Time taken: 0.3276042938232422\n",
            "Batch Number: 195 Loss: 2.4191415309906006 Time taken: 0.3211212158203125\n",
            "Batch Number: 196 Loss: 2.437203884124756 Time taken: 0.3259003162384033\n",
            "Batch Number: 197 Loss: 2.421184539794922 Time taken: 0.31989169120788574\n",
            "Batch Number: 198 Loss: 2.3988966941833496 Time taken: 0.31450843811035156\n",
            "Batch Number: 199 Loss: 2.424179792404175 Time taken: 0.3168525695800781\n",
            "Batch Number: 200 Loss: 2.4141197204589844 Time taken: 0.34149813652038574\n",
            "Batch Number: 201 Loss: 2.404479742050171 Time taken: 0.3171195983886719\n",
            "Batch Number: 202 Loss: 2.426055908203125 Time taken: 0.32125425338745117\n",
            "Batch Number: 203 Loss: 2.416551113128662 Time taken: 0.33008790016174316\n",
            "Batch Number: 204 Loss: 2.4025514125823975 Time taken: 0.3101780414581299\n",
            "Batch Number: 205 Loss: 2.378410816192627 Time taken: 0.3175523281097412\n",
            "Batch Number: 206 Loss: 2.3902599811553955 Time taken: 0.34701061248779297\n",
            "Batch Number: 207 Loss: 2.3950276374816895 Time taken: 0.334134578704834\n",
            "Batch Number: 208 Loss: 2.388960123062134 Time taken: 0.3194131851196289\n",
            "Batch Number: 209 Loss: 2.383554697036743 Time taken: 0.3318307399749756\n",
            "Batch Number: 210 Loss: 2.396512269973755 Time taken: 0.31366491317749023\n",
            "Batch Number: 211 Loss: 2.398160934448242 Time taken: 0.31366968154907227\n",
            "Batch Number: 212 Loss: 2.388406753540039 Time taken: 0.33538365364074707\n",
            "Batch Number: 213 Loss: 2.3745791912078857 Time taken: 0.32390832901000977\n",
            "Batch Number: 214 Loss: 2.3727304935455322 Time taken: 0.31147217750549316\n",
            "Batch Number: 215 Loss: 2.376950979232788 Time taken: 0.33246374130249023\n",
            "Batch Number: 216 Loss: 2.379436731338501 Time taken: 0.3194913864135742\n",
            "Batch Number: 217 Loss: 2.3903095722198486 Time taken: 0.3080124855041504\n",
            "Batch Number: 218 Loss: 2.4024012088775635 Time taken: 0.32901573181152344\n",
            "Batch Number: 219 Loss: 2.4058632850646973 Time taken: 0.316852331161499\n",
            "Batch Number: 220 Loss: 2.3638553619384766 Time taken: 0.31517791748046875\n",
            "Batch Number: 221 Loss: 2.386090040206909 Time taken: 0.3174464702606201\n",
            "Batch Number: 222 Loss: 2.3558144569396973 Time taken: 0.323899507522583\n",
            "Batch Number: 223 Loss: 2.347872257232666 Time taken: 0.3131537437438965\n",
            "Batch Number: 224 Loss: 2.3424465656280518 Time taken: 0.3142387866973877\n",
            "Batch Number: 225 Loss: 2.343663454055786 Time taken: 0.3226287364959717\n",
            "Batch Number: 226 Loss: 2.3506252765655518 Time taken: 0.3195371627807617\n",
            "Batch Number: 227 Loss: 2.360285520553589 Time taken: 0.3181123733520508\n",
            "Batch Number: 228 Loss: 2.3571155071258545 Time taken: 0.32437944412231445\n",
            "Batch Number: 229 Loss: 2.3750195503234863 Time taken: 0.32486391067504883\n",
            "Batch Number: 230 Loss: 2.3592562675476074 Time taken: 0.32565736770629883\n",
            "Batch Number: 231 Loss: 2.3609535694122314 Time taken: 0.3522179126739502\n",
            "Batch Number: 232 Loss: 2.3764419555664062 Time taken: 0.33797144889831543\n",
            "Batch Number: 233 Loss: 2.3662099838256836 Time taken: 0.3202095031738281\n",
            "Batch Number: 234 Loss: 2.3785877227783203 Time taken: 0.34140610694885254\n",
            "Batch Number: 235 Loss: 2.3572728633880615 Time taken: 0.34990930557250977\n",
            "Batch Number: 236 Loss: 2.358278274536133 Time taken: 0.33643579483032227\n",
            "Batch Number: 237 Loss: 2.335313081741333 Time taken: 0.3396012783050537\n",
            "Batch Number: 238 Loss: 2.346029043197632 Time taken: 0.32989025115966797\n",
            "Batch Number: 239 Loss: 2.318157434463501 Time taken: 0.34023380279541016\n",
            "Batch Number: 240 Loss: 2.3370862007141113 Time taken: 0.3440115451812744\n",
            "Batch Number: 241 Loss: 2.304692506790161 Time taken: 0.3194234371185303\n",
            "Batch Number: 242 Loss: 2.3316433429718018 Time taken: 0.3271327018737793\n",
            "Batch Number: 243 Loss: 2.3084218502044678 Time taken: 0.33848023414611816\n",
            "Batch Number: 244 Loss: 2.319545030593872 Time taken: 0.32384681701660156\n",
            "Batch Number: 245 Loss: 2.301762580871582 Time taken: 0.30672740936279297\n",
            "Batch Number: 246 Loss: 2.3091015815734863 Time taken: 0.3339080810546875\n",
            "Batch Number: 247 Loss: 2.2975661754608154 Time taken: 0.3153347969055176\n",
            "Batch Number: 248 Loss: 2.3122804164886475 Time taken: 0.3145287036895752\n",
            "Batch Number: 249 Loss: 2.322640895843506 Time taken: 0.3327188491821289\n",
            "Batch Number: 250 Loss: 2.3020238876342773 Time taken: 0.31361842155456543\n",
            "Batch Number: 251 Loss: 2.2941415309906006 Time taken: 0.3063783645629883\n",
            "Batch Number: 252 Loss: 2.315866231918335 Time taken: 0.33149290084838867\n",
            "Batch Number: 253 Loss: 2.3029890060424805 Time taken: 0.31575608253479004\n",
            "Batch Number: 254 Loss: 2.324984550476074 Time taken: 0.3170902729034424\n",
            "Batch Number: 255 Loss: 2.3184168338775635 Time taken: 0.32790350914001465\n",
            "Batch Number: 256 Loss: 2.308468818664551 Time taken: 0.3212697505950928\n",
            "Batch Number: 257 Loss: 2.3053627014160156 Time taken: 0.306673526763916\n",
            "Batch Number: 258 Loss: 2.3295934200286865 Time taken: 0.3268246650695801\n",
            "Batch Number: 259 Loss: 2.3394579887390137 Time taken: 0.3094179630279541\n",
            "Batch Number: 260 Loss: 2.38144588470459 Time taken: 0.30846238136291504\n",
            "Batch Number: 261 Loss: 2.3227529525756836 Time taken: 0.3280627727508545\n",
            "Batch Number: 262 Loss: 2.3184640407562256 Time taken: 0.3100869655609131\n",
            "Batch Number: 263 Loss: 2.3506102561950684 Time taken: 0.30731987953186035\n",
            "Batch Number: 264 Loss: 2.336911201477051 Time taken: 0.3177454471588135\n",
            "Batch Number: 265 Loss: 2.3129894733428955 Time taken: 0.33358120918273926\n",
            "Batch Number: 266 Loss: 2.3214211463928223 Time taken: 0.3125319480895996\n",
            "Batch Number: 267 Loss: 2.337817430496216 Time taken: 0.3135805130004883\n",
            "Batch Number: 268 Loss: 2.336144208908081 Time taken: 0.3286311626434326\n",
            "Batch Number: 269 Loss: 2.3172733783721924 Time taken: 0.3128995895385742\n",
            "Batch Number: 270 Loss: 2.325676441192627 Time taken: 0.30884361267089844\n",
            "Batch Number: 271 Loss: 2.3207433223724365 Time taken: 0.3470923900604248\n",
            "Batch Number: 272 Loss: 2.338552713394165 Time taken: 0.32759618759155273\n",
            "Batch Number: 273 Loss: 2.3382718563079834 Time taken: 0.3058605194091797\n",
            "Batch Number: 274 Loss: 2.303586959838867 Time taken: 0.3365652561187744\n",
            "Batch Number: 275 Loss: 2.330955982208252 Time taken: 0.3090496063232422\n",
            "Batch Number: 276 Loss: 2.3219640254974365 Time taken: 0.3090674877166748\n",
            "Batch Number: 277 Loss: 2.3116002082824707 Time taken: 0.33737754821777344\n",
            "Batch Number: 278 Loss: 2.3119380474090576 Time taken: 0.3137214183807373\n",
            "Batch Number: 279 Loss: 2.2849771976470947 Time taken: 0.3038771152496338\n",
            "Batch Number: 280 Loss: 2.298295497894287 Time taken: 0.3184621334075928\n",
            "Batch Number: 281 Loss: 2.305828332901001 Time taken: 0.3093142509460449\n",
            "Batch Number: 282 Loss: 2.2894539833068848 Time taken: 0.31834912300109863\n",
            "Batch Number: 283 Loss: 2.2977235317230225 Time taken: 0.3266756534576416\n",
            "Batch Number: 284 Loss: 2.313474655151367 Time taken: 0.31378722190856934\n",
            "Batch Number: 285 Loss: 2.3090741634368896 Time taken: 0.3038175106048584\n",
            "Batch Number: 286 Loss: 2.296602249145508 Time taken: 0.3230292797088623\n",
            "Batch Number: 287 Loss: 2.2891886234283447 Time taken: 0.3281829357147217\n",
            "Batch Number: 288 Loss: 2.2870938777923584 Time taken: 0.3087637424468994\n",
            "Batch Number: 289 Loss: 2.3136069774627686 Time taken: 0.3126072883605957\n",
            "Batch Number: 290 Loss: 2.2867894172668457 Time taken: 0.33477306365966797\n",
            "Batch Number: 291 Loss: 2.29679799079895 Time taken: 0.31885528564453125\n",
            "Batch Number: 292 Loss: 2.2944750785827637 Time taken: 0.3170170783996582\n",
            "Batch Number: 293 Loss: 2.300947427749634 Time taken: 0.3364839553833008\n",
            "Batch Number: 294 Loss: 2.3027937412261963 Time taken: 0.3087623119354248\n",
            "Batch Number: 295 Loss: 2.2798991203308105 Time taken: 0.30347347259521484\n",
            "Batch Number: 296 Loss: 2.2860896587371826 Time taken: 0.3288838863372803\n",
            "Batch Number: 297 Loss: 2.258376121520996 Time taken: 0.31751561164855957\n",
            "Batch Number: 298 Loss: 2.28238844871521 Time taken: 0.3169369697570801\n",
            "Batch Number: 299 Loss: 2.2790327072143555 Time taken: 0.3518953323364258\n",
            "Batch Number: 300 Loss: 2.2941958904266357 Time taken: 0.3072390556335449\n",
            "Batch Number: 301 Loss: 2.3156661987304688 Time taken: 0.3197181224822998\n",
            "Batch Number: 302 Loss: 2.2865567207336426 Time taken: 0.33150744438171387\n",
            "Batch Number: 303 Loss: 2.2980575561523438 Time taken: 0.31197261810302734\n",
            "Batch Number: 304 Loss: 2.276463747024536 Time taken: 0.33537840843200684\n",
            "Batch Number: 305 Loss: 2.276780605316162 Time taken: 0.3359971046447754\n",
            "Batch Number: 306 Loss: 2.272404909133911 Time taken: 0.3121647834777832\n",
            "Batch Number: 307 Loss: 2.277940511703491 Time taken: 0.3245813846588135\n",
            "Batch Number: 308 Loss: 2.271171808242798 Time taken: 0.32677650451660156\n",
            "Batch Number: 309 Loss: 2.279346466064453 Time taken: 0.31639957427978516\n",
            "Batch Number: 310 Loss: 2.2815659046173096 Time taken: 0.30861449241638184\n",
            "Batch Number: 311 Loss: 2.2910544872283936 Time taken: 0.3214070796966553\n",
            "Batch Number: 312 Loss: 2.273702383041382 Time taken: 0.32785654067993164\n",
            "Batch Number: 313 Loss: 2.2955596446990967 Time taken: 0.3110799789428711\n",
            "Batch Number: 314 Loss: 2.273409366607666 Time taken: 0.31981968879699707\n",
            "Batch Number: 315 Loss: 2.2762553691864014 Time taken: 0.3337523937225342\n",
            "Batch Number: 316 Loss: 2.287827491760254 Time taken: 0.3152754306793213\n",
            "Batch Number: 317 Loss: 2.2810068130493164 Time taken: 0.3176760673522949\n",
            "Batch Number: 318 Loss: 2.2794296741485596 Time taken: 0.3239305019378662\n",
            "Batch Number: 319 Loss: 2.260491132736206 Time taken: 0.3086066246032715\n",
            "Batch Number: 320 Loss: 2.2705605030059814 Time taken: 0.31846117973327637\n",
            "Batch Number: 321 Loss: 2.2674970626831055 Time taken: 0.33298397064208984\n",
            "Batch Number: 322 Loss: 2.275925874710083 Time taken: 0.3099706172943115\n",
            "Batch Number: 323 Loss: 2.2627148628234863 Time taken: 0.31143832206726074\n",
            "Batch Number: 324 Loss: 2.3108608722686768 Time taken: 0.33143091201782227\n",
            "Batch Number: 325 Loss: 2.2669262886047363 Time taken: 0.32353830337524414\n",
            "Batch Number: 326 Loss: 2.284773588180542 Time taken: 0.31556272506713867\n",
            "Batch Number: 327 Loss: 2.2506422996520996 Time taken: 0.3231687545776367\n",
            "Batch Number: 328 Loss: 2.275407552719116 Time taken: 0.31793808937072754\n",
            "Batch Number: 329 Loss: 2.236511468887329 Time taken: 0.3177988529205322\n",
            "Batch Number: 330 Loss: 2.2416155338287354 Time taken: 0.32855796813964844\n",
            "Batch Number: 331 Loss: 2.2574734687805176 Time taken: 0.3064546585083008\n",
            "Batch Number: 332 Loss: 2.2601876258850098 Time taken: 0.3124372959136963\n",
            "Batch Number: 333 Loss: 2.25941801071167 Time taken: 0.3312244415283203\n",
            "Batch Number: 334 Loss: 2.2645392417907715 Time taken: 0.31200647354125977\n",
            "Batch Number: 335 Loss: 2.252851724624634 Time taken: 0.3079204559326172\n",
            "Batch Number: 336 Loss: 2.252685785293579 Time taken: 0.33623838424682617\n",
            "Batch Number: 337 Loss: 2.240576982498169 Time taken: 0.32090044021606445\n",
            "Batch Number: 338 Loss: 2.267817258834839 Time taken: 0.3107898235321045\n",
            "Batch Number: 339 Loss: 2.272878408432007 Time taken: 0.3203878402709961\n",
            "Batch Number: 340 Loss: 2.2606852054595947 Time taken: 0.3208308219909668\n",
            "Batch Number: 341 Loss: 2.2584192752838135 Time taken: 0.304884672164917\n",
            "Batch Number: 342 Loss: 2.261169910430908 Time taken: 0.32387518882751465\n",
            "Batch Number: 343 Loss: 2.2587029933929443 Time taken: 0.3259425163269043\n",
            "Batch Number: 344 Loss: 2.2505979537963867 Time taken: 0.31266045570373535\n",
            "Batch Number: 345 Loss: 2.246483325958252 Time taken: 0.3203742504119873\n",
            "Batch Number: 346 Loss: 2.251023292541504 Time taken: 0.32460451126098633\n",
            "Batch Number: 347 Loss: 2.2525367736816406 Time taken: 0.30832886695861816\n",
            "Batch Number: 348 Loss: 2.2474727630615234 Time taken: 0.31493496894836426\n",
            "Batch Number: 349 Loss: 2.2458102703094482 Time taken: 0.32543444633483887\n",
            "Batch Number: 350 Loss: 2.2321810722351074 Time taken: 0.30503273010253906\n",
            "Batch Number: 351 Loss: 2.2257893085479736 Time taken: 0.32565832138061523\n",
            "Batch Number: 352 Loss: 2.2492940425872803 Time taken: 0.3238105773925781\n",
            "Batch Number: 353 Loss: 2.255908250808716 Time taken: 0.31458330154418945\n",
            "Batch Number: 354 Loss: 2.2392399311065674 Time taken: 0.31901097297668457\n",
            "Batch Number: 355 Loss: 2.2500994205474854 Time taken: 0.32637834548950195\n",
            "Batch Number: 356 Loss: 2.243497371673584 Time taken: 0.3205578327178955\n",
            "Batch Number: 357 Loss: 2.2156455516815186 Time taken: 0.3107941150665283\n",
            "Batch Number: 358 Loss: 2.2386884689331055 Time taken: 0.32300639152526855\n",
            "Batch Number: 359 Loss: 2.23715877532959 Time taken: 0.3309624195098877\n",
            "Batch Number: 360 Loss: 2.2229819297790527 Time taken: 0.30827879905700684\n",
            "Batch Number: 361 Loss: 2.2187917232513428 Time taken: 0.3157806396484375\n",
            "Batch Number: 362 Loss: 2.2010090351104736 Time taken: 0.32652878761291504\n",
            "Batch Number: 363 Loss: 2.214015245437622 Time taken: 0.3101787567138672\n",
            "Batch Number: 364 Loss: 2.197770118713379 Time taken: 0.32924652099609375\n",
            "Batch Number: 365 Loss: 2.21945858001709 Time taken: 0.3310396671295166\n",
            "Batch Number: 366 Loss: 2.209042549133301 Time taken: 0.3152730464935303\n",
            "Batch Number: 367 Loss: 2.2158145904541016 Time taken: 0.3147761821746826\n",
            "Batch Number: 368 Loss: 2.2333431243896484 Time taken: 0.33971738815307617\n",
            "Batch Number: 369 Loss: 2.2125957012176514 Time taken: 0.328153133392334\n",
            "Batch Number: 370 Loss: 2.213454008102417 Time taken: 0.31131887435913086\n",
            "Batch Number: 371 Loss: 2.2136802673339844 Time taken: 0.31138038635253906\n",
            "Batch Number: 372 Loss: 2.223752975463867 Time taken: 0.3175051212310791\n",
            "Batch Number: 373 Loss: 2.2258801460266113 Time taken: 0.3027153015136719\n",
            "Batch Number: 374 Loss: 2.222878932952881 Time taken: 0.3228001594543457\n",
            "Batch Number: 375 Loss: 2.2196779251098633 Time taken: 0.30981874465942383\n",
            "Batch Number: 376 Loss: 2.209848403930664 Time taken: 0.30742788314819336\n",
            "Batch Number: 377 Loss: 2.213932752609253 Time taken: 0.3238182067871094\n",
            "Batch Number: 378 Loss: 2.2218101024627686 Time taken: 0.32137465476989746\n",
            "Batch Number: 379 Loss: 2.2042200565338135 Time taken: 0.312328577041626\n",
            "Batch Number: 380 Loss: 2.1926357746124268 Time taken: 0.305187463760376\n",
            "Batch Number: 381 Loss: 2.181119441986084 Time taken: 0.32643985748291016\n",
            "Batch Number: 382 Loss: 2.2027482986450195 Time taken: 0.31053876876831055\n",
            "Batch Number: 383 Loss: 2.1870837211608887 Time taken: 0.3193221092224121\n",
            "Batch Number: 384 Loss: 2.2191696166992188 Time taken: 0.32484912872314453\n",
            "Batch Number: 385 Loss: 2.200317621231079 Time taken: 0.31290388107299805\n",
            "Batch Number: 386 Loss: 2.2071547508239746 Time taken: 0.30356359481811523\n",
            "Batch Number: 387 Loss: 2.2054152488708496 Time taken: 0.31703972816467285\n",
            "Batch Number: 388 Loss: 2.209129810333252 Time taken: 0.3119053840637207\n",
            "Batch Number: 389 Loss: 2.207989454269409 Time taken: 0.3144998550415039\n",
            "Batch Number: 390 Loss: 2.2060883045196533 Time taken: 0.3084075450897217\n",
            "Batch Number: 391 Loss: 2.217038869857788 Time taken: 0.312183141708374\n",
            "Batch Number: 392 Loss: 2.2109177112579346 Time taken: 0.31448888778686523\n",
            "Batch Number: 393 Loss: 2.2042617797851562 Time taken: 0.3205890655517578\n",
            "Batch Number: 394 Loss: 2.2130308151245117 Time taken: 0.3081216812133789\n",
            "Batch Number: 395 Loss: 2.207608461380005 Time taken: 0.3074376583099365\n",
            "Batch Number: 396 Loss: 2.19075608253479 Time taken: 0.32604122161865234\n",
            "Batch Number: 397 Loss: 2.1951003074645996 Time taken: 0.32140159606933594\n",
            "Batch Number: 398 Loss: 2.191718339920044 Time taken: 0.3061497211456299\n",
            "Batch Number: 399 Loss: 2.203972816467285 Time taken: 0.3169443607330322\n",
            "Batch Number: 400 Loss: 2.195697069168091 Time taken: 0.32445669174194336\n",
            "Batch Number: 401 Loss: 2.20868182182312 Time taken: 0.3403353691101074\n",
            "Batch Number: 402 Loss: 2.2179086208343506 Time taken: 0.33266592025756836\n",
            "Batch Number: 403 Loss: 2.178086519241333 Time taken: 0.3192019462585449\n",
            "Batch Number: 404 Loss: 2.189936637878418 Time taken: 0.33161330223083496\n",
            "Batch Number: 405 Loss: 2.177701950073242 Time taken: 0.3302738666534424\n",
            "Batch Number: 406 Loss: 2.202183961868286 Time taken: 0.3260166645050049\n",
            "Batch Number: 407 Loss: 2.192068338394165 Time taken: 0.3195815086364746\n",
            "Batch Number: 408 Loss: 2.2079765796661377 Time taken: 0.3310725688934326\n",
            "Batch Number: 409 Loss: 2.189866542816162 Time taken: 0.3419530391693115\n",
            "Batch Number: 410 Loss: 2.2110214233398438 Time taken: 0.31483936309814453\n",
            "Batch Number: 411 Loss: 2.2127726078033447 Time taken: 0.3254060745239258\n",
            "Batch Number: 412 Loss: 2.2168946266174316 Time taken: 0.33093953132629395\n",
            "Batch Number: 413 Loss: 2.203514575958252 Time taken: 0.3027627468109131\n",
            "Batch Number: 414 Loss: 2.2026333808898926 Time taken: 0.30838727951049805\n",
            "Batch Number: 415 Loss: 2.2020981311798096 Time taken: 0.31058478355407715\n",
            "Batch Number: 416 Loss: 2.1830360889434814 Time taken: 0.30235815048217773\n",
            "Batch Number: 417 Loss: 2.189363479614258 Time taken: 0.3102858066558838\n",
            "Batch Number: 418 Loss: 2.1889994144439697 Time taken: 0.3134033679962158\n",
            "Batch Number: 419 Loss: 2.1970176696777344 Time taken: 0.32332921028137207\n",
            "Batch Number: 420 Loss: 2.166797637939453 Time taken: 0.3124501705169678\n",
            "Batch Number: 421 Loss: 2.1685211658477783 Time taken: 0.32257771492004395\n",
            "Batch Number: 422 Loss: 2.1692073345184326 Time taken: 0.32984423637390137\n",
            "Batch Number: 423 Loss: 2.1885859966278076 Time taken: 0.31593942642211914\n",
            "Batch Number: 424 Loss: 2.1634836196899414 Time taken: 0.3139958381652832\n",
            "Batch Number: 425 Loss: 2.170243501663208 Time taken: 0.3260207176208496\n",
            "Batch Number: 426 Loss: 2.2283167839050293 Time taken: 0.3220219612121582\n",
            "Batch Number: 427 Loss: 2.184297800064087 Time taken: 0.31937599182128906\n",
            "Batch Number: 428 Loss: 2.1986584663391113 Time taken: 0.3220694065093994\n",
            "Batch Number: 429 Loss: 2.1796536445617676 Time taken: 0.3115401268005371\n",
            "Batch Number: 430 Loss: 2.1860344409942627 Time taken: 0.31049561500549316\n",
            "Batch Number: 431 Loss: 2.1936445236206055 Time taken: 0.3238406181335449\n",
            "Batch Number: 432 Loss: 2.2115862369537354 Time taken: 0.30246973037719727\n",
            "Batch Number: 433 Loss: 2.1907958984375 Time taken: 0.31952905654907227\n",
            "Batch Number: 434 Loss: 2.199047327041626 Time taken: 0.3202815055847168\n",
            "Batch Number: 435 Loss: 2.2059013843536377 Time taken: 0.3141036033630371\n",
            "Batch Number: 436 Loss: 2.193284749984741 Time taken: 0.322037935256958\n",
            "Batch Number: 437 Loss: 2.221590518951416 Time taken: 0.32323217391967773\n",
            "Batch Number: 438 Loss: 2.196363925933838 Time taken: 0.30069589614868164\n",
            "Batch Number: 439 Loss: 2.180901050567627 Time taken: 0.3110206127166748\n",
            "Batch Number: 440 Loss: 2.1926138401031494 Time taken: 0.313748836517334\n",
            "Batch Number: 441 Loss: 2.201672077178955 Time taken: 0.32788848876953125\n",
            "Batch Number: 442 Loss: 2.1807029247283936 Time taken: 0.31718921661376953\n",
            "Batch Number: 443 Loss: 2.1595752239227295 Time taken: 0.3062119483947754\n",
            "Batch Number: 444 Loss: 2.1642956733703613 Time taken: 0.34154582023620605\n",
            "Batch Number: 445 Loss: 2.182088613510132 Time taken: 0.30585503578186035\n",
            "Batch Number: 446 Loss: 2.1890270709991455 Time taken: 0.3186006546020508\n",
            "Batch Number: 447 Loss: 2.1744871139526367 Time taken: 0.3206932544708252\n",
            "Batch Number: 448 Loss: 2.1831138134002686 Time taken: 0.30866050720214844\n",
            "Batch Number: 449 Loss: 2.1837997436523438 Time taken: 0.3131980895996094\n",
            "Batch Number: 450 Loss: 2.179805040359497 Time taken: 0.3313584327697754\n",
            "Batch Number: 451 Loss: 2.1993415355682373 Time taken: 0.31761908531188965\n",
            "Batch Number: 452 Loss: 2.2063493728637695 Time taken: 0.3166518211364746\n",
            "Batch Number: 453 Loss: 2.16475248336792 Time taken: 0.3229846954345703\n",
            "Batch Number: 454 Loss: 2.1963343620300293 Time taken: 0.31651902198791504\n",
            "Batch Number: 455 Loss: 2.183666944503784 Time taken: 0.3110203742980957\n",
            "Batch Number: 456 Loss: 2.1967992782592773 Time taken: 0.32386040687561035\n",
            "Batch Number: 457 Loss: 2.200077772140503 Time taken: 0.3125741481781006\n",
            "Batch Number: 458 Loss: 2.205794095993042 Time taken: 0.3195838928222656\n",
            "Batch Number: 459 Loss: 2.190533399581909 Time taken: 0.320054292678833\n",
            "Batch Number: 460 Loss: 2.1777079105377197 Time taken: 0.3053467273712158\n",
            "Batch Number: 461 Loss: 2.1936838626861572 Time taken: 0.3184666633605957\n",
            "Batch Number: 462 Loss: 2.1806063652038574 Time taken: 0.31250452995300293\n",
            "Batch Number: 463 Loss: 2.1681435108184814 Time taken: 0.32384300231933594\n",
            "Batch Number: 464 Loss: 2.207695484161377 Time taken: 0.3149440288543701\n",
            "Batch Number: 465 Loss: 2.185300350189209 Time taken: 0.31529688835144043\n",
            "Batch Number: 466 Loss: 2.1733036041259766 Time taken: 0.3257272243499756\n",
            "Batch Number: 467 Loss: 2.1959431171417236 Time taken: 0.31271910667419434\n",
            "Batch Number: 468 Loss: 2.186352252960205 Time taken: 0.31198692321777344\n",
            "Batch Number: 469 Loss: 2.1813700199127197 Time taken: 0.31920814514160156\n",
            "Batch Number: 470 Loss: 2.183382511138916 Time taken: 0.2977418899536133\n",
            "Batch Number: 471 Loss: 2.19974422454834 Time taken: 0.32013535499572754\n",
            "Batch Number: 472 Loss: 2.1881582736968994 Time taken: 0.33284997940063477\n",
            "Batch Number: 473 Loss: 2.2254481315612793 Time taken: 0.30315637588500977\n",
            "Batch Number: 474 Loss: 2.1823689937591553 Time taken: 0.3159635066986084\n",
            "Batch Number: 475 Loss: 2.1650688648223877 Time taken: 0.3198280334472656\n",
            "Batch Number: 476 Loss: 2.188340425491333 Time taken: 0.307558536529541\n",
            "Batch Number: 477 Loss: 2.185624361038208 Time taken: 0.3142883777618408\n",
            "Batch Number: 478 Loss: 2.16403865814209 Time taken: 0.31871891021728516\n",
            "Batch Number: 479 Loss: 2.181248426437378 Time taken: 0.32665038108825684\n",
            "Batch Number: 480 Loss: 2.1843831539154053 Time taken: 0.31944751739501953\n",
            "Batch Number: 481 Loss: 2.1823432445526123 Time taken: 0.3117983341217041\n",
            "Batch Number: 482 Loss: 2.1876888275146484 Time taken: 0.32483386993408203\n",
            "Batch Number: 483 Loss: 2.184763193130493 Time taken: 0.3199324607849121\n",
            "Batch Number: 484 Loss: 2.1566426753997803 Time taken: 0.3174431324005127\n",
            "Batch Number: 485 Loss: 2.1721479892730713 Time taken: 0.32828187942504883\n",
            "Batch Number: 486 Loss: 2.1791083812713623 Time taken: 0.316417932510376\n",
            "Batch Number: 487 Loss: 2.1958487033843994 Time taken: 0.3291809558868408\n",
            "Batch Number: 488 Loss: 2.1762466430664062 Time taken: 0.33298158645629883\n",
            "Batch Number: 489 Loss: 2.154391050338745 Time taken: 0.32119274139404297\n",
            "Batch Number: 490 Loss: 2.1705076694488525 Time taken: 0.30899620056152344\n",
            "Batch Number: 491 Loss: 2.1697640419006348 Time taken: 0.30855441093444824\n",
            "Batch Number: 492 Loss: 2.1623222827911377 Time taken: 0.3008594512939453\n",
            "Batch Number: 493 Loss: 2.1791656017303467 Time taken: 0.3198728561401367\n",
            "Batch Number: 494 Loss: 2.1734700202941895 Time taken: 0.3162081241607666\n",
            "Batch Number: 495 Loss: 2.1668882369995117 Time taken: 0.312725305557251\n",
            "Batch Number: 496 Loss: 2.1672675609588623 Time taken: 0.3297386169433594\n",
            "Batch Number: 497 Loss: 2.1803178787231445 Time taken: 0.31548452377319336\n",
            "Batch Number: 498 Loss: 2.1809003353118896 Time taken: 0.30978870391845703\n",
            "Batch Number: 499 Loss: 2.170959711074829 Time taken: 0.3231201171875\n",
            "Batch Number: 500 Loss: 2.1727662086486816 Time taken: 0.32182884216308594\n",
            "Batch Number: 501 Loss: 2.1671643257141113 Time taken: 0.31859254837036133\n",
            "Batch Number: 502 Loss: 2.1627655029296875 Time taken: 0.32428860664367676\n",
            "Batch Number: 503 Loss: 2.172297716140747 Time taken: 0.3149869441986084\n",
            "Batch Number: 504 Loss: 2.1452243328094482 Time taken: 0.32396697998046875\n",
            "Batch Number: 505 Loss: 2.1857292652130127 Time taken: 0.32501649856567383\n",
            "Batch Number: 506 Loss: 2.1580560207366943 Time taken: 0.32256197929382324\n",
            "Batch Number: 507 Loss: 2.1639015674591064 Time taken: 0.32664918899536133\n",
            "Batch Number: 508 Loss: 2.178004026412964 Time taken: 0.32665467262268066\n",
            "Batch Number: 509 Loss: 2.163759231567383 Time taken: 0.31359291076660156\n",
            "Batch Number: 510 Loss: 2.183739185333252 Time taken: 0.33000659942626953\n",
            "Batch Number: 511 Loss: 2.1760239601135254 Time taken: 0.3189253807067871\n",
            "Batch Number: 512 Loss: 2.1587681770324707 Time taken: 0.3119018077850342\n",
            "Batch Number: 513 Loss: 2.1514906883239746 Time taken: 0.3186986446380615\n",
            "Batch Number: 514 Loss: 2.1606054306030273 Time taken: 0.3200044631958008\n",
            "Batch Number: 515 Loss: 2.173341751098633 Time taken: 0.3180081844329834\n",
            "Batch Number: 516 Loss: 2.1543469429016113 Time taken: 0.3214752674102783\n",
            "Batch Number: 517 Loss: 2.1634175777435303 Time taken: 0.3224034309387207\n",
            "Batch Number: 518 Loss: 2.1826117038726807 Time taken: 0.31710219383239746\n",
            "Batch Number: 519 Loss: 2.1658880710601807 Time taken: 0.31817126274108887\n",
            "Batch Number: 520 Loss: 2.1721994876861572 Time taken: 0.3207266330718994\n",
            "Batch Number: 521 Loss: 2.1599061489105225 Time taken: 0.31767964363098145\n",
            "Batch Number: 522 Loss: 2.179274320602417 Time taken: 0.3179781436920166\n",
            "Batch Number: 523 Loss: 2.1670987606048584 Time taken: 0.3141038417816162\n",
            "Batch Number: 524 Loss: 2.150881052017212 Time taken: 0.33965492248535156\n",
            "Batch Number: 525 Loss: 2.1636784076690674 Time taken: 0.32567930221557617\n",
            "Batch Number: 526 Loss: 2.1573569774627686 Time taken: 0.30747032165527344\n",
            "Batch Number: 527 Loss: 2.1678571701049805 Time taken: 0.3222038745880127\n",
            "Batch Number: 528 Loss: 2.1528525352478027 Time taken: 0.3083202838897705\n",
            "Batch Number: 529 Loss: 2.155723810195923 Time taken: 0.3249781131744385\n",
            "Batch Number: 530 Loss: 2.169987440109253 Time taken: 0.3239326477050781\n",
            "Batch Number: 531 Loss: 2.1529104709625244 Time taken: 0.30758047103881836\n",
            "Batch Number: 532 Loss: 2.161853551864624 Time taken: 0.32900166511535645\n",
            "Batch Number: 533 Loss: 2.1414036750793457 Time taken: 0.3157315254211426\n",
            "Batch Number: 534 Loss: 2.1198770999908447 Time taken: 0.3057525157928467\n",
            "Batch Number: 535 Loss: 2.155736207962036 Time taken: 0.3239774703979492\n",
            "Batch Number: 536 Loss: 2.1317496299743652 Time taken: 0.3211507797241211\n",
            "Batch Number: 537 Loss: 2.122990846633911 Time taken: 0.30164337158203125\n",
            "Batch Number: 538 Loss: 2.1409897804260254 Time taken: 0.31023359298706055\n",
            "Batch Number: 539 Loss: 2.1527867317199707 Time taken: 0.3133988380432129\n",
            "Batch Number: 540 Loss: 2.1196203231811523 Time taken: 0.30727648735046387\n",
            "Batch Number: 541 Loss: 2.132551670074463 Time taken: 0.32212162017822266\n",
            "Batch Number: 542 Loss: 2.1366255283355713 Time taken: 0.30850791931152344\n",
            "Batch Number: 543 Loss: 2.1243815422058105 Time taken: 0.3156247138977051\n",
            "Batch Number: 544 Loss: 2.131462335586548 Time taken: 0.3140888214111328\n",
            "Batch Number: 545 Loss: 2.1260669231414795 Time taken: 0.3167233467102051\n",
            "Batch Number: 546 Loss: 2.1538960933685303 Time taken: 0.3095736503601074\n",
            "Batch Number: 547 Loss: 2.117480516433716 Time taken: 0.3006558418273926\n",
            "Batch Number: 548 Loss: 2.1361775398254395 Time taken: 0.31494712829589844\n",
            "Batch Number: 549 Loss: 2.1283609867095947 Time taken: 0.3228931427001953\n",
            "Batch Number: 550 Loss: 2.1254231929779053 Time taken: 0.311490535736084\n",
            "Batch Number: 551 Loss: 2.1467766761779785 Time taken: 0.32332515716552734\n",
            "Batch Number: 552 Loss: 2.1471874713897705 Time taken: 0.3204467296600342\n",
            "Batch Number: 553 Loss: 2.131978750228882 Time taken: 0.3222637176513672\n",
            "Batch Number: 554 Loss: 2.1593401432037354 Time taken: 0.32283878326416016\n",
            "Batch Number: 555 Loss: 2.1577610969543457 Time taken: 0.3386366367340088\n",
            "Batch Number: 556 Loss: 2.122667074203491 Time taken: 0.3131692409515381\n",
            "Batch Number: 557 Loss: 2.128871202468872 Time taken: 0.3219294548034668\n",
            "Batch Number: 558 Loss: 2.1302616596221924 Time taken: 0.32502031326293945\n",
            "Batch Number: 559 Loss: 2.126863956451416 Time taken: 0.3145618438720703\n",
            "Batch Number: 560 Loss: 2.1255903244018555 Time taken: 0.3321371078491211\n",
            "Batch Number: 561 Loss: 2.1331350803375244 Time taken: 0.324190616607666\n",
            "Batch Number: 562 Loss: 2.118967294692993 Time taken: 0.3092386722564697\n",
            "Batch Number: 563 Loss: 2.1363000869750977 Time taken: 0.3206930160522461\n",
            "Batch Number: 564 Loss: 2.1471309661865234 Time taken: 0.3163619041442871\n",
            "Batch Number: 565 Loss: 2.1274845600128174 Time taken: 0.3107178211212158\n",
            "Batch Number: 566 Loss: 2.1071155071258545 Time taken: 0.3147315979003906\n",
            "Batch Number: 567 Loss: 2.1196353435516357 Time taken: 0.32272911071777344\n",
            "Batch Number: 568 Loss: 2.1180970668792725 Time taken: 0.3137502670288086\n",
            "Batch Number: 569 Loss: 2.1278982162475586 Time taken: 0.30977869033813477\n",
            "Batch Number: 570 Loss: 2.132939100265503 Time taken: 0.328845739364624\n",
            "Batch Number: 571 Loss: 2.1165919303894043 Time taken: 0.32532310485839844\n",
            "Batch Number: 572 Loss: 2.122227668762207 Time taken: 0.3143138885498047\n",
            "Batch Number: 573 Loss: 2.117875337600708 Time taken: 0.32396578788757324\n",
            "Batch Number: 574 Loss: 2.1278154850006104 Time taken: 0.32508015632629395\n",
            "Batch Number: 575 Loss: 2.118426561355591 Time taken: 0.3115410804748535\n",
            "Batch Number: 576 Loss: 2.117877244949341 Time taken: 0.32968640327453613\n",
            "Batch Number: 577 Loss: 2.1124114990234375 Time taken: 0.33699870109558105\n",
            "Batch Number: 578 Loss: 2.1255013942718506 Time taken: 0.3143465518951416\n",
            "Batch Number: 579 Loss: 2.1363577842712402 Time taken: 0.32433342933654785\n",
            "Batch Number: 580 Loss: 2.1258630752563477 Time taken: 0.31891608238220215\n",
            "Batch Number: 581 Loss: 2.138340950012207 Time taken: 0.3172774314880371\n",
            "Batch Number: 582 Loss: 2.1361351013183594 Time taken: 0.3164076805114746\n",
            "Batch Number: 583 Loss: 2.113542318344116 Time taken: 0.3215456008911133\n",
            "Batch Number: 584 Loss: 2.1220598220825195 Time taken: 0.30477476119995117\n",
            "Batch Number: 585 Loss: 2.112041473388672 Time taken: 0.32251548767089844\n",
            "Batch Number: 586 Loss: 2.1214189529418945 Time taken: 0.3145883083343506\n",
            "Batch Number: 587 Loss: 2.125945806503296 Time taken: 0.3001687526702881\n",
            "Batch Number: 588 Loss: 2.11568546295166 Time taken: 0.3055546283721924\n",
            "Batch Number: 589 Loss: 2.1167097091674805 Time taken: 0.3293006420135498\n",
            "Batch Number: 590 Loss: 2.1202688217163086 Time taken: 0.30867767333984375\n",
            "Batch Number: 591 Loss: 2.130542278289795 Time taken: 0.3042747974395752\n",
            "Batch Number: 592 Loss: 2.120497465133667 Time taken: 0.3330695629119873\n",
            "Batch Number: 593 Loss: 2.140683650970459 Time taken: 0.3152163028717041\n",
            "Batch Number: 594 Loss: 2.132246255874634 Time taken: 0.301877498626709\n",
            "Batch Number: 595 Loss: 2.127408981323242 Time taken: 0.3189411163330078\n",
            "Batch Number: 596 Loss: 2.1316802501678467 Time taken: 0.33664417266845703\n",
            "Batch Number: 597 Loss: 2.116360664367676 Time taken: 0.30977773666381836\n",
            "Batch Number: 598 Loss: 2.0994980335235596 Time taken: 0.3264946937561035\n",
            "Batch Number: 599 Loss: 2.1354820728302 Time taken: 0.32312560081481934\n",
            "Batch Number: 600 Loss: 2.124129295349121 Time taken: 0.3131070137023926\n",
            "Batch Number: 601 Loss: 2.1164989471435547 Time taken: 0.32475829124450684\n",
            "Batch Number: 602 Loss: 2.0985724925994873 Time taken: 0.3264648914337158\n",
            "Batch Number: 603 Loss: 2.1109609603881836 Time taken: 0.30613064765930176\n",
            "Batch Number: 604 Loss: 2.087078094482422 Time taken: 0.32395029067993164\n",
            "Batch Number: 605 Loss: 2.1087636947631836 Time taken: 0.3227381706237793\n",
            "Batch Number: 606 Loss: 2.122624158859253 Time taken: 0.3176405429840088\n",
            "Batch Number: 607 Loss: 2.1177303791046143 Time taken: 0.32286763191223145\n",
            "Batch Number: 608 Loss: 2.1153206825256348 Time taken: 0.35164952278137207\n",
            "Batch Number: 609 Loss: 2.0976345539093018 Time taken: 0.3154175281524658\n",
            "Batch Number: 610 Loss: 2.1082687377929688 Time taken: 0.32063722610473633\n",
            "Batch Number: 611 Loss: 2.1213536262512207 Time taken: 0.3203125\n",
            "Batch Number: 612 Loss: 2.1193764209747314 Time taken: 0.3197154998779297\n",
            "Batch Number: 613 Loss: 2.1254372596740723 Time taken: 0.31191492080688477\n",
            "Batch Number: 614 Loss: 2.1169960498809814 Time taken: 0.3267233371734619\n",
            "Batch Number: 615 Loss: 2.139575481414795 Time taken: 0.31418395042419434\n",
            "Batch Number: 616 Loss: 2.118992567062378 Time taken: 0.30893492698669434\n",
            "Batch Number: 617 Loss: 2.09648060798645 Time taken: 0.3488767147064209\n",
            "Batch Number: 618 Loss: 2.122243881225586 Time taken: 0.3191843032836914\n",
            "Batch Number: 619 Loss: 2.1187069416046143 Time taken: 0.31979894638061523\n",
            "Batch Number: 620 Loss: 2.1035706996917725 Time taken: 0.3329148292541504\n",
            "Batch Number: 621 Loss: 2.0929744243621826 Time taken: 0.3085000514984131\n",
            "Batch Number: 622 Loss: 2.088702440261841 Time taken: 0.32348155975341797\n",
            "Batch Number: 623 Loss: 2.0912907123565674 Time taken: 0.32953906059265137\n",
            "Batch Number: 624 Loss: 2.0931854248046875 Time taken: 0.31400251388549805\n",
            "Batch Number: 625 Loss: 2.1018407344818115 Time taken: 0.3113701343536377\n",
            "Batch Number: 626 Loss: 2.119046211242676 Time taken: 0.33715105056762695\n",
            "Batch Number: 627 Loss: 2.1070590019226074 Time taken: 0.32398557662963867\n",
            "Batch Number: 628 Loss: 2.194413185119629 Time taken: 0.31340599060058594\n",
            "Batch Number: 629 Loss: 2.1208508014678955 Time taken: 0.3246293067932129\n",
            "Batch Number: 630 Loss: 2.1161859035491943 Time taken: 0.3270454406738281\n",
            "Batch Number: 631 Loss: 2.127913475036621 Time taken: 0.31769275665283203\n",
            "Batch Number: 632 Loss: 2.166141986846924 Time taken: 0.3179666996002197\n",
            "Batch Number: 633 Loss: 2.1431894302368164 Time taken: 0.3334653377532959\n",
            "Batch Number: 634 Loss: 2.1654858589172363 Time taken: 0.31035447120666504\n",
            "Batch Number: 635 Loss: 2.159456491470337 Time taken: 0.3131682872772217\n",
            "Batch Number: 636 Loss: 2.1617729663848877 Time taken: 0.3214735984802246\n",
            "Batch Number: 637 Loss: 2.1658670902252197 Time taken: 0.3079965114593506\n",
            "Batch Number: 638 Loss: 2.1793718338012695 Time taken: 0.30793190002441406\n",
            "Batch Number: 639 Loss: 2.232490062713623 Time taken: 0.3330953121185303\n",
            "Batch Number: 640 Loss: 2.751345634460449 Time taken: 0.3145616054534912\n",
            "Batch Number: 641 Loss: 2.786208152770996 Time taken: 0.31125664710998535\n",
            "Batch Number: 642 Loss: 3.808109998703003 Time taken: 0.3334205150604248\n",
            "Batch Number: 643 Loss: 3.441481351852417 Time taken: 0.3131072521209717\n",
            "Batch Number: 644 Loss: 3.230269432067871 Time taken: 0.31444668769836426\n",
            "Batch Number: 645 Loss: 2.8608779907226562 Time taken: 0.32648634910583496\n",
            "Batch Number: 646 Loss: 2.9095311164855957 Time taken: 0.31223630905151367\n",
            "Batch Number: 647 Loss: 2.6818299293518066 Time taken: 0.31136631965637207\n",
            "Batch Number: 648 Loss: 2.6231799125671387 Time taken: 0.32874321937561035\n",
            "Batch Number: 649 Loss: 2.6670405864715576 Time taken: 0.3099210262298584\n",
            "Batch Number: 650 Loss: 2.6534390449523926 Time taken: 0.3154137134552002\n",
            "Batch Number: 651 Loss: 2.637221097946167 Time taken: 0.3287999629974365\n",
            "Batch Number: 652 Loss: 2.511037588119507 Time taken: 0.31675004959106445\n",
            "Batch Number: 653 Loss: 2.4700241088867188 Time taken: 0.3126373291015625\n",
            "Batch Number: 654 Loss: 2.432610511779785 Time taken: 0.3242979049682617\n",
            "Batch Number: 655 Loss: 2.42857027053833 Time taken: 0.33019208908081055\n",
            "Batch Number: 656 Loss: 2.4348740577697754 Time taken: 0.31003761291503906\n",
            "Batch Number: 657 Loss: 2.370438814163208 Time taken: 0.3143186569213867\n",
            "Batch Number: 658 Loss: 2.3576691150665283 Time taken: 0.31764960289001465\n",
            "Batch Number: 659 Loss: 2.3649325370788574 Time taken: 0.3083367347717285\n",
            "Batch Number: 660 Loss: 2.3268628120422363 Time taken: 0.3111388683319092\n",
            "Batch Number: 661 Loss: 2.3583121299743652 Time taken: 0.33522701263427734\n",
            "Batch Number: 662 Loss: 2.365657329559326 Time taken: 0.31663012504577637\n",
            "Batch Number: 663 Loss: 2.311880350112915 Time taken: 0.3099327087402344\n",
            "Batch Number: 664 Loss: 2.3012914657592773 Time taken: 0.3262770175933838\n",
            "Batch Number: 665 Loss: 2.2793030738830566 Time taken: 0.31754183769226074\n",
            "Batch Number: 666 Loss: 2.2888238430023193 Time taken: 0.3084688186645508\n",
            "Batch Number: 667 Loss: 2.247653007507324 Time taken: 0.33344602584838867\n",
            "Batch Number: 668 Loss: 2.2700557708740234 Time taken: 0.3113982677459717\n",
            "Batch Number: 669 Loss: 2.250486373901367 Time taken: 0.30655598640441895\n",
            "Batch Number: 670 Loss: 2.246049404144287 Time taken: 0.34387683868408203\n",
            "Batch Number: 671 Loss: 2.2466859817504883 Time taken: 0.3088188171386719\n",
            "Batch Number: 672 Loss: 2.232870578765869 Time taken: 0.2999587059020996\n",
            "Batch Number: 673 Loss: 2.2388370037078857 Time taken: 0.3187096118927002\n",
            "Batch Number: 674 Loss: 2.2352328300476074 Time taken: 0.3107280731201172\n",
            "Batch Number: 675 Loss: 2.210451126098633 Time taken: 0.304532527923584\n",
            "Batch Number: 676 Loss: 2.218564510345459 Time taken: 0.31658005714416504\n",
            "Batch Number: 677 Loss: 2.2176735401153564 Time taken: 0.3234708309173584\n",
            "Batch Number: 678 Loss: 2.1779301166534424 Time taken: 0.3067049980163574\n",
            "Batch Number: 679 Loss: 2.197819471359253 Time taken: 0.31746959686279297\n",
            "Batch Number: 680 Loss: 2.2044267654418945 Time taken: 0.329648494720459\n",
            "Batch Number: 681 Loss: 2.2100322246551514 Time taken: 0.29978394508361816\n",
            "Batch Number: 682 Loss: 2.1871442794799805 Time taken: 0.29905104637145996\n",
            "Batch Number: 683 Loss: 2.192781925201416 Time taken: 0.33387041091918945\n",
            "Batch Number: 684 Loss: 2.205451488494873 Time taken: 0.3053700923919678\n",
            "Batch Number: 685 Loss: 2.189685344696045 Time taken: 0.3033888339996338\n",
            "Batch Number: 686 Loss: 2.1815760135650635 Time taken: 0.33714938163757324\n",
            "Batch Number: 687 Loss: 2.19735050201416 Time taken: 0.3144211769104004\n",
            "Batch Number: 688 Loss: 2.1643614768981934 Time taken: 0.30287909507751465\n",
            "Batch Number: 689 Loss: 2.171008348464966 Time taken: 0.31525444984436035\n",
            "Batch Number: 690 Loss: 2.1648826599121094 Time taken: 0.310901403427124\n",
            "Batch Number: 691 Loss: 2.165923833847046 Time taken: 0.3061213493347168\n",
            "Batch Number: 692 Loss: 2.167266607284546 Time taken: 0.3189516067504883\n",
            "Batch Number: 693 Loss: 2.158203363418579 Time taken: 0.30840444564819336\n",
            "Batch Number: 694 Loss: 2.1649086475372314 Time taken: 0.3019862174987793\n",
            "Batch Number: 695 Loss: 2.1502866744995117 Time taken: 0.3062143325805664\n",
            "Batch Number: 696 Loss: 2.163787364959717 Time taken: 0.3286886215209961\n",
            "Batch Number: 697 Loss: 2.180039167404175 Time taken: 0.3046584129333496\n",
            "Batch Number: 698 Loss: 2.169975519180298 Time taken: 0.3184983730316162\n",
            "Batch Number: 699 Loss: 2.1606626510620117 Time taken: 0.3298323154449463\n",
            "Batch Number: 700 Loss: 2.13493275642395 Time taken: 0.3119955062866211\n",
            "Batch Number: 701 Loss: 2.1587209701538086 Time taken: 0.3070967197418213\n",
            "Batch Number: 702 Loss: 2.157877206802368 Time taken: 0.3232135772705078\n",
            "Batch Number: 703 Loss: 2.1479740142822266 Time taken: 0.3068244457244873\n",
            "Batch Number: 704 Loss: 2.126098394393921 Time taken: 0.3047187328338623\n",
            "Batch Number: 705 Loss: 2.1347806453704834 Time taken: 0.31723666191101074\n",
            "Batch Number: 706 Loss: 2.1356468200683594 Time taken: 0.311767578125\n",
            "Batch Number: 707 Loss: 2.12935471534729 Time taken: 0.3181192874908447\n",
            "Batch Number: 708 Loss: 2.136958122253418 Time taken: 0.32346558570861816\n",
            "Batch Number: 709 Loss: 2.126023292541504 Time taken: 0.30515265464782715\n",
            "Batch Number: 710 Loss: 2.1414108276367188 Time taken: 0.31381964683532715\n",
            "Batch Number: 711 Loss: 2.1385562419891357 Time taken: 0.3058147430419922\n",
            "Batch Number: 712 Loss: 2.141489267349243 Time taken: 0.32738375663757324\n",
            "Batch Number: 713 Loss: 2.1470248699188232 Time taken: 0.31734681129455566\n",
            "Batch Number: 714 Loss: 2.1363942623138428 Time taken: 0.30798816680908203\n",
            "Batch Number: 715 Loss: 2.145031690597534 Time taken: 0.3352832794189453\n",
            "Batch Number: 716 Loss: 2.1533279418945312 Time taken: 0.3078491687774658\n",
            "Batch Number: 717 Loss: 2.125612735748291 Time taken: 0.3121318817138672\n",
            "Batch Number: 718 Loss: 2.1344830989837646 Time taken: 0.33138394355773926\n",
            "Batch Number: 719 Loss: 2.1374311447143555 Time taken: 0.30968475341796875\n",
            "Batch Number: 720 Loss: 2.1234073638916016 Time taken: 0.30791544914245605\n",
            "Batch Number: 721 Loss: 2.1484901905059814 Time taken: 0.33205628395080566\n",
            "Batch Number: 722 Loss: 2.1081745624542236 Time taken: 0.31505346298217773\n",
            "Batch Number: 723 Loss: 2.1073975563049316 Time taken: 0.3244478702545166\n",
            "Batch Number: 724 Loss: 2.088763475418091 Time taken: 0.32820630073547363\n",
            "Batch Number: 725 Loss: 2.090104341506958 Time taken: 0.31038951873779297\n",
            "Batch Number: 726 Loss: 2.1233789920806885 Time taken: 0.3094608783721924\n",
            "Batch Number: 727 Loss: 2.1068389415740967 Time taken: 0.3192636966705322\n",
            "Batch Number: 728 Loss: 2.081035852432251 Time taken: 0.31214165687561035\n",
            "Batch Number: 729 Loss: 2.096200466156006 Time taken: 0.30536484718322754\n",
            "Batch Number: 730 Loss: 2.0854384899139404 Time taken: 0.3087935447692871\n",
            "Batch Number: 731 Loss: 2.107091188430786 Time taken: 0.32062244415283203\n",
            "Batch Number: 732 Loss: 2.1276466846466064 Time taken: 0.31061387062072754\n",
            "Batch Number: 733 Loss: 2.1039788722991943 Time taken: 0.30499887466430664\n",
            "Batch Number: 734 Loss: 2.1137726306915283 Time taken: 0.3409454822540283\n",
            "Batch Number: 735 Loss: 2.1167614459991455 Time taken: 0.3265223503112793\n",
            "Batch Number: 736 Loss: 2.091890811920166 Time taken: 0.3147928714752197\n",
            "Batch Number: 737 Loss: 2.1231515407562256 Time taken: 0.3371269702911377\n",
            "Batch Number: 738 Loss: 2.1239471435546875 Time taken: 0.3175978660583496\n",
            "Batch Number: 739 Loss: 2.0989480018615723 Time taken: 0.3201408386230469\n",
            "Batch Number: 740 Loss: 2.0859580039978027 Time taken: 0.3475210666656494\n",
            "Batch Number: 741 Loss: 2.0966551303863525 Time taken: 0.31027865409851074\n",
            "Batch Number: 742 Loss: 2.1109554767608643 Time taken: 0.31949830055236816\n",
            "Batch Number: 743 Loss: 2.1025853157043457 Time taken: 0.3456001281738281\n",
            "Batch Number: 744 Loss: 2.1008455753326416 Time taken: 0.33070826530456543\n",
            "Batch Number: 745 Loss: 2.0892698764801025 Time taken: 0.31838226318359375\n",
            "Batch Number: 746 Loss: 2.104788064956665 Time taken: 0.3473942279815674\n",
            "Batch Number: 747 Loss: 2.1035122871398926 Time taken: 0.3185105323791504\n",
            "Batch Number: 748 Loss: 2.1144042015075684 Time taken: 0.3158907890319824\n",
            "Batch Number: 749 Loss: 2.1115078926086426 Time taken: 0.3395516872406006\n",
            "Batch Number: 750 Loss: 2.089982748031616 Time taken: 0.3258395195007324\n",
            "Batch Number: 751 Loss: 2.1123085021972656 Time taken: 0.32584261894226074\n",
            "Batch Number: 752 Loss: 2.102463960647583 Time taken: 0.33086085319519043\n",
            "Batch Number: 753 Loss: 2.0894289016723633 Time taken: 0.32735586166381836\n",
            "Batch Number: 754 Loss: 2.079596996307373 Time taken: 0.3227677345275879\n",
            "Batch Number: 755 Loss: 2.08541202545166 Time taken: 0.33543944358825684\n",
            "Batch Number: 756 Loss: 2.073751449584961 Time taken: 0.3083016872406006\n",
            "Batch Number: 757 Loss: 2.095226764678955 Time taken: 0.32393598556518555\n",
            "Batch Number: 758 Loss: 2.110163688659668 Time taken: 0.33585166931152344\n",
            "Batch Number: 759 Loss: 2.116024971008301 Time taken: 0.326092004776001\n",
            "Batch Number: 760 Loss: 2.093074321746826 Time taken: 0.31556272506713867\n",
            "Batch Number: 761 Loss: 2.1211555004119873 Time taken: 0.3370809555053711\n",
            "Batch Number: 762 Loss: 2.091029405593872 Time taken: 0.34003448486328125\n",
            "Batch Number: 763 Loss: 2.089617967605591 Time taken: 0.3372206687927246\n",
            "Batch Number: 764 Loss: 2.082869291305542 Time taken: 0.3282780647277832\n",
            "Batch Number: 765 Loss: 2.063915252685547 Time taken: 0.3121986389160156\n",
            "Batch Number: 766 Loss: 2.0790603160858154 Time taken: 0.31864404678344727\n",
            "Batch Number: 767 Loss: 2.119619607925415 Time taken: 0.3164553642272949\n",
            "Batch Number: 768 Loss: 2.097352981567383 Time taken: 0.3277623653411865\n",
            "Batch Number: 769 Loss: 2.080528736114502 Time taken: 0.31578993797302246\n",
            "Batch Number: 770 Loss: 2.0919008255004883 Time taken: 0.31609129905700684\n",
            "Batch Number: 771 Loss: 2.095677614212036 Time taken: 0.3276946544647217\n",
            "Batch Number: 772 Loss: 2.099900722503662 Time taken: 0.3173673152923584\n",
            "Batch Number: 773 Loss: 2.0985677242279053 Time taken: 0.31936073303222656\n",
            "Batch Number: 774 Loss: 2.1169228553771973 Time taken: 0.3247649669647217\n",
            "Batch Number: 775 Loss: 2.1060147285461426 Time taken: 0.3138554096221924\n",
            "Batch Number: 776 Loss: 2.1073079109191895 Time taken: 0.31746864318847656\n",
            "Batch Number: 777 Loss: 2.0755558013916016 Time taken: 0.3102610111236572\n",
            "Batch Number: 778 Loss: 2.066112995147705 Time taken: 0.3081066608428955\n",
            "Batch Number: 779 Loss: 2.085343360900879 Time taken: 0.31341552734375\n",
            "Batch Number: 780 Loss: 2.0857787132263184 Time taken: 0.3248870372772217\n",
            "Batch Number: 781 Loss: 2.0936970710754395 Time taken: 0.30867671966552734\n",
            "Batch Number: 782 Loss: 2.059202194213867 Time taken: 0.31203198432922363\n",
            "Batch Number: 783 Loss: 2.071488857269287 Time taken: 0.3313026428222656\n",
            "Batch Number: 784 Loss: 2.08967924118042 Time taken: 0.30779218673706055\n",
            "Batch Number: 785 Loss: 2.0759425163269043 Time taken: 0.3175809383392334\n",
            "Batch Number: 786 Loss: 2.080705404281616 Time taken: 0.3257734775543213\n",
            "Batch Number: 787 Loss: 2.064265489578247 Time taken: 0.3118441104888916\n",
            "Batch Number: 788 Loss: 2.087015390396118 Time taken: 0.3110635280609131\n",
            "Batch Number: 789 Loss: 2.070993423461914 Time taken: 0.3236408233642578\n",
            "Batch Number: 790 Loss: 2.0817670822143555 Time taken: 0.3198251724243164\n",
            "Batch Number: 791 Loss: 2.0890612602233887 Time taken: 0.3092203140258789\n",
            "Batch Number: 792 Loss: 2.0883615016937256 Time taken: 0.3151252269744873\n",
            "Batch Number: 793 Loss: 2.108152151107788 Time taken: 0.334078311920166\n",
            "Batch Number: 794 Loss: 2.1085710525512695 Time taken: 0.30698585510253906\n",
            "Batch Number: 795 Loss: 2.0926151275634766 Time taken: 0.307873010635376\n",
            "Batch Number: 796 Loss: 2.0928754806518555 Time taken: 0.3325638771057129\n",
            "Batch Number: 797 Loss: 2.08943772315979 Time taken: 0.31061339378356934\n",
            "Batch Number: 798 Loss: 2.0859034061431885 Time taken: 0.3217456340789795\n",
            "Batch Number: 799 Loss: 2.0851306915283203 Time taken: 0.31992363929748535\n",
            "Batch Number: 800 Loss: 2.0641369819641113 Time taken: 0.3154594898223877\n",
            "Batch Number: 801 Loss: 2.0818300247192383 Time taken: 0.3072638511657715\n",
            "Batch Number: 802 Loss: 2.0875139236450195 Time taken: 0.3271298408508301\n",
            "Batch Number: 803 Loss: 2.08760142326355 Time taken: 0.30649471282958984\n",
            "Batch Number: 804 Loss: 2.0574867725372314 Time taken: 0.3097574710845947\n",
            "Batch Number: 805 Loss: 2.0671699047088623 Time taken: 0.3265235424041748\n",
            "Batch Number: 806 Loss: 2.078094244003296 Time taken: 0.306168794631958\n",
            "Batch Number: 807 Loss: 2.0698635578155518 Time taken: 0.3060729503631592\n",
            "Batch Number: 808 Loss: 2.0818793773651123 Time taken: 0.3037378787994385\n",
            "Batch Number: 809 Loss: 2.078578472137451 Time taken: 0.3187735080718994\n",
            "Batch Number: 810 Loss: 2.067350387573242 Time taken: 0.3085935115814209\n",
            "Batch Number: 811 Loss: 2.078836679458618 Time taken: 0.30623960494995117\n",
            "Batch Number: 812 Loss: 2.0999529361724854 Time taken: 0.30916714668273926\n",
            "Batch Number: 813 Loss: 2.096959352493286 Time taken: 0.31473708152770996\n",
            "Batch Number: 814 Loss: 2.1092658042907715 Time taken: 0.31397366523742676\n",
            "Batch Number: 815 Loss: 2.083094358444214 Time taken: 0.31583571434020996\n",
            "Batch Number: 816 Loss: 2.0798637866973877 Time taken: 0.31302905082702637\n",
            "Batch Number: 817 Loss: 2.1397294998168945 Time taken: 0.30574727058410645\n",
            "Batch Number: 818 Loss: 2.0993659496307373 Time taken: 0.31680941581726074\n",
            "Batch Number: 819 Loss: 2.1028189659118652 Time taken: 0.3109886646270752\n",
            "Batch Number: 820 Loss: 2.1010897159576416 Time taken: 0.30693483352661133\n",
            "Batch Number: 821 Loss: 2.0734217166900635 Time taken: 0.3200490474700928\n",
            "Batch Number: 822 Loss: 2.110121011734009 Time taken: 0.3050410747528076\n",
            "Batch Number: 823 Loss: 2.1058497428894043 Time taken: 0.30713391304016113\n",
            "Batch Number: 824 Loss: 2.084703207015991 Time taken: 0.3146040439605713\n",
            "Batch Number: 825 Loss: 2.0863215923309326 Time taken: 0.31487512588500977\n",
            "Batch Number: 826 Loss: 2.0975701808929443 Time taken: 0.30806851387023926\n",
            "Batch Number: 827 Loss: 2.108527660369873 Time taken: 0.3208608627319336\n",
            "Batch Number: 828 Loss: 2.0905120372772217 Time taken: 0.3111391067504883\n",
            "Batch Number: 829 Loss: 2.0922091007232666 Time taken: 0.3135416507720947\n",
            "Batch Number: 830 Loss: 2.1154513359069824 Time taken: 0.31393909454345703\n",
            "Batch Number: 831 Loss: 2.0898499488830566 Time taken: 0.31215715408325195\n",
            "Batch Number: 832 Loss: 2.0856196880340576 Time taken: 0.3117971420288086\n",
            "Batch Number: 833 Loss: 2.081963300704956 Time taken: 0.3135707378387451\n",
            "Batch Number: 834 Loss: 2.1027722358703613 Time taken: 0.3179960250854492\n",
            "Batch Number: 835 Loss: 2.1003386974334717 Time taken: 0.3095402717590332\n",
            "Batch Number: 836 Loss: 2.0927281379699707 Time taken: 0.3075375556945801\n",
            "Batch Number: 837 Loss: 2.081340789794922 Time taken: 0.3224050998687744\n",
            "Batch Number: 838 Loss: 2.0816025733947754 Time taken: 0.3197650909423828\n",
            "Batch Number: 839 Loss: 2.0989887714385986 Time taken: 0.30731892585754395\n",
            "Batch Number: 840 Loss: 2.058605909347534 Time taken: 0.3117496967315674\n",
            "Batch Number: 841 Loss: 2.064108371734619 Time taken: 0.32486844062805176\n",
            "Batch Number: 842 Loss: 2.092456817626953 Time taken: 0.3086094856262207\n",
            "Batch Number: 843 Loss: 2.06723690032959 Time taken: 0.310685396194458\n",
            "Batch Number: 844 Loss: 2.0931992530822754 Time taken: 0.32106709480285645\n",
            "Batch Number: 845 Loss: 2.0808050632476807 Time taken: 0.31076812744140625\n",
            "Batch Number: 846 Loss: 2.0465404987335205 Time taken: 0.3168802261352539\n",
            "Batch Number: 847 Loss: 2.0780773162841797 Time taken: 0.3198816776275635\n",
            "Batch Number: 848 Loss: 2.0559051036834717 Time taken: 0.31130504608154297\n",
            "Batch Number: 849 Loss: 2.067582368850708 Time taken: 0.3094656467437744\n",
            "Batch Number: 850 Loss: 2.079010009765625 Time taken: 0.3143610954284668\n",
            "Batch Number: 851 Loss: 2.0760726928710938 Time taken: 0.3071925640106201\n",
            "Batch Number: 852 Loss: 2.083239793777466 Time taken: 0.30909252166748047\n",
            "Batch Number: 853 Loss: 2.0673739910125732 Time taken: 0.30559206008911133\n",
            "Batch Number: 854 Loss: 2.1067519187927246 Time taken: 0.3140528202056885\n",
            "Batch Number: 855 Loss: 2.06425142288208 Time taken: 0.3109896183013916\n",
            "Batch Number: 856 Loss: 2.052046537399292 Time taken: 0.31391000747680664\n",
            "Batch Number: 857 Loss: 2.068288564682007 Time taken: 0.32399749755859375\n",
            "Batch Number: 858 Loss: 2.053321361541748 Time taken: 0.30205678939819336\n",
            "Batch Number: 859 Loss: 2.088785409927368 Time taken: 0.31401753425598145\n",
            "Batch Number: 860 Loss: 2.096369504928589 Time taken: 0.31919336318969727\n",
            "Batch Number: 861 Loss: 2.0759329795837402 Time taken: 0.30293822288513184\n",
            "Batch Number: 862 Loss: 2.0696189403533936 Time taken: 0.316448450088501\n",
            "Batch Number: 863 Loss: 2.0805411338806152 Time taken: 0.3178577423095703\n",
            "Batch Number: 864 Loss: 2.095273971557617 Time taken: 0.30359959602355957\n",
            "Batch Number: 865 Loss: 2.0707390308380127 Time taken: 0.3290290832519531\n",
            "Batch Number: 866 Loss: 2.0592193603515625 Time taken: 0.31867432594299316\n",
            "Batch Number: 867 Loss: 2.047046184539795 Time taken: 0.3059985637664795\n",
            "Batch Number: 868 Loss: 2.0376338958740234 Time taken: 0.31568002700805664\n",
            "Batch Number: 869 Loss: 2.037733793258667 Time taken: 0.318225622177124\n",
            "Batch Number: 870 Loss: 2.036386728286743 Time taken: 0.30721187591552734\n",
            "Batch Number: 871 Loss: 2.09234619140625 Time taken: 0.32443904876708984\n",
            "Batch Number: 872 Loss: 2.0821478366851807 Time taken: 0.3072030544281006\n",
            "Batch Number: 873 Loss: 2.035355806350708 Time taken: 0.31964826583862305\n",
            "Batch Number: 874 Loss: 2.073584794998169 Time taken: 0.32444334030151367\n",
            "Batch Number: 875 Loss: 2.0730836391448975 Time taken: 0.32608866691589355\n",
            "Batch Number: 876 Loss: 2.0720982551574707 Time taken: 0.3125722408294678\n",
            "Batch Number: 877 Loss: 2.0684542655944824 Time taken: 0.3175814151763916\n",
            "Batch Number: 878 Loss: 2.0769596099853516 Time taken: 0.3178431987762451\n",
            "Batch Number: 879 Loss: 2.0679171085357666 Time taken: 0.3302338123321533\n",
            "Batch Number: 880 Loss: 2.068817615509033 Time taken: 0.3057699203491211\n",
            "Batch Number: 881 Loss: 2.0463907718658447 Time taken: 0.3302140235900879\n",
            "Batch Number: 882 Loss: 2.04807186126709 Time taken: 0.312161922454834\n",
            "Batch Number: 883 Loss: 2.0595123767852783 Time taken: 0.3016526699066162\n",
            "Batch Number: 884 Loss: 2.0560929775238037 Time taken: 0.314939022064209\n",
            "Batch Number: 885 Loss: 2.049170970916748 Time taken: 0.31899046897888184\n",
            "Batch Number: 886 Loss: 2.0454344749450684 Time taken: 0.30411458015441895\n",
            "Batch Number: 887 Loss: 2.047441244125366 Time taken: 0.3192737102508545\n",
            "Batch Number: 888 Loss: 2.0620174407958984 Time taken: 0.3139164447784424\n",
            "Batch Number: 889 Loss: 2.044821262359619 Time taken: 0.3099031448364258\n",
            "Batch Number: 890 Loss: 2.0393240451812744 Time taken: 0.32451319694519043\n",
            "Batch Number: 891 Loss: 2.05269455909729 Time taken: 0.3076910972595215\n",
            "Batch Number: 892 Loss: 2.031567096710205 Time taken: 0.32271623611450195\n",
            "Batch Number: 893 Loss: 2.0620851516723633 Time taken: 0.31548380851745605\n",
            "Batch Number: 894 Loss: 2.0473599433898926 Time taken: 0.31028056144714355\n",
            "Batch Number: 895 Loss: 2.0550737380981445 Time taken: 0.3073902130126953\n",
            "Batch Number: 896 Loss: 2.0371174812316895 Time taken: 0.3015260696411133\n",
            "Batch Number: 897 Loss: 2.0468862056732178 Time taken: 0.31281018257141113\n",
            "Batch Number: 898 Loss: 2.044459819793701 Time taken: 0.31732726097106934\n",
            "Batch Number: 899 Loss: 2.0259900093078613 Time taken: 0.30733609199523926\n",
            "Batch Number: 900 Loss: 2.039712429046631 Time taken: 0.3047933578491211\n",
            "Batch Number: 901 Loss: 2.0176024436950684 Time taken: 0.32865142822265625\n",
            "Batch Number: 902 Loss: 2.0445356369018555 Time taken: 0.3041410446166992\n",
            "Batch Number: 903 Loss: 2.033600091934204 Time taken: 0.3219447135925293\n",
            "Batch Number: 904 Loss: 2.0220789909362793 Time taken: 0.317493200302124\n",
            "Batch Number: 905 Loss: 1.9961713552474976 Time taken: 0.3112223148345947\n",
            "Batch Number: 906 Loss: 2.01939058303833 Time taken: 0.3082618713378906\n",
            "Batch Number: 907 Loss: 2.024780511856079 Time taken: 0.30674171447753906\n",
            "Batch Number: 908 Loss: 2.01594877243042 Time taken: 0.30894970893859863\n",
            "Batch Number: 909 Loss: 2.0228898525238037 Time taken: 0.31182432174682617\n",
            "Batch Number: 910 Loss: 2.0324387550354004 Time taken: 0.31947922706604004\n",
            "Batch Number: 911 Loss: 2.0456864833831787 Time taken: 0.3136606216430664\n",
            "Batch Number: 912 Loss: 2.0117688179016113 Time taken: 0.2988119125366211\n",
            "Batch Number: 913 Loss: 2.0636093616485596 Time taken: 0.31470537185668945\n",
            "Batch Number: 914 Loss: 2.0397093296051025 Time taken: 0.3148834705352783\n",
            "Batch Number: 915 Loss: 2.0383527278900146 Time taken: 0.3020322322845459\n",
            "Batch Number: 916 Loss: 2.0587756633758545 Time taken: 0.32079076766967773\n",
            "Batch Number: 917 Loss: 2.031818151473999 Time taken: 0.3153386116027832\n",
            "Batch Number: 918 Loss: 2.034632444381714 Time taken: 0.3094770908355713\n",
            "Batch Number: 919 Loss: 2.0199360847473145 Time taken: 0.3172914981842041\n",
            "Batch Number: 920 Loss: 2.0244743824005127 Time taken: 0.3107492923736572\n",
            "Batch Number: 921 Loss: 2.037306547164917 Time taken: 0.31142640113830566\n",
            "Batch Number: 922 Loss: 2.02470326423645 Time taken: 0.31946492195129395\n",
            "Batch Number: 923 Loss: 2.028404951095581 Time taken: 0.3140089511871338\n",
            "Batch Number: 924 Loss: 2.038851737976074 Time taken: 0.31836795806884766\n",
            "Batch Number: 925 Loss: 2.023247718811035 Time taken: 0.3119039535522461\n",
            "Batch Number: 926 Loss: 2.01664662361145 Time taken: 0.3052198886871338\n",
            "Batch Number: 927 Loss: 2.0160725116729736 Time taken: 0.31513285636901855\n",
            "Batch Number: 928 Loss: 2.0130739212036133 Time taken: 0.3223555088043213\n",
            "Batch Number: 929 Loss: 2.0114128589630127 Time taken: 0.3047919273376465\n",
            "Batch Number: 930 Loss: 2.010035276412964 Time taken: 0.3358490467071533\n",
            "Batch Number: 931 Loss: 2.017925977706909 Time taken: 0.3138539791107178\n",
            "Batch Number: 932 Loss: 2.0313968658447266 Time taken: 0.32167696952819824\n",
            "Batch Number: 933 Loss: 2.0116827487945557 Time taken: 0.3119814395904541\n",
            "Batch Number: 934 Loss: 2.0361413955688477 Time taken: 0.32436060905456543\n",
            "Batch Number: 935 Loss: 2.0293028354644775 Time taken: 0.31829118728637695\n",
            "Batch Number: 936 Loss: 2.036356210708618 Time taken: 0.31062817573547363\n",
            "Batch Number: 937 Loss: 2.017559766769409 Time taken: 0.3163304328918457\n",
            "Batch Number: 938 Loss: 2.0173568725585938 Time taken: 0.3189103603363037\n",
            "Batch Number: 939 Loss: 2.050830602645874 Time taken: 0.3253517150878906\n",
            "Batch Number: 940 Loss: 2.051652193069458 Time taken: 0.31890153884887695\n",
            "Batch Number: 941 Loss: 2.063708782196045 Time taken: 0.32620882987976074\n",
            "Batch Number: 942 Loss: 2.069849729537964 Time taken: 0.3035104274749756\n",
            "Batch Number: 943 Loss: 2.0193827152252197 Time taken: 0.30772972106933594\n",
            "Batch Number: 944 Loss: 2.0230259895324707 Time taken: 0.3208041191101074\n",
            "Batch Number: 945 Loss: 2.0178072452545166 Time taken: 0.3137223720550537\n",
            "Batch Number: 946 Loss: 2.046642303466797 Time taken: 0.31827712059020996\n",
            "Batch Number: 947 Loss: 2.054572343826294 Time taken: 0.32285642623901367\n",
            "Batch Number: 948 Loss: 2.029538869857788 Time taken: 0.3107593059539795\n",
            "Batch Number: 949 Loss: 2.0277836322784424 Time taken: 0.3166933059692383\n",
            "Batch Number: 950 Loss: 2.023587226867676 Time taken: 0.31174278259277344\n",
            "Batch Number: 951 Loss: 2.040894031524658 Time taken: 0.3181486129760742\n",
            "Batch Number: 952 Loss: 2.0332698822021484 Time taken: 0.32158422470092773\n",
            "Batch Number: 953 Loss: 2.0432450771331787 Time taken: 0.31330180168151855\n",
            "Batch Number: 954 Loss: 2.040788412094116 Time taken: 0.3313901424407959\n",
            "Batch Number: 955 Loss: 2.0226712226867676 Time taken: 0.32361340522766113\n",
            "Batch Number: 956 Loss: 2.040346145629883 Time taken: 0.3068995475769043\n",
            "Batch Number: 957 Loss: 2.0460455417633057 Time taken: 0.30693531036376953\n",
            "Batch Number: 958 Loss: 2.0272772312164307 Time taken: 0.3093910217285156\n",
            "Batch Number: 959 Loss: 2.0163137912750244 Time taken: 0.31684017181396484\n",
            "Batch Number: 960 Loss: 2.0180351734161377 Time taken: 0.315427303314209\n",
            "Batch Number: 961 Loss: 1.9961069822311401 Time taken: 0.30460214614868164\n",
            "Batch Number: 962 Loss: 2.036947250366211 Time taken: 0.3176851272583008\n",
            "Batch Number: 963 Loss: 2.0314855575561523 Time taken: 0.32463717460632324\n",
            "Batch Number: 964 Loss: 2.0127923488616943 Time taken: 0.3121039867401123\n",
            "Batch Number: 965 Loss: 1.9988977909088135 Time taken: 0.32233142852783203\n",
            "Batch Number: 966 Loss: 2.022047519683838 Time taken: 0.32176923751831055\n",
            "Batch Number: 967 Loss: 2.0085933208465576 Time taken: 0.31117916107177734\n",
            "Batch Number: 968 Loss: 2.0272953510284424 Time taken: 0.32739806175231934\n",
            "Batch Number: 969 Loss: 2.020080327987671 Time taken: 0.3108975887298584\n",
            "Batch Number: 970 Loss: 2.0226938724517822 Time taken: 0.3088653087615967\n",
            "Batch Number: 971 Loss: 1.997542381286621 Time taken: 0.3224449157714844\n",
            "Batch Number: 972 Loss: 2.031465530395508 Time taken: 0.3101024627685547\n",
            "Batch Number: 973 Loss: 2.012972831726074 Time taken: 0.3097524642944336\n",
            "Batch Number: 974 Loss: 2.0418856143951416 Time taken: 0.3099184036254883\n",
            "Batch Number: 975 Loss: 2.039856433868408 Time taken: 0.3072085380554199\n",
            "Batch Number: 976 Loss: 2.039121389389038 Time taken: 0.32565736770629883\n",
            "Batch Number: 977 Loss: 2.034137725830078 Time taken: 0.309736967086792\n",
            "Batch Number: 978 Loss: 2.037952423095703 Time taken: 0.31690192222595215\n",
            "Batch Number: 979 Loss: 2.018296241760254 Time taken: 0.32122182846069336\n",
            "Batch Number: 980 Loss: 2.026223659515381 Time taken: 0.31221890449523926\n",
            "Batch Number: 981 Loss: 2.008615255355835 Time taken: 0.31966185569763184\n",
            "Batch Number: 982 Loss: 1.9888406991958618 Time taken: 0.32280898094177246\n",
            "Batch Number: 983 Loss: 2.0087292194366455 Time taken: 0.31101489067077637\n",
            "Batch Number: 984 Loss: 2.0026233196258545 Time taken: 0.3231160640716553\n",
            "Batch Number: 985 Loss: 1.9884752035140991 Time taken: 0.31834864616394043\n",
            "Batch Number: 986 Loss: 2.0114200115203857 Time taken: 0.3249988555908203\n",
            "Batch Number: 987 Loss: 2.016242504119873 Time taken: 0.31890153884887695\n",
            "Batch Number: 988 Loss: 1.9857691526412964 Time taken: 0.32115864753723145\n",
            "Batch Number: 989 Loss: 1.9976907968521118 Time taken: 0.3256042003631592\n",
            "Batch Number: 990 Loss: 2.006326198577881 Time taken: 0.3272819519042969\n",
            "Batch Number: 991 Loss: 2.0127604007720947 Time taken: 0.3165724277496338\n",
            "Batch Number: 992 Loss: 2.0401668548583984 Time taken: 0.3105502128601074\n",
            "Batch Number: 993 Loss: 2.026852607727051 Time taken: 0.317615270614624\n",
            "Batch Number: 994 Loss: 2.022512674331665 Time taken: 0.31092143058776855\n",
            "Batch Number: 995 Loss: 2.0658812522888184 Time taken: 0.30925559997558594\n",
            "Batch Number: 996 Loss: 2.033766508102417 Time taken: 0.32588672637939453\n",
            "Batch Number: 997 Loss: 2.042841672897339 Time taken: 0.3226351737976074\n",
            "Batch Number: 998 Loss: 2.028351306915283 Time taken: 0.315903902053833\n",
            "Batch Number: 999 Loss: 2.0079941749572754 Time taken: 0.3170769214630127\n",
            "Batch Number: 1000 Loss: 2.0393226146698 Time taken: 0.32445645332336426\n",
            "Batch Number: 1001 Loss: 2.033874273300171 Time taken: 0.3188502788543701\n",
            "Batch Number: 1002 Loss: 2.0279977321624756 Time taken: 0.3063521385192871\n",
            "Batch Number: 1003 Loss: 2.0442957878112793 Time taken: 0.32394909858703613\n",
            "Batch Number: 1004 Loss: 2.031214952468872 Time taken: 0.3132667541503906\n",
            "Batch Number: 1005 Loss: 2.0532660484313965 Time taken: 0.3115980625152588\n",
            "Batch Number: 1006 Loss: 2.025280475616455 Time taken: 0.3243367671966553\n",
            "Batch Number: 1007 Loss: 2.067370653152466 Time taken: 0.3205738067626953\n",
            "Batch Number: 1008 Loss: 2.0365774631500244 Time taken: 0.3208436965942383\n",
            "Batch Number: 1009 Loss: 2.0188519954681396 Time taken: 0.32198667526245117\n",
            "Batch Number: 1010 Loss: 2.051609516143799 Time taken: 0.3229250907897949\n",
            "Batch Number: 1011 Loss: 2.053907871246338 Time taken: 0.30748867988586426\n",
            "Batch Number: 1012 Loss: 2.032308340072632 Time taken: 0.31467247009277344\n",
            "Batch Number: 1013 Loss: 2.0427310466766357 Time taken: 0.3103766441345215\n",
            "Batch Number: 1014 Loss: 2.046295166015625 Time taken: 0.310962438583374\n",
            "Batch Number: 1015 Loss: 2.060534715652466 Time taken: 0.31946706771850586\n",
            "Batch Number: 1016 Loss: 2.0340311527252197 Time taken: 0.3203458786010742\n",
            "Batch Number: 1017 Loss: 2.02791428565979 Time taken: 0.30788302421569824\n",
            "Batch Number: 1018 Loss: 2.0266213417053223 Time taken: 0.30996203422546387\n",
            "Batch Number: 1019 Loss: 2.0300958156585693 Time taken: 0.32570409774780273\n",
            "Batch Number: 1020 Loss: 2.0002975463867188 Time taken: 0.30833888053894043\n",
            "Batch Number: 1021 Loss: 2.0163705348968506 Time taken: 0.30500102043151855\n",
            "Batch Number: 1022 Loss: 2.0139286518096924 Time taken: 0.3260669708251953\n",
            "Batch Number: 1023 Loss: 1.9943042993545532 Time taken: 0.3174266815185547\n",
            "Batch Number: 1024 Loss: 2.001051664352417 Time taken: 0.30051326751708984\n",
            "Batch Number: 1025 Loss: 2.0277111530303955 Time taken: 0.3251664638519287\n",
            "Batch Number: 1026 Loss: 2.022001028060913 Time taken: 0.31848597526550293\n",
            "Batch Number: 1027 Loss: 1.9955722093582153 Time taken: 0.30616259574890137\n",
            "Batch Number: 1028 Loss: 2.0044116973876953 Time taken: 0.3278837203979492\n",
            "Batch Number: 1029 Loss: 1.9882506132125854 Time taken: 0.323378324508667\n",
            "Batch Number: 1030 Loss: 1.9910175800323486 Time taken: 0.30649399757385254\n",
            "Batch Number: 1031 Loss: 2.0035996437072754 Time taken: 0.31637024879455566\n",
            "Batch Number: 1032 Loss: 2.0190861225128174 Time taken: 0.31525158882141113\n",
            "Batch Number: 1033 Loss: 2.0011274814605713 Time taken: 0.3071775436401367\n",
            "Batch Number: 1034 Loss: 2.0099403858184814 Time taken: 0.32317090034484863\n",
            "Batch Number: 1035 Loss: 1.9966752529144287 Time taken: 0.3161959648132324\n",
            "Batch Number: 1036 Loss: 2.0218143463134766 Time taken: 0.31731486320495605\n",
            "Batch Number: 1037 Loss: 1.979332447052002 Time taken: 0.3163440227508545\n",
            "Batch Number: 1038 Loss: 1.9759151935577393 Time taken: 0.3253011703491211\n",
            "Batch Number: 1039 Loss: 1.9803917407989502 Time taken: 0.30797815322875977\n",
            "Batch Number: 1040 Loss: 2.029489278793335 Time taken: 0.3041965961456299\n",
            "Batch Number: 1041 Loss: 2.030946969985962 Time taken: 0.3128330707550049\n",
            "Batch Number: 1042 Loss: 2.0068411827087402 Time taken: 0.3191108703613281\n",
            "Batch Number: 1043 Loss: 2.018414258956909 Time taken: 0.3151063919067383\n",
            "Batch Number: 1044 Loss: 2.0120809078216553 Time taken: 0.3270833492279053\n",
            "Batch Number: 1045 Loss: 2.0025484561920166 Time taken: 0.32771849632263184\n",
            "Batch Number: 1046 Loss: 1.9874358177185059 Time taken: 0.3070945739746094\n",
            "Batch Number: 1047 Loss: 1.9812060594558716 Time taken: 0.3084836006164551\n",
            "Batch Number: 1048 Loss: 1.9994142055511475 Time taken: 0.30953001976013184\n",
            "Batch Number: 1049 Loss: 1.997025489807129 Time taken: 0.3187277317047119\n",
            "Batch Number: 1050 Loss: 2.0148983001708984 Time taken: 0.3177814483642578\n",
            "Batch Number: 1051 Loss: 1.9934287071228027 Time taken: 0.3163580894470215\n",
            "Batch Number: 1052 Loss: 1.982413411140442 Time taken: 0.30773282051086426\n",
            "Batch Number: 1053 Loss: 2.0130016803741455 Time taken: 0.3087158203125\n",
            "Batch Number: 1054 Loss: 1.9986345767974854 Time taken: 0.31863927841186523\n",
            "Batch Number: 1055 Loss: 2.012016773223877 Time taken: 0.3049924373626709\n",
            "Batch Number: 1056 Loss: 2.007868528366089 Time taken: 0.30513668060302734\n",
            "Batch Number: 1057 Loss: 2.0177671909332275 Time taken: 0.33411431312561035\n",
            "Batch Number: 1058 Loss: 2.009550094604492 Time taken: 0.3343992233276367\n",
            "Batch Number: 1059 Loss: 2.017726421356201 Time taken: 0.31797099113464355\n",
            "Batch Number: 1060 Loss: 2.014072895050049 Time taken: 0.32932329177856445\n",
            "Batch Number: 1061 Loss: 1.9910448789596558 Time taken: 0.3161468505859375\n",
            "Batch Number: 1062 Loss: 1.9999538660049438 Time taken: 0.3120262622833252\n",
            "Batch Number: 1063 Loss: 1.9719505310058594 Time taken: 0.3309135437011719\n",
            "Batch Number: 1064 Loss: 2.002481698989868 Time taken: 0.3164975643157959\n",
            "Batch Number: 1065 Loss: 1.989601731300354 Time taken: 0.315310001373291\n",
            "Batch Number: 1066 Loss: 1.9984793663024902 Time taken: 0.3190286159515381\n",
            "Batch Number: 1067 Loss: 1.9809986352920532 Time taken: 0.30974912643432617\n",
            "Batch Number: 1068 Loss: 1.9829684495925903 Time taken: 0.3101012706756592\n",
            "Batch Number: 1069 Loss: 1.9981361627578735 Time taken: 0.32408976554870605\n",
            "Batch Number: 1070 Loss: 2.0096235275268555 Time taken: 0.31993770599365234\n",
            "Batch Number: 1071 Loss: 2.0019917488098145 Time taken: 0.31363821029663086\n",
            "Batch Number: 1072 Loss: 1.9834508895874023 Time taken: 0.31871533393859863\n",
            "Batch Number: 1073 Loss: 1.9936202764511108 Time taken: 0.31894516944885254\n",
            "Batch Number: 1074 Loss: 1.9941128492355347 Time taken: 0.31197142601013184\n",
            "Batch Number: 1075 Loss: 1.9773566722869873 Time taken: 0.3040275573730469\n",
            "Batch Number: 1076 Loss: 1.9784932136535645 Time taken: 0.3277709484100342\n",
            "Batch Number: 1077 Loss: 1.9939442873001099 Time taken: 0.31224775314331055\n",
            "Batch Number: 1078 Loss: 1.9745107889175415 Time taken: 0.30487871170043945\n",
            "Batch Number: 1079 Loss: 1.9735082387924194 Time taken: 0.3264195919036865\n",
            "Batch Number: 1080 Loss: 1.95414400100708 Time taken: 0.3013947010040283\n",
            "Batch Number: 1081 Loss: 1.9747580289840698 Time taken: 0.3047940731048584\n",
            "Batch Number: 1082 Loss: 1.9696627855300903 Time taken: 0.3298165798187256\n",
            "Batch Number: 1083 Loss: 1.9629040956497192 Time taken: 0.30797600746154785\n",
            "Batch Number: 1084 Loss: 1.9527920484542847 Time taken: 0.30570340156555176\n",
            "Batch Number: 1085 Loss: 1.9767252206802368 Time taken: 0.31856584548950195\n",
            "Batch Number: 1086 Loss: 1.9652013778686523 Time taken: 0.3161153793334961\n",
            "Batch Number: 1087 Loss: 1.9680839776992798 Time taken: 0.3050971031188965\n",
            "Batch Number: 1088 Loss: 1.963607668876648 Time taken: 0.3213694095611572\n",
            "Batch Number: 1089 Loss: 1.9510546922683716 Time taken: 0.3124232292175293\n",
            "Batch Number: 1090 Loss: 1.9605275392532349 Time taken: 0.3049888610839844\n",
            "Batch Number: 1091 Loss: 1.9882843494415283 Time taken: 0.3064918518066406\n",
            "Batch Number: 1092 Loss: 1.9432023763656616 Time taken: 0.3289966583251953\n",
            "Batch Number: 1093 Loss: 1.9737356901168823 Time taken: 0.3086705207824707\n",
            "Batch Number: 1094 Loss: 1.976752758026123 Time taken: 0.3089778423309326\n",
            "Batch Number: 1095 Loss: 1.9761624336242676 Time taken: 0.3299741744995117\n",
            "Batch Number: 1096 Loss: 1.9895917177200317 Time taken: 0.30738067626953125\n",
            "Batch Number: 1097 Loss: 1.962805986404419 Time taken: 0.30866289138793945\n",
            "Batch Number: 1098 Loss: 1.9601271152496338 Time taken: 0.324923038482666\n",
            "Batch Number: 1099 Loss: 1.9677382707595825 Time taken: 0.3082146644592285\n",
            "Batch Number: 1100 Loss: 1.9692867994308472 Time taken: 0.3151240348815918\n",
            "Batch Number: 1101 Loss: 1.9274622201919556 Time taken: 0.3212294578552246\n",
            "Batch Number: 1102 Loss: 1.9707683324813843 Time taken: 0.304304838180542\n",
            "Batch Number: 1103 Loss: 1.9672352075576782 Time taken: 0.3133366107940674\n",
            "Batch Number: 1104 Loss: 1.9740930795669556 Time taken: 0.3189699649810791\n",
            "Batch Number: 1105 Loss: 1.973772644996643 Time taken: 0.31426024436950684\n",
            "Batch Number: 1106 Loss: 1.9653220176696777 Time taken: 0.313709020614624\n",
            "Batch Number: 1107 Loss: 1.990583062171936 Time taken: 0.3071448802947998\n",
            "Batch Number: 1108 Loss: 1.9671967029571533 Time taken: 0.3376307487487793\n",
            "Batch Number: 1109 Loss: 1.9560537338256836 Time taken: 0.3050117492675781\n",
            "Batch Number: 1110 Loss: 1.978742241859436 Time taken: 0.31098461151123047\n",
            "Batch Number: 1111 Loss: 1.978046178817749 Time taken: 0.3347599506378174\n",
            "Batch Number: 1112 Loss: 1.9876402616500854 Time taken: 0.30740952491760254\n",
            "Batch Number: 1113 Loss: 1.9491862058639526 Time taken: 0.3046131134033203\n",
            "Batch Number: 1114 Loss: 1.977739930152893 Time taken: 0.32855963706970215\n",
            "Batch Number: 1115 Loss: 1.9664525985717773 Time taken: 0.31063318252563477\n",
            "Batch Number: 1116 Loss: 1.9897770881652832 Time taken: 0.3093068599700928\n",
            "Batch Number: 1117 Loss: 1.9536716938018799 Time taken: 0.324779748916626\n",
            "Batch Number: 1118 Loss: 1.9787061214447021 Time taken: 0.32141542434692383\n",
            "Batch Number: 1119 Loss: 1.979777455329895 Time taken: 0.3122234344482422\n",
            "Batch Number: 1120 Loss: 1.9610559940338135 Time taken: 0.3091409206390381\n",
            "Batch Number: 1121 Loss: 1.9661462306976318 Time taken: 0.31255626678466797\n",
            "Batch Number: 1122 Loss: 1.982128620147705 Time taken: 0.31665587425231934\n",
            "Batch Number: 1123 Loss: 1.9906697273254395 Time taken: 0.30245041847229004\n",
            "Batch Number: 1124 Loss: 1.978826880455017 Time taken: 0.31519103050231934\n",
            "Batch Number: 1125 Loss: 1.9479643106460571 Time taken: 0.29865264892578125\n",
            "Batch Number: 1126 Loss: 1.9733294248580933 Time taken: 0.30697035789489746\n",
            "Batch Number: 1127 Loss: 1.9542542695999146 Time taken: 0.33812856674194336\n",
            "Batch Number: 1128 Loss: 1.9544918537139893 Time taken: 0.30478405952453613\n",
            "Batch Number: 1129 Loss: 1.9583516120910645 Time taken: 0.2949032783508301\n",
            "Batch Number: 1130 Loss: 1.9870402812957764 Time taken: 0.3218564987182617\n",
            "Batch Number: 1131 Loss: 1.9701358079910278 Time taken: 0.31400108337402344\n",
            "Batch Number: 1132 Loss: 1.9598817825317383 Time taken: 0.3059666156768799\n",
            "Batch Number: 1133 Loss: 1.986445426940918 Time taken: 0.3274705410003662\n",
            "Batch Number: 1134 Loss: 1.9834809303283691 Time taken: 0.30314064025878906\n",
            "Batch Number: 1135 Loss: 1.970137119293213 Time taken: 0.296933650970459\n",
            "Batch Number: 1136 Loss: 1.9657561779022217 Time taken: 0.31357884407043457\n",
            "Batch Number: 1137 Loss: 1.9779566526412964 Time taken: 0.32593607902526855\n",
            "Batch Number: 1138 Loss: 1.9693036079406738 Time taken: 0.30495786666870117\n",
            "Batch Number: 1139 Loss: 1.9712674617767334 Time taken: 0.3123650550842285\n",
            "Batch Number: 1140 Loss: 1.9666459560394287 Time taken: 0.32486510276794434\n",
            "Batch Number: 1141 Loss: 1.9371777772903442 Time taken: 0.3075838088989258\n",
            "Batch Number: 1142 Loss: 1.9517234563827515 Time taken: 0.30495405197143555\n",
            "Batch Number: 1143 Loss: 1.9554734230041504 Time taken: 0.3206055164337158\n",
            "Batch Number: 1144 Loss: 1.9499082565307617 Time taken: 0.30216217041015625\n",
            "Batch Number: 1145 Loss: 1.9513481855392456 Time taken: 0.30269455909729004\n",
            "Batch Number: 1146 Loss: 1.9402985572814941 Time taken: 0.32157158851623535\n",
            "Batch Number: 1147 Loss: 1.9722976684570312 Time taken: 0.3073298931121826\n",
            "Batch Number: 1148 Loss: 1.9735593795776367 Time taken: 0.3033103942871094\n",
            "Batch Number: 1149 Loss: 1.9469578266143799 Time taken: 0.3150622844696045\n",
            "Batch Number: 1150 Loss: 1.955483317375183 Time taken: 0.3039703369140625\n",
            "Batch Number: 1151 Loss: 1.962921142578125 Time taken: 0.30108094215393066\n",
            "Batch Number: 1152 Loss: 1.969080924987793 Time taken: 0.3153495788574219\n",
            "Batch Number: 1153 Loss: 1.9695477485656738 Time taken: 0.312824010848999\n",
            "Batch Number: 1154 Loss: 1.9901155233383179 Time taken: 0.2979910373687744\n",
            "Batch Number: 1155 Loss: 1.977487325668335 Time taken: 0.3026425838470459\n",
            "Batch Number: 1156 Loss: 2.0083608627319336 Time taken: 0.32169342041015625\n",
            "Batch Number: 1157 Loss: 2.0094027519226074 Time taken: 0.29573845863342285\n",
            "Batch Number: 1158 Loss: 1.9947789907455444 Time taken: 0.30436086654663086\n",
            "Batch Number: 1159 Loss: 1.995519757270813 Time taken: 0.32053065299987793\n",
            "Batch Number: 1160 Loss: 1.9727874994277954 Time taken: 0.3017239570617676\n",
            "Batch Number: 1161 Loss: 1.9723544120788574 Time taken: 0.3049049377441406\n",
            "Batch Number: 1162 Loss: 1.9649643898010254 Time taken: 0.3171989917755127\n",
            "Batch Number: 1163 Loss: 1.9759783744812012 Time taken: 0.30838990211486816\n",
            "Batch Number: 1164 Loss: 1.958365559577942 Time taken: 0.29823923110961914\n",
            "Batch Number: 1165 Loss: 1.9653016328811646 Time taken: 0.30979108810424805\n",
            "Batch Number: 1166 Loss: 1.9613734483718872 Time taken: 0.31322312355041504\n",
            "Batch Number: 1167 Loss: 1.9597681760787964 Time taken: 0.30605196952819824\n",
            "Batch Number: 1168 Loss: 1.9456257820129395 Time taken: 0.30710554122924805\n",
            "Batch Number: 1169 Loss: 1.9578360319137573 Time taken: 0.33631014823913574\n",
            "Batch Number: 1170 Loss: 1.960859775543213 Time taken: 0.30489087104797363\n",
            "Batch Number: 1171 Loss: 1.976676344871521 Time taken: 0.30396318435668945\n",
            "Batch Number: 1172 Loss: 1.9575499296188354 Time taken: 0.3336343765258789\n",
            "Batch Number: 1173 Loss: 1.963361382484436 Time taken: 0.3106698989868164\n",
            "Batch Number: 1174 Loss: 1.9892560243606567 Time taken: 0.31566929817199707\n",
            "Batch Number: 1175 Loss: 2.001955270767212 Time taken: 0.3340308666229248\n",
            "Batch Number: 1176 Loss: 1.965141773223877 Time taken: 0.32349634170532227\n",
            "Batch Number: 1177 Loss: 2.0014328956604004 Time taken: 0.3451364040374756\n",
            "Batch Number: 1178 Loss: 1.987438678741455 Time taken: 0.3419039249420166\n",
            "Batch Number: 1179 Loss: 1.9805275201797485 Time taken: 0.323122501373291\n",
            "Batch Number: 1180 Loss: 1.9854624271392822 Time taken: 0.33254265785217285\n",
            "Batch Number: 1181 Loss: 1.9748293161392212 Time taken: 0.33411407470703125\n",
            "Batch Number: 1182 Loss: 1.9655964374542236 Time taken: 0.30637145042419434\n",
            "Batch Number: 1183 Loss: 1.9891051054000854 Time taken: 0.3173034191131592\n",
            "Batch Number: 1184 Loss: 2.00620174407959 Time taken: 0.3433995246887207\n",
            "Batch Number: 1185 Loss: 1.98161780834198 Time taken: 0.3159646987915039\n",
            "Batch Number: 1186 Loss: 1.9747004508972168 Time taken: 0.3227231502532959\n",
            "Batch Number: 1187 Loss: 1.9885121583938599 Time taken: 0.33298563957214355\n",
            "Batch Number: 1188 Loss: 1.9980778694152832 Time taken: 0.31620287895202637\n",
            "Batch Number: 1189 Loss: 1.9620072841644287 Time taken: 0.3262782096862793\n",
            "Batch Number: 1190 Loss: 1.9749128818511963 Time taken: 0.3136920928955078\n",
            "Batch Number: 1191 Loss: 1.9829381704330444 Time taken: 0.3022329807281494\n",
            "Batch Number: 1192 Loss: 1.9863598346710205 Time taken: 0.3114292621612549\n",
            "Batch Number: 1193 Loss: 1.9874159097671509 Time taken: 0.310640811920166\n",
            "Batch Number: 1194 Loss: 1.9835995435714722 Time taken: 0.32203197479248047\n",
            "Batch Number: 1195 Loss: 1.9687474966049194 Time taken: 0.30827760696411133\n",
            "Batch Number: 1196 Loss: 1.9699699878692627 Time taken: 0.3028395175933838\n",
            "Batch Number: 1197 Loss: 1.990071177482605 Time taken: 0.34381723403930664\n",
            "Batch Number: 1198 Loss: 1.9607501029968262 Time taken: 0.31308794021606445\n",
            "Batch Number: 1199 Loss: 1.9917415380477905 Time taken: 0.30542755126953125\n",
            "Batch Number: 1200 Loss: 1.9459363222122192 Time taken: 0.3291313648223877\n",
            "Batch Number: 1201 Loss: 1.9669212102890015 Time taken: 0.3061201572418213\n",
            "Batch Number: 1202 Loss: 1.9576700925827026 Time taken: 0.30420780181884766\n",
            "Batch Number: 1203 Loss: 1.9591277837753296 Time taken: 0.31731462478637695\n",
            "Batch Number: 1204 Loss: 1.9431633949279785 Time taken: 0.3056759834289551\n",
            "Batch Number: 1205 Loss: 1.9485527276992798 Time taken: 0.30031800270080566\n",
            "Batch Number: 1206 Loss: 1.9729679822921753 Time taken: 0.31340551376342773\n",
            "Batch Number: 1207 Loss: 1.932777762413025 Time taken: 0.3152351379394531\n",
            "Batch Number: 1208 Loss: 1.956890344619751 Time taken: 0.30137157440185547\n",
            "Batch Number: 1209 Loss: 1.9341163635253906 Time taken: 0.31714820861816406\n",
            "Batch Number: 1210 Loss: 1.9601634740829468 Time taken: 0.3170952796936035\n",
            "Batch Number: 1211 Loss: 1.946677565574646 Time taken: 0.3121938705444336\n",
            "Batch Number: 1212 Loss: 1.9337822198867798 Time taken: 0.2995569705963135\n",
            "Batch Number: 1213 Loss: 1.9620832204818726 Time taken: 0.3146951198577881\n",
            "Batch Number: 1214 Loss: 1.952337622642517 Time taken: 0.2939319610595703\n",
            "Batch Number: 1215 Loss: 1.979294776916504 Time taken: 0.31142449378967285\n",
            "Batch Number: 1216 Loss: 1.939179539680481 Time taken: 0.32086849212646484\n",
            "Batch Number: 1217 Loss: 1.9258114099502563 Time taken: 0.3061792850494385\n",
            "Batch Number: 1218 Loss: 1.946440577507019 Time taken: 0.30674290657043457\n",
            "Batch Number: 1219 Loss: 1.9187226295471191 Time taken: 0.3357725143432617\n",
            "Batch Number: 1220 Loss: 1.933847188949585 Time taken: 0.3089447021484375\n",
            "Batch Number: 1221 Loss: 1.9253613948822021 Time taken: 0.31125521659851074\n",
            "Batch Number: 1222 Loss: 1.9556152820587158 Time taken: 0.3154447078704834\n",
            "Batch Number: 1223 Loss: 1.966453194618225 Time taken: 0.317899227142334\n",
            "Batch Number: 1224 Loss: 1.9519438743591309 Time taken: 0.30877137184143066\n",
            "Batch Number: 1225 Loss: 1.9526511430740356 Time taken: 0.31314802169799805\n",
            "Batch Number: 1226 Loss: 1.9708373546600342 Time taken: 0.3280620574951172\n",
            "Batch Number: 1227 Loss: 1.9274126291275024 Time taken: 0.30312204360961914\n",
            "Batch Number: 1228 Loss: 1.9400427341461182 Time taken: 0.3271341323852539\n",
            "Batch Number: 1229 Loss: 1.9354642629623413 Time taken: 0.3264036178588867\n",
            "Batch Number: 1230 Loss: 1.9589812755584717 Time taken: 0.3138711452484131\n",
            "Batch Number: 1231 Loss: 1.942710518836975 Time taken: 0.3158223628997803\n",
            "Batch Number: 1232 Loss: 1.9472163915634155 Time taken: 0.3231923580169678\n",
            "Batch Number: 1233 Loss: 1.9325306415557861 Time taken: 0.3273129463195801\n",
            "Batch Number: 1234 Loss: 1.9573427438735962 Time taken: 0.31281089782714844\n",
            "Batch Number: 1235 Loss: 1.96149480342865 Time taken: 0.3120877742767334\n",
            "Batch Number: 1236 Loss: 1.9465423822402954 Time taken: 0.30190515518188477\n",
            "Batch Number: 1237 Loss: 1.9609367847442627 Time taken: 0.2953920364379883\n",
            "Batch Number: 1238 Loss: 1.9538198709487915 Time taken: 0.3099653720855713\n",
            "Batch Number: 1239 Loss: 1.9431095123291016 Time taken: 0.3015456199645996\n",
            "Batch Number: 1240 Loss: 1.9662418365478516 Time taken: 0.30358076095581055\n",
            "Batch Number: 1241 Loss: 1.9395869970321655 Time taken: 0.30754804611206055\n",
            "Batch Number: 1242 Loss: 1.958150029182434 Time taken: 0.3125920295715332\n",
            "Batch Number: 1243 Loss: 1.955245852470398 Time taken: 0.30324745178222656\n",
            "Batch Number: 1244 Loss: 1.9421013593673706 Time taken: 0.3100142478942871\n",
            "Batch Number: 1245 Loss: 1.9580163955688477 Time taken: 0.3170292377471924\n",
            "Batch Number: 1246 Loss: 1.9447271823883057 Time taken: 0.31137847900390625\n",
            "Batch Number: 1247 Loss: 1.942597508430481 Time taken: 0.30901598930358887\n",
            "Batch Number: 1248 Loss: 1.915796160697937 Time taken: 0.3151884078979492\n",
            "Batch Number: 1249 Loss: 1.9376134872436523 Time taken: 0.30242252349853516\n",
            "Batch Number: 1250 Loss: 1.9437153339385986 Time taken: 0.31110501289367676\n",
            "Batch Number: 1251 Loss: 1.9185373783111572 Time taken: 0.326812744140625\n",
            "Batch Number: 1252 Loss: 1.9377003908157349 Time taken: 0.3072528839111328\n",
            "Batch Number: 1253 Loss: 1.923987865447998 Time taken: 0.3065800666809082\n",
            "Batch Number: 1254 Loss: 1.94096040725708 Time taken: 0.3130452632904053\n",
            "Batch Number: 1255 Loss: 1.934718132019043 Time taken: 0.3207406997680664\n",
            "Batch Number: 1256 Loss: 1.9108842611312866 Time taken: 0.30567312240600586\n",
            "Batch Number: 1257 Loss: 1.9392011165618896 Time taken: 0.31244993209838867\n",
            "Batch Number: 1258 Loss: 1.9240529537200928 Time taken: 0.31881260871887207\n",
            "Batch Number: 1259 Loss: 1.9032824039459229 Time taken: 0.30976295471191406\n",
            "Batch Number: 1260 Loss: 1.8870947360992432 Time taken: 0.30591917037963867\n",
            "Batch Number: 1261 Loss: 1.9442936182022095 Time taken: 0.31008100509643555\n",
            "Batch Number: 1262 Loss: 1.9163678884506226 Time taken: 0.30780816078186035\n",
            "Batch Number: 1263 Loss: 1.9034504890441895 Time taken: 0.31363654136657715\n",
            "Batch Number: 1264 Loss: 1.8923346996307373 Time taken: 0.3137664794921875\n",
            "Batch Number: 1265 Loss: 1.9092931747436523 Time taken: 0.30211353302001953\n",
            "Batch Number: 1266 Loss: 1.9117941856384277 Time taken: 0.3022654056549072\n",
            "Batch Number: 1267 Loss: 1.9354010820388794 Time taken: 0.3149912357330322\n",
            "Batch Number: 1268 Loss: 1.922392725944519 Time taken: 0.3104555606842041\n",
            "Batch Number: 1269 Loss: 1.9085853099822998 Time taken: 0.3002927303314209\n",
            "Batch Number: 1270 Loss: 1.9164087772369385 Time taken: 0.31142354011535645\n",
            "Batch Number: 1271 Loss: 1.9224740266799927 Time taken: 0.3146696090698242\n",
            "Batch Number: 1272 Loss: 1.927812933921814 Time taken: 0.30147242546081543\n",
            "Batch Number: 1273 Loss: 1.9202044010162354 Time taken: 0.30822014808654785\n",
            "Batch Number: 1274 Loss: 1.919940710067749 Time taken: 0.31700921058654785\n",
            "Batch Number: 1275 Loss: 1.9422367811203003 Time taken: 0.3045616149902344\n",
            "Batch Number: 1276 Loss: 1.9217654466629028 Time taken: 0.31466197967529297\n",
            "Batch Number: 1277 Loss: 1.914912223815918 Time taken: 0.3201751708984375\n",
            "Batch Number: 1278 Loss: 1.900901198387146 Time taken: 0.312511682510376\n",
            "Batch Number: 1279 Loss: 1.9172122478485107 Time taken: 0.3202826976776123\n",
            "Batch Number: 1280 Loss: 1.8935824632644653 Time taken: 0.31572604179382324\n",
            "Batch Number: 1281 Loss: 1.9112831354141235 Time taken: 0.3044593334197998\n",
            "Batch Number: 1282 Loss: 1.903346061706543 Time taken: 0.31193041801452637\n",
            "Batch Number: 1283 Loss: 1.8994102478027344 Time taken: 0.328127384185791\n",
            "Batch Number: 1284 Loss: 1.9071868658065796 Time taken: 0.3199481964111328\n",
            "Batch Number: 1285 Loss: 1.926458716392517 Time taken: 0.30811238288879395\n",
            "Batch Number: 1286 Loss: 1.9253218173980713 Time taken: 0.3193976879119873\n",
            "Batch Number: 1287 Loss: 1.9214966297149658 Time taken: 0.32076096534729004\n",
            "Batch Number: 1288 Loss: 1.9036997556686401 Time taken: 0.31014323234558105\n",
            "Batch Number: 1289 Loss: 1.893588900566101 Time taken: 0.3214693069458008\n",
            "Batch Number: 1290 Loss: 1.8971507549285889 Time taken: 0.32317090034484863\n",
            "Batch Number: 1291 Loss: 1.8984073400497437 Time taken: 0.30217504501342773\n",
            "Batch Number: 1292 Loss: 1.9117190837860107 Time taken: 0.3107950687408447\n",
            "Batch Number: 1293 Loss: 1.910546064376831 Time taken: 0.3165857791900635\n",
            "Batch Number: 1294 Loss: 1.9273388385772705 Time taken: 0.30708932876586914\n",
            "Batch Number: 1295 Loss: 1.9378012418746948 Time taken: 0.3120608329772949\n",
            "Batch Number: 1296 Loss: 1.9011828899383545 Time taken: 0.3167076110839844\n",
            "Batch Number: 1297 Loss: 1.912155270576477 Time taken: 0.3057093620300293\n",
            "Batch Number: 1298 Loss: 1.93727445602417 Time taken: 0.3123350143432617\n",
            "Batch Number: 1299 Loss: 1.9127497673034668 Time taken: 0.30887794494628906\n",
            "Batch Number: 1300 Loss: 1.943652868270874 Time taken: 0.3290293216705322\n",
            "Batch Number: 1301 Loss: 1.9250203371047974 Time taken: 0.30279111862182617\n",
            "Batch Number: 1302 Loss: 1.9295154809951782 Time taken: 0.3086113929748535\n",
            "Batch Number: 1303 Loss: 1.9130362272262573 Time taken: 0.32770848274230957\n",
            "Batch Number: 1304 Loss: 1.90794837474823 Time taken: 0.30251526832580566\n",
            "Batch Number: 1305 Loss: 1.922918438911438 Time taken: 0.31787681579589844\n",
            "Batch Number: 1306 Loss: 1.9184008836746216 Time taken: 0.3198215961456299\n",
            "Batch Number: 1307 Loss: 1.909589171409607 Time taken: 0.30191874504089355\n",
            "Batch Number: 1308 Loss: 1.9073145389556885 Time taken: 0.3179929256439209\n",
            "Batch Number: 1309 Loss: 1.8834788799285889 Time taken: 0.3329803943634033\n",
            "Batch Number: 1310 Loss: 1.9086472988128662 Time taken: 0.30265188217163086\n",
            "Batch Number: 1311 Loss: 1.93769109249115 Time taken: 0.3224787712097168\n",
            "Batch Number: 1312 Loss: 1.9142839908599854 Time taken: 0.2986328601837158\n",
            "Batch Number: 1313 Loss: 1.9167885780334473 Time taken: 0.29692506790161133\n",
            "Batch Number: 1314 Loss: 1.9100966453552246 Time taken: 0.30623388290405273\n",
            "Batch Number: 1315 Loss: 1.951076626777649 Time taken: 0.30368494987487793\n",
            "Batch Number: 1316 Loss: 1.8790838718414307 Time taken: 0.3154878616333008\n",
            "Batch Number: 1317 Loss: 1.9367036819458008 Time taken: 0.3112144470214844\n",
            "Batch Number: 1318 Loss: 1.9137042760849 Time taken: 0.3072044849395752\n",
            "Batch Number: 1319 Loss: 1.9117454290390015 Time taken: 0.3277244567871094\n",
            "Batch Number: 1320 Loss: 1.913680076599121 Time taken: 0.30647706985473633\n",
            "Batch Number: 1321 Loss: 1.8857448101043701 Time taken: 0.3179337978363037\n",
            "Batch Number: 1322 Loss: 1.9002033472061157 Time taken: 0.3216519355773926\n",
            "Batch Number: 1323 Loss: 1.8992621898651123 Time taken: 0.3084385395050049\n",
            "Batch Number: 1324 Loss: 1.8842544555664062 Time taken: 0.31936192512512207\n",
            "Batch Number: 1325 Loss: 1.8957798480987549 Time taken: 0.31102967262268066\n",
            "Batch Number: 1326 Loss: 1.9005647897720337 Time taken: 0.3359193801879883\n",
            "Batch Number: 1327 Loss: 1.9142566919326782 Time taken: 0.31874513626098633\n",
            "Batch Number: 1328 Loss: 1.9086027145385742 Time taken: 0.3353292942047119\n",
            "Batch Number: 1329 Loss: 1.8991179466247559 Time taken: 0.3086228370666504\n",
            "Batch Number: 1330 Loss: 1.9116129875183105 Time taken: 0.31106138229370117\n",
            "Batch Number: 1331 Loss: 1.8842779397964478 Time taken: 0.3143441677093506\n",
            "Batch Number: 1332 Loss: 1.9139025211334229 Time taken: 0.304119348526001\n",
            "Batch Number: 1333 Loss: 1.9392890930175781 Time taken: 0.3171229362487793\n",
            "Batch Number: 1334 Loss: 1.95743989944458 Time taken: 0.3051636219024658\n",
            "Batch Number: 1335 Loss: 1.9552030563354492 Time taken: 0.3198535442352295\n",
            "Batch Number: 1336 Loss: 1.928934097290039 Time taken: 0.32184839248657227\n",
            "Batch Number: 1337 Loss: 1.9289363622665405 Time taken: 0.3034353256225586\n",
            "Batch Number: 1338 Loss: 1.9152296781539917 Time taken: 0.316852331161499\n",
            "Batch Number: 1339 Loss: 1.9163326025009155 Time taken: 0.31432151794433594\n",
            "Batch Number: 1340 Loss: 1.9035345315933228 Time taken: 0.3086056709289551\n",
            "Batch Number: 1341 Loss: 1.9219731092453003 Time taken: 0.3219876289367676\n",
            "Batch Number: 1342 Loss: 1.8894144296646118 Time taken: 0.30796051025390625\n",
            "Batch Number: 1343 Loss: 1.9329737424850464 Time taken: 0.3166325092315674\n",
            "Batch Number: 1344 Loss: 1.9028021097183228 Time taken: 0.324737548828125\n",
            "Batch Number: 1345 Loss: 1.9121071100234985 Time taken: 0.3047146797180176\n",
            "Batch Number: 1346 Loss: 1.9048415422439575 Time taken: 0.31426286697387695\n",
            "Batch Number: 1347 Loss: 1.883765697479248 Time taken: 0.32576847076416016\n",
            "Batch Number: 1348 Loss: 1.8913482427597046 Time taken: 0.30707502365112305\n",
            "Batch Number: 1349 Loss: 1.8988126516342163 Time taken: 0.3137850761413574\n",
            "Batch Number: 1350 Loss: 1.899613857269287 Time taken: 0.30522775650024414\n",
            "Batch Number: 1351 Loss: 1.9001127481460571 Time taken: 0.3173866271972656\n",
            "Batch Number: 1352 Loss: 1.8943920135498047 Time taken: 0.31725001335144043\n",
            "Batch Number: 1353 Loss: 1.8944846391677856 Time taken: 0.3051717281341553\n",
            "Batch Number: 1354 Loss: 1.9154092073440552 Time taken: 0.318878173828125\n",
            "Batch Number: 1355 Loss: 1.927317500114441 Time taken: 0.3207976818084717\n",
            "Batch Number: 1356 Loss: 1.9496325254440308 Time taken: 0.306673526763916\n",
            "Batch Number: 1357 Loss: 1.9357980489730835 Time taken: 0.32826924324035645\n",
            "Batch Number: 1358 Loss: 1.9166890382766724 Time taken: 0.3160543441772461\n",
            "Batch Number: 1359 Loss: 1.9384403228759766 Time taken: 0.3133714199066162\n",
            "Batch Number: 1360 Loss: 1.931882619857788 Time taken: 0.3180267810821533\n",
            "Batch Number: 1361 Loss: 1.9311021566390991 Time taken: 0.31066203117370605\n",
            "Batch Number: 1362 Loss: 1.9263867139816284 Time taken: 0.3154873847961426\n",
            "Batch Number: 1363 Loss: 1.914763331413269 Time taken: 0.3201022148132324\n",
            "Batch Number: 1364 Loss: 1.9322874546051025 Time taken: 0.32747483253479004\n",
            "Batch Number: 1365 Loss: 1.9350121021270752 Time taken: 0.31768012046813965\n",
            "Batch Number: 1366 Loss: 1.9406335353851318 Time taken: 0.3164527416229248\n",
            "Batch Number: 1367 Loss: 1.915178894996643 Time taken: 0.32637619972229004\n",
            "Batch Number: 1368 Loss: 1.9305803775787354 Time taken: 0.3157689571380615\n",
            "Batch Number: 1369 Loss: 1.9351551532745361 Time taken: 0.30643558502197266\n",
            "Batch Number: 1370 Loss: 1.9646974802017212 Time taken: 0.3142571449279785\n",
            "Batch Number: 1371 Loss: 1.9245789051055908 Time taken: 0.33052921295166016\n",
            "Batch Number: 1372 Loss: 1.9431726932525635 Time taken: 0.3082296848297119\n",
            "Batch Number: 1373 Loss: 1.9283469915390015 Time taken: 0.31710243225097656\n",
            "Batch Number: 1374 Loss: 1.921563982963562 Time taken: 0.32238245010375977\n",
            "Batch Number: 1375 Loss: 1.922026515007019 Time taken: 0.31391477584838867\n",
            "Batch Number: 1376 Loss: 1.9064898490905762 Time taken: 0.3077273368835449\n",
            "Batch Number: 1377 Loss: 1.9160183668136597 Time taken: 0.31838345527648926\n",
            "Batch Number: 1378 Loss: 1.91643488407135 Time taken: 0.3086564540863037\n",
            "Batch Number: 1379 Loss: 1.9197242259979248 Time taken: 0.31102466583251953\n",
            "Batch Number: 1380 Loss: 1.918070673942566 Time taken: 0.31117677688598633\n",
            "Batch Number: 1381 Loss: 1.925583004951477 Time taken: 0.3133857250213623\n",
            "Batch Number: 1382 Loss: 1.9154131412506104 Time taken: 0.3086860179901123\n",
            "Batch Number: 1383 Loss: 1.913221836090088 Time taken: 0.30975890159606934\n",
            "Batch Number: 1384 Loss: 1.9045909643173218 Time taken: 0.3146474361419678\n",
            "Batch Number: 1385 Loss: 1.9047719240188599 Time taken: 0.3122885227203369\n",
            "Batch Number: 1386 Loss: 1.9003087282180786 Time taken: 0.3008584976196289\n",
            "Batch Number: 1387 Loss: 1.9133411645889282 Time taken: 0.31812572479248047\n",
            "Batch Number: 1388 Loss: 1.9106508493423462 Time taken: 0.3151671886444092\n",
            "Batch Number: 1389 Loss: 1.8806836605072021 Time taken: 0.31511664390563965\n",
            "Batch Number: 1390 Loss: 1.8599189519882202 Time taken: 0.3186004161834717\n",
            "Batch Number: 1391 Loss: 1.8991197347640991 Time taken: 0.3071117401123047\n",
            "Batch Number: 1392 Loss: 1.9154988527297974 Time taken: 0.31262636184692383\n",
            "Batch Number: 1393 Loss: 1.9095028638839722 Time taken: 0.3227357864379883\n",
            "Batch Number: 1394 Loss: 1.9180753231048584 Time taken: 0.3115720748901367\n",
            "Batch Number: 1395 Loss: 1.9032050371170044 Time taken: 0.32085156440734863\n",
            "Batch Number: 1396 Loss: 1.8911856412887573 Time taken: 0.31188344955444336\n",
            "Batch Number: 1397 Loss: 1.8651295900344849 Time taken: 0.30524396896362305\n",
            "Batch Number: 1398 Loss: 1.890308141708374 Time taken: 0.31538987159729004\n",
            "Batch Number: 1399 Loss: 1.8840969800949097 Time taken: 0.3234841823577881\n",
            "Batch Number: 1400 Loss: 1.890336036682129 Time taken: 0.3094899654388428\n",
            "Batch Number: 1401 Loss: 1.9064273834228516 Time taken: 0.312363862991333\n",
            "Batch Number: 1402 Loss: 1.9138659238815308 Time taken: 0.30558228492736816\n",
            "Batch Number: 1403 Loss: 1.9084211587905884 Time taken: 0.31015515327453613\n",
            "Batch Number: 1404 Loss: 1.909706473350525 Time taken: 0.3165855407714844\n",
            "Batch Number: 1405 Loss: 1.9176503419876099 Time taken: 0.32103967666625977\n",
            "Batch Number: 1406 Loss: 1.891363501548767 Time taken: 0.3104698657989502\n",
            "Batch Number: 1407 Loss: 1.8419833183288574 Time taken: 0.3092784881591797\n",
            "Batch Number: 1408 Loss: 1.8797458410263062 Time taken: 0.32873964309692383\n",
            "Batch Number: 1409 Loss: 1.86305570602417 Time taken: 0.3218832015991211\n",
            "Batch Number: 1410 Loss: 1.9036024808883667 Time taken: 0.30611109733581543\n",
            "Batch Number: 1411 Loss: 1.8904064893722534 Time taken: 0.31778693199157715\n",
            "Batch Number: 1412 Loss: 1.9079208374023438 Time taken: 0.31354689598083496\n",
            "Batch Number: 1413 Loss: 1.8972316980361938 Time taken: 0.30298519134521484\n",
            "Batch Number: 1414 Loss: 1.8987939357757568 Time taken: 0.3165302276611328\n",
            "Batch Number: 1415 Loss: 1.9000544548034668 Time taken: 0.31181931495666504\n",
            "Batch Number: 1416 Loss: 1.9016306400299072 Time taken: 0.3034226894378662\n",
            "Batch Number: 1417 Loss: 1.9035910367965698 Time taken: 0.3152008056640625\n",
            "Batch Number: 1418 Loss: 1.9262019395828247 Time taken: 0.3038508892059326\n",
            "Batch Number: 1419 Loss: 1.9127470254898071 Time taken: 0.3204019069671631\n",
            "Batch Number: 1420 Loss: 1.8737951517105103 Time taken: 0.3108518123626709\n",
            "Batch Number: 1421 Loss: 1.8817929029464722 Time taken: 0.31635522842407227\n",
            "Batch Number: 1422 Loss: 1.9071921110153198 Time taken: 0.32651662826538086\n",
            "Batch Number: 1423 Loss: 1.9055668115615845 Time taken: 0.3115732669830322\n",
            "Batch Number: 1424 Loss: 1.9020086526870728 Time taken: 0.32235050201416016\n",
            "Batch Number: 1425 Loss: 1.9061888456344604 Time taken: 0.30834269523620605\n",
            "Batch Number: 1426 Loss: 1.8878233432769775 Time taken: 0.3142542839050293\n",
            "Batch Number: 1427 Loss: 1.8859131336212158 Time taken: 0.3259875774383545\n",
            "Batch Number: 1428 Loss: 1.8609505891799927 Time taken: 0.3199887275695801\n",
            "Batch Number: 1429 Loss: 1.8543509244918823 Time taken: 0.307758092880249\n",
            "Batch Number: 1430 Loss: 1.8760461807250977 Time taken: 0.31894898414611816\n",
            "Batch Number: 1431 Loss: 1.8683969974517822 Time taken: 0.30809736251831055\n",
            "Batch Number: 1432 Loss: 1.8931479454040527 Time taken: 0.30794620513916016\n",
            "Batch Number: 1433 Loss: 1.891344428062439 Time taken: 0.3206021785736084\n",
            "Batch Number: 1434 Loss: 1.872065544128418 Time taken: 0.3162660598754883\n",
            "Batch Number: 1435 Loss: 1.8761463165283203 Time taken: 0.3062002658843994\n",
            "Batch Number: 1436 Loss: 1.8844988346099854 Time taken: 0.3100852966308594\n",
            "Batch Number: 1437 Loss: 1.9099866151809692 Time taken: 0.32309770584106445\n",
            "Batch Number: 1438 Loss: 1.8965139389038086 Time taken: 0.31116366386413574\n",
            "Batch Number: 1439 Loss: 1.8989996910095215 Time taken: 0.30665087699890137\n",
            "Batch Number: 1440 Loss: 1.8784914016723633 Time taken: 0.31731152534484863\n",
            "Batch Number: 1441 Loss: 1.9024852514266968 Time taken: 0.31905174255371094\n",
            "Batch Number: 1442 Loss: 1.8776710033416748 Time taken: 0.3061091899871826\n",
            "Batch Number: 1443 Loss: 1.845254898071289 Time taken: 0.3173060417175293\n",
            "Batch Number: 1444 Loss: 1.8636704683303833 Time taken: 0.3273029327392578\n",
            "Batch Number: 1445 Loss: 1.8579070568084717 Time taken: 0.30692291259765625\n",
            "Batch Number: 1446 Loss: 1.8441040515899658 Time taken: 0.31748437881469727\n",
            "Batch Number: 1447 Loss: 1.85346519947052 Time taken: 0.3204970359802246\n",
            "Batch Number: 1448 Loss: 1.8614110946655273 Time taken: 0.3069174289703369\n",
            "Batch Number: 1449 Loss: 1.8487483263015747 Time taken: 0.31051206588745117\n",
            "Batch Number: 1450 Loss: 1.8534258604049683 Time taken: 0.3111450672149658\n",
            "Batch Number: 1451 Loss: 1.8602712154388428 Time taken: 0.3146977424621582\n",
            "Batch Number: 1452 Loss: 1.8772739171981812 Time taken: 0.31597280502319336\n",
            "Batch Number: 1453 Loss: 1.8778045177459717 Time taken: 0.31034040451049805\n",
            "Batch Number: 1454 Loss: 1.9000720977783203 Time taken: 0.30757570266723633\n",
            "Batch Number: 1455 Loss: 1.8984448909759521 Time taken: 0.30539464950561523\n",
            "Batch Number: 1456 Loss: 1.8845645189285278 Time taken: 0.321854829788208\n",
            "Batch Number: 1457 Loss: 1.8861784934997559 Time taken: 0.316051721572876\n",
            "Batch Number: 1458 Loss: 1.857637643814087 Time taken: 0.31021928787231445\n",
            "Batch Number: 1459 Loss: 1.8708999156951904 Time taken: 0.31845855712890625\n",
            "Batch Number: 1460 Loss: 1.8586839437484741 Time taken: 0.30723094940185547\n",
            "Batch Number: 1461 Loss: 1.8654853105545044 Time taken: 0.3215780258178711\n",
            "Batch Number: 1462 Loss: 1.8355361223220825 Time taken: 0.318220853805542\n",
            "Batch Number: 1463 Loss: 1.8784010410308838 Time taken: 0.3150777816772461\n",
            "Batch Number: 1464 Loss: 1.9005780220031738 Time taken: 0.30311155319213867\n",
            "Batch Number: 1465 Loss: 1.8774425983428955 Time taken: 0.3148171901702881\n",
            "Batch Number: 1466 Loss: 1.8671854734420776 Time taken: 0.3168768882751465\n",
            "Batch Number: 1467 Loss: 1.8651999235153198 Time taken: 0.3000142574310303\n",
            "Batch Number: 1468 Loss: 1.864119052886963 Time taken: 0.3125035762786865\n",
            "Batch Number: 1469 Loss: 1.8524881601333618 Time taken: 0.31403636932373047\n",
            "Batch Number: 1470 Loss: 1.8700850009918213 Time taken: 0.31070995330810547\n",
            "Batch Number: 1471 Loss: 1.8432166576385498 Time taken: 0.313274621963501\n",
            "Batch Number: 1472 Loss: 1.8810406923294067 Time taken: 0.32696962356567383\n",
            "Batch Number: 1473 Loss: 1.879602074623108 Time taken: 0.30785298347473145\n",
            "Batch Number: 1474 Loss: 1.890089511871338 Time taken: 0.3087897300720215\n",
            "Batch Number: 1475 Loss: 1.873130440711975 Time taken: 0.3149542808532715\n",
            "Batch Number: 1476 Loss: 1.8525047302246094 Time taken: 0.3129923343658447\n",
            "Batch Number: 1477 Loss: 1.868666172027588 Time taken: 0.3084986209869385\n",
            "Batch Number: 1478 Loss: 1.8834195137023926 Time taken: 0.32396602630615234\n",
            "Batch Number: 1479 Loss: 1.8920716047286987 Time taken: 0.3149869441986084\n",
            "Batch Number: 1480 Loss: 1.878639578819275 Time taken: 0.3158607482910156\n",
            "Batch Number: 1481 Loss: 1.8735558986663818 Time taken: 0.3118600845336914\n",
            "Batch Number: 1482 Loss: 1.8518846035003662 Time taken: 0.3179142475128174\n",
            "Batch Number: 1483 Loss: 1.8916001319885254 Time taken: 0.31148600578308105\n",
            "Batch Number: 1484 Loss: 1.8478989601135254 Time taken: 0.3196587562561035\n",
            "Batch Number: 1485 Loss: 1.864098310470581 Time taken: 0.32517361640930176\n",
            "Batch Number: 1486 Loss: 1.868508219718933 Time taken: 0.31917572021484375\n",
            "Batch Number: 1487 Loss: 1.86161208152771 Time taken: 0.3148787021636963\n",
            "Batch Number: 1488 Loss: 1.8770442008972168 Time taken: 0.3218531608581543\n",
            "Batch Number: 1489 Loss: 1.8547093868255615 Time taken: 0.3163144588470459\n",
            "Batch Number: 1490 Loss: 1.8418986797332764 Time taken: 0.3021676540374756\n",
            "Batch Number: 1491 Loss: 1.879202127456665 Time taken: 0.3246128559112549\n",
            "Batch Number: 1492 Loss: 1.8562719821929932 Time taken: 0.3173336982727051\n",
            "Batch Number: 1493 Loss: 1.8451714515686035 Time taken: 0.3154470920562744\n",
            "Batch Number: 1494 Loss: 1.8852301836013794 Time taken: 0.32148098945617676\n",
            "Batch Number: 1495 Loss: 1.8862509727478027 Time taken: 0.3128070831298828\n",
            "Batch Number: 1496 Loss: 1.8665932416915894 Time taken: 0.30965638160705566\n",
            "Batch Number: 1497 Loss: 1.8623031377792358 Time taken: 0.3220639228820801\n",
            "Batch Number: 1498 Loss: 1.855824589729309 Time taken: 0.31606626510620117\n",
            "Batch Number: 1499 Loss: 1.8749805688858032 Time taken: 0.3094148635864258\n",
            "Batch Number: 1500 Loss: 1.8619118928909302 Time taken: 0.31200528144836426\n",
            "Batch Number: 1501 Loss: 1.855967402458191 Time taken: 0.31705379486083984\n",
            "Batch Number: 1502 Loss: 1.8512561321258545 Time taken: 0.3052201271057129\n",
            "Batch Number: 1503 Loss: 1.841204047203064 Time taken: 0.31307387351989746\n",
            "Batch Number: 1504 Loss: 1.8486248254776 Time taken: 0.31482481956481934\n",
            "Batch Number: 1505 Loss: 1.8594329357147217 Time taken: 0.3094656467437744\n",
            "Batch Number: 1506 Loss: 1.842145323753357 Time taken: 0.31616830825805664\n",
            "Batch Number: 1507 Loss: 1.856832504272461 Time taken: 0.32776856422424316\n",
            "Batch Number: 1508 Loss: 1.8601365089416504 Time taken: 0.3167746067047119\n",
            "Batch Number: 1509 Loss: 1.8698383569717407 Time taken: 0.312150239944458\n",
            "Batch Number: 1510 Loss: 1.86182701587677 Time taken: 0.3246192932128906\n",
            "Batch Number: 1511 Loss: 1.8609462976455688 Time taken: 0.30826258659362793\n",
            "Batch Number: 1512 Loss: 1.8590697050094604 Time taken: 0.3095667362213135\n",
            "Batch Number: 1513 Loss: 1.8554173707962036 Time taken: 0.32764172554016113\n",
            "Batch Number: 1514 Loss: 1.9278100728988647 Time taken: 0.3052241802215576\n",
            "Batch Number: 1515 Loss: 1.9363670349121094 Time taken: 0.3040344715118408\n",
            "Batch Number: 1516 Loss: 1.8791825771331787 Time taken: 0.3251955509185791\n",
            "Batch Number: 1517 Loss: 1.8975529670715332 Time taken: 0.33368945121765137\n",
            "Batch Number: 1518 Loss: 1.893508791923523 Time taken: 0.3095576763153076\n",
            "Batch Number: 1519 Loss: 1.875441551208496 Time taken: 0.3211953639984131\n",
            "Batch Number: 1520 Loss: 1.8600554466247559 Time taken: 0.3128988742828369\n",
            "Batch Number: 1521 Loss: 1.872245192527771 Time taken: 0.31001901626586914\n",
            "Batch Number: 1522 Loss: 1.8975762128829956 Time taken: 0.3088223934173584\n",
            "Batch Number: 1523 Loss: 1.8876497745513916 Time taken: 0.3242185115814209\n",
            "Batch Number: 1524 Loss: 1.8645938634872437 Time taken: 0.3061256408691406\n",
            "Batch Number: 1525 Loss: 1.8392789363861084 Time taken: 0.3020913600921631\n",
            "Batch Number: 1526 Loss: 1.8708995580673218 Time taken: 0.3285086154937744\n",
            "Batch Number: 1527 Loss: 1.8533315658569336 Time taken: 0.30219554901123047\n",
            "Batch Number: 1528 Loss: 1.837422251701355 Time taken: 0.30972790718078613\n",
            "Batch Number: 1529 Loss: 1.8527618646621704 Time taken: 0.32153892517089844\n",
            "Batch Number: 1530 Loss: 1.8337668180465698 Time taken: 0.30513858795166016\n",
            "Batch Number: 1531 Loss: 1.8689045906066895 Time taken: 0.3172721862792969\n",
            "Batch Number: 1532 Loss: 1.8794294595718384 Time taken: 0.325239896774292\n",
            "Batch Number: 1533 Loss: 1.8486952781677246 Time taken: 0.3093712329864502\n",
            "Batch Number: 1534 Loss: 1.8775714635849 Time taken: 0.31072497367858887\n",
            "Batch Number: 1535 Loss: 1.88785982131958 Time taken: 0.30982327461242676\n",
            "Batch Number: 1536 Loss: 1.9010593891143799 Time taken: 0.30787038803100586\n",
            "Batch Number: 1537 Loss: 1.8985134363174438 Time taken: 0.3081550598144531\n",
            "Batch Number: 1538 Loss: 1.8635902404785156 Time taken: 0.30908894538879395\n",
            "Batch Number: 1539 Loss: 1.8884024620056152 Time taken: 0.32206106185913086\n",
            "Batch Number: 1540 Loss: 1.889452338218689 Time taken: 0.3061046600341797\n",
            "Batch Number: 1541 Loss: 1.8981328010559082 Time taken: 0.30725550651550293\n",
            "Batch Number: 1542 Loss: 1.8836263418197632 Time taken: 0.3333568572998047\n",
            "Batch Number: 1543 Loss: 1.8829809427261353 Time taken: 0.30837106704711914\n",
            "Batch Number: 1544 Loss: 1.8838845491409302 Time taken: 0.31062769889831543\n",
            "Batch Number: 1545 Loss: 1.8853857517242432 Time taken: 0.32001352310180664\n",
            "Batch Number: 1546 Loss: 1.9147193431854248 Time taken: 0.30707597732543945\n",
            "Batch Number: 1547 Loss: 1.8883533477783203 Time taken: 0.3054168224334717\n",
            "Batch Number: 1548 Loss: 1.8570753335952759 Time taken: 0.32153940200805664\n",
            "Batch Number: 1549 Loss: 1.9066487550735474 Time taken: 0.31607866287231445\n",
            "Batch Number: 1550 Loss: 1.8899599313735962 Time taken: 0.3159158229827881\n",
            "Batch Number: 1551 Loss: 1.9085719585418701 Time taken: 0.3239626884460449\n",
            "Batch Number: 1552 Loss: 1.8955681324005127 Time taken: 0.3087611198425293\n",
            "Batch Number: 1553 Loss: 1.901319980621338 Time taken: 0.30872488021850586\n",
            "Batch Number: 1554 Loss: 1.8896760940551758 Time taken: 0.3066673278808594\n",
            "Batch Number: 1555 Loss: 1.8945249319076538 Time taken: 0.3183462619781494\n",
            "Batch Number: 1556 Loss: 1.8677560091018677 Time taken: 0.30654096603393555\n",
            "Batch Number: 1557 Loss: 1.8831777572631836 Time taken: 0.31383419036865234\n",
            "Batch Number: 1558 Loss: 1.8570775985717773 Time taken: 0.3280024528503418\n",
            "Batch Number: 1559 Loss: 1.8875449895858765 Time taken: 0.3206162452697754\n",
            "Batch Number: 1560 Loss: 1.842353105545044 Time taken: 0.3061177730560303\n",
            "Batch Number: 1561 Loss: 1.8980906009674072 Time taken: 0.3275930881500244\n",
            "Batch Number: 1562 Loss: 1.8586260080337524 Time taken: 0.3146064281463623\n",
            "Batch Number: 1563 Loss: 1.8802008628845215 Time taken: 0.3038637638092041\n",
            "Batch Number: 1564 Loss: 1.8652058839797974 Time taken: 0.32244110107421875\n",
            "Batch Number: 1565 Loss: 1.847909688949585 Time taken: 0.3051109313964844\n",
            "Batch Number: 1566 Loss: 1.877693772315979 Time taken: 0.30894947052001953\n",
            "Batch Number: 1567 Loss: 1.8439096212387085 Time taken: 0.31455016136169434\n",
            "Batch Number: 1568 Loss: 1.851905107498169 Time taken: 0.32066941261291504\n",
            "Batch Number: 1569 Loss: 1.8435909748077393 Time taken: 0.30420494079589844\n",
            "Batch Number: 1570 Loss: 1.8503791093826294 Time taken: 0.3213677406311035\n",
            "Batch Number: 1571 Loss: 1.853053092956543 Time taken: 0.30367326736450195\n",
            "Batch Number: 1572 Loss: 1.8497377634048462 Time taken: 0.3067481517791748\n",
            "Batch Number: 1573 Loss: 1.8255629539489746 Time taken: 0.300520658493042\n",
            "Batch Number: 1574 Loss: 1.888273000717163 Time taken: 0.32697510719299316\n",
            "Batch Number: 1575 Loss: 1.8463599681854248 Time taken: 0.3125479221343994\n",
            "Batch Number: 1576 Loss: 1.8323094844818115 Time taken: 0.3012096881866455\n",
            "Batch Number: 1577 Loss: 1.809977412223816 Time taken: 0.3253467082977295\n",
            "Batch Number: 1578 Loss: 1.843940019607544 Time taken: 0.32023024559020996\n",
            "Batch Number: 1579 Loss: 1.8115313053131104 Time taken: 0.31076502799987793\n",
            "Batch Number: 1580 Loss: 1.8278567790985107 Time taken: 0.3293783664703369\n",
            "Batch Number: 1581 Loss: 1.8685600757598877 Time taken: 0.30443692207336426\n",
            "Batch Number: 1582 Loss: 1.893007516860962 Time taken: 0.30756378173828125\n",
            "Batch Number: 1583 Loss: 1.8722727298736572 Time taken: 0.3129143714904785\n",
            "Batch Number: 1584 Loss: 1.8443903923034668 Time taken: 0.3041524887084961\n",
            "Batch Number: 1585 Loss: 1.860220193862915 Time taken: 0.30727386474609375\n",
            "Batch Number: 1586 Loss: 1.841048240661621 Time taken: 0.31266260147094727\n",
            "Batch Number: 1587 Loss: 1.8474217653274536 Time taken: 0.314647912979126\n",
            "Batch Number: 1588 Loss: 1.834067940711975 Time taken: 0.304624080657959\n",
            "Batch Number: 1589 Loss: 1.8596701622009277 Time taken: 0.3049795627593994\n",
            "Batch Number: 1590 Loss: 1.851965069770813 Time taken: 0.32224488258361816\n",
            "Batch Number: 1591 Loss: 1.858621597290039 Time taken: 0.3088996410369873\n",
            "Batch Number: 1592 Loss: 1.8528529405593872 Time taken: 0.3081505298614502\n",
            "Batch Number: 1593 Loss: 1.8538309335708618 Time taken: 0.33121728897094727\n",
            "Batch Number: 1594 Loss: 1.857873797416687 Time taken: 0.30559444427490234\n",
            "Batch Number: 1595 Loss: 1.8513298034667969 Time taken: 0.3191051483154297\n",
            "Batch Number: 1596 Loss: 1.8678293228149414 Time taken: 0.33154749870300293\n",
            "Batch Number: 1597 Loss: 1.854251742362976 Time taken: 0.3050570487976074\n",
            "Batch Number: 1598 Loss: 1.8584692478179932 Time taken: 0.30144786834716797\n",
            "Batch Number: 1599 Loss: 1.8643831014633179 Time taken: 0.32666516304016113\n",
            "Batch Number: 1600 Loss: 1.8643468618392944 Time taken: 0.3080101013183594\n",
            "Batch Number: 1601 Loss: 1.835170865058899 Time taken: 0.3126487731933594\n",
            "Batch Number: 1602 Loss: 1.8562657833099365 Time taken: 0.31552696228027344\n",
            "Batch Number: 1603 Loss: 1.8585546016693115 Time taken: 0.3178880214691162\n",
            "Batch Number: 1604 Loss: 1.8304147720336914 Time taken: 0.30251264572143555\n",
            "Batch Number: 1605 Loss: 1.8650190830230713 Time taken: 0.3174169063568115\n",
            "Batch Number: 1606 Loss: 1.8423848152160645 Time taken: 0.32178568840026855\n",
            "Batch Number: 1607 Loss: 1.849541425704956 Time taken: 0.3081796169281006\n",
            "Batch Number: 1608 Loss: 1.8281131982803345 Time taken: 0.30969762802124023\n",
            "Batch Number: 1609 Loss: 1.8383225202560425 Time taken: 0.32394862174987793\n",
            "Batch Number: 1610 Loss: 1.851294994354248 Time taken: 0.3191702365875244\n",
            "Batch Number: 1611 Loss: 1.8436110019683838 Time taken: 0.32373785972595215\n",
            "Batch Number: 1612 Loss: 1.827633023262024 Time taken: 0.33199191093444824\n",
            "Batch Number: 1613 Loss: 1.855485200881958 Time taken: 0.3057725429534912\n",
            "Batch Number: 1614 Loss: 1.8427238464355469 Time taken: 0.3081486225128174\n",
            "Batch Number: 1615 Loss: 1.8430399894714355 Time taken: 0.32317066192626953\n",
            "Batch Number: 1616 Loss: 1.831532597541809 Time taken: 0.30684494972229004\n",
            "Batch Number: 1617 Loss: 1.8187357187271118 Time taken: 0.3025839328765869\n",
            "Batch Number: 1618 Loss: 1.8303630352020264 Time taken: 0.3171513080596924\n",
            "Batch Number: 1619 Loss: 1.8411211967468262 Time taken: 0.31319093704223633\n",
            "Batch Number: 1620 Loss: 1.8083299398422241 Time taken: 0.30730700492858887\n",
            "Batch Number: 1621 Loss: 1.8319346904754639 Time taken: 0.3179624080657959\n",
            "Batch Number: 1622 Loss: 1.8216991424560547 Time taken: 0.3170027732849121\n",
            "Batch Number: 1623 Loss: 1.7816342115402222 Time taken: 0.30693483352661133\n",
            "Batch Number: 1624 Loss: 1.8280984163284302 Time taken: 0.3071300983428955\n",
            "Batch Number: 1625 Loss: 1.7866911888122559 Time taken: 0.31966328620910645\n",
            "Batch Number: 1626 Loss: 1.8149853944778442 Time taken: 0.309157133102417\n",
            "Batch Number: 1627 Loss: 1.8023262023925781 Time taken: 0.30441713333129883\n",
            "Batch Number: 1628 Loss: 1.8138113021850586 Time taken: 0.3294816017150879\n",
            "Batch Number: 1629 Loss: 1.8652031421661377 Time taken: 0.3146209716796875\n",
            "Batch Number: 1630 Loss: 1.8631409406661987 Time taken: 0.30472874641418457\n",
            "Batch Number: 1631 Loss: 1.837445855140686 Time taken: 0.3211643695831299\n",
            "Batch Number: 1632 Loss: 1.8233462572097778 Time taken: 0.3100903034210205\n",
            "Batch Number: 1633 Loss: 1.863669753074646 Time taken: 0.30620384216308594\n",
            "Batch Number: 1634 Loss: 1.8505231142044067 Time taken: 0.3283665180206299\n",
            "Batch Number: 1635 Loss: 1.8329262733459473 Time taken: 0.3057379722595215\n",
            "Batch Number: 1636 Loss: 1.8607617616653442 Time taken: 0.307661771774292\n",
            "Batch Number: 1637 Loss: 1.8416730165481567 Time taken: 0.30982184410095215\n",
            "Batch Number: 1638 Loss: 1.81618332862854 Time taken: 0.31531715393066406\n",
            "Batch Number: 1639 Loss: 1.8148540258407593 Time taken: 0.30497241020202637\n",
            "Batch Number: 1640 Loss: 1.8205459117889404 Time taken: 0.3220970630645752\n",
            "Batch Number: 1641 Loss: 1.8231351375579834 Time taken: 0.3173942565917969\n",
            "Batch Number: 1642 Loss: 1.8296223878860474 Time taken: 0.30692434310913086\n",
            "Batch Number: 1643 Loss: 1.8206279277801514 Time taken: 0.31075215339660645\n",
            "Batch Number: 1644 Loss: 1.820920705795288 Time taken: 0.3259871006011963\n",
            "Batch Number: 1645 Loss: 1.8293814659118652 Time taken: 0.3031730651855469\n",
            "Batch Number: 1646 Loss: 1.8371522426605225 Time taken: 0.3083202838897705\n",
            "Batch Number: 1647 Loss: 1.8146923780441284 Time taken: 0.3220069408416748\n",
            "Batch Number: 1648 Loss: 1.8331184387207031 Time taken: 0.30669665336608887\n",
            "Batch Number: 1649 Loss: 1.8324323892593384 Time taken: 0.311629056930542\n",
            "Batch Number: 1650 Loss: 1.8264180421829224 Time taken: 0.32364916801452637\n",
            "Batch Number: 1651 Loss: 1.8301756381988525 Time taken: 0.3110368251800537\n",
            "Batch Number: 1652 Loss: 1.8176401853561401 Time taken: 0.30850696563720703\n",
            "Batch Number: 1653 Loss: 1.8450872898101807 Time taken: 0.31846094131469727\n",
            "Batch Number: 1654 Loss: 1.827785611152649 Time taken: 0.3166542053222656\n",
            "Batch Number: 1655 Loss: 1.8221238851547241 Time taken: 0.30708885192871094\n",
            "Batch Number: 1656 Loss: 1.813460350036621 Time taken: 0.31215453147888184\n",
            "Batch Number: 1657 Loss: 1.8375039100646973 Time taken: 0.32445430755615234\n",
            "Batch Number: 1658 Loss: 1.8528398275375366 Time taken: 0.30557799339294434\n",
            "Batch Number: 1659 Loss: 1.8318464756011963 Time taken: 0.32236456871032715\n",
            "Batch Number: 1660 Loss: 1.8083183765411377 Time taken: 0.31836748123168945\n",
            "Batch Number: 1661 Loss: 1.8090087175369263 Time taken: 0.30802106857299805\n",
            "Batch Number: 1662 Loss: 1.84333074092865 Time taken: 0.30750560760498047\n",
            "Batch Number: 1663 Loss: 1.8313400745391846 Time taken: 0.32538700103759766\n",
            "Batch Number: 1664 Loss: 1.800401210784912 Time taken: 0.30356717109680176\n",
            "Batch Number: 1665 Loss: 1.8419185876846313 Time taken: 0.3088054656982422\n",
            "Batch Number: 1666 Loss: 1.845232605934143 Time taken: 0.32697033882141113\n",
            "Batch Number: 1667 Loss: 1.8170627355575562 Time taken: 0.3116879463195801\n",
            "Batch Number: 1668 Loss: 1.8309330940246582 Time taken: 0.3115415573120117\n",
            "Batch Number: 1669 Loss: 1.809537649154663 Time taken: 0.3140749931335449\n",
            "Batch Number: 1670 Loss: 1.846732258796692 Time taken: 0.3167111873626709\n",
            "Batch Number: 1671 Loss: 1.8238511085510254 Time taken: 0.3050849437713623\n",
            "Batch Number: 1672 Loss: 1.8138186931610107 Time taken: 0.31944775581359863\n",
            "Batch Number: 1673 Loss: 1.8107523918151855 Time taken: 0.32346582412719727\n",
            "Batch Number: 1674 Loss: 1.8418551683425903 Time taken: 0.30741381645202637\n",
            "Batch Number: 1675 Loss: 1.868057131767273 Time taken: 0.31246066093444824\n",
            "Batch Number: 1676 Loss: 1.8286399841308594 Time taken: 0.31999826431274414\n",
            "Batch Number: 1677 Loss: 1.8104966878890991 Time taken: 0.3131897449493408\n",
            "Batch Number: 1678 Loss: 1.81484055519104 Time taken: 0.3143889904022217\n",
            "Batch Number: 1679 Loss: 1.8234409093856812 Time taken: 0.3189046382904053\n",
            "Batch Number: 1680 Loss: 1.7950130701065063 Time taken: 0.3057265281677246\n",
            "Batch Number: 1681 Loss: 1.814413070678711 Time taken: 0.3091564178466797\n",
            "Batch Number: 1682 Loss: 1.8030561208724976 Time taken: 0.3237893581390381\n",
            "Batch Number: 1683 Loss: 1.8227907419204712 Time taken: 0.3086404800415039\n",
            "Batch Number: 1684 Loss: 1.8136260509490967 Time taken: 0.2970583438873291\n",
            "Batch Number: 1685 Loss: 1.818459391593933 Time taken: 0.3166985511779785\n",
            "Batch Number: 1686 Loss: 1.822089672088623 Time taken: 0.3233308792114258\n",
            "Batch Number: 1687 Loss: 1.8338472843170166 Time taken: 0.3017699718475342\n",
            "Batch Number: 1688 Loss: 1.8424221277236938 Time taken: 0.31871581077575684\n",
            "Batch Number: 1689 Loss: 1.8120678663253784 Time taken: 0.3213164806365967\n",
            "Batch Number: 1690 Loss: 1.8309519290924072 Time taken: 0.30788755416870117\n",
            "Batch Number: 1691 Loss: 1.8156818151474 Time taken: 0.3133552074432373\n",
            "Batch Number: 1692 Loss: 1.8099679946899414 Time taken: 0.33093929290771484\n",
            "Batch Number: 1693 Loss: 1.8384815454483032 Time taken: 0.30884814262390137\n",
            "Batch Number: 1694 Loss: 1.8289790153503418 Time taken: 0.3108816146850586\n",
            "Batch Number: 1695 Loss: 1.820214867591858 Time taken: 0.31844162940979004\n",
            "Batch Number: 1696 Loss: 1.8326400518417358 Time taken: 0.30491089820861816\n",
            "Batch Number: 1697 Loss: 1.8199753761291504 Time taken: 0.3096499443054199\n",
            "Batch Number: 1698 Loss: 1.8419787883758545 Time taken: 0.309140682220459\n",
            "Batch Number: 1699 Loss: 1.8631038665771484 Time taken: 0.31134963035583496\n",
            "Batch Number: 1700 Loss: 1.9551516771316528 Time taken: 0.30386900901794434\n",
            "Batch Number: 1701 Loss: 1.8709086179733276 Time taken: 0.323941707611084\n",
            "Batch Number: 1702 Loss: 1.9106290340423584 Time taken: 0.31194496154785156\n",
            "Batch Number: 1703 Loss: 1.9193207025527954 Time taken: 0.32526588439941406\n",
            "Batch Number: 1704 Loss: 1.8945937156677246 Time taken: 0.3202342987060547\n",
            "Batch Number: 1705 Loss: 1.8656662702560425 Time taken: 0.3422055244445801\n",
            "Batch Number: 1706 Loss: 1.8553847074508667 Time taken: 0.32274508476257324\n",
            "Batch Number: 1707 Loss: 1.8422995805740356 Time taken: 0.32116150856018066\n",
            "Batch Number: 1708 Loss: 1.8526054620742798 Time taken: 0.32372379302978516\n",
            "Batch Number: 1709 Loss: 1.860121250152588 Time taken: 0.32143139839172363\n",
            "Batch Number: 1710 Loss: 1.8193047046661377 Time taken: 0.3229715824127197\n",
            "Batch Number: 1711 Loss: 1.8475271463394165 Time taken: 0.3400437831878662\n",
            "Batch Number: 1712 Loss: 1.8511592149734497 Time taken: 0.3173987865447998\n",
            "Batch Number: 1713 Loss: 1.8679206371307373 Time taken: 0.34163546562194824\n",
            "Batch Number: 1714 Loss: 1.8411368131637573 Time taken: 0.328535795211792\n",
            "Batch Number: 1715 Loss: 1.853829264640808 Time taken: 0.32700562477111816\n",
            "Batch Number: 1716 Loss: 1.862061619758606 Time taken: 0.33090662956237793\n",
            "Batch Number: 1717 Loss: 1.8634074926376343 Time taken: 0.32433509826660156\n",
            "Batch Number: 1718 Loss: 1.8681039810180664 Time taken: 0.3170013427734375\n",
            "Batch Number: 1719 Loss: 1.8753516674041748 Time taken: 0.32550573348999023\n",
            "Batch Number: 1720 Loss: 1.8461180925369263 Time taken: 0.33272576332092285\n",
            "Batch Number: 1721 Loss: 1.8630849123001099 Time taken: 0.32309770584106445\n",
            "Batch Number: 1722 Loss: 1.8381614685058594 Time taken: 0.32469797134399414\n",
            "Batch Number: 1723 Loss: 1.8419982194900513 Time taken: 0.32848310470581055\n",
            "Batch Number: 1724 Loss: 1.8726850748062134 Time taken: 0.31104421615600586\n",
            "Batch Number: 1725 Loss: 1.8319917917251587 Time taken: 0.3208277225494385\n",
            "Batch Number: 1726 Loss: 1.8552210330963135 Time taken: 0.3224046230316162\n",
            "Batch Number: 1727 Loss: 1.869415283203125 Time taken: 0.3177056312561035\n",
            "Batch Number: 1728 Loss: 1.8641242980957031 Time taken: 0.32457947731018066\n",
            "Batch Number: 1729 Loss: 1.8697139024734497 Time taken: 0.32350873947143555\n",
            "Batch Number: 1730 Loss: 1.8451569080352783 Time taken: 0.3235948085784912\n",
            "Batch Number: 1731 Loss: 1.866097092628479 Time taken: 0.3203089237213135\n",
            "Batch Number: 1732 Loss: 1.86896550655365 Time taken: 0.32415246963500977\n",
            "Batch Number: 1733 Loss: 1.8571752309799194 Time taken: 0.31637096405029297\n",
            "Batch Number: 1734 Loss: 1.872017502784729 Time taken: 0.30916714668273926\n",
            "Batch Number: 1735 Loss: 1.8573015928268433 Time taken: 0.3085143566131592\n",
            "Batch Number: 1736 Loss: 1.852131724357605 Time taken: 0.31913089752197266\n",
            "Batch Number: 1737 Loss: 1.8188737630844116 Time taken: 0.3124265670776367\n",
            "Batch Number: 1738 Loss: 1.828352451324463 Time taken: 0.31742048263549805\n",
            "Batch Number: 1739 Loss: 1.8196719884872437 Time taken: 0.3204348087310791\n",
            "Batch Number: 1740 Loss: 1.8382580280303955 Time taken: 0.30592775344848633\n",
            "Batch Number: 1741 Loss: 1.8324902057647705 Time taken: 0.3202674388885498\n",
            "Batch Number: 1742 Loss: 1.8449641466140747 Time taken: 0.3157315254211426\n",
            "Batch Number: 1743 Loss: 1.8254421949386597 Time taken: 0.3036532402038574\n",
            "Batch Number: 1744 Loss: 1.8329216241836548 Time taken: 0.3180253505706787\n",
            "Batch Number: 1745 Loss: 1.836309790611267 Time taken: 0.3248109817504883\n",
            "Batch Number: 1746 Loss: 1.8441189527511597 Time taken: 0.3067348003387451\n",
            "Batch Number: 1747 Loss: 1.8351725339889526 Time taken: 0.3494076728820801\n",
            "Batch Number: 1748 Loss: 1.8133078813552856 Time taken: 0.3138084411621094\n",
            "Batch Number: 1749 Loss: 1.8433473110198975 Time taken: 0.30541324615478516\n",
            "Batch Number: 1750 Loss: 1.8126806020736694 Time taken: 0.310868501663208\n",
            "Batch Number: 1751 Loss: 1.83243727684021 Time taken: 0.3175632953643799\n",
            "Batch Number: 1752 Loss: 1.8225724697113037 Time taken: 0.3130686283111572\n",
            "Batch Number: 1753 Loss: 1.7880805730819702 Time taken: 0.3148043155670166\n",
            "Batch Number: 1754 Loss: 1.8329488039016724 Time taken: 0.30789875984191895\n",
            "Batch Number: 1755 Loss: 1.8261327743530273 Time taken: 0.32692980766296387\n",
            "Batch Number: 1756 Loss: 1.7839285135269165 Time taken: 0.31769728660583496\n",
            "Batch Number: 1757 Loss: 1.8219693899154663 Time taken: 0.30863070487976074\n",
            "Batch Number: 1758 Loss: 1.7953388690948486 Time taken: 0.315096378326416\n",
            "Batch Number: 1759 Loss: 1.818861722946167 Time taken: 0.31139302253723145\n",
            "Batch Number: 1760 Loss: 1.8304598331451416 Time taken: 0.32181501388549805\n",
            "Batch Number: 1761 Loss: 1.8185535669326782 Time taken: 0.3158378601074219\n",
            "Batch Number: 1762 Loss: 1.831472396850586 Time taken: 0.30025339126586914\n",
            "Batch Number: 1763 Loss: 1.8496761322021484 Time taken: 0.32225489616394043\n",
            "Batch Number: 1764 Loss: 1.8388360738754272 Time taken: 0.3156254291534424\n",
            "Batch Number: 1765 Loss: 1.8216819763183594 Time taken: 0.3006765842437744\n",
            "Batch Number: 1766 Loss: 1.819844365119934 Time taken: 0.31549668312072754\n",
            "Batch Number: 1767 Loss: 1.808889389038086 Time taken: 0.3162844181060791\n",
            "Batch Number: 1768 Loss: 1.8157017230987549 Time taken: 0.3049750328063965\n",
            "Batch Number: 1769 Loss: 1.8101438283920288 Time taken: 0.32041287422180176\n",
            "Batch Number: 1770 Loss: 1.8274588584899902 Time taken: 0.30935144424438477\n",
            "Batch Number: 1771 Loss: 1.7986160516738892 Time taken: 0.32174038887023926\n",
            "Batch Number: 1772 Loss: 1.810989499092102 Time taken: 0.32166242599487305\n",
            "Batch Number: 1773 Loss: 1.8192836046218872 Time taken: 0.3092682361602783\n",
            "Batch Number: 1774 Loss: 1.825601577758789 Time taken: 0.31961584091186523\n",
            "Batch Number: 1775 Loss: 1.801308512687683 Time taken: 0.31270265579223633\n",
            "Batch Number: 1776 Loss: 1.8242378234863281 Time taken: 0.3044905662536621\n",
            "Batch Number: 1777 Loss: 1.8291934728622437 Time taken: 0.3145029544830322\n",
            "Batch Number: 1778 Loss: 1.8175907135009766 Time taken: 0.30742383003234863\n",
            "Batch Number: 1779 Loss: 1.8124969005584717 Time taken: 0.31733226776123047\n",
            "Batch Number: 1780 Loss: 1.830842137336731 Time taken: 0.3267529010772705\n",
            "Batch Number: 1781 Loss: 1.805619478225708 Time taken: 0.306995153427124\n",
            "Batch Number: 1782 Loss: 1.8129608631134033 Time taken: 0.3214092254638672\n",
            "Batch Number: 1783 Loss: 1.8249735832214355 Time taken: 0.3120536804199219\n",
            "Batch Number: 1784 Loss: 1.8295830488204956 Time taken: 0.29956555366516113\n",
            "Batch Number: 1785 Loss: 1.812869906425476 Time taken: 0.31090855598449707\n",
            "Batch Number: 1786 Loss: 1.7971950769424438 Time taken: 0.31327199935913086\n",
            "Batch Number: 1787 Loss: 1.8007594347000122 Time taken: 0.3125932216644287\n",
            "Batch Number: 1788 Loss: 1.7837809324264526 Time taken: 0.31035637855529785\n",
            "Batch Number: 1789 Loss: 1.7925560474395752 Time taken: 0.303164005279541\n",
            "Batch Number: 1790 Loss: 1.7949014902114868 Time taken: 0.3286576271057129\n",
            "Batch Number: 1791 Loss: 1.8119189739227295 Time taken: 0.314434289932251\n",
            "Batch Number: 1792 Loss: 1.7885311841964722 Time taken: 0.2981076240539551\n",
            "Batch Number: 1793 Loss: 1.7879650592803955 Time taken: 0.3107132911682129\n",
            "Batch Number: 1794 Loss: 1.8050333261489868 Time taken: 0.30570244789123535\n",
            "Batch Number: 1795 Loss: 1.804194450378418 Time taken: 0.3083677291870117\n",
            "Batch Number: 1796 Loss: 1.782612681388855 Time taken: 0.3118760585784912\n",
            "Batch Number: 1797 Loss: 1.7972123622894287 Time taken: 0.302692174911499\n",
            "Batch Number: 1798 Loss: 1.7934060096740723 Time taken: 0.3196690082550049\n",
            "Batch Number: 1799 Loss: 1.7839784622192383 Time taken: 0.3251993656158447\n",
            "Batch Number: 1800 Loss: 1.8243350982666016 Time taken: 0.3114769458770752\n",
            "Batch Number: 1801 Loss: 1.7994951009750366 Time taken: 0.3160688877105713\n",
            "Batch Number: 1802 Loss: 1.7712054252624512 Time taken: 0.3038513660430908\n",
            "Batch Number: 1803 Loss: 1.7750558853149414 Time taken: 0.3161323070526123\n",
            "Batch Number: 1804 Loss: 1.770281434059143 Time taken: 0.3147282600402832\n",
            "Batch Number: 1805 Loss: 1.7780485153198242 Time taken: 0.304302453994751\n",
            "Batch Number: 1806 Loss: 1.772435188293457 Time taken: 0.3160116672515869\n",
            "Batch Number: 1807 Loss: 1.7559221982955933 Time taken: 0.31471705436706543\n",
            "Batch Number: 1808 Loss: 1.7933048009872437 Time taken: 0.3112952709197998\n",
            "Batch Number: 1809 Loss: 1.799899935722351 Time taken: 0.3222537040710449\n",
            "Batch Number: 1810 Loss: 1.7815499305725098 Time taken: 0.32413625717163086\n",
            "Batch Number: 1811 Loss: 1.7861627340316772 Time taken: 0.3032834529876709\n",
            "Batch Number: 1812 Loss: 1.7949100732803345 Time taken: 0.3069579601287842\n",
            "Batch Number: 1813 Loss: 1.7940889596939087 Time taken: 0.3035879135131836\n",
            "Batch Number: 1814 Loss: 1.7792340517044067 Time taken: 0.3163008689880371\n",
            "Batch Number: 1815 Loss: 1.805404782295227 Time taken: 0.3195369243621826\n",
            "Batch Number: 1816 Loss: 1.8012864589691162 Time taken: 0.3104104995727539\n",
            "Batch Number: 1817 Loss: 1.7939987182617188 Time taken: 0.31671833992004395\n",
            "Batch Number: 1818 Loss: 1.788030743598938 Time taken: 0.31237101554870605\n",
            "Batch Number: 1819 Loss: 1.7760236263275146 Time taken: 0.31871962547302246\n",
            "Batch Number: 1820 Loss: 1.7843875885009766 Time taken: 0.3315846920013428\n",
            "Batch Number: 1821 Loss: 1.78215491771698 Time taken: 0.3076028823852539\n",
            "Batch Number: 1822 Loss: 1.7747830152511597 Time taken: 0.31856393814086914\n",
            "Batch Number: 1823 Loss: 1.8032408952713013 Time taken: 0.30940937995910645\n",
            "Batch Number: 1824 Loss: 1.777357816696167 Time taken: 0.30182743072509766\n",
            "Batch Number: 1825 Loss: 1.7670016288757324 Time taken: 0.30895376205444336\n",
            "Batch Number: 1826 Loss: 1.7897398471832275 Time taken: 0.3213512897491455\n",
            "Batch Number: 1827 Loss: 1.7899197340011597 Time taken: 0.3128015995025635\n",
            "Batch Number: 1828 Loss: 1.7865080833435059 Time taken: 0.3237466812133789\n",
            "Batch Number: 1829 Loss: 1.7948269844055176 Time taken: 0.30809640884399414\n",
            "Batch Number: 1830 Loss: 1.7745864391326904 Time taken: 0.31781530380249023\n",
            "Batch Number: 1831 Loss: 1.7855300903320312 Time taken: 0.3163943290710449\n",
            "Batch Number: 1832 Loss: 1.7858096361160278 Time taken: 0.308819055557251\n",
            "Batch Number: 1833 Loss: 1.7860355377197266 Time taken: 0.32866454124450684\n",
            "Batch Number: 1834 Loss: 1.7864711284637451 Time taken: 0.30727434158325195\n",
            "Batch Number: 1835 Loss: 1.784477949142456 Time taken: 0.3044092655181885\n",
            "Batch Number: 1836 Loss: 1.7937253713607788 Time taken: 0.3187839984893799\n",
            "Batch Number: 1837 Loss: 1.7941534519195557 Time taken: 0.31745100021362305\n",
            "Batch Number: 1838 Loss: 1.8103481531143188 Time taken: 0.3235948085784912\n",
            "Batch Number: 1839 Loss: 1.8130475282669067 Time taken: 0.31798577308654785\n",
            "Batch Number: 1840 Loss: 1.8028525114059448 Time taken: 0.31023120880126953\n",
            "Batch Number: 1841 Loss: 1.7868773937225342 Time taken: 0.312305212020874\n",
            "Batch Number: 1842 Loss: 1.8241477012634277 Time taken: 0.3178560733795166\n",
            "Batch Number: 1843 Loss: 1.7867839336395264 Time taken: 0.3110158443450928\n",
            "Batch Number: 1844 Loss: 1.8167253732681274 Time taken: 0.30863451957702637\n",
            "Batch Number: 1845 Loss: 1.7721223831176758 Time taken: 0.32236337661743164\n",
            "Batch Number: 1846 Loss: 1.7889652252197266 Time taken: 0.3103981018066406\n",
            "Batch Number: 1847 Loss: 1.773185133934021 Time taken: 0.31406545639038086\n",
            "Batch Number: 1848 Loss: 1.7808918952941895 Time taken: 0.321544885635376\n",
            "Batch Number: 1849 Loss: 1.780288577079773 Time taken: 0.3208506107330322\n",
            "Batch Number: 1850 Loss: 1.7909830808639526 Time taken: 0.3212113380432129\n",
            "Batch Number: 1851 Loss: 1.7905018329620361 Time taken: 0.3032541275024414\n",
            "Batch Number: 1852 Loss: 1.8226580619812012 Time taken: 0.3108491897583008\n",
            "Batch Number: 1853 Loss: 1.778204083442688 Time taken: 0.30228590965270996\n",
            "Batch Number: 1854 Loss: 1.7933310270309448 Time taken: 0.328202486038208\n",
            "Batch Number: 1855 Loss: 1.7787631750106812 Time taken: 0.30821990966796875\n",
            "Batch Number: 1856 Loss: 1.7845262289047241 Time taken: 0.31264805793762207\n",
            "Batch Number: 1857 Loss: 1.7409660816192627 Time taken: 0.32160139083862305\n",
            "Batch Number: 1858 Loss: 1.8141249418258667 Time taken: 0.3150663375854492\n",
            "Batch Number: 1859 Loss: 1.7772362232208252 Time taken: 0.3045639991760254\n",
            "Batch Number: 1860 Loss: 1.7523607015609741 Time taken: 0.3211853504180908\n",
            "Batch Number: 1861 Loss: 1.7772557735443115 Time taken: 0.3162853717803955\n",
            "Batch Number: 1862 Loss: 1.7639967203140259 Time taken: 0.3045535087585449\n",
            "Batch Number: 1863 Loss: 1.7982604503631592 Time taken: 0.3224625587463379\n",
            "Batch Number: 1864 Loss: 1.775818943977356 Time taken: 0.32016444206237793\n",
            "Batch Number: 1865 Loss: 1.7953367233276367 Time taken: 0.31104016304016113\n",
            "Batch Number: 1866 Loss: 1.7660822868347168 Time taken: 0.31345057487487793\n",
            "Batch Number: 1867 Loss: 1.7800791263580322 Time taken: 0.30632638931274414\n",
            "Batch Number: 1868 Loss: 1.7836500406265259 Time taken: 0.31037187576293945\n",
            "Batch Number: 1869 Loss: 1.774491548538208 Time taken: 0.30873608589172363\n",
            "Batch Number: 1870 Loss: 1.775192141532898 Time taken: 0.3279430866241455\n",
            "Batch Number: 1871 Loss: 1.7810653448104858 Time taken: 0.3193192481994629\n",
            "Batch Number: 1872 Loss: 1.8056981563568115 Time taken: 0.30861830711364746\n",
            "Batch Number: 1873 Loss: 1.8054554462432861 Time taken: 0.32534027099609375\n",
            "Batch Number: 1874 Loss: 1.814524531364441 Time taken: 0.30616331100463867\n",
            "Batch Number: 1875 Loss: 1.7756845951080322 Time taken: 0.2984433174133301\n",
            "Batch Number: 1876 Loss: 1.7876415252685547 Time taken: 0.32190966606140137\n",
            "Batch Number: 1877 Loss: 1.8125531673431396 Time taken: 0.3220195770263672\n",
            "Batch Number: 1878 Loss: 1.783214807510376 Time taken: 0.31964898109436035\n",
            "Batch Number: 1879 Loss: 1.7845866680145264 Time taken: 0.3137331008911133\n",
            "Batch Number: 1880 Loss: 1.754548192024231 Time taken: 0.31766510009765625\n",
            "Batch Number: 1881 Loss: 1.803694248199463 Time taken: 0.2989366054534912\n",
            "Batch Number: 1882 Loss: 1.7607314586639404 Time taken: 0.31137871742248535\n",
            "Batch Number: 1883 Loss: 1.784044861793518 Time taken: 0.3210632801055908\n",
            "Batch Number: 1884 Loss: 1.7770034074783325 Time taken: 0.305804967880249\n",
            "Batch Number: 1885 Loss: 1.7782227993011475 Time taken: 0.31189417839050293\n",
            "Batch Number: 1886 Loss: 1.795485019683838 Time taken: 0.3196084499359131\n",
            "Batch Number: 1887 Loss: 1.7640659809112549 Time taken: 0.30829548835754395\n",
            "Batch Number: 1888 Loss: 1.7499676942825317 Time taken: 0.31749701499938965\n",
            "Batch Number: 1889 Loss: 1.754258394241333 Time taken: 0.302962064743042\n",
            "Batch Number: 1890 Loss: 1.7746816873550415 Time taken: 0.3005244731903076\n",
            "Batch Number: 1891 Loss: 1.7652347087860107 Time taken: 0.3049960136413574\n",
            "Batch Number: 1892 Loss: 1.7942633628845215 Time taken: 0.3141019344329834\n",
            "Batch Number: 1893 Loss: 1.7949206829071045 Time taken: 0.3142364025115967\n",
            "Batch Number: 1894 Loss: 1.7985211610794067 Time taken: 0.30338549613952637\n",
            "Batch Number: 1895 Loss: 1.7911019325256348 Time taken: 0.3054506778717041\n",
            "Batch Number: 1896 Loss: 1.8186179399490356 Time taken: 0.2992565631866455\n",
            "Batch Number: 1897 Loss: 1.794546365737915 Time taken: 0.29790568351745605\n",
            "Batch Number: 1898 Loss: 1.8185415267944336 Time taken: 0.30248570442199707\n",
            "Batch Number: 1899 Loss: 1.8291115760803223 Time taken: 0.30773448944091797\n",
            "Batch Number: 1900 Loss: 1.7738646268844604 Time taken: 0.3018608093261719\n",
            "Batch Number: 1901 Loss: 1.8120375871658325 Time taken: 0.30426764488220215\n",
            "Batch Number: 1902 Loss: 1.7975393533706665 Time taken: 0.318162202835083\n",
            "Batch Number: 1903 Loss: 1.7847933769226074 Time taken: 0.3212850093841553\n",
            "Batch Number: 1904 Loss: 1.7990808486938477 Time taken: 0.30573391914367676\n",
            "Batch Number: 1905 Loss: 1.824762225151062 Time taken: 0.31462740898132324\n",
            "Batch Number: 1906 Loss: 1.805654525756836 Time taken: 0.3208587169647217\n",
            "Batch Number: 1907 Loss: 1.8182491064071655 Time taken: 0.306624174118042\n",
            "Batch Number: 1908 Loss: 1.8045225143432617 Time taken: 0.3125181198120117\n",
            "Batch Number: 1909 Loss: 1.8443925380706787 Time taken: 0.31461238861083984\n",
            "Batch Number: 1910 Loss: 1.8162482976913452 Time taken: 0.3033015727996826\n",
            "Batch Number: 1911 Loss: 1.8045060634613037 Time taken: 0.31569814682006836\n",
            "Batch Number: 1912 Loss: 1.8291740417480469 Time taken: 0.3120427131652832\n",
            "Batch Number: 1913 Loss: 1.8478196859359741 Time taken: 0.30409860610961914\n",
            "Batch Number: 1914 Loss: 1.8122437000274658 Time taken: 0.30465054512023926\n",
            "Batch Number: 1915 Loss: 1.7946441173553467 Time taken: 0.31831812858581543\n",
            "Batch Number: 1916 Loss: 1.7863678932189941 Time taken: 0.30484509468078613\n",
            "Batch Number: 1917 Loss: 1.786723017692566 Time taken: 0.3063161373138428\n",
            "Batch Number: 1918 Loss: 1.8148739337921143 Time taken: 0.31730055809020996\n",
            "Batch Number: 1919 Loss: 1.791438102722168 Time taken: 0.31136083602905273\n",
            "Batch Number: 1920 Loss: 1.7823034524917603 Time taken: 0.3048124313354492\n",
            "Batch Number: 1921 Loss: 1.8037022352218628 Time taken: 0.3146626949310303\n",
            "Batch Number: 1922 Loss: 1.8133275508880615 Time taken: 0.3159017562866211\n",
            "Batch Number: 1923 Loss: 1.819391131401062 Time taken: 0.3221578598022461\n",
            "Batch Number: 1924 Loss: 1.7799327373504639 Time taken: 0.31844544410705566\n",
            "Batch Number: 1925 Loss: 1.7905151844024658 Time taken: 0.32018470764160156\n",
            "Batch Number: 1926 Loss: 1.8067785501480103 Time taken: 0.31229734420776367\n",
            "Batch Number: 1927 Loss: 1.797622799873352 Time taken: 0.3152294158935547\n",
            "Batch Number: 1928 Loss: 1.7925093173980713 Time taken: 0.3183939456939697\n",
            "Batch Number: 1929 Loss: 1.7749130725860596 Time taken: 0.30782651901245117\n",
            "Batch Number: 1930 Loss: 1.799879789352417 Time taken: 0.3061044216156006\n",
            "Batch Number: 1931 Loss: 1.7941497564315796 Time taken: 0.3224005699157715\n",
            "Batch Number: 1932 Loss: 1.790035367012024 Time taken: 0.30810093879699707\n",
            "Batch Number: 1933 Loss: 1.7619794607162476 Time taken: 0.3138155937194824\n",
            "Batch Number: 1934 Loss: 1.7727452516555786 Time taken: 0.32714200019836426\n",
            "Batch Number: 1935 Loss: 1.7715411186218262 Time taken: 0.311509370803833\n",
            "Batch Number: 1936 Loss: 1.7392727136611938 Time taken: 0.2957448959350586\n",
            "Batch Number: 1937 Loss: 1.7582734823226929 Time taken: 0.32808589935302734\n",
            "Batch Number: 1938 Loss: 1.7599557638168335 Time taken: 0.3096621036529541\n",
            "Batch Number: 1939 Loss: 1.7591739892959595 Time taken: 0.30773210525512695\n",
            "Batch Number: 1940 Loss: 1.7850720882415771 Time taken: 0.32108187675476074\n",
            "Batch Number: 1941 Loss: 1.766340970993042 Time taken: 0.3128688335418701\n",
            "Batch Number: 1942 Loss: 1.7978380918502808 Time taken: 0.3090322017669678\n",
            "Batch Number: 1943 Loss: 1.7837028503417969 Time taken: 0.32045531272888184\n",
            "Batch Number: 1944 Loss: 1.7724460363388062 Time taken: 0.32775139808654785\n",
            "Batch Number: 1945 Loss: 1.7661243677139282 Time taken: 0.3016810417175293\n",
            "Batch Number: 1946 Loss: 1.8030613660812378 Time taken: 0.3165011405944824\n",
            "Batch Number: 1947 Loss: 1.7526485919952393 Time taken: 0.316540002822876\n",
            "Batch Number: 1948 Loss: 1.7686975002288818 Time taken: 0.30009889602661133\n",
            "Batch Number: 1949 Loss: 1.774119257926941 Time taken: 0.30745363235473633\n",
            "Batch Number: 1950 Loss: 1.8082857131958008 Time taken: 0.3215196132659912\n",
            "Batch Number: 1951 Loss: 1.7799625396728516 Time taken: 0.30931997299194336\n",
            "Batch Number: 1952 Loss: 1.7835017442703247 Time taken: 0.3123197555541992\n",
            "Batch Number: 1953 Loss: 1.7805968523025513 Time taken: 0.32903528213500977\n",
            "Batch Number: 1954 Loss: 1.7954610586166382 Time taken: 0.30790138244628906\n",
            "Batch Number: 1955 Loss: 1.8058428764343262 Time taken: 0.3066980838775635\n",
            "Batch Number: 1956 Loss: 1.7838619947433472 Time taken: 0.32354140281677246\n",
            "Batch Number: 1957 Loss: 1.785720705986023 Time taken: 0.31362438201904297\n",
            "Batch Number: 1958 Loss: 1.8012460470199585 Time taken: 0.30568552017211914\n",
            "Batch Number: 1959 Loss: 1.7819092273712158 Time taken: 0.3158893585205078\n",
            "Batch Number: 1960 Loss: 1.759799838066101 Time taken: 0.31095457077026367\n",
            "Batch Number: 1961 Loss: 1.782699465751648 Time taken: 0.3072025775909424\n",
            "Batch Number: 1962 Loss: 1.7932370901107788 Time taken: 0.3181161880493164\n",
            "Batch Number: 1963 Loss: 1.801316261291504 Time taken: 0.32334446907043457\n",
            "Batch Number: 1964 Loss: 1.7761449813842773 Time taken: 0.3073282241821289\n",
            "Batch Number: 1965 Loss: 1.7829393148422241 Time taken: 0.3087778091430664\n",
            "Batch Number: 1966 Loss: 1.765206217765808 Time taken: 0.3257298469543457\n",
            "Batch Number: 1967 Loss: 1.7831240892410278 Time taken: 0.3083016872406006\n",
            "Batch Number: 1968 Loss: 1.7659709453582764 Time taken: 0.30361223220825195\n",
            "Batch Number: 1969 Loss: 1.772963285446167 Time taken: 0.32179856300354004\n",
            "Batch Number: 1970 Loss: 1.7638355493545532 Time taken: 0.31304192543029785\n",
            "Batch Number: 1971 Loss: 1.7606563568115234 Time taken: 0.3072359561920166\n",
            "Batch Number: 1972 Loss: 1.7684065103530884 Time taken: 0.33206844329833984\n",
            "Batch Number: 1973 Loss: 1.7732629776000977 Time taken: 0.3090639114379883\n",
            "Batch Number: 1974 Loss: 1.7763831615447998 Time taken: 0.30687403678894043\n",
            "Batch Number: 1975 Loss: 1.755906343460083 Time taken: 0.31129980087280273\n",
            "Batch Number: 1976 Loss: 1.7420939207077026 Time taken: 0.31925082206726074\n",
            "Batch Number: 1977 Loss: 1.7648627758026123 Time taken: 0.31009960174560547\n",
            "Batch Number: 1978 Loss: 1.7593706846237183 Time taken: 0.3060622215270996\n",
            "Batch Number: 1979 Loss: 1.7293386459350586 Time taken: 0.32968688011169434\n",
            "Batch Number: 1980 Loss: 1.7150355577468872 Time taken: 0.30788493156433105\n",
            "Batch Number: 1981 Loss: 1.7664086818695068 Time taken: 0.3122701644897461\n",
            "Batch Number: 1982 Loss: 1.7557681798934937 Time taken: 0.32442450523376465\n",
            "Batch Number: 1983 Loss: 1.7337288856506348 Time taken: 0.3098921775817871\n",
            "Batch Number: 1984 Loss: 1.7178072929382324 Time taken: 0.3093702793121338\n",
            "Batch Number: 1985 Loss: 1.7245821952819824 Time taken: 0.3286623954772949\n",
            "Batch Number: 1986 Loss: 1.761033058166504 Time taken: 0.30835914611816406\n",
            "Batch Number: 1987 Loss: 1.7512547969818115 Time taken: 0.3142244815826416\n",
            "Batch Number: 1988 Loss: 1.7273354530334473 Time taken: 0.3261730670928955\n",
            "Batch Number: 1989 Loss: 1.7379239797592163 Time taken: 0.30727243423461914\n",
            "Batch Number: 1990 Loss: 1.7352173328399658 Time taken: 0.31429028511047363\n",
            "Batch Number: 1991 Loss: 1.722115159034729 Time taken: 0.32141685485839844\n",
            "Batch Number: 1992 Loss: 1.7377668619155884 Time taken: 0.3086104393005371\n",
            "Batch Number: 1993 Loss: 1.7598586082458496 Time taken: 0.3050241470336914\n",
            "Batch Number: 1994 Loss: 1.7742723226547241 Time taken: 0.32987380027770996\n",
            "Batch Number: 1995 Loss: 1.776369333267212 Time taken: 0.3158121109008789\n",
            "Batch Number: 1996 Loss: 1.7403351068496704 Time taken: 0.3072221279144287\n",
            "Batch Number: 1997 Loss: 1.776041030883789 Time taken: 0.3106193542480469\n",
            "Batch Number: 1998 Loss: 1.7542147636413574 Time taken: 0.32334280014038086\n",
            "Batch Number: 1999 Loss: 1.7371960878372192 Time taken: 0.30922865867614746\n",
            "Batch Number: 2000 Loss: 1.7581396102905273 Time taken: 0.2981271743774414\n",
            "Batch Number: 2001 Loss: 1.7306828498840332 Time taken: 0.3233497142791748\n",
            "Batch Number: 2002 Loss: 1.7186064720153809 Time taken: 0.30910825729370117\n",
            "Batch Number: 2003 Loss: 1.756242275238037 Time taken: 0.31035327911376953\n",
            "Batch Number: 2004 Loss: 1.7633044719696045 Time taken: 0.3249971866607666\n",
            "Batch Number: 2005 Loss: 1.7868388891220093 Time taken: 0.30733156204223633\n",
            "Batch Number: 2006 Loss: 1.7450867891311646 Time taken: 0.31100893020629883\n",
            "Batch Number: 2007 Loss: 1.7440476417541504 Time taken: 0.3210427761077881\n",
            "Batch Number: 2008 Loss: 1.7311846017837524 Time taken: 0.3046741485595703\n",
            "Batch Number: 2009 Loss: 1.7822357416152954 Time taken: 0.31557297706604004\n",
            "Batch Number: 2010 Loss: 1.729020357131958 Time taken: 0.3178234100341797\n",
            "Batch Number: 2011 Loss: 1.7345778942108154 Time taken: 0.3060312271118164\n",
            "Batch Number: 2012 Loss: 1.7484383583068848 Time taken: 0.3067028522491455\n",
            "Batch Number: 2013 Loss: 1.739004373550415 Time taken: 0.3131401538848877\n",
            "Batch Number: 2014 Loss: 1.7452763319015503 Time taken: 0.3297433853149414\n",
            "Batch Number: 2015 Loss: 1.7300809621810913 Time taken: 0.3081181049346924\n",
            "Batch Number: 2016 Loss: 1.7740051746368408 Time taken: 0.31955599784851074\n",
            "Batch Number: 2017 Loss: 1.7581510543823242 Time taken: 0.3283226490020752\n",
            "Batch Number: 2018 Loss: 1.7753971815109253 Time taken: 0.3054800033569336\n",
            "Batch Number: 2019 Loss: 1.7750266790390015 Time taken: 0.30978822708129883\n",
            "Batch Number: 2020 Loss: 1.7570496797561646 Time taken: 0.3222463130950928\n",
            "Batch Number: 2021 Loss: 1.7750742435455322 Time taken: 0.30448007583618164\n",
            "Batch Number: 2022 Loss: 1.7498544454574585 Time taken: 0.3090543746948242\n",
            "Batch Number: 2023 Loss: 1.7572132349014282 Time taken: 0.32450318336486816\n",
            "Batch Number: 2024 Loss: 1.7461235523223877 Time taken: 0.3068969249725342\n",
            "Batch Number: 2025 Loss: 1.7367651462554932 Time taken: 0.3223724365234375\n",
            "Batch Number: 2026 Loss: 1.7491004467010498 Time taken: 0.32193875312805176\n",
            "Batch Number: 2027 Loss: 1.7597631216049194 Time taken: 0.2972722053527832\n",
            "Batch Number: 2028 Loss: 1.7603175640106201 Time taken: 0.30209779739379883\n",
            "Batch Number: 2029 Loss: 1.7351747751235962 Time taken: 0.31616902351379395\n",
            "Batch Number: 2030 Loss: 1.7514194250106812 Time taken: 0.312044620513916\n",
            "Batch Number: 2031 Loss: 1.750653624534607 Time taken: 0.3087913990020752\n",
            "Batch Number: 2032 Loss: 1.777545690536499 Time taken: 0.31011080741882324\n",
            "Batch Number: 2033 Loss: 1.750932216644287 Time taken: 0.3329958915710449\n",
            "Batch Number: 2034 Loss: 1.7719252109527588 Time taken: 0.309917688369751\n",
            "Batch Number: 2035 Loss: 1.739351511001587 Time taken: 0.3038904666900635\n",
            "Batch Number: 2036 Loss: 1.7608903646469116 Time taken: 0.33350491523742676\n",
            "Batch Number: 2037 Loss: 1.7494449615478516 Time taken: 0.3027479648590088\n",
            "Batch Number: 2038 Loss: 1.763565182685852 Time taken: 0.30919480323791504\n",
            "Batch Number: 2039 Loss: 1.750435709953308 Time taken: 0.3275938034057617\n",
            "Batch Number: 2040 Loss: 1.7174793481826782 Time taken: 0.30900049209594727\n",
            "Batch Number: 2041 Loss: 1.7364811897277832 Time taken: 0.3037393093109131\n",
            "Batch Number: 2042 Loss: 1.7287002801895142 Time taken: 0.32599759101867676\n",
            "Batch Number: 2043 Loss: 1.732555866241455 Time taken: 0.31400394439697266\n",
            "Batch Number: 2044 Loss: 1.7503769397735596 Time taken: 0.30849432945251465\n",
            "Batch Number: 2045 Loss: 1.7502859830856323 Time taken: 0.31461668014526367\n",
            "Batch Number: 2046 Loss: 1.7396776676177979 Time taken: 0.307192325592041\n",
            "Batch Number: 2047 Loss: 1.7362970113754272 Time taken: 0.3096795082092285\n",
            "Batch Number: 2048 Loss: 1.7502413988113403 Time taken: 0.3195650577545166\n",
            "Batch Number: 2049 Loss: 1.7574201822280884 Time taken: 0.31999850273132324\n",
            "Batch Number: 2050 Loss: 1.761484980583191 Time taken: 0.3110525608062744\n",
            "Batch Number: 2051 Loss: 1.7514317035675049 Time taken: 0.31653666496276855\n",
            "Batch Number: 2052 Loss: 1.7621484994888306 Time taken: 0.3287835121154785\n",
            "Batch Number: 2053 Loss: 1.7587730884552002 Time taken: 0.30391407012939453\n",
            "Batch Number: 2054 Loss: 1.7668728828430176 Time taken: 0.3049025535583496\n",
            "Batch Number: 2055 Loss: 1.7788424491882324 Time taken: 0.30950236320495605\n",
            "Batch Number: 2056 Loss: 1.774477243423462 Time taken: 0.2951314449310303\n",
            "Batch Number: 2057 Loss: 1.7888821363449097 Time taken: 0.29871368408203125\n",
            "Batch Number: 2058 Loss: 1.7819749116897583 Time taken: 0.324662446975708\n",
            "Batch Number: 2059 Loss: 1.7500301599502563 Time taken: 0.3057713508605957\n",
            "Batch Number: 2060 Loss: 1.7538135051727295 Time taken: 0.30662059783935547\n",
            "Batch Number: 2061 Loss: 1.750123143196106 Time taken: 0.3184850215911865\n",
            "Batch Number: 2062 Loss: 1.7575479745864868 Time taken: 0.31490516662597656\n",
            "Batch Number: 2063 Loss: 1.726286768913269 Time taken: 0.3065807819366455\n",
            "Batch Number: 2064 Loss: 1.7323334217071533 Time taken: 0.3218190670013428\n",
            "Batch Number: 2065 Loss: 1.7458971738815308 Time taken: 0.3205704689025879\n",
            "Batch Number: 2066 Loss: 1.7295740842819214 Time taken: 0.31278514862060547\n",
            "Batch Number: 2067 Loss: 1.7303311824798584 Time taken: 0.3215165138244629\n",
            "Batch Number: 2068 Loss: 1.7712807655334473 Time taken: 0.32367539405822754\n",
            "Batch Number: 2069 Loss: 1.7171577215194702 Time taken: 0.31647467613220215\n",
            "Batch Number: 2070 Loss: 1.740126371383667 Time taken: 0.3107743263244629\n",
            "Batch Number: 2071 Loss: 1.7309609651565552 Time taken: 0.3217775821685791\n",
            "Batch Number: 2072 Loss: 1.7367757558822632 Time taken: 0.30478668212890625\n",
            "Batch Number: 2073 Loss: 1.7840019464492798 Time taken: 0.3050074577331543\n",
            "Batch Number: 2074 Loss: 1.7601238489151 Time taken: 0.32425713539123535\n",
            "Batch Number: 2075 Loss: 1.7696655988693237 Time taken: 0.312910795211792\n",
            "Batch Number: 2076 Loss: 1.7709914445877075 Time taken: 0.30330491065979004\n",
            "Batch Number: 2077 Loss: 1.76957368850708 Time taken: 0.3251352310180664\n",
            "Batch Number: 2078 Loss: 1.7797901630401611 Time taken: 0.3074920177459717\n",
            "Batch Number: 2079 Loss: 1.7770317792892456 Time taken: 0.30878329277038574\n",
            "Batch Number: 2080 Loss: 1.758743405342102 Time taken: 0.3193776607513428\n",
            "Batch Number: 2081 Loss: 1.7623522281646729 Time taken: 0.3182063102722168\n",
            "Batch Number: 2082 Loss: 1.7740564346313477 Time taken: 0.3065989017486572\n",
            "Batch Number: 2083 Loss: 1.773117184638977 Time taken: 0.3125267028808594\n",
            "Batch Number: 2084 Loss: 1.7764530181884766 Time taken: 0.31839847564697266\n",
            "Batch Number: 2085 Loss: 1.755053997039795 Time taken: 0.3110203742980957\n",
            "Batch Number: 2086 Loss: 1.7761973142623901 Time taken: 0.3107645511627197\n",
            "Batch Number: 2087 Loss: 1.784688949584961 Time taken: 0.3119535446166992\n",
            "Batch Number: 2088 Loss: 1.779697299003601 Time taken: 0.31819868087768555\n",
            "Batch Number: 2089 Loss: 1.780409812927246 Time taken: 0.307370662689209\n",
            "Batch Number: 2090 Loss: 1.807841420173645 Time taken: 0.3201470375061035\n",
            "Batch Number: 2091 Loss: 1.7684916257858276 Time taken: 0.31070494651794434\n",
            "Batch Number: 2092 Loss: 1.8017593622207642 Time taken: 0.3118119239807129\n",
            "Batch Number: 2093 Loss: 1.7635318040847778 Time taken: 0.31903696060180664\n",
            "Batch Number: 2094 Loss: 1.7893446683883667 Time taken: 0.303753137588501\n",
            "Batch Number: 2095 Loss: 1.7895094156265259 Time taken: 0.30436086654663086\n",
            "Batch Number: 2096 Loss: 1.7411984205245972 Time taken: 0.31192898750305176\n",
            "Batch Number: 2097 Loss: 1.7763676643371582 Time taken: 0.31799840927124023\n",
            "Batch Number: 2098 Loss: 1.7426893711090088 Time taken: 0.31644177436828613\n",
            "Batch Number: 2099 Loss: 1.7491568326950073 Time taken: 0.30966711044311523\n",
            "Batch Number: 2100 Loss: 1.7720881700515747 Time taken: 0.33145713806152344\n",
            "Batch Number: 2101 Loss: 1.788382887840271 Time taken: 0.30602431297302246\n",
            "Batch Number: 2102 Loss: 1.7733908891677856 Time taken: 0.299685001373291\n",
            "Batch Number: 2103 Loss: 1.783203125 Time taken: 0.31536293029785156\n",
            "Batch Number: 2104 Loss: 1.7414900064468384 Time taken: 0.3011908531188965\n",
            "Batch Number: 2105 Loss: 1.7704789638519287 Time taken: 0.30295777320861816\n",
            "Batch Number: 2106 Loss: 1.7686740159988403 Time taken: 0.32761383056640625\n",
            "Batch Number: 2107 Loss: 1.7484793663024902 Time taken: 0.31231188774108887\n",
            "Batch Number: 2108 Loss: 1.7679792642593384 Time taken: 0.3076488971710205\n",
            "Batch Number: 2109 Loss: 1.7542043924331665 Time taken: 0.32846522331237793\n",
            "Batch Number: 2110 Loss: 1.726272702217102 Time taken: 0.3113279342651367\n",
            "Batch Number: 2111 Loss: 1.7813149690628052 Time taken: 0.30365419387817383\n",
            "Batch Number: 2112 Loss: 1.7694878578186035 Time taken: 0.3195364475250244\n",
            "Batch Number: 2113 Loss: 1.7605847120285034 Time taken: 0.3240630626678467\n",
            "Batch Number: 2114 Loss: 1.7447911500930786 Time taken: 0.31572961807250977\n",
            "Batch Number: 2115 Loss: 1.7423481941223145 Time taken: 0.3218064308166504\n",
            "Batch Number: 2116 Loss: 1.7047326564788818 Time taken: 0.33333802223205566\n",
            "Batch Number: 2117 Loss: 1.713212013244629 Time taken: 0.3033006191253662\n",
            "Batch Number: 2118 Loss: 1.7526733875274658 Time taken: 0.30787038803100586\n",
            "Batch Number: 2119 Loss: 1.7679487466812134 Time taken: 0.32775187492370605\n",
            "Batch Number: 2120 Loss: 1.748116135597229 Time taken: 0.3122293949127197\n",
            "Batch Number: 2121 Loss: 1.757644534111023 Time taken: 0.31426429748535156\n",
            "Batch Number: 2122 Loss: 1.7293529510498047 Time taken: 0.32164525985717773\n",
            "Batch Number: 2123 Loss: 1.7416960000991821 Time taken: 0.31383299827575684\n",
            "Batch Number: 2124 Loss: 1.74726402759552 Time taken: 0.3049776554107666\n",
            "Batch Number: 2125 Loss: 1.7697359323501587 Time taken: 0.31038522720336914\n",
            "Batch Number: 2126 Loss: 1.7648497819900513 Time taken: 0.3009665012359619\n",
            "Batch Number: 2127 Loss: 1.7540533542633057 Time taken: 0.3113217353820801\n",
            "Batch Number: 2128 Loss: 1.7312051057815552 Time taken: 0.35490918159484863\n",
            "Batch Number: 2129 Loss: 1.7532427310943604 Time taken: 0.3222172260284424\n",
            "Batch Number: 2130 Loss: 1.7354590892791748 Time taken: 0.3143167495727539\n",
            "Batch Number: 2131 Loss: 1.7375438213348389 Time taken: 0.32160401344299316\n",
            "Batch Number: 2132 Loss: 1.7530945539474487 Time taken: 0.33674073219299316\n",
            "Batch Number: 2133 Loss: 1.74715256690979 Time taken: 0.3253519535064697\n",
            "Batch Number: 2134 Loss: 1.74763822555542 Time taken: 0.3290531635284424\n",
            "Batch Number: 2135 Loss: 1.7600984573364258 Time taken: 0.32886385917663574\n",
            "Batch Number: 2136 Loss: 1.7352948188781738 Time taken: 0.32083582878112793\n",
            "Batch Number: 2137 Loss: 1.7551161050796509 Time taken: 0.32459044456481934\n",
            "Batch Number: 2138 Loss: 1.756481409072876 Time taken: 0.32474756240844727\n",
            "Batch Number: 2139 Loss: 1.7543596029281616 Time taken: 0.31632018089294434\n",
            "Batch Number: 2140 Loss: 1.7523448467254639 Time taken: 0.3390350341796875\n",
            "Batch Number: 2141 Loss: 1.7397218942642212 Time taken: 0.32233357429504395\n",
            "Batch Number: 2142 Loss: 1.7373512983322144 Time taken: 0.31340622901916504\n",
            "Batch Number: 2143 Loss: 1.7625775337219238 Time taken: 0.3219180107116699\n",
            "Batch Number: 2144 Loss: 1.7557955980300903 Time taken: 0.32221078872680664\n",
            "Batch Number: 2145 Loss: 1.7337769269943237 Time taken: 0.3062880039215088\n",
            "Batch Number: 2146 Loss: 1.7581440210342407 Time taken: 0.32129693031311035\n",
            "Batch Number: 2147 Loss: 1.7314965724945068 Time taken: 0.322559118270874\n",
            "Batch Number: 2148 Loss: 1.7421882152557373 Time taken: 0.3055741786956787\n",
            "Batch Number: 2149 Loss: 1.7241792678833008 Time taken: 0.31801629066467285\n",
            "Batch Number: 2150 Loss: 1.7441048622131348 Time taken: 0.3290283679962158\n",
            "Batch Number: 2151 Loss: 1.7231872081756592 Time taken: 0.31151747703552246\n",
            "Batch Number: 2152 Loss: 1.7086855173110962 Time taken: 0.31258177757263184\n",
            "Batch Number: 2153 Loss: 1.7319130897521973 Time taken: 0.32105398178100586\n",
            "Batch Number: 2154 Loss: 1.729021430015564 Time taken: 0.3087191581726074\n",
            "Batch Number: 2155 Loss: 1.7303762435913086 Time taken: 0.3063528537750244\n",
            "Batch Number: 2156 Loss: 1.7314376831054688 Time taken: 0.31930017471313477\n",
            "Batch Number: 2157 Loss: 1.7276626825332642 Time taken: 0.32381343841552734\n",
            "Batch Number: 2158 Loss: 1.7077733278274536 Time taken: 0.3127729892730713\n",
            "Batch Number: 2159 Loss: 1.7378334999084473 Time taken: 0.31983065605163574\n",
            "Batch Number: 2160 Loss: 1.7276781797409058 Time taken: 0.3183434009552002\n",
            "Batch Number: 2161 Loss: 1.7293190956115723 Time taken: 0.31128740310668945\n",
            "Batch Number: 2162 Loss: 1.7101224660873413 Time taken: 0.31923627853393555\n",
            "Batch Number: 2163 Loss: 1.7126350402832031 Time taken: 0.317990779876709\n",
            "Batch Number: 2164 Loss: 1.6963192224502563 Time taken: 0.3098480701446533\n",
            "Batch Number: 2165 Loss: 1.6876150369644165 Time taken: 0.31583261489868164\n",
            "Batch Number: 2166 Loss: 1.6967185735702515 Time taken: 0.3161461353302002\n",
            "Batch Number: 2167 Loss: 1.7227047681808472 Time taken: 0.3074023723602295\n",
            "Batch Number: 2168 Loss: 1.7092026472091675 Time taken: 0.30820465087890625\n",
            "Batch Number: 2169 Loss: 1.7261909246444702 Time taken: 0.3119645118713379\n",
            "Batch Number: 2170 Loss: 1.7206592559814453 Time taken: 0.3028740882873535\n",
            "Batch Number: 2171 Loss: 1.686785101890564 Time taken: 0.30937838554382324\n",
            "Batch Number: 2172 Loss: 1.6998378038406372 Time taken: 0.3058342933654785\n",
            "Batch Number: 2173 Loss: 1.7338402271270752 Time taken: 0.3082089424133301\n",
            "Batch Number: 2174 Loss: 1.719810962677002 Time taken: 0.31064605712890625\n",
            "Batch Number: 2175 Loss: 1.762805700302124 Time taken: 0.31697630882263184\n",
            "Batch Number: 2176 Loss: 1.6908385753631592 Time taken: 0.3181183338165283\n",
            "Batch Number: 2177 Loss: 1.717634916305542 Time taken: 0.31914687156677246\n",
            "Batch Number: 2178 Loss: 1.7023049592971802 Time taken: 0.3099336624145508\n",
            "Batch Number: 2179 Loss: 1.7305761575698853 Time taken: 0.3219308853149414\n",
            "Batch Number: 2180 Loss: 1.7051327228546143 Time taken: 0.3119821548461914\n",
            "Batch Number: 2181 Loss: 1.707777500152588 Time taken: 0.3224482536315918\n",
            "Batch Number: 2182 Loss: 1.7114084959030151 Time taken: 0.3122987747192383\n",
            "Batch Number: 2183 Loss: 1.7354429960250854 Time taken: 0.3073694705963135\n",
            "Batch Number: 2184 Loss: 1.7203631401062012 Time taken: 0.31334996223449707\n",
            "Batch Number: 2185 Loss: 1.718354344367981 Time taken: 0.3125617504119873\n",
            "Batch Number: 2186 Loss: 1.7160093784332275 Time taken: 0.30761241912841797\n",
            "Batch Number: 2187 Loss: 1.7273272275924683 Time taken: 0.31356000900268555\n",
            "Batch Number: 2188 Loss: 1.7190922498703003 Time taken: 0.30590367317199707\n",
            "Batch Number: 2189 Loss: 1.7121460437774658 Time taken: 0.3162531852722168\n",
            "Batch Number: 2190 Loss: 1.7130378484725952 Time taken: 0.31013059616088867\n",
            "Batch Number: 2191 Loss: 1.7236782312393188 Time taken: 0.3032538890838623\n",
            "Batch Number: 2192 Loss: 1.7200536727905273 Time taken: 0.31856560707092285\n",
            "Batch Number: 2193 Loss: 1.7214152812957764 Time taken: 0.30668044090270996\n",
            "Batch Number: 2194 Loss: 1.7312017679214478 Time taken: 0.30617666244506836\n",
            "Batch Number: 2195 Loss: 1.694525957107544 Time taken: 0.31714677810668945\n",
            "Batch Number: 2196 Loss: 1.7364848852157593 Time taken: 0.3048393726348877\n",
            "Batch Number: 2197 Loss: 1.7116721868515015 Time taken: 0.3171851634979248\n",
            "Batch Number: 2198 Loss: 1.7272366285324097 Time taken: 0.3194141387939453\n",
            "Batch Number: 2199 Loss: 1.7358096837997437 Time taken: 0.3003072738647461\n",
            "Batch Number: 2200 Loss: 1.719467282295227 Time taken: 0.31224918365478516\n",
            "Batch Number: 2201 Loss: 1.7556939125061035 Time taken: 0.30133724212646484\n",
            "Batch Number: 2202 Loss: 1.750131607055664 Time taken: 0.3115274906158447\n",
            "Batch Number: 2203 Loss: 1.7083064317703247 Time taken: 0.3069908618927002\n",
            "Batch Number: 2204 Loss: 1.7120646238327026 Time taken: 0.29715704917907715\n",
            "Batch Number: 2205 Loss: 1.691673755645752 Time taken: 0.30577754974365234\n",
            "Batch Number: 2206 Loss: 1.7129148244857788 Time taken: 0.3069486618041992\n",
            "Batch Number: 2207 Loss: 1.7446277141571045 Time taken: 0.31632351875305176\n",
            "Batch Number: 2208 Loss: 1.703311800956726 Time taken: 0.3206617832183838\n",
            "Batch Number: 2209 Loss: 1.7124587297439575 Time taken: 0.30710577964782715\n",
            "Batch Number: 2210 Loss: 1.7131763696670532 Time taken: 0.31671810150146484\n",
            "Batch Number: 2211 Loss: 1.7173959016799927 Time taken: 0.3071448802947998\n",
            "Batch Number: 2212 Loss: 1.732381820678711 Time taken: 0.30988168716430664\n",
            "Batch Number: 2213 Loss: 1.7368861436843872 Time taken: 0.3353762626647949\n",
            "Batch Number: 2214 Loss: 1.738669991493225 Time taken: 0.3093886375427246\n",
            "Batch Number: 2215 Loss: 1.7232375144958496 Time taken: 0.30399322509765625\n",
            "Batch Number: 2216 Loss: 1.7185801267623901 Time taken: 0.3163447380065918\n",
            "Batch Number: 2217 Loss: 1.7060338258743286 Time taken: 0.3046729564666748\n",
            "Batch Number: 2218 Loss: 1.7046048641204834 Time taken: 0.31934595108032227\n",
            "Batch Number: 2219 Loss: 1.7260311841964722 Time taken: 0.31812191009521484\n",
            "Batch Number: 2220 Loss: 1.7103946208953857 Time taken: 0.30829358100891113\n",
            "Batch Number: 2221 Loss: 1.7144447565078735 Time taken: 0.320828914642334\n",
            "Batch Number: 2222 Loss: 1.6997058391571045 Time taken: 0.3139610290527344\n",
            "Batch Number: 2223 Loss: 1.706721305847168 Time taken: 0.30629777908325195\n",
            "Batch Number: 2224 Loss: 1.7171070575714111 Time taken: 0.321901798248291\n",
            "Batch Number: 2225 Loss: 1.7326873540878296 Time taken: 0.30872297286987305\n",
            "Batch Number: 2226 Loss: 1.710918664932251 Time taken: 0.32132744789123535\n",
            "Batch Number: 2227 Loss: 1.7152258157730103 Time taken: 0.3148770332336426\n",
            "Batch Number: 2228 Loss: 1.7235146760940552 Time taken: 0.3100087642669678\n",
            "Batch Number: 2229 Loss: 1.72209894657135 Time taken: 0.32303714752197266\n",
            "Batch Number: 2230 Loss: 1.7112135887145996 Time taken: 0.31459498405456543\n",
            "Batch Number: 2231 Loss: 1.7029205560684204 Time taken: 0.3083622455596924\n",
            "Batch Number: 2232 Loss: 1.7263795137405396 Time taken: 0.3165433406829834\n",
            "Batch Number: 2233 Loss: 1.7523248195648193 Time taken: 0.31313061714172363\n",
            "Batch Number: 2234 Loss: 1.7532689571380615 Time taken: 0.30527210235595703\n",
            "Batch Number: 2235 Loss: 1.7279963493347168 Time taken: 0.3193552494049072\n",
            "Batch Number: 2236 Loss: 1.7391618490219116 Time taken: 0.31540846824645996\n",
            "Batch Number: 2237 Loss: 1.7332117557525635 Time taken: 0.3116147518157959\n",
            "Batch Number: 2238 Loss: 1.7333415746688843 Time taken: 0.31253767013549805\n",
            "Batch Number: 2239 Loss: 1.7103251218795776 Time taken: 0.31886768341064453\n",
            "Batch Number: 2240 Loss: 1.7640520334243774 Time taken: 0.31662678718566895\n",
            "Batch Number: 2241 Loss: 1.7906368970870972 Time taken: 0.30727171897888184\n",
            "Batch Number: 2242 Loss: 1.730963110923767 Time taken: 0.3075566291809082\n",
            "Batch Number: 2243 Loss: 1.7216609716415405 Time taken: 0.3145434856414795\n",
            "Batch Number: 2244 Loss: 1.7058428525924683 Time taken: 0.3088219165802002\n",
            "Batch Number: 2245 Loss: 1.7231664657592773 Time taken: 0.31451869010925293\n",
            "Batch Number: 2246 Loss: 1.698960781097412 Time taken: 0.31647562980651855\n",
            "Batch Number: 2247 Loss: 1.7133415937423706 Time taken: 0.3127765655517578\n",
            "Batch Number: 2248 Loss: 1.712531566619873 Time taken: 0.3192932605743408\n",
            "Batch Number: 2249 Loss: 1.723105549812317 Time taken: 0.31023526191711426\n",
            "Batch Number: 2250 Loss: 1.6949225664138794 Time taken: 0.3065214157104492\n",
            "Batch Number: 2251 Loss: 1.749254584312439 Time taken: 0.3171505928039551\n",
            "Batch Number: 2252 Loss: 1.7508859634399414 Time taken: 0.3073539733886719\n",
            "Batch Number: 2253 Loss: 1.7459056377410889 Time taken: 0.321392297744751\n",
            "Batch Number: 2254 Loss: 1.7529343366622925 Time taken: 0.3118915557861328\n",
            "Batch Number: 2255 Loss: 1.7146706581115723 Time taken: 0.3027768135070801\n",
            "Batch Number: 2256 Loss: 1.7248165607452393 Time taken: 0.31261730194091797\n",
            "Batch Number: 2257 Loss: 1.7387385368347168 Time taken: 0.31378674507141113\n",
            "Batch Number: 2258 Loss: 1.7403392791748047 Time taken: 0.32187485694885254\n",
            "Batch Number: 2259 Loss: 1.7180471420288086 Time taken: 0.32935380935668945\n",
            "Batch Number: 2260 Loss: 1.73740816116333 Time taken: 0.3168513774871826\n",
            "Batch Number: 2261 Loss: 1.7213103771209717 Time taken: 0.31101250648498535\n",
            "Batch Number: 2262 Loss: 1.7525323629379272 Time taken: 0.3195319175720215\n",
            "Batch Number: 2263 Loss: 1.7497220039367676 Time taken: 0.311509370803833\n",
            "Batch Number: 2264 Loss: 1.7364857196807861 Time taken: 0.3207106590270996\n",
            "Batch Number: 2265 Loss: 1.7701677083969116 Time taken: 0.3239281177520752\n",
            "Batch Number: 2266 Loss: 1.7741271257400513 Time taken: 0.31281375885009766\n",
            "Batch Number: 2267 Loss: 1.7832175493240356 Time taken: 0.31507182121276855\n",
            "Batch Number: 2268 Loss: 1.752297043800354 Time taken: 0.3097414970397949\n",
            "Batch Number: 2269 Loss: 1.7780821323394775 Time taken: 0.3183629512786865\n",
            "Batch Number: 2270 Loss: 1.7813656330108643 Time taken: 0.30809617042541504\n",
            "Batch Number: 2271 Loss: 1.7593574523925781 Time taken: 0.3082611560821533\n",
            "Batch Number: 2272 Loss: 1.7742106914520264 Time taken: 0.32463622093200684\n",
            "Batch Number: 2273 Loss: 1.7722338438034058 Time taken: 0.31729769706726074\n",
            "Batch Number: 2274 Loss: 1.7511361837387085 Time taken: 0.3095896244049072\n",
            "Batch Number: 2275 Loss: 1.7319992780685425 Time taken: 0.3209807872772217\n",
            "Batch Number: 2276 Loss: 1.735910415649414 Time taken: 0.31572651863098145\n",
            "Batch Number: 2277 Loss: 1.7119837999343872 Time taken: 0.3012979030609131\n",
            "Batch Number: 2278 Loss: 1.7313218116760254 Time taken: 0.3168306350708008\n",
            "Batch Number: 2279 Loss: 1.7325598001480103 Time taken: 0.3107109069824219\n",
            "Batch Number: 2280 Loss: 1.7250381708145142 Time taken: 0.3190727233886719\n",
            "Batch Number: 2281 Loss: 1.7434415817260742 Time taken: 0.3230929374694824\n",
            "Batch Number: 2282 Loss: 1.7354356050491333 Time taken: 0.31334352493286133\n",
            "Batch Number: 2283 Loss: 1.7243834733963013 Time taken: 0.30037450790405273\n",
            "Batch Number: 2284 Loss: 1.720526933670044 Time taken: 0.3026602268218994\n",
            "Batch Number: 2285 Loss: 1.733932375907898 Time taken: 0.3255884647369385\n",
            "Batch Number: 2286 Loss: 1.7345027923583984 Time taken: 0.3168447017669678\n",
            "Batch Number: 2287 Loss: 1.706189751625061 Time taken: 0.3099043369293213\n",
            "Batch Number: 2288 Loss: 1.7156341075897217 Time taken: 0.31547975540161133\n",
            "Batch Number: 2289 Loss: 1.7341581583023071 Time taken: 0.31215524673461914\n",
            "Batch Number: 2290 Loss: 1.7224242687225342 Time taken: 0.30577802658081055\n",
            "Batch Number: 2291 Loss: 1.726966381072998 Time taken: 0.31912755966186523\n",
            "Batch Number: 2292 Loss: 1.73981511592865 Time taken: 0.3130459785461426\n",
            "Batch Number: 2293 Loss: 1.7207289934158325 Time taken: 0.3224451541900635\n",
            "Batch Number: 2294 Loss: 1.686267375946045 Time taken: 0.31615328788757324\n",
            "Batch Number: 2295 Loss: 1.7230224609375 Time taken: 0.31008481979370117\n",
            "Batch Number: 2296 Loss: 1.7374167442321777 Time taken: 0.31407594680786133\n",
            "Batch Number: 2297 Loss: 1.682790994644165 Time taken: 0.31827855110168457\n",
            "Batch Number: 2298 Loss: 1.699865460395813 Time taken: 0.3195762634277344\n",
            "Batch Number: 2299 Loss: 1.702353835105896 Time taken: 0.3051636219024658\n",
            "Batch Number: 2300 Loss: 1.7019622325897217 Time taken: 0.3092198371887207\n",
            "Batch Number: 2301 Loss: 1.7348248958587646 Time taken: 0.31360387802124023\n",
            "Batch Number: 2302 Loss: 1.7336983680725098 Time taken: 0.3067035675048828\n",
            "Batch Number: 2303 Loss: 1.7286639213562012 Time taken: 0.3167867660522461\n",
            "Batch Number: 2304 Loss: 1.7159229516983032 Time taken: 0.3168008327484131\n",
            "Batch Number: 2305 Loss: 1.7191493511199951 Time taken: 0.3169882297515869\n",
            "Batch Number: 2306 Loss: 1.770021677017212 Time taken: 0.3054962158203125\n",
            "Batch Number: 2307 Loss: 1.683916449546814 Time taken: 0.32910966873168945\n",
            "Batch Number: 2308 Loss: 1.6959662437438965 Time taken: 0.31276988983154297\n",
            "Batch Number: 2309 Loss: 1.7143100500106812 Time taken: 0.306502103805542\n",
            "Batch Number: 2310 Loss: 1.7134568691253662 Time taken: 0.3278477191925049\n",
            "Batch Number: 2311 Loss: 1.7087693214416504 Time taken: 0.3162720203399658\n",
            "Batch Number: 2312 Loss: 1.7335827350616455 Time taken: 0.3034656047821045\n",
            "Batch Number: 2313 Loss: 1.7051339149475098 Time taken: 0.3214547634124756\n",
            "Batch Number: 2314 Loss: 1.7084918022155762 Time taken: 0.3214232921600342\n",
            "Batch Number: 2315 Loss: 1.731742024421692 Time taken: 0.3061046600341797\n",
            "Batch Number: 2316 Loss: 1.7193634510040283 Time taken: 0.31832385063171387\n",
            "Batch Number: 2317 Loss: 1.733313798904419 Time taken: 0.3152477741241455\n",
            "Batch Number: 2318 Loss: 1.7256340980529785 Time taken: 0.332061767578125\n",
            "Batch Number: 2319 Loss: 1.7057113647460938 Time taken: 0.3161170482635498\n",
            "Batch Number: 2320 Loss: 1.7210252285003662 Time taken: 0.31090497970581055\n",
            "Batch Number: 2321 Loss: 1.7038720846176147 Time taken: 0.30987000465393066\n",
            "Batch Number: 2322 Loss: 1.7124683856964111 Time taken: 0.3058793544769287\n",
            "Batch Number: 2323 Loss: 1.7243857383728027 Time taken: 0.3094482421875\n",
            "Batch Number: 2324 Loss: 1.7183641195297241 Time taken: 0.318514347076416\n",
            "Batch Number: 2325 Loss: 1.7389013767242432 Time taken: 0.3033289909362793\n",
            "Batch Number: 2326 Loss: 1.6974753141403198 Time taken: 0.314957857131958\n",
            "Batch Number: 2327 Loss: 1.7151402235031128 Time taken: 0.3254575729370117\n",
            "Batch Number: 2328 Loss: 1.6803321838378906 Time taken: 0.30589914321899414\n",
            "Batch Number: 2329 Loss: 1.7057485580444336 Time taken: 0.30766749382019043\n",
            "Batch Number: 2330 Loss: 1.7048180103302002 Time taken: 0.3117353916168213\n",
            "Batch Number: 2331 Loss: 1.7052451372146606 Time taken: 0.29486560821533203\n",
            "Batch Number: 2332 Loss: 1.7241525650024414 Time taken: 0.31847453117370605\n",
            "Batch Number: 2333 Loss: 1.6819802522659302 Time taken: 0.3173792362213135\n",
            "Batch Number: 2334 Loss: 1.6921316385269165 Time taken: 0.30786919593811035\n",
            "Batch Number: 2335 Loss: 1.6839181184768677 Time taken: 0.30750155448913574\n",
            "Batch Number: 2336 Loss: 1.7049980163574219 Time taken: 0.32500243186950684\n",
            "Batch Number: 2337 Loss: 1.6936349868774414 Time taken: 0.31400251388549805\n",
            "Batch Number: 2338 Loss: 1.685227632522583 Time taken: 0.3062760829925537\n",
            "Batch Number: 2339 Loss: 1.6922633647918701 Time taken: 0.3124697208404541\n",
            "Batch Number: 2340 Loss: 1.7021735906600952 Time taken: 0.31595897674560547\n",
            "Batch Number: 2341 Loss: 1.6907280683517456 Time taken: 0.3029952049255371\n",
            "Batch Number: 2342 Loss: 1.70098876953125 Time taken: 0.31671762466430664\n",
            "Batch Number: 2343 Loss: 1.6708545684814453 Time taken: 0.3148367404937744\n",
            "Batch Number: 2344 Loss: 1.662981390953064 Time taken: 0.3117811679840088\n",
            "Batch Number: 2345 Loss: 1.6506255865097046 Time taken: 0.31692934036254883\n",
            "Batch Number: 2346 Loss: 1.6811765432357788 Time taken: 0.32469654083251953\n",
            "Batch Number: 2347 Loss: 1.676875114440918 Time taken: 0.31000685691833496\n",
            "Batch Number: 2348 Loss: 1.664903998374939 Time taken: 0.3144962787628174\n",
            "Batch Number: 2349 Loss: 1.6724603176116943 Time taken: 0.3153829574584961\n",
            "Batch Number: 2350 Loss: 1.6816396713256836 Time taken: 0.3078954219818115\n",
            "Batch Number: 2351 Loss: 1.6591448783874512 Time taken: 0.31159210205078125\n",
            "Batch Number: 2352 Loss: 1.702437400817871 Time taken: 0.31207275390625\n",
            "Batch Number: 2353 Loss: 1.7143352031707764 Time taken: 0.3078932762145996\n",
            "Batch Number: 2354 Loss: 1.7171200513839722 Time taken: 0.3066403865814209\n",
            "Batch Number: 2355 Loss: 1.7077751159667969 Time taken: 0.3232276439666748\n",
            "Batch Number: 2356 Loss: 1.7040802240371704 Time taken: 0.319354772567749\n",
            "Batch Number: 2357 Loss: 1.6806669235229492 Time taken: 0.3108248710632324\n",
            "Batch Number: 2358 Loss: 1.6874960660934448 Time taken: 0.31208086013793945\n",
            "Batch Number: 2359 Loss: 1.6791954040527344 Time taken: 0.31787848472595215\n",
            "Batch Number: 2360 Loss: 1.683021903038025 Time taken: 0.305767297744751\n",
            "Batch Number: 2361 Loss: 1.6902925968170166 Time taken: 0.32213807106018066\n",
            "Batch Number: 2362 Loss: 1.6594147682189941 Time taken: 0.3109152317047119\n",
            "Batch Number: 2363 Loss: 1.6920006275177002 Time taken: 0.3039984703063965\n",
            "Batch Number: 2364 Loss: 1.699169397354126 Time taken: 0.3149068355560303\n",
            "Batch Number: 2365 Loss: 1.7014716863632202 Time taken: 0.31630492210388184\n",
            "Batch Number: 2366 Loss: 1.7140421867370605 Time taken: 0.3046143054962158\n",
            "Batch Number: 2367 Loss: 1.6898127794265747 Time taken: 0.32651734352111816\n",
            "Batch Number: 2368 Loss: 1.68584144115448 Time taken: 0.3195514678955078\n",
            "Batch Number: 2369 Loss: 1.6869076490402222 Time taken: 0.31026315689086914\n",
            "Batch Number: 2370 Loss: 1.6746097803115845 Time taken: 0.3047008514404297\n",
            "Batch Number: 2371 Loss: 1.6649550199508667 Time taken: 0.32538676261901855\n",
            "Batch Number: 2372 Loss: 1.6666271686553955 Time taken: 0.31096506118774414\n",
            "Batch Number: 2373 Loss: 1.683834195137024 Time taken: 0.3115391731262207\n",
            "Batch Number: 2374 Loss: 1.680708885192871 Time taken: 0.3256042003631592\n",
            "Batch Number: 2375 Loss: 1.6942335367202759 Time taken: 0.30783963203430176\n",
            "Batch Number: 2376 Loss: 1.6774157285690308 Time taken: 0.30460238456726074\n",
            "Batch Number: 2377 Loss: 1.682982325553894 Time taken: 0.3176555633544922\n",
            "Batch Number: 2378 Loss: 1.7306052446365356 Time taken: 0.314403772354126\n",
            "Batch Number: 2379 Loss: 1.6704826354980469 Time taken: 0.3125131130218506\n",
            "Batch Number: 2380 Loss: 1.7149046659469604 Time taken: 0.3116016387939453\n",
            "Batch Number: 2381 Loss: 1.7018942832946777 Time taken: 0.31351423263549805\n",
            "Batch Number: 2382 Loss: 1.6893117427825928 Time taken: 0.3121042251586914\n",
            "Batch Number: 2383 Loss: 1.6712905168533325 Time taken: 0.32223939895629883\n",
            "Batch Number: 2384 Loss: 1.6871492862701416 Time taken: 0.312122106552124\n",
            "Batch Number: 2385 Loss: 1.6977757215499878 Time taken: 0.3072638511657715\n",
            "Batch Number: 2386 Loss: 1.7035539150238037 Time taken: 0.304978609085083\n",
            "Batch Number: 2387 Loss: 1.684837818145752 Time taken: 0.3309288024902344\n",
            "Batch Number: 2388 Loss: 1.6942857503890991 Time taken: 0.3083839416503906\n",
            "Batch Number: 2389 Loss: 1.694071650505066 Time taken: 0.30609631538391113\n",
            "Batch Number: 2390 Loss: 1.6941795349121094 Time taken: 0.3218088150024414\n",
            "Batch Number: 2391 Loss: 1.690903902053833 Time taken: 0.306229829788208\n",
            "Batch Number: 2392 Loss: 1.7115569114685059 Time taken: 0.30706310272216797\n",
            "Batch Number: 2393 Loss: 1.7097132205963135 Time taken: 0.32530665397644043\n",
            "Batch Number: 2394 Loss: 1.7040390968322754 Time taken: 0.30479931831359863\n",
            "Batch Number: 2395 Loss: 1.7053468227386475 Time taken: 0.2964200973510742\n",
            "Batch Number: 2396 Loss: 1.6709623336791992 Time taken: 0.3011891841888428\n",
            "Batch Number: 2397 Loss: 1.686674952507019 Time taken: 0.3121771812438965\n",
            "Batch Number: 2398 Loss: 1.7095283269882202 Time taken: 0.30522847175598145\n",
            "Batch Number: 2399 Loss: 1.6978399753570557 Time taken: 0.3046903610229492\n",
            "Batch Number: 2400 Loss: 1.7087152004241943 Time taken: 0.3173859119415283\n",
            "Batch Number: 2401 Loss: 1.6710065603256226 Time taken: 0.3307490348815918\n",
            "Batch Number: 2402 Loss: 1.6520711183547974 Time taken: 0.30769920349121094\n",
            "Batch Number: 2403 Loss: 1.6772466897964478 Time taken: 0.32332897186279297\n",
            "Batch Number: 2404 Loss: 1.6822433471679688 Time taken: 0.3104584217071533\n",
            "Batch Number: 2405 Loss: 1.6971322298049927 Time taken: 0.30745983123779297\n",
            "Batch Number: 2406 Loss: 1.6837395429611206 Time taken: 0.3147695064544678\n",
            "Batch Number: 2407 Loss: 1.658137321472168 Time taken: 0.30898404121398926\n",
            "Batch Number: 2408 Loss: 1.6822891235351562 Time taken: 0.305316686630249\n",
            "Batch Number: 2409 Loss: 1.7007066011428833 Time taken: 0.3156750202178955\n",
            "Batch Number: 2410 Loss: 1.6874864101409912 Time taken: 0.3099536895751953\n",
            "Batch Number: 2411 Loss: 1.694740653038025 Time taken: 0.30459094047546387\n",
            "Batch Number: 2412 Loss: 1.7038272619247437 Time taken: 0.3117978572845459\n",
            "Batch Number: 2413 Loss: 1.698137879371643 Time taken: 0.30998921394348145\n",
            "Batch Number: 2414 Loss: 1.719994306564331 Time taken: 0.3028604984283447\n",
            "Batch Number: 2415 Loss: 1.7120041847229004 Time taken: 0.30612635612487793\n",
            "Batch Number: 2416 Loss: 1.701617956161499 Time taken: 0.32834935188293457\n",
            "Batch Number: 2417 Loss: 1.714794397354126 Time taken: 0.3071112632751465\n",
            "Batch Number: 2418 Loss: 1.7146729230880737 Time taken: 0.3047933578491211\n",
            "Batch Number: 2419 Loss: 1.6977074146270752 Time taken: 0.3342933654785156\n",
            "Batch Number: 2420 Loss: 1.688367486000061 Time taken: 0.31066441535949707\n",
            "Batch Number: 2421 Loss: 1.7133392095565796 Time taken: 0.30607175827026367\n",
            "Batch Number: 2422 Loss: 1.6777937412261963 Time taken: 0.3227577209472656\n",
            "Batch Number: 2423 Loss: 1.6838427782058716 Time taken: 0.3091590404510498\n",
            "Batch Number: 2424 Loss: 1.6692663431167603 Time taken: 0.3038499355316162\n",
            "Batch Number: 2425 Loss: 1.6817080974578857 Time taken: 0.3226804733276367\n",
            "Batch Number: 2426 Loss: 1.706843376159668 Time taken: 0.30949926376342773\n",
            "Batch Number: 2427 Loss: 1.6777456998825073 Time taken: 0.30602145195007324\n",
            "Batch Number: 2428 Loss: 1.6525380611419678 Time taken: 0.3107419013977051\n",
            "Batch Number: 2429 Loss: 1.6407016515731812 Time taken: 0.3223612308502197\n",
            "Batch Number: 2430 Loss: 1.6886146068572998 Time taken: 0.30594301223754883\n",
            "Batch Number: 2431 Loss: 1.652721643447876 Time taken: 0.31572914123535156\n",
            "Batch Number: 2432 Loss: 1.674487829208374 Time taken: 0.31382131576538086\n",
            "Batch Number: 2433 Loss: 1.7016123533248901 Time taken: 0.3072216510772705\n",
            "Batch Number: 2434 Loss: 1.710309624671936 Time taken: 0.30411553382873535\n",
            "Batch Number: 2435 Loss: 1.6964010000228882 Time taken: 0.3259131908416748\n",
            "Batch Number: 2436 Loss: 1.713829755783081 Time taken: 0.30095911026000977\n",
            "Batch Number: 2437 Loss: 1.6953204870224 Time taken: 0.30556249618530273\n",
            "Batch Number: 2438 Loss: 1.7149577140808105 Time taken: 0.3390216827392578\n",
            "Batch Number: 2439 Loss: 1.683946967124939 Time taken: 0.31002306938171387\n",
            "Batch Number: 2440 Loss: 1.7181073427200317 Time taken: 0.3117814064025879\n",
            "Batch Number: 2441 Loss: 1.690626621246338 Time taken: 0.32547545433044434\n",
            "Batch Number: 2442 Loss: 1.70195472240448 Time taken: 0.30953216552734375\n",
            "Batch Number: 2443 Loss: 1.7091237306594849 Time taken: 0.3076496124267578\n",
            "Batch Number: 2444 Loss: 1.7207844257354736 Time taken: 0.31487369537353516\n",
            "Batch Number: 2445 Loss: 1.7249152660369873 Time taken: 0.3082747459411621\n",
            "Batch Number: 2446 Loss: 1.7353630065917969 Time taken: 0.3073303699493408\n",
            "Batch Number: 2447 Loss: 1.7565414905548096 Time taken: 0.30077552795410156\n",
            "Batch Number: 2448 Loss: 1.7598079442977905 Time taken: 0.3216736316680908\n",
            "Batch Number: 2449 Loss: 1.826866865158081 Time taken: 0.2956047058105469\n",
            "Batch Number: 2450 Loss: 1.7779772281646729 Time taken: 0.3012375831604004\n",
            "Batch Number: 2451 Loss: 1.7523150444030762 Time taken: 0.31900691986083984\n",
            "Batch Number: 2452 Loss: 1.7854639291763306 Time taken: 0.31278347969055176\n",
            "Batch Number: 2453 Loss: 1.7546815872192383 Time taken: 0.3038291931152344\n",
            "Batch Number: 2454 Loss: 1.7202727794647217 Time taken: 0.323225736618042\n",
            "Batch Number: 2455 Loss: 1.7274655103683472 Time taken: 0.3140401840209961\n",
            "Batch Number: 2456 Loss: 1.7425802946090698 Time taken: 0.3071627616882324\n",
            "Batch Number: 2457 Loss: 1.7099300622940063 Time taken: 0.3266022205352783\n",
            "Batch Number: 2458 Loss: 1.6954388618469238 Time taken: 0.30560970306396484\n",
            "Batch Number: 2459 Loss: 1.7533869743347168 Time taken: 0.3016533851623535\n",
            "Batch Number: 2460 Loss: 1.699735403060913 Time taken: 0.3123202323913574\n",
            "Batch Number: 2461 Loss: 1.7322660684585571 Time taken: 0.31868433952331543\n",
            "Batch Number: 2462 Loss: 1.729106068611145 Time taken: 0.31186771392822266\n",
            "Batch Number: 2463 Loss: 1.7036980390548706 Time taken: 0.30621767044067383\n",
            "Batch Number: 2464 Loss: 1.703315258026123 Time taken: 0.3327510356903076\n",
            "Batch Number: 2465 Loss: 1.7109984159469604 Time taken: 0.3134129047393799\n",
            "Batch Number: 2466 Loss: 1.7089662551879883 Time taken: 0.3041210174560547\n",
            "Batch Number: 2467 Loss: 1.7063966989517212 Time taken: 0.3245704174041748\n",
            "Batch Number: 2468 Loss: 1.6765484809875488 Time taken: 0.3075864315032959\n",
            "Batch Number: 2469 Loss: 1.709118366241455 Time taken: 0.30749011039733887\n",
            "Batch Number: 2470 Loss: 1.7281489372253418 Time taken: 0.3227822780609131\n",
            "Batch Number: 2471 Loss: 1.7179564237594604 Time taken: 0.3080732822418213\n",
            "Batch Number: 2472 Loss: 1.6746139526367188 Time taken: 0.2937960624694824\n",
            "Batch Number: 2473 Loss: 1.6937164068222046 Time taken: 0.31552791595458984\n",
            "Batch Number: 2474 Loss: 1.6984254121780396 Time taken: 0.30399489402770996\n",
            "Batch Number: 2475 Loss: 1.6958553791046143 Time taken: 0.301694393157959\n",
            "Batch Number: 2476 Loss: 1.6847070455551147 Time taken: 0.31527256965637207\n",
            "Batch Number: 2477 Loss: 1.6714344024658203 Time taken: 0.31317687034606934\n",
            "Batch Number: 2478 Loss: 1.674658179283142 Time taken: 0.30449533462524414\n",
            "Batch Number: 2479 Loss: 1.6546677350997925 Time taken: 0.30453062057495117\n",
            "Batch Number: 2480 Loss: 1.6975537538528442 Time taken: 0.32201504707336426\n",
            "Batch Number: 2481 Loss: 1.7107363939285278 Time taken: 0.302783727645874\n",
            "Batch Number: 2482 Loss: 1.6586174964904785 Time taken: 0.3066287040710449\n",
            "Batch Number: 2483 Loss: 1.7294001579284668 Time taken: 0.3287203311920166\n",
            "Batch Number: 2484 Loss: 1.6933839321136475 Time taken: 0.30583643913269043\n",
            "Batch Number: 2485 Loss: 1.716101050376892 Time taken: 0.30068206787109375\n",
            "Batch Number: 2486 Loss: 1.6554362773895264 Time taken: 0.31908512115478516\n",
            "Batch Number: 2487 Loss: 1.6811870336532593 Time taken: 0.300708532333374\n",
            "Batch Number: 2488 Loss: 1.6940712928771973 Time taken: 0.317901611328125\n",
            "Batch Number: 2489 Loss: 1.6968908309936523 Time taken: 0.33226704597473145\n",
            "Batch Number: 2490 Loss: 1.700396180152893 Time taken: 0.30292344093322754\n",
            "Batch Number: 2491 Loss: 1.6868231296539307 Time taken: 0.31659436225891113\n",
            "Batch Number: 2492 Loss: 1.687180757522583 Time taken: 0.30806803703308105\n",
            "Batch Number: 2493 Loss: 1.6882120370864868 Time taken: 0.3188951015472412\n",
            "Batch Number: 2494 Loss: 1.6924235820770264 Time taken: 0.30907273292541504\n",
            "Batch Number: 2495 Loss: 1.69854736328125 Time taken: 0.30635786056518555\n",
            "Batch Number: 2496 Loss: 1.6980681419372559 Time taken: 0.33759188652038574\n",
            "Batch Number: 2497 Loss: 1.6881810426712036 Time taken: 0.3084523677825928\n",
            "Batch Number: 2498 Loss: 1.69541597366333 Time taken: 0.306490421295166\n",
            "Batch Number: 2499 Loss: 1.7253187894821167 Time taken: 0.3275001049041748\n",
            "Batch Number: 2500 Loss: 1.7179094552993774 Time taken: 0.31295013427734375\n",
            "Batch Number: 2501 Loss: 1.7129651308059692 Time taken: 0.3061990737915039\n",
            "Batch Number: 2502 Loss: 1.6899139881134033 Time taken: 0.32289576530456543\n",
            "Batch Number: 2503 Loss: 1.6943347454071045 Time taken: 0.30729198455810547\n",
            "Batch Number: 2504 Loss: 1.6770769357681274 Time taken: 0.3043212890625\n",
            "Batch Number: 2505 Loss: 1.6746083498001099 Time taken: 0.31855010986328125\n",
            "Batch Number: 2506 Loss: 1.695488452911377 Time taken: 0.3033921718597412\n",
            "Batch Number: 2507 Loss: 1.643883466720581 Time taken: 0.3134145736694336\n",
            "Batch Number: 2508 Loss: 1.680433750152588 Time taken: 0.31406521797180176\n",
            "Batch Number: 2509 Loss: 1.6930583715438843 Time taken: 0.33188605308532715\n",
            "Batch Number: 2510 Loss: 1.7086536884307861 Time taken: 0.30603957176208496\n",
            "Batch Number: 2511 Loss: 1.6728301048278809 Time taken: 0.3178257942199707\n",
            "Batch Number: 2512 Loss: 1.6927157640457153 Time taken: 0.3194620609283447\n",
            "Batch Number: 2513 Loss: 1.68067467212677 Time taken: 0.30635690689086914\n",
            "Batch Number: 2514 Loss: 1.672039270401001 Time taken: 0.31816554069519043\n",
            "Batch Number: 2515 Loss: 1.675954818725586 Time taken: 0.3221116065979004\n",
            "Batch Number: 2516 Loss: 1.6891121864318848 Time taken: 0.30870556831359863\n",
            "Batch Number: 2517 Loss: 1.6585156917572021 Time taken: 0.29786062240600586\n",
            "Batch Number: 2518 Loss: 1.6611518859863281 Time taken: 0.31575965881347656\n",
            "Batch Number: 2519 Loss: 1.6462098360061646 Time taken: 0.297487735748291\n",
            "Batch Number: 2520 Loss: 1.6921144723892212 Time taken: 0.301605224609375\n",
            "Batch Number: 2521 Loss: 1.6560463905334473 Time taken: 0.32558441162109375\n",
            "Batch Number: 2522 Loss: 1.6658439636230469 Time taken: 0.31476569175720215\n",
            "Batch Number: 2523 Loss: 1.6677392721176147 Time taken: 0.30521702766418457\n",
            "Batch Number: 2524 Loss: 1.6405988931655884 Time taken: 0.31449317932128906\n",
            "Batch Number: 2525 Loss: 1.6455588340759277 Time taken: 0.32233715057373047\n",
            "Batch Number: 2526 Loss: 1.6448509693145752 Time taken: 0.3070094585418701\n",
            "Batch Number: 2527 Loss: 1.6615850925445557 Time taken: 0.31325840950012207\n",
            "Batch Number: 2528 Loss: 1.6431149244308472 Time taken: 0.31920361518859863\n",
            "Batch Number: 2529 Loss: 1.6465463638305664 Time taken: 0.30196189880371094\n",
            "Batch Number: 2530 Loss: 1.6329312324523926 Time taken: 0.3143596649169922\n",
            "Batch Number: 2531 Loss: 1.6726491451263428 Time taken: 0.316178560256958\n",
            "Batch Number: 2532 Loss: 1.6506166458129883 Time taken: 0.3003225326538086\n",
            "Batch Number: 2533 Loss: 1.6435821056365967 Time taken: 0.3068385124206543\n",
            "Batch Number: 2534 Loss: 1.6785396337509155 Time taken: 0.3453702926635742\n",
            "Batch Number: 2535 Loss: 1.65990149974823 Time taken: 0.30123138427734375\n",
            "Batch Number: 2536 Loss: 1.6634564399719238 Time taken: 0.30751800537109375\n",
            "Batch Number: 2537 Loss: 1.6565065383911133 Time taken: 0.32640647888183594\n",
            "Batch Number: 2538 Loss: 1.677076816558838 Time taken: 0.30130743980407715\n",
            "Batch Number: 2539 Loss: 1.6676372289657593 Time taken: 0.30046796798706055\n",
            "Batch Number: 2540 Loss: 1.6550472974777222 Time taken: 0.3188660144805908\n",
            "Batch Number: 2541 Loss: 1.6580158472061157 Time taken: 0.31907129287719727\n",
            "Batch Number: 2542 Loss: 1.6425305604934692 Time taken: 0.3086092472076416\n",
            "Batch Number: 2543 Loss: 1.666586995124817 Time taken: 0.3148536682128906\n",
            "Batch Number: 2544 Loss: 1.6771384477615356 Time taken: 0.3183631896972656\n",
            "Batch Number: 2545 Loss: 1.6712517738342285 Time taken: 0.30794262886047363\n",
            "Batch Number: 2546 Loss: 1.666143536567688 Time taken: 0.31310415267944336\n",
            "Batch Number: 2547 Loss: 1.6643131971359253 Time taken: 0.31856751441955566\n",
            "Batch Number: 2548 Loss: 1.651076316833496 Time taken: 0.31031179428100586\n",
            "Batch Number: 2549 Loss: 1.6730594635009766 Time taken: 0.29648351669311523\n",
            "Batch Number: 2550 Loss: 1.6592576503753662 Time taken: 0.3173973560333252\n",
            "Batch Number: 2551 Loss: 1.6662050485610962 Time taken: 0.31681323051452637\n",
            "Batch Number: 2552 Loss: 1.679714322090149 Time taken: 0.3036956787109375\n",
            "Batch Number: 2553 Loss: 1.6640862226486206 Time taken: 0.3273956775665283\n",
            "Batch Number: 2554 Loss: 1.6630033254623413 Time taken: 0.30367517471313477\n",
            "Batch Number: 2555 Loss: 1.6619771718978882 Time taken: 0.3083336353302002\n",
            "Batch Number: 2556 Loss: 1.6572542190551758 Time taken: 0.3228929042816162\n",
            "Batch Number: 2557 Loss: 1.6805146932601929 Time taken: 0.3209819793701172\n",
            "Batch Number: 2558 Loss: 1.6654236316680908 Time taken: 0.29948973655700684\n",
            "Batch Number: 2559 Loss: 1.6728194952011108 Time taken: 0.31184911727905273\n",
            "Batch Number: 2560 Loss: 1.6765704154968262 Time taken: 0.32118654251098633\n",
            "Batch Number: 2561 Loss: 1.683783769607544 Time taken: 0.30385398864746094\n",
            "Batch Number: 2562 Loss: 1.6566647291183472 Time taken: 0.312408447265625\n",
            "Batch Number: 2563 Loss: 1.6574759483337402 Time taken: 0.3170144557952881\n",
            "Batch Number: 2564 Loss: 1.6846370697021484 Time taken: 0.30199766159057617\n",
            "Batch Number: 2565 Loss: 1.6456891298294067 Time taken: 0.3065028190612793\n",
            "Batch Number: 2566 Loss: 1.6335381269454956 Time taken: 0.35210371017456055\n",
            "Batch Number: 2567 Loss: 1.6992485523223877 Time taken: 0.31015467643737793\n",
            "Batch Number: 2568 Loss: 1.641128420829773 Time taken: 0.3059272766113281\n",
            "Batch Number: 2569 Loss: 1.654720425605774 Time taken: 0.3227243423461914\n",
            "Batch Number: 2570 Loss: 1.690698504447937 Time taken: 0.3199145793914795\n",
            "Batch Number: 2571 Loss: 1.6528445482254028 Time taken: 0.31026291847229004\n",
            "Batch Number: 2572 Loss: 1.6761987209320068 Time taken: 0.3204195499420166\n",
            "Batch Number: 2573 Loss: 1.7002454996109009 Time taken: 0.3131084442138672\n",
            "Batch Number: 2574 Loss: 1.6584131717681885 Time taken: 0.3120245933532715\n",
            "Batch Number: 2575 Loss: 1.6621747016906738 Time taken: 0.3164701461791992\n",
            "Batch Number: 2576 Loss: 1.6771093606948853 Time taken: 0.31677722930908203\n",
            "Batch Number: 2577 Loss: 1.6600592136383057 Time taken: 0.304675817489624\n",
            "Batch Number: 2578 Loss: 1.657536506652832 Time taken: 0.3146507740020752\n",
            "Batch Number: 2579 Loss: 1.673717975616455 Time taken: 0.31926631927490234\n",
            "Batch Number: 2580 Loss: 1.6761155128479004 Time taken: 0.32068443298339844\n",
            "Batch Number: 2581 Loss: 1.67059326171875 Time taken: 0.31038594245910645\n",
            "Batch Number: 2582 Loss: 1.659478783607483 Time taken: 0.3217353820800781\n",
            "Batch Number: 2583 Loss: 1.6381192207336426 Time taken: 0.3075408935546875\n",
            "Batch Number: 2584 Loss: 1.6491618156433105 Time taken: 0.31055617332458496\n",
            "Batch Number: 2585 Loss: 1.650741457939148 Time taken: 0.30815815925598145\n",
            "Batch Number: 2586 Loss: 1.6588834524154663 Time taken: 0.311173677444458\n",
            "Batch Number: 2587 Loss: 1.6622425317764282 Time taken: 0.31250572204589844\n",
            "Batch Number: 2588 Loss: 1.6606448888778687 Time taken: 0.3235945701599121\n",
            "Batch Number: 2589 Loss: 1.6545125246047974 Time taken: 0.3059375286102295\n",
            "Batch Number: 2590 Loss: 1.6529642343521118 Time taken: 0.318617582321167\n",
            "Batch Number: 2591 Loss: 1.656685709953308 Time taken: 0.32791829109191895\n",
            "Batch Number: 2592 Loss: 1.6896460056304932 Time taken: 0.315537691116333\n",
            "Batch Number: 2593 Loss: 1.6970213651657104 Time taken: 0.2985036373138428\n",
            "Batch Number: 2594 Loss: 1.6653575897216797 Time taken: 0.32111120223999023\n",
            "Batch Number: 2595 Loss: 1.6787230968475342 Time taken: 0.319460391998291\n",
            "Batch Number: 2596 Loss: 1.6885160207748413 Time taken: 0.3104548454284668\n",
            "Batch Number: 2597 Loss: 1.6700483560562134 Time taken: 0.3155641555786133\n",
            "Batch Number: 2598 Loss: 1.6632423400878906 Time taken: 0.32089948654174805\n",
            "Batch Number: 2599 Loss: 1.6564805507659912 Time taken: 0.3099358081817627\n",
            "Batch Number: 2600 Loss: 1.6755492687225342 Time taken: 0.3107187747955322\n",
            "Batch Number: 2601 Loss: 1.647026777267456 Time taken: 0.3135688304901123\n",
            "Batch Number: 2602 Loss: 1.6658554077148438 Time taken: 0.3124732971191406\n",
            "Batch Number: 2603 Loss: 1.662665843963623 Time taken: 0.3113536834716797\n",
            "Batch Number: 2604 Loss: 1.6460410356521606 Time taken: 0.3110079765319824\n",
            "Batch Number: 2605 Loss: 1.6653411388397217 Time taken: 0.3051493167877197\n",
            "Batch Number: 2606 Loss: 1.6512113809585571 Time taken: 0.3128697872161865\n",
            "Batch Number: 2607 Loss: 1.6268839836120605 Time taken: 0.3229050636291504\n",
            "Batch Number: 2608 Loss: 1.6557129621505737 Time taken: 0.3198223114013672\n",
            "Batch Number: 2609 Loss: 1.6566792726516724 Time taken: 0.30830812454223633\n",
            "Batch Number: 2610 Loss: 1.6637109518051147 Time taken: 0.32070446014404297\n",
            "Batch Number: 2611 Loss: 1.6242655515670776 Time taken: 0.3233020305633545\n",
            "Batch Number: 2612 Loss: 1.679092288017273 Time taken: 0.30416178703308105\n",
            "Batch Number: 2613 Loss: 1.6508697271347046 Time taken: 0.3165934085845947\n",
            "Batch Number: 2614 Loss: 1.7014423608779907 Time taken: 0.3233039379119873\n",
            "Batch Number: 2615 Loss: 1.696662425994873 Time taken: 0.31121087074279785\n",
            "Batch Number: 2616 Loss: 1.6533766984939575 Time taken: 0.32767701148986816\n",
            "Batch Number: 2617 Loss: 1.6994003057479858 Time taken: 0.3220179080963135\n",
            "Batch Number: 2618 Loss: 1.7044732570648193 Time taken: 0.30262112617492676\n",
            "Batch Number: 2619 Loss: 1.6705224514007568 Time taken: 0.31566309928894043\n",
            "Batch Number: 2620 Loss: 1.6610710620880127 Time taken: 0.3163261413574219\n",
            "Batch Number: 2621 Loss: 1.6767871379852295 Time taken: 0.30437421798706055\n",
            "Batch Number: 2622 Loss: 1.697492003440857 Time taken: 0.3081521987915039\n",
            "Batch Number: 2623 Loss: 1.661577582359314 Time taken: 0.31378746032714844\n",
            "Batch Number: 2624 Loss: 1.6888656616210938 Time taken: 0.30915188789367676\n",
            "Batch Number: 2625 Loss: 1.6733332872390747 Time taken: 0.3112640380859375\n",
            "Batch Number: 2626 Loss: 1.696370244026184 Time taken: 0.3212409019470215\n",
            "Batch Number: 2627 Loss: 1.716457486152649 Time taken: 0.3098134994506836\n",
            "Batch Number: 2628 Loss: 1.6947619915008545 Time taken: 0.31141185760498047\n",
            "Batch Number: 2629 Loss: 1.6998217105865479 Time taken: 0.3192121982574463\n",
            "Batch Number: 2630 Loss: 1.7269628047943115 Time taken: 0.31096911430358887\n",
            "Batch Number: 2631 Loss: 1.7300258874893188 Time taken: 0.30522584915161133\n",
            "Batch Number: 2632 Loss: 1.6722171306610107 Time taken: 0.31716060638427734\n",
            "Batch Number: 2633 Loss: 1.696951985359192 Time taken: 0.31472325325012207\n",
            "Batch Number: 2634 Loss: 1.7053502798080444 Time taken: 0.3118572235107422\n",
            "Batch Number: 2635 Loss: 1.7031971216201782 Time taken: 0.3191099166870117\n",
            "Batch Number: 2636 Loss: 1.7262393236160278 Time taken: 0.31315159797668457\n",
            "Batch Number: 2637 Loss: 1.7149686813354492 Time taken: 0.30124568939208984\n",
            "Batch Number: 2638 Loss: 1.7103756666183472 Time taken: 0.30921173095703125\n",
            "Batch Number: 2639 Loss: 1.6640496253967285 Time taken: 0.31050896644592285\n",
            "Batch Number: 2640 Loss: 1.6914303302764893 Time taken: 0.32611584663391113\n",
            "Batch Number: 2641 Loss: 1.6980317831039429 Time taken: 0.3170459270477295\n",
            "Batch Number: 2642 Loss: 1.6892207860946655 Time taken: 0.3118605613708496\n",
            "Batch Number: 2643 Loss: 1.685982584953308 Time taken: 0.3171536922454834\n",
            "Batch Number: 2644 Loss: 1.7145261764526367 Time taken: 0.30797410011291504\n",
            "Batch Number: 2645 Loss: 1.665276050567627 Time taken: 0.3150205612182617\n",
            "Batch Number: 2646 Loss: 1.6873003244400024 Time taken: 0.3166837692260742\n",
            "Batch Number: 2647 Loss: 1.67487633228302 Time taken: 0.3040292263031006\n",
            "Batch Number: 2648 Loss: 1.6831985712051392 Time taken: 0.3112154006958008\n",
            "Batch Number: 2649 Loss: 1.6899619102478027 Time taken: 0.3035142421722412\n",
            "Batch Number: 2650 Loss: 1.6679527759552002 Time taken: 0.31448912620544434\n",
            "Batch Number: 2651 Loss: 1.6644593477249146 Time taken: 0.32050275802612305\n",
            "Batch Number: 2652 Loss: 1.685766339302063 Time taken: 0.3235287666320801\n",
            "Batch Number: 2653 Loss: 1.6907398700714111 Time taken: 0.31437230110168457\n",
            "Batch Number: 2654 Loss: 1.6797508001327515 Time taken: 0.3213386535644531\n",
            "Batch Number: 2655 Loss: 1.657778024673462 Time taken: 0.31502604484558105\n",
            "Batch Number: 2656 Loss: 1.615316390991211 Time taken: 0.30678534507751465\n",
            "Batch Number: 2657 Loss: 1.657326579093933 Time taken: 0.3108043670654297\n",
            "Batch Number: 2658 Loss: 1.6501078605651855 Time taken: 0.30797266960144043\n",
            "Batch Number: 2659 Loss: 1.6434484720230103 Time taken: 0.312835693359375\n",
            "Batch Number: 2660 Loss: 1.6646443605422974 Time taken: 0.3127288818359375\n",
            "Batch Number: 2661 Loss: 1.6442492008209229 Time taken: 0.3070797920227051\n",
            "Batch Number: 2662 Loss: 1.6678264141082764 Time taken: 0.3246731758117676\n",
            "Batch Number: 2663 Loss: 1.6781654357910156 Time taken: 0.3137047290802002\n",
            "Batch Number: 2664 Loss: 1.7124375104904175 Time taken: 0.3122990131378174\n",
            "Batch Number: 2665 Loss: 1.7086743116378784 Time taken: 0.3169839382171631\n",
            "Batch Number: 2666 Loss: 1.6486499309539795 Time taken: 0.30649375915527344\n",
            "Batch Number: 2667 Loss: 1.6844440698623657 Time taken: 0.31300878524780273\n",
            "Batch Number: 2668 Loss: 1.6823453903198242 Time taken: 0.311617374420166\n",
            "Batch Number: 2669 Loss: 1.6732441186904907 Time taken: 0.3080291748046875\n",
            "Batch Number: 2670 Loss: 1.6471422910690308 Time taken: 0.32228636741638184\n",
            "Batch Number: 2671 Loss: 1.6813666820526123 Time taken: 0.31807374954223633\n",
            "Batch Number: 2672 Loss: 1.6683896780014038 Time taken: 0.315152645111084\n",
            "Batch Number: 2673 Loss: 1.6499009132385254 Time taken: 0.32273197174072266\n",
            "Batch Number: 2674 Loss: 1.6443368196487427 Time taken: 0.31548285484313965\n",
            "Batch Number: 2675 Loss: 1.6629600524902344 Time taken: 0.3270533084869385\n",
            "Batch Number: 2676 Loss: 1.6769040822982788 Time taken: 0.3244321346282959\n",
            "Batch Number: 2677 Loss: 1.673231601715088 Time taken: 0.3220994472503662\n",
            "Batch Number: 2678 Loss: 1.6972042322158813 Time taken: 0.3265993595123291\n",
            "Batch Number: 2679 Loss: 1.683064579963684 Time taken: 0.32382798194885254\n",
            "Batch Number: 2680 Loss: 1.663948655128479 Time taken: 0.3265261650085449\n",
            "Batch Number: 2681 Loss: 1.6489830017089844 Time taken: 0.3450582027435303\n",
            "Batch Number: 2682 Loss: 1.6651499271392822 Time taken: 0.3280191421508789\n",
            "Batch Number: 2683 Loss: 1.6577057838439941 Time taken: 0.32129335403442383\n",
            "Batch Number: 2684 Loss: 1.6652867794036865 Time taken: 0.3361351490020752\n",
            "Batch Number: 2685 Loss: 1.6599736213684082 Time taken: 0.33406853675842285\n",
            "Batch Number: 2686 Loss: 1.6563915014266968 Time taken: 0.31862473487854004\n",
            "Batch Number: 2687 Loss: 1.668230414390564 Time taken: 0.3434333801269531\n",
            "Batch Number: 2688 Loss: 1.6640206575393677 Time taken: 0.3318657875061035\n",
            "Batch Number: 2689 Loss: 1.6518985033035278 Time taken: 0.314802885055542\n",
            "Batch Number: 2690 Loss: 1.6372060775756836 Time taken: 0.330106258392334\n",
            "Batch Number: 2691 Loss: 1.6568630933761597 Time taken: 0.333085298538208\n",
            "Batch Number: 2692 Loss: 1.6614134311676025 Time taken: 0.31493520736694336\n",
            "Batch Number: 2693 Loss: 1.6630057096481323 Time taken: 0.3293285369873047\n",
            "Batch Number: 2694 Loss: 1.6448231935501099 Time taken: 0.3186373710632324\n",
            "Batch Number: 2695 Loss: 1.6703779697418213 Time taken: 0.3293747901916504\n",
            "Batch Number: 2696 Loss: 1.6509195566177368 Time taken: 0.3390474319458008\n",
            "Batch Number: 2697 Loss: 1.6225000619888306 Time taken: 0.3235154151916504\n",
            "Batch Number: 2698 Loss: 1.6362009048461914 Time taken: 0.32552504539489746\n",
            "Batch Number: 2699 Loss: 1.6545335054397583 Time taken: 0.3310718536376953\n",
            "Batch Number: 2700 Loss: 1.6521728038787842 Time taken: 0.31693005561828613\n",
            "Batch Number: 2701 Loss: 1.6530126333236694 Time taken: 0.32312655448913574\n",
            "Batch Number: 2702 Loss: 1.609671711921692 Time taken: 0.338665246963501\n",
            "Batch Number: 2703 Loss: 1.6296316385269165 Time taken: 0.312880277633667\n",
            "Batch Number: 2704 Loss: 1.6028778553009033 Time taken: 0.321941614151001\n",
            "Batch Number: 2705 Loss: 1.62984299659729 Time taken: 0.31949591636657715\n",
            "Batch Number: 2706 Loss: 1.6161487102508545 Time taken: 0.311842679977417\n",
            "Batch Number: 2707 Loss: 1.6328332424163818 Time taken: 0.3208169937133789\n",
            "Batch Number: 2708 Loss: 1.6114132404327393 Time taken: 0.3101952075958252\n",
            "Batch Number: 2709 Loss: 1.6242573261260986 Time taken: 0.3082244396209717\n",
            "Batch Number: 2710 Loss: 1.634993314743042 Time taken: 0.31357264518737793\n",
            "Batch Number: 2711 Loss: 1.6112852096557617 Time taken: 0.306563138961792\n",
            "Batch Number: 2712 Loss: 1.6361209154129028 Time taken: 0.32186245918273926\n",
            "Batch Number: 2713 Loss: 1.6597660779953003 Time taken: 0.3128852844238281\n",
            "Batch Number: 2714 Loss: 1.6418938636779785 Time taken: 0.306182861328125\n",
            "Batch Number: 2715 Loss: 1.659199595451355 Time taken: 0.32433438301086426\n",
            "Batch Number: 2716 Loss: 1.6549363136291504 Time taken: 0.3144819736480713\n",
            "Batch Number: 2717 Loss: 1.660016655921936 Time taken: 0.3060281276702881\n",
            "Batch Number: 2718 Loss: 1.6457475423812866 Time taken: 0.318206787109375\n",
            "Batch Number: 2719 Loss: 1.6320993900299072 Time taken: 0.311931848526001\n",
            "Batch Number: 2720 Loss: 1.618928074836731 Time taken: 0.3085770606994629\n",
            "Batch Number: 2721 Loss: 1.6290894746780396 Time taken: 0.3078594207763672\n",
            "Batch Number: 2722 Loss: 1.6325558423995972 Time taken: 0.3104870319366455\n",
            "Batch Number: 2723 Loss: 1.6272263526916504 Time taken: 0.31313204765319824\n",
            "Batch Number: 2724 Loss: 1.6334844827651978 Time taken: 0.3173820972442627\n",
            "Batch Number: 2725 Loss: 1.636618733406067 Time taken: 0.30113911628723145\n",
            "Batch Number: 2726 Loss: 1.6495130062103271 Time taken: 0.31757330894470215\n",
            "Batch Number: 2727 Loss: 1.646196961402893 Time taken: 0.30409741401672363\n",
            "Batch Number: 2728 Loss: 1.612060546875 Time taken: 0.3154466152191162\n",
            "Batch Number: 2729 Loss: 1.626428246498108 Time taken: 0.3168947696685791\n",
            "Batch Number: 2730 Loss: 1.6184850931167603 Time taken: 0.3063664436340332\n",
            "Batch Number: 2731 Loss: 1.6309869289398193 Time taken: 0.3151514530181885\n",
            "Batch Number: 2732 Loss: 1.626461148262024 Time taken: 0.3109560012817383\n",
            "Batch Number: 2733 Loss: 1.6323920488357544 Time taken: 0.31324028968811035\n",
            "Batch Number: 2734 Loss: 1.6586201190948486 Time taken: 0.3187601566314697\n",
            "Batch Number: 2735 Loss: 1.63023841381073 Time taken: 0.30402159690856934\n",
            "Batch Number: 2736 Loss: 1.6379915475845337 Time taken: 0.29659128189086914\n",
            "Batch Number: 2737 Loss: 1.6413297653198242 Time taken: 0.31089138984680176\n",
            "Batch Number: 2738 Loss: 1.6589295864105225 Time taken: 0.2935037612915039\n",
            "Batch Number: 2739 Loss: 1.6627694368362427 Time taken: 0.3088984489440918\n",
            "Batch Number: 2740 Loss: 1.669447660446167 Time taken: 0.2975764274597168\n",
            "Batch Number: 2741 Loss: 1.6981664896011353 Time taken: 0.3198528289794922\n",
            "Batch Number: 2742 Loss: 1.643492579460144 Time taken: 0.3181920051574707\n",
            "Batch Number: 2743 Loss: 1.6554781198501587 Time taken: 0.30983710289001465\n",
            "Batch Number: 2744 Loss: 1.654154896736145 Time taken: 0.3225069046020508\n",
            "Batch Number: 2745 Loss: 1.6149001121520996 Time taken: 0.3102571964263916\n",
            "Batch Number: 2746 Loss: 1.6464816331863403 Time taken: 0.3076903820037842\n",
            "Batch Number: 2747 Loss: 1.6532127857208252 Time taken: 0.31750988960266113\n",
            "Batch Number: 2748 Loss: 1.6436264514923096 Time taken: 0.3058764934539795\n",
            "Batch Number: 2749 Loss: 1.6399930715560913 Time taken: 0.3047308921813965\n",
            "Batch Number: 2750 Loss: 1.6536413431167603 Time taken: 0.31409525871276855\n",
            "Batch Number: 2751 Loss: 1.6445857286453247 Time taken: 0.3145601749420166\n",
            "Batch Number: 2752 Loss: 1.6619453430175781 Time taken: 0.30025696754455566\n",
            "Batch Number: 2753 Loss: 1.6066194772720337 Time taken: 0.3031790256500244\n",
            "Batch Number: 2754 Loss: 1.63930344581604 Time taken: 0.317413330078125\n",
            "Batch Number: 2755 Loss: 1.6649744510650635 Time taken: 0.31955528259277344\n",
            "Batch Number: 2756 Loss: 1.6201016902923584 Time taken: 0.30550050735473633\n",
            "Batch Number: 2757 Loss: 1.6616566181182861 Time taken: 0.31432342529296875\n",
            "Batch Number: 2758 Loss: 1.6569408178329468 Time taken: 0.3164966106414795\n",
            "Batch Number: 2759 Loss: 1.635107159614563 Time taken: 0.3081541061401367\n",
            "Batch Number: 2760 Loss: 1.6358208656311035 Time taken: 0.3101212978363037\n",
            "Batch Number: 2761 Loss: 1.6117461919784546 Time taken: 0.3243582248687744\n",
            "Batch Number: 2762 Loss: 1.6239265203475952 Time taken: 0.3088822364807129\n",
            "Batch Number: 2763 Loss: 1.6158493757247925 Time taken: 0.3160257339477539\n",
            "Batch Number: 2764 Loss: 1.621564507484436 Time taken: 0.3128058910369873\n",
            "Batch Number: 2765 Loss: 1.6200165748596191 Time taken: 0.308652400970459\n",
            "Batch Number: 2766 Loss: 1.6140825748443604 Time taken: 0.31517481803894043\n",
            "Batch Number: 2767 Loss: 1.6448101997375488 Time taken: 0.30554819107055664\n",
            "Batch Number: 2768 Loss: 1.6454219818115234 Time taken: 0.3088798522949219\n",
            "Batch Number: 2769 Loss: 1.6539909839630127 Time taken: 0.30745863914489746\n",
            "Batch Number: 2770 Loss: 1.6617836952209473 Time taken: 0.3130335807800293\n",
            "Batch Number: 2771 Loss: 1.6491237878799438 Time taken: 0.3158888816833496\n",
            "Batch Number: 2772 Loss: 1.6776665449142456 Time taken: 0.3075590133666992\n",
            "Batch Number: 2773 Loss: 1.6607670783996582 Time taken: 0.3151514530181885\n",
            "Batch Number: 2774 Loss: 1.6606061458587646 Time taken: 0.30937671661376953\n",
            "Batch Number: 2775 Loss: 1.6381839513778687 Time taken: 0.30802059173583984\n",
            "Batch Number: 2776 Loss: 1.6370362043380737 Time taken: 0.3092687129974365\n",
            "Batch Number: 2777 Loss: 1.6609302759170532 Time taken: 0.30896949768066406\n",
            "Batch Number: 2778 Loss: 1.6413623094558716 Time taken: 0.2981832027435303\n",
            "Batch Number: 2779 Loss: 1.6690326929092407 Time taken: 0.3182058334350586\n",
            "Batch Number: 2780 Loss: 1.651771068572998 Time taken: 0.30179929733276367\n",
            "Batch Number: 2781 Loss: 1.6393002271652222 Time taken: 0.30161452293395996\n",
            "Batch Number: 2782 Loss: 1.6415642499923706 Time taken: 0.3083469867706299\n",
            "Batch Number: 2783 Loss: 1.6291755437850952 Time taken: 0.31456732749938965\n",
            "Batch Number: 2784 Loss: 1.6234463453292847 Time taken: 0.3072633743286133\n",
            "Batch Number: 2785 Loss: 1.6186603307724 Time taken: 0.30588269233703613\n",
            "Batch Number: 2786 Loss: 1.6270742416381836 Time taken: 0.31995511054992676\n",
            "Batch Number: 2787 Loss: 1.6317436695098877 Time taken: 0.3182032108306885\n",
            "Batch Number: 2788 Loss: 1.6353565454483032 Time taken: 0.30228638648986816\n",
            "Batch Number: 2789 Loss: 1.6276206970214844 Time taken: 0.31653761863708496\n",
            "Batch Number: 2790 Loss: 1.6051918268203735 Time taken: 0.31258344650268555\n",
            "Batch Number: 2791 Loss: 1.64107084274292 Time taken: 0.3024144172668457\n",
            "Batch Number: 2792 Loss: 1.642575979232788 Time taken: 0.30247926712036133\n",
            "Batch Number: 2793 Loss: 1.6341677904129028 Time taken: 0.3060486316680908\n",
            "Batch Number: 2794 Loss: 1.6565358638763428 Time taken: 0.3020024299621582\n",
            "Batch Number: 2795 Loss: 1.6804864406585693 Time taken: 0.31188201904296875\n",
            "Batch Number: 2796 Loss: 1.6684191226959229 Time taken: 0.32032108306884766\n",
            "Batch Number: 2797 Loss: 1.680901050567627 Time taken: 0.3018224239349365\n",
            "Batch Number: 2798 Loss: 1.6544442176818848 Time taken: 0.3083994388580322\n",
            "Batch Number: 2799 Loss: 1.6498090028762817 Time taken: 0.31783485412597656\n",
            "Batch Number: 2800 Loss: 1.6479392051696777 Time taken: 0.306105375289917\n",
            "Batch Number: 2801 Loss: 1.6573771238327026 Time taken: 0.30121636390686035\n",
            "Batch Number: 2802 Loss: 1.655098557472229 Time taken: 0.30609917640686035\n",
            "Batch Number: 2803 Loss: 1.6387293338775635 Time taken: 0.3050665855407715\n",
            "Batch Number: 2804 Loss: 1.681572675704956 Time taken: 0.3025472164154053\n",
            "Batch Number: 2805 Loss: 1.693748116493225 Time taken: 0.30373120307922363\n",
            "Batch Number: 2806 Loss: 1.7114819288253784 Time taken: 0.3137047290802002\n",
            "Batch Number: 2807 Loss: 1.69300377368927 Time taken: 0.3050696849822998\n",
            "Batch Number: 2808 Loss: 1.703025221824646 Time taken: 0.30525851249694824\n",
            "Batch Number: 2809 Loss: 1.6858081817626953 Time taken: 0.30556154251098633\n",
            "Batch Number: 2810 Loss: 1.7028155326843262 Time taken: 0.30590009689331055\n",
            "Batch Number: 2811 Loss: 1.690447211265564 Time taken: 0.30712270736694336\n",
            "Batch Number: 2812 Loss: 1.6667492389678955 Time taken: 0.3291201591491699\n",
            "Batch Number: 2813 Loss: 1.6724269390106201 Time taken: 0.2984006404876709\n",
            "Batch Number: 2814 Loss: 1.6697064638137817 Time taken: 0.3040738105773926\n",
            "Batch Number: 2815 Loss: 1.6641279458999634 Time taken: 0.3295886516571045\n",
            "Batch Number: 2816 Loss: 1.6519503593444824 Time taken: 0.31343579292297363\n",
            "Batch Number: 2817 Loss: 1.6691514253616333 Time taken: 0.3097550868988037\n",
            "Batch Number: 2818 Loss: 1.6490453481674194 Time taken: 0.3171823024749756\n",
            "Batch Number: 2819 Loss: 1.6557862758636475 Time taken: 0.3130192756652832\n",
            "Batch Number: 2820 Loss: 1.6547048091888428 Time taken: 0.2989649772644043\n",
            "Batch Number: 2821 Loss: 1.673145055770874 Time taken: 0.3125932216644287\n",
            "Batch Number: 2822 Loss: 1.6481027603149414 Time taken: 0.3093743324279785\n",
            "Batch Number: 2823 Loss: 1.6511871814727783 Time taken: 0.3053581714630127\n",
            "Batch Number: 2824 Loss: 1.6663460731506348 Time taken: 0.2976076602935791\n",
            "Batch Number: 2825 Loss: 1.6333307027816772 Time taken: 0.33162403106689453\n",
            "Batch Number: 2826 Loss: 1.6368300914764404 Time taken: 0.3083655834197998\n",
            "Batch Number: 2827 Loss: 1.6298478841781616 Time taken: 0.3029165267944336\n",
            "Batch Number: 2828 Loss: 1.6284489631652832 Time taken: 0.32380199432373047\n",
            "Batch Number: 2829 Loss: 1.6564873456954956 Time taken: 0.3062567710876465\n",
            "Batch Number: 2830 Loss: 1.654313564300537 Time taken: 0.3011295795440674\n",
            "Batch Number: 2831 Loss: 1.6862810850143433 Time taken: 0.3269004821777344\n",
            "Batch Number: 2832 Loss: 1.6275105476379395 Time taken: 0.30803680419921875\n",
            "Batch Number: 2833 Loss: 1.6563822031021118 Time taken: 0.3177449703216553\n",
            "Batch Number: 2834 Loss: 1.642072319984436 Time taken: 0.3131742477416992\n",
            "Batch Number: 2835 Loss: 1.6305402517318726 Time taken: 0.30673885345458984\n",
            "Batch Number: 2836 Loss: 1.6021958589553833 Time taken: 0.3054921627044678\n",
            "Batch Number: 2837 Loss: 1.6192604303359985 Time taken: 0.31517648696899414\n",
            "Batch Number: 2838 Loss: 1.623847484588623 Time taken: 0.3139209747314453\n",
            "Batch Number: 2839 Loss: 1.6142020225524902 Time taken: 0.31176042556762695\n",
            "Batch Number: 2840 Loss: 1.634653925895691 Time taken: 0.30830836296081543\n",
            "Batch Number: 2841 Loss: 1.6314417123794556 Time taken: 0.3265397548675537\n",
            "Batch Number: 2842 Loss: 1.6686996221542358 Time taken: 0.30411529541015625\n",
            "Batch Number: 2843 Loss: 1.7147265672683716 Time taken: 0.31455445289611816\n",
            "Batch Number: 2844 Loss: 1.6589168310165405 Time taken: 0.32219862937927246\n",
            "Batch Number: 2845 Loss: 1.6419233083724976 Time taken: 0.3078789710998535\n",
            "Batch Number: 2846 Loss: 1.665381908416748 Time taken: 0.3022763729095459\n",
            "Batch Number: 2847 Loss: 1.606521487236023 Time taken: 0.3195357322692871\n",
            "Batch Number: 2848 Loss: 1.6550877094268799 Time taken: 0.31118249893188477\n",
            "Batch Number: 2849 Loss: 1.6496037244796753 Time taken: 0.3059420585632324\n",
            "Batch Number: 2850 Loss: 1.636949896812439 Time taken: 0.3175017833709717\n",
            "Batch Number: 2851 Loss: 1.627869725227356 Time taken: 0.32663989067077637\n",
            "Batch Number: 2852 Loss: 1.6674145460128784 Time taken: 0.30886220932006836\n",
            "Batch Number: 2853 Loss: 1.6306180953979492 Time taken: 0.3117513656616211\n",
            "Batch Number: 2854 Loss: 1.6300454139709473 Time taken: 0.3145582675933838\n",
            "Batch Number: 2855 Loss: 1.642497181892395 Time taken: 0.30274105072021484\n",
            "Batch Number: 2856 Loss: 1.6174607276916504 Time taken: 0.30031895637512207\n",
            "Batch Number: 2857 Loss: 1.6509402990341187 Time taken: 0.33070993423461914\n",
            "Batch Number: 2858 Loss: 1.6419689655303955 Time taken: 0.30409860610961914\n",
            "Batch Number: 2859 Loss: 1.6708040237426758 Time taken: 0.31577038764953613\n",
            "Batch Number: 2860 Loss: 1.650299072265625 Time taken: 0.33088111877441406\n",
            "Batch Number: 2861 Loss: 1.6243436336517334 Time taken: 0.3132894039154053\n",
            "Batch Number: 2862 Loss: 1.6433372497558594 Time taken: 0.30527400970458984\n",
            "Batch Number: 2863 Loss: 1.654099464416504 Time taken: 0.3365898132324219\n",
            "Batch Number: 2864 Loss: 1.6305361986160278 Time taken: 0.3156554698944092\n",
            "Batch Number: 2865 Loss: 1.6528830528259277 Time taken: 0.30503153800964355\n",
            "Batch Number: 2866 Loss: 1.6572015285491943 Time taken: 0.31884050369262695\n",
            "Batch Number: 2867 Loss: 1.629510760307312 Time taken: 0.30478477478027344\n",
            "Batch Number: 2868 Loss: 1.6334567070007324 Time taken: 0.30054426193237305\n",
            "Batch Number: 2869 Loss: 1.6308845281600952 Time taken: 0.32160210609436035\n",
            "Batch Number: 2870 Loss: 1.618444800376892 Time taken: 0.3166654109954834\n",
            "Batch Number: 2871 Loss: 1.6399247646331787 Time taken: 0.30441951751708984\n",
            "Batch Number: 2872 Loss: 1.6196449995040894 Time taken: 0.3078007698059082\n",
            "Batch Number: 2873 Loss: 1.6558750867843628 Time taken: 0.32258152961730957\n",
            "Batch Number: 2874 Loss: 1.646986722946167 Time taken: 0.3000657558441162\n",
            "Batch Number: 2875 Loss: 1.708030343055725 Time taken: 0.32392215728759766\n",
            "Batch Number: 2876 Loss: 1.65498685836792 Time taken: 0.33290815353393555\n",
            "Batch Number: 2877 Loss: 1.660177230834961 Time taken: 0.307053804397583\n",
            "Batch Number: 2878 Loss: 1.633591890335083 Time taken: 0.3042125701904297\n",
            "Batch Number: 2879 Loss: 1.629206657409668 Time taken: 0.32300400733947754\n",
            "Batch Number: 2880 Loss: 1.6522586345672607 Time taken: 0.29853272438049316\n",
            "Batch Number: 2881 Loss: 1.6454397439956665 Time taken: 0.30422520637512207\n",
            "Batch Number: 2882 Loss: 1.619721531867981 Time taken: 0.3235197067260742\n",
            "Batch Number: 2883 Loss: 1.641721487045288 Time taken: 0.3096184730529785\n",
            "Batch Number: 2884 Loss: 1.5872111320495605 Time taken: 0.30226802825927734\n",
            "Batch Number: 2885 Loss: 1.5817170143127441 Time taken: 0.3287997245788574\n",
            "Batch Number: 2886 Loss: 1.613463282585144 Time taken: 0.30843424797058105\n",
            "Batch Number: 2887 Loss: 1.6222217082977295 Time taken: 0.30678844451904297\n",
            "Batch Number: 2888 Loss: 1.6378415822982788 Time taken: 0.30150341987609863\n",
            "Batch Number: 2889 Loss: 1.6031297445297241 Time taken: 0.3258993625640869\n",
            "Batch Number: 2890 Loss: 1.6210441589355469 Time taken: 0.3061487674713135\n",
            "Batch Number: 2891 Loss: 1.606661081314087 Time taken: 0.3270258903503418\n",
            "Batch Number: 2892 Loss: 1.5878344774246216 Time taken: 0.33069658279418945\n",
            "Batch Number: 2893 Loss: 1.6397496461868286 Time taken: 0.30750298500061035\n",
            "Batch Number: 2894 Loss: 1.6521291732788086 Time taken: 0.30580735206604004\n",
            "Batch Number: 2895 Loss: 1.622888445854187 Time taken: 0.32846498489379883\n",
            "Batch Number: 2896 Loss: 1.6163727045059204 Time taken: 0.31763291358947754\n",
            "Batch Number: 2897 Loss: 1.6396898031234741 Time taken: 0.30281591415405273\n",
            "Batch Number: 2898 Loss: 1.6117992401123047 Time taken: 0.32207369804382324\n",
            "Batch Number: 2899 Loss: 1.6100834608078003 Time taken: 0.3112912178039551\n",
            "Batch Number: 2900 Loss: 1.6031712293624878 Time taken: 0.3040456771850586\n",
            "Batch Number: 2901 Loss: 1.605141043663025 Time taken: 0.3173379898071289\n",
            "Batch Number: 2902 Loss: 1.6314618587493896 Time taken: 0.3095366954803467\n",
            "Batch Number: 2903 Loss: 1.6071484088897705 Time taken: 0.3096127510070801\n",
            "Batch Number: 2904 Loss: 1.6188321113586426 Time taken: 0.3065633773803711\n",
            "Batch Number: 2905 Loss: 1.6107412576675415 Time taken: 0.3207085132598877\n",
            "Batch Number: 2906 Loss: 1.6289805173873901 Time taken: 0.30562424659729004\n",
            "Batch Number: 2907 Loss: 1.6113531589508057 Time taken: 0.30788493156433105\n",
            "Batch Number: 2908 Loss: 1.6172382831573486 Time taken: 0.32381296157836914\n",
            "Batch Number: 2909 Loss: 1.6321179866790771 Time taken: 0.3003051280975342\n",
            "Batch Number: 2910 Loss: 1.5855026245117188 Time taken: 0.30763864517211914\n",
            "Batch Number: 2911 Loss: 1.6081918478012085 Time taken: 0.3292665481567383\n",
            "Batch Number: 2912 Loss: 1.6062391996383667 Time taken: 0.3095104694366455\n",
            "Batch Number: 2913 Loss: 1.6092857122421265 Time taken: 0.30809545516967773\n",
            "Batch Number: 2914 Loss: 1.6468563079833984 Time taken: 0.32138538360595703\n",
            "Batch Number: 2915 Loss: 1.616280198097229 Time taken: 0.30223774909973145\n",
            "Batch Number: 2916 Loss: 1.6184196472167969 Time taken: 0.3070805072784424\n",
            "Batch Number: 2917 Loss: 1.6161566972732544 Time taken: 0.3165702819824219\n",
            "Batch Number: 2918 Loss: 1.6681358814239502 Time taken: 0.30616188049316406\n",
            "Batch Number: 2919 Loss: 1.646653652191162 Time taken: 0.3049466609954834\n",
            "Batch Number: 2920 Loss: 1.6565585136413574 Time taken: 0.30838847160339355\n",
            "Batch Number: 2921 Loss: 1.6528315544128418 Time taken: 0.3124203681945801\n",
            "Batch Number: 2922 Loss: 1.639319658279419 Time taken: 0.2987675666809082\n",
            "Batch Number: 2923 Loss: 1.6229292154312134 Time taken: 0.3061513900756836\n",
            "Batch Number: 2924 Loss: 1.579304814338684 Time taken: 0.3241698741912842\n",
            "Batch Number: 2925 Loss: 1.595699429512024 Time taken: 0.3063013553619385\n",
            "Batch Number: 2926 Loss: 1.6517809629440308 Time taken: 0.3028872013092041\n",
            "Batch Number: 2927 Loss: 1.5886449813842773 Time taken: 0.33069324493408203\n",
            "Batch Number: 2928 Loss: 1.6006025075912476 Time taken: 0.3095836639404297\n",
            "Batch Number: 2929 Loss: 1.6132392883300781 Time taken: 0.30910205841064453\n",
            "Batch Number: 2930 Loss: 1.6120530366897583 Time taken: 0.3103756904602051\n",
            "Batch Number: 2931 Loss: 1.6130053997039795 Time taken: 0.3082718849182129\n",
            "Batch Number: 2932 Loss: 1.6594209671020508 Time taken: 0.29802942276000977\n",
            "Batch Number: 2933 Loss: 1.6108262538909912 Time taken: 0.3103015422821045\n",
            "Batch Number: 2934 Loss: 1.6110769510269165 Time taken: 0.3215341567993164\n",
            "Batch Number: 2935 Loss: 1.6165771484375 Time taken: 0.298306941986084\n",
            "Batch Number: 2936 Loss: 1.6142165660858154 Time taken: 0.3038980960845947\n",
            "Batch Number: 2937 Loss: 1.6122804880142212 Time taken: 0.32840681076049805\n",
            "Batch Number: 2938 Loss: 1.6296337842941284 Time taken: 0.30510902404785156\n",
            "Batch Number: 2939 Loss: 1.6417193412780762 Time taken: 0.30468177795410156\n",
            "Batch Number: 2940 Loss: 1.621009349822998 Time taken: 0.3357369899749756\n",
            "Batch Number: 2941 Loss: 1.625747561454773 Time taken: 0.30382227897644043\n",
            "Batch Number: 2942 Loss: 1.6063536405563354 Time taken: 0.30537867546081543\n",
            "Batch Number: 2943 Loss: 1.606522798538208 Time taken: 0.31748390197753906\n",
            "Batch Number: 2944 Loss: 1.6144222021102905 Time taken: 0.3095672130584717\n",
            "Batch Number: 2945 Loss: 1.6403694152832031 Time taken: 0.3021121025085449\n",
            "Batch Number: 2946 Loss: 1.611459732055664 Time taken: 0.31462883949279785\n",
            "Batch Number: 2947 Loss: 1.6251039505004883 Time taken: 0.3087639808654785\n",
            "Batch Number: 2948 Loss: 1.6393736600875854 Time taken: 0.30619359016418457\n",
            "Batch Number: 2949 Loss: 1.6208840608596802 Time taken: 0.30568909645080566\n",
            "Batch Number: 2950 Loss: 1.607649803161621 Time taken: 0.33186984062194824\n",
            "Batch Number: 2951 Loss: 1.6150093078613281 Time taken: 0.30885863304138184\n",
            "Batch Number: 2952 Loss: 1.6516867876052856 Time taken: 0.3105480670928955\n",
            "Batch Number: 2953 Loss: 1.6309634447097778 Time taken: 0.32601070404052734\n",
            "Batch Number: 2954 Loss: 1.6371707916259766 Time taken: 0.30641889572143555\n",
            "Batch Number: 2955 Loss: 1.639024019241333 Time taken: 0.30496740341186523\n",
            "Batch Number: 2956 Loss: 1.6439169645309448 Time taken: 0.3228878974914551\n",
            "Batch Number: 2957 Loss: 1.6486115455627441 Time taken: 0.30611419677734375\n",
            "Batch Number: 2958 Loss: 1.6362965106964111 Time taken: 0.3072023391723633\n",
            "Batch Number: 2959 Loss: 1.5959590673446655 Time taken: 0.32697248458862305\n",
            "Batch Number: 2960 Loss: 1.6236857175827026 Time taken: 0.30588746070861816\n",
            "Batch Number: 2961 Loss: 1.620705246925354 Time taken: 0.30751633644104004\n",
            "Batch Number: 2962 Loss: 1.6053037643432617 Time taken: 0.304980993270874\n",
            "Batch Number: 2963 Loss: 1.6022672653198242 Time taken: 0.31110215187072754\n",
            "Batch Number: 2964 Loss: 1.5932873487472534 Time taken: 0.3068509101867676\n",
            "Batch Number: 2965 Loss: 1.5996100902557373 Time taken: 0.31852197647094727\n",
            "Batch Number: 2966 Loss: 1.5895572900772095 Time taken: 0.3119335174560547\n",
            "Batch Number: 2967 Loss: 1.5762766599655151 Time taken: 0.3044013977050781\n",
            "Batch Number: 2968 Loss: 1.5846298933029175 Time taken: 0.30603814125061035\n",
            "Batch Number: 2969 Loss: 1.599866271018982 Time taken: 0.3298940658569336\n",
            "Batch Number: 2970 Loss: 1.6210180521011353 Time taken: 0.3136920928955078\n",
            "Batch Number: 2971 Loss: 1.6444021463394165 Time taken: 0.3024625778198242\n",
            "Batch Number: 2972 Loss: 1.6099750995635986 Time taken: 0.32062864303588867\n",
            "Batch Number: 2973 Loss: 1.6627000570297241 Time taken: 0.30836033821105957\n",
            "Batch Number: 2974 Loss: 1.6485188007354736 Time taken: 0.30954527854919434\n",
            "Batch Number: 2975 Loss: 1.6523739099502563 Time taken: 0.3155801296234131\n",
            "Batch Number: 2976 Loss: 1.6438562870025635 Time taken: 0.311007022857666\n",
            "Batch Number: 2977 Loss: 1.6512150764465332 Time taken: 0.30444788932800293\n",
            "Batch Number: 2978 Loss: 1.6276100873947144 Time taken: 0.3134291172027588\n",
            "Batch Number: 2979 Loss: 1.6487208604812622 Time taken: 0.3116936683654785\n",
            "Batch Number: 2980 Loss: 1.6351356506347656 Time taken: 0.30306506156921387\n",
            "Batch Number: 2981 Loss: 1.649877667427063 Time taken: 0.31246161460876465\n",
            "Batch Number: 2982 Loss: 1.6519502401351929 Time taken: 0.32061767578125\n",
            "Batch Number: 2983 Loss: 1.660291075706482 Time taken: 0.3010280132293701\n",
            "Batch Number: 2984 Loss: 1.669607162475586 Time taken: 0.3070952892303467\n",
            "Batch Number: 2985 Loss: 1.650498390197754 Time taken: 0.3304579257965088\n",
            "Batch Number: 2986 Loss: 1.6564710140228271 Time taken: 0.3076808452606201\n",
            "Batch Number: 2987 Loss: 1.6754159927368164 Time taken: 0.3050532341003418\n",
            "Batch Number: 2988 Loss: 1.6624236106872559 Time taken: 0.32016515731811523\n",
            "Batch Number: 2989 Loss: 1.6765708923339844 Time taken: 0.3027007579803467\n",
            "Batch Number: 2990 Loss: 1.6891191005706787 Time taken: 0.31102514266967773\n",
            "Batch Number: 2991 Loss: 1.699172019958496 Time taken: 0.3185868263244629\n",
            "Batch Number: 2992 Loss: 1.6289668083190918 Time taken: 0.3096950054168701\n",
            "Batch Number: 2993 Loss: 1.654800295829773 Time taken: 0.29711031913757324\n",
            "Batch Number: 2994 Loss: 1.623834252357483 Time taken: 0.310056209564209\n",
            "Batch Number: 2995 Loss: 1.6564909219741821 Time taken: 0.32749080657958984\n",
            "Batch Number: 2996 Loss: 1.63643479347229 Time taken: 0.31517553329467773\n",
            "Batch Number: 2997 Loss: 1.6420859098434448 Time taken: 0.3114278316497803\n",
            "Batch Number: 2998 Loss: 1.6100307703018188 Time taken: 0.32221484184265137\n",
            "Batch Number: 2999 Loss: 1.6436480283737183 Time taken: 0.304401159286499\n",
            "Batch Number: 3000 Loss: 1.6390150785446167 Time taken: 0.3151693344116211\n",
            "Batch Number: 3001 Loss: 1.6440659761428833 Time taken: 0.32024526596069336\n",
            "Batch Number: 3002 Loss: 1.6627825498580933 Time taken: 0.2979464530944824\n",
            "Batch Number: 3003 Loss: 1.633846402168274 Time taken: 0.29488253593444824\n",
            "Batch Number: 3004 Loss: 1.6086074113845825 Time taken: 0.33175206184387207\n",
            "Batch Number: 3005 Loss: 1.624321460723877 Time taken: 0.3060600757598877\n",
            "Batch Number: 3006 Loss: 1.6359813213348389 Time taken: 0.31357622146606445\n",
            "Batch Number: 3007 Loss: 1.6321662664413452 Time taken: 0.31485438346862793\n",
            "Batch Number: 3008 Loss: 1.6483389139175415 Time taken: 0.31642794609069824\n",
            "Batch Number: 3009 Loss: 1.6340630054473877 Time taken: 0.30854129791259766\n",
            "Batch Number: 3010 Loss: 1.6399823427200317 Time taken: 0.30841660499572754\n",
            "Batch Number: 3011 Loss: 1.6244101524353027 Time taken: 0.32091569900512695\n",
            "Batch Number: 3012 Loss: 1.6081129312515259 Time taken: 0.30548691749572754\n",
            "Batch Number: 3013 Loss: 1.6513603925704956 Time taken: 0.31275486946105957\n",
            "Batch Number: 3014 Loss: 1.61247718334198 Time taken: 0.32604241371154785\n",
            "Batch Number: 3015 Loss: 1.6112594604492188 Time taken: 0.31012415885925293\n",
            "Batch Number: 3016 Loss: 1.5727773904800415 Time taken: 0.31212949752807617\n",
            "Batch Number: 3017 Loss: 1.603389859199524 Time taken: 0.3217182159423828\n",
            "Batch Number: 3018 Loss: 1.5984001159667969 Time taken: 0.3085758686065674\n",
            "Batch Number: 3019 Loss: 1.6142085790634155 Time taken: 0.31252050399780273\n",
            "Batch Number: 3020 Loss: 1.6109615564346313 Time taken: 0.3118295669555664\n",
            "Batch Number: 3021 Loss: 1.6468204259872437 Time taken: 0.2929501533508301\n",
            "Batch Number: 3022 Loss: 1.6525052785873413 Time taken: 0.2959442138671875\n",
            "Batch Number: 3023 Loss: 1.63645339012146 Time taken: 0.320753812789917\n",
            "Batch Number: 3024 Loss: 1.6038111448287964 Time taken: 0.3121323585510254\n",
            "Batch Number: 3025 Loss: 1.6453685760498047 Time taken: 0.2910926342010498\n",
            "Batch Number: 3026 Loss: 1.6020315885543823 Time taken: 0.3093898296356201\n",
            "Batch Number: 3027 Loss: 1.6334949731826782 Time taken: 0.3145921230316162\n",
            "Batch Number: 3028 Loss: 1.624021291732788 Time taken: 0.2990450859069824\n",
            "Batch Number: 3029 Loss: 1.6135189533233643 Time taken: 0.3229024410247803\n",
            "Batch Number: 3030 Loss: 1.6388602256774902 Time taken: 0.3148610591888428\n",
            "Batch Number: 3031 Loss: 1.6028169393539429 Time taken: 0.29694318771362305\n",
            "Batch Number: 3032 Loss: 1.6356871128082275 Time taken: 0.2993955612182617\n",
            "Batch Number: 3033 Loss: 1.6078721284866333 Time taken: 0.30841588973999023\n",
            "Batch Number: 3034 Loss: 1.6129562854766846 Time taken: 0.3023662567138672\n",
            "Batch Number: 3035 Loss: 1.616054654121399 Time taken: 0.29943156242370605\n",
            "Batch Number: 3036 Loss: 1.6354374885559082 Time taken: 0.31067681312561035\n",
            "Batch Number: 3037 Loss: 1.6600761413574219 Time taken: 0.31769227981567383\n",
            "Batch Number: 3038 Loss: 1.6450598239898682 Time taken: 0.30222272872924805\n",
            "Batch Number: 3039 Loss: 1.6538455486297607 Time taken: 0.31967854499816895\n",
            "Batch Number: 3040 Loss: 1.6368716955184937 Time taken: 0.31897950172424316\n",
            "Batch Number: 3041 Loss: 1.6422626972198486 Time taken: 0.3025195598602295\n",
            "Batch Number: 3042 Loss: 1.623845100402832 Time taken: 0.3111557960510254\n",
            "Batch Number: 3043 Loss: 1.6422327756881714 Time taken: 0.3175523281097412\n",
            "Batch Number: 3044 Loss: 1.6239837408065796 Time taken: 0.3006899356842041\n",
            "Batch Number: 3045 Loss: 1.6188483238220215 Time taken: 0.29911255836486816\n",
            "Batch Number: 3046 Loss: 1.598905324935913 Time taken: 0.3127889633178711\n",
            "Batch Number: 3047 Loss: 1.5830328464508057 Time taken: 0.3044297695159912\n",
            "Batch Number: 3048 Loss: 1.6009392738342285 Time taken: 0.30385780334472656\n",
            "Batch Number: 3049 Loss: 1.6071605682373047 Time taken: 0.3261997699737549\n",
            "Batch Number: 3050 Loss: 1.618580937385559 Time taken: 0.3240370750427246\n",
            "Batch Number: 3051 Loss: 1.622596025466919 Time taken: 0.3055284023284912\n",
            "Batch Number: 3052 Loss: 1.6395671367645264 Time taken: 0.3099503517150879\n",
            "Batch Number: 3053 Loss: 1.6052501201629639 Time taken: 0.3140232563018799\n",
            "Batch Number: 3054 Loss: 1.6112613677978516 Time taken: 0.29749083518981934\n",
            "Batch Number: 3055 Loss: 1.5997332334518433 Time taken: 0.3074829578399658\n",
            "Batch Number: 3056 Loss: 1.621532678604126 Time taken: 0.3130354881286621\n",
            "Batch Number: 3057 Loss: 1.6121881008148193 Time taken: 0.3028426170349121\n",
            "Batch Number: 3058 Loss: 1.6063404083251953 Time taken: 0.3096046447753906\n",
            "Batch Number: 3059 Loss: 1.6031296253204346 Time taken: 0.326002836227417\n",
            "Batch Number: 3060 Loss: 1.6172329187393188 Time taken: 0.3095097541809082\n",
            "Batch Number: 3061 Loss: 1.6068195104599 Time taken: 0.3099837303161621\n",
            "Batch Number: 3062 Loss: 1.5926895141601562 Time taken: 0.31476712226867676\n",
            "Batch Number: 3063 Loss: 1.5958681106567383 Time taken: 0.3097817897796631\n",
            "Batch Number: 3064 Loss: 1.5651320219039917 Time taken: 0.307858943939209\n",
            "Batch Number: 3065 Loss: 1.5470020771026611 Time taken: 0.3267641067504883\n",
            "Batch Number: 3066 Loss: 1.5800094604492188 Time taken: 0.31951141357421875\n",
            "Batch Number: 3067 Loss: 1.5902389287948608 Time taken: 0.3030881881713867\n",
            "Batch Number: 3068 Loss: 1.6043330430984497 Time taken: 0.31596970558166504\n",
            "Batch Number: 3069 Loss: 1.6006724834442139 Time taken: 0.3142526149749756\n",
            "Batch Number: 3070 Loss: 1.5610822439193726 Time taken: 0.3156602382659912\n",
            "Batch Number: 3071 Loss: 1.6148253679275513 Time taken: 0.3155522346496582\n",
            "Batch Number: 3072 Loss: 1.580783724784851 Time taken: 0.32503509521484375\n",
            "Batch Number: 3073 Loss: 1.605674147605896 Time taken: 0.318650484085083\n",
            "Batch Number: 3074 Loss: 1.6009464263916016 Time taken: 0.3131277561187744\n",
            "Batch Number: 3075 Loss: 1.6211740970611572 Time taken: 0.32523083686828613\n",
            "Batch Number: 3076 Loss: 1.604046106338501 Time taken: 0.31275081634521484\n",
            "Batch Number: 3077 Loss: 1.5950381755828857 Time taken: 0.3136005401611328\n",
            "Batch Number: 3078 Loss: 1.5970934629440308 Time taken: 0.31293606758117676\n",
            "Batch Number: 3079 Loss: 1.5870020389556885 Time taken: 0.31017208099365234\n",
            "Batch Number: 3080 Loss: 1.5889652967453003 Time taken: 0.3096306324005127\n",
            "Batch Number: 3081 Loss: 1.5777230262756348 Time taken: 0.3297309875488281\n",
            "Batch Number: 3082 Loss: 1.5998674631118774 Time taken: 0.32793498039245605\n",
            "Batch Number: 3083 Loss: 1.6056849956512451 Time taken: 0.31297779083251953\n",
            "Batch Number: 3084 Loss: 1.610067367553711 Time taken: 0.32543301582336426\n",
            "Batch Number: 3085 Loss: 1.5892589092254639 Time taken: 0.32520151138305664\n",
            "Batch Number: 3086 Loss: 1.5868134498596191 Time taken: 0.32314491271972656\n",
            "Batch Number: 3087 Loss: 1.5785250663757324 Time taken: 0.3136332035064697\n",
            "Batch Number: 3088 Loss: 1.591525912284851 Time taken: 0.32412195205688477\n",
            "Batch Number: 3089 Loss: 1.6023329496383667 Time taken: 0.3036513328552246\n",
            "Batch Number: 3090 Loss: 1.602784276008606 Time taken: 0.324077844619751\n",
            "Batch Number: 3091 Loss: 1.5830981731414795 Time taken: 0.3168792724609375\n",
            "Batch Number: 3092 Loss: 1.593069076538086 Time taken: 0.3130931854248047\n",
            "Batch Number: 3093 Loss: 1.5985883474349976 Time taken: 0.32785725593566895\n",
            "Batch Number: 3094 Loss: 1.5810412168502808 Time taken: 0.3245389461517334\n",
            "Batch Number: 3095 Loss: 1.5848520994186401 Time taken: 0.3121769428253174\n",
            "Batch Number: 3096 Loss: 1.5984293222427368 Time taken: 0.3280041217803955\n",
            "Batch Number: 3097 Loss: 1.6131404638290405 Time taken: 0.32633256912231445\n",
            "Batch Number: 3098 Loss: 1.6078965663909912 Time taken: 0.3126988410949707\n",
            "Batch Number: 3099 Loss: 1.6231952905654907 Time taken: 0.32424402236938477\n",
            "Batch Number: 3100 Loss: 1.622387170791626 Time taken: 0.3134329319000244\n",
            "Batch Number: 3101 Loss: 1.5957225561141968 Time taken: 0.30805158615112305\n",
            "Batch Number: 3102 Loss: 1.6350741386413574 Time taken: 0.3195912837982178\n",
            "Batch Number: 3103 Loss: 1.6370537281036377 Time taken: 0.31548070907592773\n",
            "Batch Number: 3104 Loss: 1.6100035905838013 Time taken: 0.30444788932800293\n",
            "Batch Number: 3105 Loss: 1.5801401138305664 Time taken: 0.3105049133300781\n",
            "Batch Number: 3106 Loss: 1.618235468864441 Time taken: 0.304105281829834\n",
            "Batch Number: 3107 Loss: 1.5934088230133057 Time taken: 0.3135519027709961\n",
            "Batch Number: 3108 Loss: 1.6208882331848145 Time taken: 0.3097250461578369\n",
            "Batch Number: 3109 Loss: 1.6174272298812866 Time taken: 0.30521154403686523\n",
            "Batch Number: 3110 Loss: 1.581031084060669 Time taken: 0.31047916412353516\n",
            "Batch Number: 3111 Loss: 1.6082955598831177 Time taken: 0.3067340850830078\n",
            "Batch Number: 3112 Loss: 1.6115459203720093 Time taken: 0.3145887851715088\n",
            "Batch Number: 3113 Loss: 1.5838661193847656 Time taken: 0.3093135356903076\n",
            "Batch Number: 3114 Loss: 1.6064858436584473 Time taken: 0.30181002616882324\n",
            "Batch Number: 3115 Loss: 1.574406623840332 Time taken: 0.308483362197876\n",
            "Batch Number: 3116 Loss: 1.6013367176055908 Time taken: 0.3079054355621338\n",
            "Batch Number: 3117 Loss: 1.6182447671890259 Time taken: 0.30643415451049805\n",
            "Batch Number: 3118 Loss: 1.5903388261795044 Time taken: 0.3140525817871094\n",
            "Batch Number: 3119 Loss: 1.6133086681365967 Time taken: 0.30278515815734863\n",
            "Batch Number: 3120 Loss: 1.5999294519424438 Time taken: 0.3193833827972412\n",
            "Batch Number: 3121 Loss: 1.5745259523391724 Time taken: 0.3155710697174072\n",
            "Batch Number: 3122 Loss: 1.592846155166626 Time taken: 0.304807186126709\n",
            "Batch Number: 3123 Loss: 1.573264718055725 Time taken: 0.3179349899291992\n",
            "Batch Number: 3124 Loss: 1.595254898071289 Time taken: 0.3035736083984375\n",
            "Batch Number: 3125 Loss: 1.5901391506195068 Time taken: 0.30996227264404297\n",
            "Batch Number: 3126 Loss: 1.6031771898269653 Time taken: 0.3094298839569092\n",
            "Batch Number: 3127 Loss: 1.6065468788146973 Time taken: 0.3055875301361084\n",
            "Batch Number: 3128 Loss: 1.6155915260314941 Time taken: 0.30756163597106934\n",
            "Batch Number: 3129 Loss: 1.6044793128967285 Time taken: 0.3059265613555908\n",
            "Batch Number: 3130 Loss: 1.6001224517822266 Time taken: 0.3077399730682373\n",
            "Batch Number: 3131 Loss: 1.6503793001174927 Time taken: 0.31673312187194824\n",
            "Batch Number: 3132 Loss: 1.5872321128845215 Time taken: 0.30029916763305664\n",
            "Batch Number: 3133 Loss: 1.6229560375213623 Time taken: 0.31864190101623535\n",
            "Batch Number: 3134 Loss: 1.6128787994384766 Time taken: 0.3070073127746582\n",
            "Batch Number: 3135 Loss: 1.6417659521102905 Time taken: 0.29703640937805176\n",
            "Batch Number: 3136 Loss: 1.6277209520339966 Time taken: 0.3180093765258789\n",
            "Batch Number: 3137 Loss: 1.6281754970550537 Time taken: 0.3134949207305908\n",
            "Batch Number: 3138 Loss: 1.6191948652267456 Time taken: 0.30602574348449707\n",
            "Batch Number: 3139 Loss: 1.6092497110366821 Time taken: 0.3130984306335449\n",
            "Batch Number: 3140 Loss: 1.6128528118133545 Time taken: 0.3067026138305664\n",
            "Batch Number: 3141 Loss: 1.611369252204895 Time taken: 0.31705570220947266\n",
            "Batch Number: 3142 Loss: 1.6071734428405762 Time taken: 0.3159041404724121\n",
            "Batch Number: 3143 Loss: 1.597934603691101 Time taken: 0.3039684295654297\n",
            "Batch Number: 3144 Loss: 1.574791669845581 Time taken: 0.3114931583404541\n",
            "Batch Number: 3145 Loss: 1.5990593433380127 Time taken: 0.3136758804321289\n",
            "Batch Number: 3146 Loss: 1.5927430391311646 Time taken: 0.3140723705291748\n",
            "Batch Number: 3147 Loss: 1.576702356338501 Time taken: 0.31165266036987305\n",
            "Batch Number: 3148 Loss: 1.5624639987945557 Time taken: 0.3049137592315674\n",
            "Batch Number: 3149 Loss: 1.5790464878082275 Time taken: 0.3192787170410156\n",
            "Batch Number: 3150 Loss: 1.5821551084518433 Time taken: 0.3160533905029297\n",
            "Batch Number: 3151 Loss: 1.587556004524231 Time taken: 0.3061978816986084\n",
            "Batch Number: 3152 Loss: 1.583458662033081 Time taken: 0.32100677490234375\n",
            "Batch Number: 3153 Loss: 1.6321436166763306 Time taken: 0.3129749298095703\n",
            "Batch Number: 3154 Loss: 1.6198993921279907 Time taken: 0.3044164180755615\n",
            "Batch Number: 3155 Loss: 1.6056454181671143 Time taken: 0.3062410354614258\n",
            "Batch Number: 3156 Loss: 1.627557635307312 Time taken: 0.30301570892333984\n",
            "Batch Number: 3157 Loss: 1.5949937105178833 Time taken: 0.31194114685058594\n",
            "Batch Number: 3158 Loss: 1.6366212368011475 Time taken: 0.30808329582214355\n",
            "Batch Number: 3159 Loss: 1.6490776538848877 Time taken: 0.32299327850341797\n",
            "Batch Number: 3160 Loss: 1.6262608766555786 Time taken: 0.3183927536010742\n",
            "Batch Number: 3161 Loss: 1.639420986175537 Time taken: 0.3027322292327881\n",
            "Batch Number: 3162 Loss: 1.6150027513504028 Time taken: 0.3110806941986084\n",
            "Batch Number: 3163 Loss: 1.6157516241073608 Time taken: 0.30422377586364746\n",
            "Batch Number: 3164 Loss: 1.6049773693084717 Time taken: 0.2973310947418213\n",
            "Batch Number: 3165 Loss: 1.616714596748352 Time taken: 0.31768274307250977\n",
            "Batch Number: 3166 Loss: 1.628983974456787 Time taken: 0.30729198455810547\n",
            "Batch Number: 3167 Loss: 1.6411000490188599 Time taken: 0.3152782917022705\n",
            "Batch Number: 3168 Loss: 1.6699899435043335 Time taken: 0.3174002170562744\n",
            "Batch Number: 3169 Loss: 1.6278092861175537 Time taken: 0.30664539337158203\n",
            "Batch Number: 3170 Loss: 1.64457106590271 Time taken: 0.30585718154907227\n",
            "Batch Number: 3171 Loss: 1.668373703956604 Time taken: 0.3072335720062256\n",
            "Batch Number: 3172 Loss: 1.7025922536849976 Time taken: 0.3061184883117676\n",
            "Batch Number: 3173 Loss: 1.6510635614395142 Time taken: 0.31656789779663086\n",
            "Batch Number: 3174 Loss: 1.660318374633789 Time taken: 0.2981421947479248\n",
            "Batch Number: 3175 Loss: 1.6235404014587402 Time taken: 0.3192715644836426\n",
            "Batch Number: 3176 Loss: 1.6253119707107544 Time taken: 0.3100404739379883\n",
            "Batch Number: 3177 Loss: 1.6048321723937988 Time taken: 0.31108832359313965\n",
            "Batch Number: 3178 Loss: 1.6172473430633545 Time taken: 0.3169119358062744\n",
            "Batch Number: 3179 Loss: 1.6422735452651978 Time taken: 0.313413143157959\n",
            "Batch Number: 3180 Loss: 1.6469885110855103 Time taken: 0.3060164451599121\n",
            "Batch Number: 3181 Loss: 1.612900733947754 Time taken: 0.31789207458496094\n",
            "Batch Number: 3182 Loss: 1.6127989292144775 Time taken: 0.3056373596191406\n",
            "Batch Number: 3183 Loss: 1.617302656173706 Time taken: 0.3072960376739502\n",
            "Batch Number: 3184 Loss: 1.6226286888122559 Time taken: 0.31797075271606445\n",
            "Batch Number: 3185 Loss: 1.618630290031433 Time taken: 0.31805849075317383\n",
            "Batch Number: 3186 Loss: 1.6224936246871948 Time taken: 0.3060739040374756\n",
            "Batch Number: 3187 Loss: 1.6188621520996094 Time taken: 0.30913877487182617\n",
            "Batch Number: 3188 Loss: 1.6159744262695312 Time taken: 0.30406618118286133\n",
            "Batch Number: 3189 Loss: 1.6064403057098389 Time taken: 0.3193974494934082\n",
            "Batch Number: 3190 Loss: 1.617214322090149 Time taken: 0.3105597496032715\n",
            "Batch Number: 3191 Loss: 1.6258447170257568 Time taken: 0.3135557174682617\n",
            "Batch Number: 3192 Loss: 1.5858149528503418 Time taken: 0.31609296798706055\n",
            "Batch Number: 3193 Loss: 1.5934754610061646 Time taken: 0.31238865852355957\n",
            "Batch Number: 3194 Loss: 1.5916458368301392 Time taken: 0.3277406692504883\n",
            "Batch Number: 3195 Loss: 1.581153392791748 Time taken: 0.3089721202850342\n",
            "Batch Number: 3196 Loss: 1.5686631202697754 Time taken: 0.2992730140686035\n",
            "Batch Number: 3197 Loss: 1.6055468320846558 Time taken: 0.3175201416015625\n",
            "Batch Number: 3198 Loss: 1.5834705829620361 Time taken: 0.3094944953918457\n",
            "Batch Number: 3199 Loss: 1.5611721277236938 Time taken: 0.304340124130249\n",
            "Batch Number: 3200 Loss: 1.6292383670806885 Time taken: 0.30733513832092285\n",
            "Batch Number: 3201 Loss: 1.6577098369598389 Time taken: 0.3101639747619629\n",
            "Batch Number: 3202 Loss: 1.6194528341293335 Time taken: 0.30613040924072266\n",
            "Batch Number: 3203 Loss: 1.6300616264343262 Time taken: 0.3150913715362549\n",
            "Batch Number: 3204 Loss: 1.6024848222732544 Time taken: 0.31192970275878906\n",
            "Batch Number: 3205 Loss: 1.6026486158370972 Time taken: 0.31455206871032715\n",
            "Batch Number: 3206 Loss: 1.6144839525222778 Time taken: 0.30281496047973633\n",
            "Batch Number: 3207 Loss: 1.6239949464797974 Time taken: 0.3127126693725586\n",
            "Batch Number: 3208 Loss: 1.6057900190353394 Time taken: 0.31505584716796875\n",
            "Batch Number: 3209 Loss: 1.6057188510894775 Time taken: 0.3045322895050049\n",
            "Batch Number: 3210 Loss: 1.6200037002563477 Time taken: 0.30759477615356445\n",
            "Batch Number: 3211 Loss: 1.620901346206665 Time taken: 0.31081199645996094\n",
            "Batch Number: 3212 Loss: 1.622145175933838 Time taken: 0.3149127960205078\n",
            "Batch Number: 3213 Loss: 1.6192598342895508 Time taken: 0.31154561042785645\n",
            "Batch Number: 3214 Loss: 1.6179823875427246 Time taken: 0.31071949005126953\n",
            "Batch Number: 3215 Loss: 1.6137982606887817 Time taken: 0.31081318855285645\n",
            "Batch Number: 3216 Loss: 1.6070671081542969 Time taken: 0.3108837604522705\n",
            "Batch Number: 3217 Loss: 1.6131317615509033 Time taken: 0.31746745109558105\n",
            "Batch Number: 3218 Loss: 1.6362266540527344 Time taken: 0.3024287223815918\n",
            "Batch Number: 3219 Loss: 1.611220359802246 Time taken: 0.30394935607910156\n",
            "Batch Number: 3220 Loss: 1.6179648637771606 Time taken: 0.31534528732299805\n",
            "Batch Number: 3221 Loss: 1.6306885480880737 Time taken: 0.32238292694091797\n",
            "Batch Number: 3222 Loss: 1.6369225978851318 Time taken: 0.29938840866088867\n",
            "Batch Number: 3223 Loss: 1.5875972509384155 Time taken: 0.32082152366638184\n",
            "Batch Number: 3224 Loss: 1.6029695272445679 Time taken: 0.3097846508026123\n",
            "Batch Number: 3225 Loss: 1.6105276346206665 Time taken: 0.2994372844696045\n",
            "Batch Number: 3226 Loss: 1.6028927564620972 Time taken: 0.3139026165008545\n",
            "Batch Number: 3227 Loss: 1.5823293924331665 Time taken: 0.3053619861602783\n",
            "Batch Number: 3228 Loss: 1.600502610206604 Time taken: 0.30954980850219727\n",
            "Batch Number: 3229 Loss: 1.6014752388000488 Time taken: 0.3139479160308838\n",
            "Batch Number: 3230 Loss: 1.5769989490509033 Time taken: 0.3170347213745117\n",
            "Batch Number: 3231 Loss: 1.6097716093063354 Time taken: 0.31297802925109863\n",
            "Batch Number: 3232 Loss: 1.5923473834991455 Time taken: 0.30283188819885254\n",
            "Batch Number: 3233 Loss: 1.5814076662063599 Time taken: 0.31849026679992676\n",
            "Batch Number: 3234 Loss: 1.5817899703979492 Time taken: 0.2979860305786133\n",
            "Batch Number: 3235 Loss: 1.5789997577667236 Time taken: 0.2999386787414551\n",
            "Batch Number: 3236 Loss: 1.5863534212112427 Time taken: 0.31845879554748535\n",
            "Batch Number: 3237 Loss: 1.575429916381836 Time taken: 0.3092777729034424\n",
            "Batch Number: 3238 Loss: 1.5953255891799927 Time taken: 0.3042275905609131\n",
            "Batch Number: 3239 Loss: 1.5788519382476807 Time taken: 0.30919790267944336\n",
            "Batch Number: 3240 Loss: 1.6120349168777466 Time taken: 0.31357502937316895\n",
            "Batch Number: 3241 Loss: 1.5855982303619385 Time taken: 0.30213093757629395\n",
            "Batch Number: 3242 Loss: 1.5760498046875 Time taken: 0.3234982490539551\n",
            "Batch Number: 3243 Loss: 1.5565179586410522 Time taken: 0.31987953186035156\n",
            "Batch Number: 3244 Loss: 1.571351170539856 Time taken: 0.3036339282989502\n",
            "Batch Number: 3245 Loss: 1.5450799465179443 Time taken: 0.3105344772338867\n",
            "Batch Number: 3246 Loss: 1.5635173320770264 Time taken: 0.32428550720214844\n",
            "Batch Number: 3247 Loss: 1.5791704654693604 Time taken: 0.3052995204925537\n",
            "Batch Number: 3248 Loss: 1.5398483276367188 Time taken: 0.30806541442871094\n",
            "Batch Number: 3249 Loss: 1.5402055978775024 Time taken: 0.3198111057281494\n",
            "Batch Number: 3250 Loss: 1.5555343627929688 Time taken: 0.30593037605285645\n",
            "Batch Number: 3251 Loss: 1.5762640237808228 Time taken: 0.30719947814941406\n",
            "Batch Number: 3252 Loss: 1.5760283470153809 Time taken: 0.32355427742004395\n",
            "Batch Number: 3253 Loss: 1.6133759021759033 Time taken: 0.3053104877471924\n",
            "Batch Number: 3254 Loss: 1.5825914144515991 Time taken: 0.31041669845581055\n",
            "Batch Number: 3255 Loss: 1.6050502061843872 Time taken: 0.3153808116912842\n",
            "Batch Number: 3256 Loss: 1.576460838317871 Time taken: 0.30651068687438965\n",
            "Batch Number: 3257 Loss: 1.5723929405212402 Time taken: 0.3041527271270752\n",
            "Batch Number: 3258 Loss: 1.5780857801437378 Time taken: 0.31430697441101074\n",
            "Batch Number: 3259 Loss: 1.5785725116729736 Time taken: 0.3066680431365967\n",
            "Batch Number: 3260 Loss: 1.5959436893463135 Time taken: 0.310805082321167\n",
            "Batch Number: 3261 Loss: 1.5642318725585938 Time taken: 0.30391383171081543\n",
            "Batch Number: 3262 Loss: 1.5854449272155762 Time taken: 0.3310070037841797\n",
            "Batch Number: 3263 Loss: 1.5726357698440552 Time taken: 0.2998025417327881\n",
            "Batch Number: 3264 Loss: 1.576939582824707 Time taken: 0.3139462471008301\n",
            "Batch Number: 3265 Loss: 1.5838525295257568 Time taken: 0.32745838165283203\n",
            "Batch Number: 3266 Loss: 1.5899053812026978 Time taken: 0.3070197105407715\n",
            "Batch Number: 3267 Loss: 1.5819191932678223 Time taken: 0.29825329780578613\n",
            "Batch Number: 3268 Loss: 1.5874403715133667 Time taken: 0.32013845443725586\n",
            "Batch Number: 3269 Loss: 1.5700511932373047 Time taken: 0.31200742721557617\n",
            "Batch Number: 3270 Loss: 1.5545295476913452 Time taken: 0.30473828315734863\n",
            "Batch Number: 3271 Loss: 1.572208285331726 Time taken: 0.3206355571746826\n",
            "Batch Number: 3272 Loss: 1.5669862031936646 Time taken: 0.338397741317749\n",
            "Batch Number: 3273 Loss: 1.5618661642074585 Time taken: 0.30934572219848633\n",
            "Batch Number: 3274 Loss: 1.5791031122207642 Time taken: 0.32795071601867676\n",
            "Batch Number: 3275 Loss: 1.5893774032592773 Time taken: 0.31382322311401367\n",
            "Batch Number: 3276 Loss: 1.6111139059066772 Time taken: 0.3002181053161621\n",
            "Batch Number: 3277 Loss: 1.6092872619628906 Time taken: 0.31029438972473145\n",
            "Batch Number: 3278 Loss: 1.6001726388931274 Time taken: 0.31116437911987305\n",
            "Batch Number: 3279 Loss: 1.5977190732955933 Time taken: 0.31299781799316406\n",
            "Batch Number: 3280 Loss: 1.5737429857254028 Time taken: 0.2998366355895996\n",
            "Batch Number: 3281 Loss: 1.590861201286316 Time taken: 0.3211510181427002\n",
            "Batch Number: 3282 Loss: 1.578390121459961 Time taken: 0.30387330055236816\n",
            "Batch Number: 3283 Loss: 1.6103109121322632 Time taken: 0.2973203659057617\n",
            "Batch Number: 3284 Loss: 1.5752124786376953 Time taken: 0.31719493865966797\n",
            "Batch Number: 3285 Loss: 1.5871291160583496 Time taken: 0.30738091468811035\n",
            "Batch Number: 3286 Loss: 1.5752383470535278 Time taken: 0.30864739418029785\n",
            "Batch Number: 3287 Loss: 1.5758693218231201 Time taken: 0.31566405296325684\n",
            "Batch Number: 3288 Loss: 1.5804461240768433 Time taken: 0.3065328598022461\n",
            "Batch Number: 3289 Loss: 1.599219560623169 Time taken: 0.29198670387268066\n",
            "Batch Number: 3290 Loss: 1.5887733697891235 Time taken: 0.29821276664733887\n",
            "Batch Number: 3291 Loss: 1.5806629657745361 Time taken: 0.31868457794189453\n",
            "Batch Number: 3292 Loss: 1.588323712348938 Time taken: 0.30510640144348145\n",
            "Batch Number: 3293 Loss: 1.5690587759017944 Time taken: 0.30300259590148926\n",
            "Batch Number: 3294 Loss: 1.5858417749404907 Time taken: 0.3267035484313965\n",
            "Batch Number: 3295 Loss: 1.5898191928863525 Time taken: 0.3032348155975342\n",
            "Batch Number: 3296 Loss: 1.5976685285568237 Time taken: 0.30284595489501953\n",
            "Batch Number: 3297 Loss: 1.5645506381988525 Time taken: 0.3218071460723877\n",
            "Batch Number: 3298 Loss: 1.574295997619629 Time taken: 0.293548583984375\n",
            "Batch Number: 3299 Loss: 1.5602267980575562 Time taken: 0.29282402992248535\n",
            "Batch Number: 3300 Loss: 1.5715317726135254 Time taken: 0.3036036491394043\n",
            "Batch Number: 3301 Loss: 1.562965989112854 Time taken: 0.3151359558105469\n",
            "Batch Number: 3302 Loss: 1.6132619380950928 Time taken: 0.30252742767333984\n",
            "Batch Number: 3303 Loss: 1.586199164390564 Time taken: 0.3057286739349365\n",
            "Batch Number: 3304 Loss: 1.5747674703598022 Time taken: 0.3176229000091553\n",
            "Batch Number: 3305 Loss: 1.5497987270355225 Time taken: 0.30936336517333984\n",
            "Batch Number: 3306 Loss: 1.5869251489639282 Time taken: 0.30118370056152344\n",
            "Batch Number: 3307 Loss: 1.5980726480484009 Time taken: 0.3294656276702881\n",
            "Batch Number: 3308 Loss: 1.5852535963058472 Time taken: 0.3123509883880615\n",
            "Batch Number: 3309 Loss: 1.5790421962738037 Time taken: 0.310596227645874\n",
            "Batch Number: 3310 Loss: 1.5662096738815308 Time taken: 0.3257300853729248\n",
            "Batch Number: 3311 Loss: 1.5928174257278442 Time taken: 0.29589104652404785\n",
            "Batch Number: 3312 Loss: 1.6015629768371582 Time taken: 0.30475902557373047\n",
            "Batch Number: 3313 Loss: 1.6021451950073242 Time taken: 0.31632566452026367\n",
            "Batch Number: 3314 Loss: 1.582201361656189 Time taken: 0.295473575592041\n",
            "Batch Number: 3315 Loss: 1.5822266340255737 Time taken: 0.30074548721313477\n",
            "Batch Number: 3316 Loss: 1.614356279373169 Time taken: 0.31033778190612793\n",
            "Batch Number: 3317 Loss: 1.6021684408187866 Time taken: 0.3097410202026367\n",
            "Batch Number: 3318 Loss: 1.5772936344146729 Time taken: 0.3069946765899658\n",
            "Batch Number: 3319 Loss: 1.5857843160629272 Time taken: 0.30036163330078125\n",
            "Batch Number: 3320 Loss: 1.5896586179733276 Time taken: 0.31760692596435547\n",
            "Batch Number: 3321 Loss: 1.5765807628631592 Time taken: 0.30490732192993164\n",
            "Batch Number: 3322 Loss: 1.6032874584197998 Time taken: 0.3043098449707031\n",
            "Batch Number: 3323 Loss: 1.6069780588150024 Time taken: 0.32185816764831543\n",
            "Batch Number: 3324 Loss: 1.5930309295654297 Time taken: 0.30718278884887695\n",
            "Batch Number: 3325 Loss: 1.5919499397277832 Time taken: 0.30990099906921387\n",
            "Batch Number: 3326 Loss: 1.5967563390731812 Time taken: 0.32302165031433105\n",
            "Batch Number: 3327 Loss: 1.5854374170303345 Time taken: 0.3022725582122803\n",
            "Batch Number: 3328 Loss: 1.570824146270752 Time taken: 0.3002047538757324\n",
            "Batch Number: 3329 Loss: 1.588906168937683 Time taken: 0.3142509460449219\n",
            "Batch Number: 3330 Loss: 1.5708787441253662 Time taken: 0.3042600154876709\n",
            "Batch Number: 3331 Loss: 1.597493290901184 Time taken: 0.3023264408111572\n",
            "Batch Number: 3332 Loss: 1.5835014581680298 Time taken: 0.3009648323059082\n",
            "Batch Number: 3333 Loss: 1.5745484828948975 Time taken: 0.32433056831359863\n",
            "Batch Number: 3334 Loss: 1.5725334882736206 Time taken: 0.30078577995300293\n",
            "Batch Number: 3335 Loss: 1.588923454284668 Time taken: 0.30896711349487305\n",
            "Batch Number: 3336 Loss: 1.6288965940475464 Time taken: 0.32020115852355957\n",
            "Batch Number: 3337 Loss: 1.6285395622253418 Time taken: 0.31018757820129395\n",
            "Batch Number: 3338 Loss: 1.5912762880325317 Time taken: 0.30687952041625977\n",
            "Batch Number: 3339 Loss: 1.6398200988769531 Time taken: 0.3117368221282959\n",
            "Batch Number: 3340 Loss: 1.6105232238769531 Time taken: 0.3019716739654541\n",
            "Batch Number: 3341 Loss: 1.5923351049423218 Time taken: 0.29769396781921387\n",
            "Batch Number: 3342 Loss: 1.620856761932373 Time taken: 0.30257081985473633\n",
            "Batch Number: 3343 Loss: 1.6090863943099976 Time taken: 0.310929536819458\n",
            "Batch Number: 3344 Loss: 1.6438045501708984 Time taken: 0.29386019706726074\n",
            "Batch Number: 3345 Loss: 1.6359118223190308 Time taken: 0.29776692390441895\n",
            "Batch Number: 3346 Loss: 1.614661455154419 Time taken: 0.3237588405609131\n",
            "Batch Number: 3347 Loss: 1.629406452178955 Time taken: 0.3038356304168701\n",
            "Batch Number: 3348 Loss: 1.6487842798233032 Time taken: 0.29710865020751953\n",
            "Batch Number: 3349 Loss: 1.6161251068115234 Time taken: 0.3239760398864746\n",
            "Batch Number: 3350 Loss: 1.6226692199707031 Time taken: 0.3153343200683594\n",
            "Batch Number: 3351 Loss: 1.6762969493865967 Time taken: 0.3037886619567871\n",
            "Batch Number: 3352 Loss: 1.6353058815002441 Time taken: 0.3187265396118164\n",
            "Batch Number: 3353 Loss: 1.6295911073684692 Time taken: 0.30250024795532227\n",
            "Batch Number: 3354 Loss: 1.6248431205749512 Time taken: 0.3092689514160156\n",
            "Batch Number: 3355 Loss: 1.5903527736663818 Time taken: 0.3096327781677246\n",
            "Batch Number: 3356 Loss: 1.6188204288482666 Time taken: 0.31267714500427246\n",
            "Batch Number: 3357 Loss: 1.5943034887313843 Time taken: 0.3023641109466553\n",
            "Batch Number: 3358 Loss: 1.5742958784103394 Time taken: 0.3005251884460449\n",
            "Batch Number: 3359 Loss: 1.5953900814056396 Time taken: 0.331906795501709\n",
            "Batch Number: 3360 Loss: 1.5765819549560547 Time taken: 0.3015732765197754\n",
            "Batch Number: 3361 Loss: 1.5821126699447632 Time taken: 0.30237627029418945\n",
            "Batch Number: 3362 Loss: 1.6076669692993164 Time taken: 0.3218865394592285\n",
            "Batch Number: 3363 Loss: 1.5872455835342407 Time taken: 0.2975485324859619\n",
            "Batch Number: 3364 Loss: 1.593336582183838 Time taken: 0.3015010356903076\n",
            "Batch Number: 3365 Loss: 1.589934229850769 Time taken: 0.3168492317199707\n",
            "Batch Number: 3366 Loss: 1.5917251110076904 Time taken: 0.30507755279541016\n",
            "Batch Number: 3367 Loss: 1.5942431688308716 Time taken: 0.3052196502685547\n",
            "Batch Number: 3368 Loss: 1.6010955572128296 Time taken: 0.3135256767272949\n",
            "Batch Number: 3369 Loss: 1.6066359281539917 Time taken: 0.3163180351257324\n",
            "Batch Number: 3370 Loss: 1.5980408191680908 Time taken: 0.31488752365112305\n",
            "Batch Number: 3371 Loss: 1.600901484489441 Time taken: 0.3049602508544922\n",
            "Batch Number: 3372 Loss: 1.5547977685928345 Time taken: 0.3122224807739258\n",
            "Batch Number: 3373 Loss: 1.602110743522644 Time taken: 0.3007686138153076\n",
            "Batch Number: 3374 Loss: 1.5790698528289795 Time taken: 0.2999396324157715\n",
            "Batch Number: 3375 Loss: 1.577309012413025 Time taken: 0.3244307041168213\n",
            "Batch Number: 3376 Loss: 1.5710620880126953 Time taken: 0.3102235794067383\n",
            "Batch Number: 3377 Loss: 1.5672467947006226 Time taken: 0.2936522960662842\n",
            "Batch Number: 3378 Loss: 1.558730959892273 Time taken: 0.31946396827697754\n",
            "Batch Number: 3379 Loss: 1.575660228729248 Time taken: 0.3113431930541992\n",
            "Batch Number: 3380 Loss: 1.5962201356887817 Time taken: 0.3106563091278076\n",
            "Batch Number: 3381 Loss: 1.6196942329406738 Time taken: 0.33133578300476074\n",
            "Batch Number: 3382 Loss: 1.5655862092971802 Time taken: 0.30927157402038574\n",
            "Batch Number: 3383 Loss: 1.6163275241851807 Time taken: 0.3091893196105957\n",
            "Batch Number: 3384 Loss: 1.6113543510437012 Time taken: 0.3156611919403076\n",
            "Batch Number: 3385 Loss: 1.6009396314620972 Time taken: 0.3176565170288086\n",
            "Batch Number: 3386 Loss: 1.606394648551941 Time taken: 0.30230212211608887\n",
            "Batch Number: 3387 Loss: 1.5875802040100098 Time taken: 0.3185243606567383\n",
            "Batch Number: 3388 Loss: 1.5996918678283691 Time taken: 0.32932472229003906\n",
            "Batch Number: 3389 Loss: 1.568508267402649 Time taken: 0.3094441890716553\n",
            "Batch Number: 3390 Loss: 1.5724656581878662 Time taken: 0.3133504390716553\n",
            "Batch Number: 3391 Loss: 1.6185287237167358 Time taken: 0.31272220611572266\n",
            "Batch Number: 3392 Loss: 1.604661226272583 Time taken: 0.29323339462280273\n",
            "Batch Number: 3393 Loss: 1.5957400798797607 Time taken: 0.2970588207244873\n",
            "Batch Number: 3394 Loss: 1.585837721824646 Time taken: 0.30048084259033203\n",
            "Batch Number: 3395 Loss: 1.5732018947601318 Time taken: 0.2897329330444336\n",
            "Batch Number: 3396 Loss: 1.578551173210144 Time taken: 0.290083646774292\n",
            "Batch Number: 3397 Loss: 1.615896224975586 Time taken: 0.30428314208984375\n",
            "Batch Number: 3398 Loss: 1.601560354232788 Time taken: 0.31095385551452637\n",
            "Batch Number: 3399 Loss: 1.6049809455871582 Time taken: 0.30453991889953613\n",
            "Batch Number: 3400 Loss: 1.5886132717132568 Time taken: 0.3062279224395752\n",
            "Batch Number: 3401 Loss: 1.595330834388733 Time taken: 0.3096756935119629\n",
            "Batch Number: 3402 Loss: 1.5890284776687622 Time taken: 0.30492639541625977\n",
            "Batch Number: 3403 Loss: 1.574629306793213 Time taken: 0.31176328659057617\n",
            "Batch Number: 3404 Loss: 1.5942624807357788 Time taken: 0.33545899391174316\n",
            "Batch Number: 3405 Loss: 1.5808840990066528 Time taken: 0.32095885276794434\n",
            "Batch Number: 3406 Loss: 1.592311978340149 Time taken: 0.3073291778564453\n",
            "Batch Number: 3407 Loss: 1.5959445238113403 Time taken: 0.3460996150970459\n",
            "Batch Number: 3408 Loss: 1.580034852027893 Time taken: 0.31340575218200684\n",
            "Batch Number: 3409 Loss: 1.5932477712631226 Time taken: 0.3051445484161377\n",
            "Batch Number: 3410 Loss: 1.5735098123550415 Time taken: 0.3266923427581787\n",
            "Batch Number: 3411 Loss: 1.565848469734192 Time taken: 0.31191587448120117\n",
            "Batch Number: 3412 Loss: 1.5974366664886475 Time taken: 0.31494784355163574\n",
            "Batch Number: 3413 Loss: 1.5734543800354004 Time taken: 0.3103756904602051\n",
            "Batch Number: 3414 Loss: 1.5767773389816284 Time taken: 0.3218259811401367\n",
            "Batch Number: 3415 Loss: 1.5516196489334106 Time taken: 0.3001871109008789\n",
            "Batch Number: 3416 Loss: 1.5665053129196167 Time taken: 0.3140697479248047\n",
            "Batch Number: 3417 Loss: 1.587720274925232 Time taken: 0.3076779842376709\n",
            "Batch Number: 3418 Loss: 1.5748670101165771 Time taken: 0.30002427101135254\n",
            "Batch Number: 3419 Loss: 1.5938068628311157 Time taken: 0.3054487705230713\n",
            "Batch Number: 3420 Loss: 1.5720607042312622 Time taken: 0.31821703910827637\n",
            "Batch Number: 3421 Loss: 1.5803625583648682 Time taken: 0.31043434143066406\n",
            "Batch Number: 3422 Loss: 1.5695688724517822 Time taken: 0.306567907333374\n",
            "Batch Number: 3423 Loss: 1.5392975807189941 Time taken: 0.31744885444641113\n",
            "Batch Number: 3424 Loss: 1.5533807277679443 Time taken: 0.3111841678619385\n",
            "Batch Number: 3425 Loss: 1.5673046112060547 Time taken: 0.29670190811157227\n",
            "Batch Number: 3426 Loss: 1.5628995895385742 Time taken: 0.31092381477355957\n",
            "Batch Number: 3427 Loss: 1.5490652322769165 Time taken: 0.31504178047180176\n",
            "Batch Number: 3428 Loss: 1.5558730363845825 Time taken: 0.3012089729309082\n",
            "Batch Number: 3429 Loss: 1.5591236352920532 Time taken: 0.3046693801879883\n",
            "Batch Number: 3430 Loss: 1.574021339416504 Time taken: 0.3098630905151367\n",
            "Batch Number: 3431 Loss: 1.5532718896865845 Time taken: 0.2929048538208008\n",
            "Batch Number: 3432 Loss: 1.5421152114868164 Time taken: 0.3002359867095947\n",
            "Batch Number: 3433 Loss: 1.595422625541687 Time taken: 0.29735422134399414\n",
            "Batch Number: 3434 Loss: 1.5990043878555298 Time taken: 0.2901904582977295\n",
            "Batch Number: 3435 Loss: 1.574957251548767 Time taken: 0.2918691635131836\n",
            "Batch Number: 3436 Loss: 1.5788449048995972 Time taken: 0.31716322898864746\n",
            "Batch Number: 3437 Loss: 1.5562940835952759 Time taken: 0.311450719833374\n",
            "Batch Number: 3438 Loss: 1.543785572052002 Time taken: 0.3102381229400635\n",
            "Batch Number: 3439 Loss: 1.5259697437286377 Time taken: 0.3050992488861084\n",
            "Batch Number: 3440 Loss: 1.536646842956543 Time taken: 0.3373074531555176\n",
            "Batch Number: 3441 Loss: 1.5612847805023193 Time taken: 0.31281518936157227\n",
            "Batch Number: 3442 Loss: 1.5524260997772217 Time taken: 0.3089258670806885\n",
            "Batch Number: 3443 Loss: 1.5514956712722778 Time taken: 0.3088550567626953\n",
            "Batch Number: 3444 Loss: 1.5710914134979248 Time taken: 0.3035597801208496\n",
            "Batch Number: 3445 Loss: 1.572973370552063 Time taken: 0.3135709762573242\n",
            "Batch Number: 3446 Loss: 1.5518829822540283 Time taken: 0.3185279369354248\n",
            "Batch Number: 3447 Loss: 1.57704496383667 Time taken: 0.3084099292755127\n",
            "Batch Number: 3448 Loss: 1.5585700273513794 Time taken: 0.31098198890686035\n",
            "Batch Number: 3449 Loss: 1.5681911706924438 Time taken: 0.31257009506225586\n",
            "Batch Number: 3450 Loss: 1.5404247045516968 Time taken: 0.30379772186279297\n",
            "Batch Number: 3451 Loss: 1.5409812927246094 Time taken: 0.3043787479400635\n",
            "Batch Number: 3452 Loss: 1.5474183559417725 Time taken: 0.3108649253845215\n",
            "Batch Number: 3453 Loss: 1.5500197410583496 Time taken: 0.3198249340057373\n",
            "Batch Number: 3454 Loss: 1.558101773262024 Time taken: 0.31468725204467773\n",
            "Batch Number: 3455 Loss: 1.550499439239502 Time taken: 0.3112149238586426\n",
            "Batch Number: 3456 Loss: 1.5731571912765503 Time taken: 0.32134532928466797\n",
            "Batch Number: 3457 Loss: 1.579943060874939 Time taken: 0.319455623626709\n",
            "Batch Number: 3458 Loss: 1.5927435159683228 Time taken: 0.3131897449493408\n",
            "Batch Number: 3459 Loss: 1.5770666599273682 Time taken: 0.3184390068054199\n",
            "Batch Number: 3460 Loss: 1.5748765468597412 Time taken: 0.3013453483581543\n",
            "Batch Number: 3461 Loss: 1.5963112115859985 Time taken: 0.30599546432495117\n",
            "Batch Number: 3462 Loss: 1.583222508430481 Time taken: 0.3336362838745117\n",
            "Batch Number: 3463 Loss: 1.5822904109954834 Time taken: 0.30457425117492676\n",
            "Batch Number: 3464 Loss: 1.555397391319275 Time taken: 0.3073303699493408\n",
            "Batch Number: 3465 Loss: 1.580388069152832 Time taken: 0.3152940273284912\n",
            "Batch Number: 3466 Loss: 1.5662171840667725 Time taken: 0.3081691265106201\n",
            "Batch Number: 3467 Loss: 1.570605993270874 Time taken: 0.3230910301208496\n",
            "Batch Number: 3468 Loss: 1.5196882486343384 Time taken: 0.30670928955078125\n",
            "Batch Number: 3469 Loss: 1.5707272291183472 Time taken: 0.3063344955444336\n",
            "Batch Number: 3470 Loss: 1.5958921909332275 Time taken: 0.3021206855773926\n",
            "Batch Number: 3471 Loss: 1.5771301984786987 Time taken: 0.31468677520751953\n",
            "Batch Number: 3472 Loss: 1.5372374057769775 Time taken: 0.3110477924346924\n",
            "Batch Number: 3473 Loss: 1.5851407051086426 Time taken: 0.3116037845611572\n",
            "Batch Number: 3474 Loss: 1.569305419921875 Time taken: 0.32640862464904785\n",
            "Batch Number: 3475 Loss: 1.5636686086654663 Time taken: 0.31818199157714844\n",
            "Batch Number: 3476 Loss: 1.5828032493591309 Time taken: 0.3012270927429199\n",
            "Batch Number: 3477 Loss: 1.5438412427902222 Time taken: 0.31206250190734863\n",
            "Batch Number: 3478 Loss: 1.5637929439544678 Time taken: 0.30733776092529297\n",
            "Batch Number: 3479 Loss: 1.5684449672698975 Time taken: 0.3059122562408447\n",
            "Batch Number: 3480 Loss: 1.5790150165557861 Time taken: 0.3105316162109375\n",
            "Batch Number: 3481 Loss: 1.5396509170532227 Time taken: 0.3085505962371826\n",
            "Batch Number: 3482 Loss: 1.5560940504074097 Time taken: 0.2964663505554199\n",
            "Batch Number: 3483 Loss: 1.5452601909637451 Time taken: 0.3174424171447754\n",
            "Batch Number: 3484 Loss: 1.5662562847137451 Time taken: 0.30558013916015625\n",
            "Batch Number: 3485 Loss: 1.5584651231765747 Time taken: 0.31241703033447266\n",
            "Batch Number: 3486 Loss: 1.5774165391921997 Time taken: 0.307387113571167\n",
            "Batch Number: 3487 Loss: 1.5607640743255615 Time taken: 0.3097710609436035\n",
            "Batch Number: 3488 Loss: 1.5499787330627441 Time taken: 0.3174583911895752\n",
            "Batch Number: 3489 Loss: 1.5844496488571167 Time taken: 0.3072702884674072\n",
            "Batch Number: 3490 Loss: 1.568536639213562 Time taken: 0.310741662979126\n",
            "Batch Number: 3491 Loss: 1.5620156526565552 Time taken: 0.32447314262390137\n",
            "Batch Number: 3492 Loss: 1.5875753164291382 Time taken: 0.3047926425933838\n",
            "Batch Number: 3493 Loss: 1.5761390924453735 Time taken: 0.3059365749359131\n",
            "Batch Number: 3494 Loss: 1.616431713104248 Time taken: 0.30892443656921387\n",
            "Batch Number: 3495 Loss: 1.5615752935409546 Time taken: 0.31166696548461914\n",
            "Batch Number: 3496 Loss: 1.598730206489563 Time taken: 0.3090672492980957\n",
            "Batch Number: 3497 Loss: 1.646997094154358 Time taken: 0.29971885681152344\n",
            "Batch Number: 3498 Loss: 1.6074427366256714 Time taken: 0.31407999992370605\n",
            "Batch Number: 3499 Loss: 1.6300805807113647 Time taken: 0.3059885501861572\n",
            "Batch Number: 3500 Loss: 1.5787931680679321 Time taken: 0.3064284324645996\n",
            "Batch Number: 3501 Loss: 1.6075109243392944 Time taken: 0.3177809715270996\n",
            "Batch Number: 3502 Loss: 1.5955876111984253 Time taken: 0.3014335632324219\n",
            "Batch Number: 3503 Loss: 1.6184329986572266 Time taken: 0.3172280788421631\n",
            "Batch Number: 3504 Loss: 1.568716287612915 Time taken: 0.3072242736816406\n",
            "Batch Number: 3505 Loss: 1.5478029251098633 Time taken: 0.3061654567718506\n",
            "Batch Number: 3506 Loss: 1.5659692287445068 Time taken: 0.31591105461120605\n",
            "Batch Number: 3507 Loss: 1.5848554372787476 Time taken: 0.3241250514984131\n",
            "Batch Number: 3508 Loss: 1.5464296340942383 Time taken: 0.3022761344909668\n",
            "Batch Number: 3509 Loss: 1.5882370471954346 Time taken: 0.31368136405944824\n",
            "Batch Number: 3510 Loss: 1.5575941801071167 Time taken: 0.30496954917907715\n",
            "Batch Number: 3511 Loss: 1.5664854049682617 Time taken: 0.30394601821899414\n",
            "Batch Number: 3512 Loss: 1.6004654169082642 Time taken: 0.3143620491027832\n",
            "Batch Number: 3513 Loss: 1.599244236946106 Time taken: 0.3063197135925293\n",
            "Batch Number: 3514 Loss: 1.5757445096969604 Time taken: 0.3103020191192627\n",
            "Batch Number: 3515 Loss: 1.5937632322311401 Time taken: 0.3020820617675781\n",
            "Batch Number: 3516 Loss: 1.5859311819076538 Time taken: 0.3136022090911865\n",
            "Batch Number: 3517 Loss: 1.5896589756011963 Time taken: 0.31284475326538086\n",
            "Batch Number: 3518 Loss: 1.604198694229126 Time taken: 0.30027270317077637\n",
            "Batch Number: 3519 Loss: 1.592824935913086 Time taken: 0.3222506046295166\n",
            "Batch Number: 3520 Loss: 1.6004751920700073 Time taken: 0.29674434661865234\n",
            "Batch Number: 3521 Loss: 1.6087651252746582 Time taken: 0.29494595527648926\n",
            "Batch Number: 3522 Loss: 1.588878870010376 Time taken: 0.30997753143310547\n",
            "Batch Number: 3523 Loss: 1.625013828277588 Time taken: 0.305187463760376\n",
            "Batch Number: 3524 Loss: 1.6154855489730835 Time taken: 0.3064603805541992\n",
            "Batch Number: 3525 Loss: 1.592524528503418 Time taken: 0.3146626949310303\n",
            "Batch Number: 3526 Loss: 1.624107837677002 Time taken: 0.30387282371520996\n",
            "Batch Number: 3527 Loss: 1.6003957986831665 Time taken: 0.3145289421081543\n",
            "Batch Number: 3528 Loss: 1.6154143810272217 Time taken: 0.3166818618774414\n",
            "Batch Number: 3529 Loss: 1.6157656908035278 Time taken: 0.2984306812286377\n",
            "Batch Number: 3530 Loss: 1.6404210329055786 Time taken: 0.30498790740966797\n",
            "Batch Number: 3531 Loss: 1.626236081123352 Time taken: 0.2961001396179199\n",
            "Batch Number: 3532 Loss: 1.6026419401168823 Time taken: 0.31528210639953613\n",
            "Batch Number: 3533 Loss: 1.6092798709869385 Time taken: 0.3088951110839844\n",
            "Batch Number: 3534 Loss: 1.5984764099121094 Time taken: 0.3032553195953369\n",
            "Batch Number: 3535 Loss: 1.620650291442871 Time taken: 0.3096127510070801\n",
            "Batch Number: 3536 Loss: 1.5958387851715088 Time taken: 0.3149983882904053\n",
            "Batch Number: 3537 Loss: 1.590959906578064 Time taken: 0.3087954521179199\n",
            "Batch Number: 3538 Loss: 1.5775362253189087 Time taken: 0.31853151321411133\n",
            "Batch Number: 3539 Loss: 1.581636667251587 Time taken: 0.3002502918243408\n",
            "Batch Number: 3540 Loss: 1.6040128469467163 Time taken: 0.32594728469848633\n",
            "Batch Number: 3541 Loss: 1.5713368654251099 Time taken: 0.3295142650604248\n",
            "Batch Number: 3542 Loss: 1.5661369562149048 Time taken: 0.2880542278289795\n",
            "Batch Number: 3543 Loss: 1.5512690544128418 Time taken: 0.3147907257080078\n",
            "Batch Number: 3544 Loss: 1.5532244443893433 Time taken: 0.3079984188079834\n",
            "Batch Number: 3545 Loss: 1.6199073791503906 Time taken: 0.3006477355957031\n",
            "Batch Number: 3546 Loss: 1.568464756011963 Time taken: 0.31604647636413574\n",
            "Batch Number: 3547 Loss: 1.5596131086349487 Time taken: 0.30491089820861816\n",
            "Batch Number: 3548 Loss: 1.5872557163238525 Time taken: 0.3129255771636963\n",
            "Batch Number: 3549 Loss: 1.5696207284927368 Time taken: 0.30635643005371094\n",
            "Batch Number: 3550 Loss: 1.5713779926300049 Time taken: 0.3149392604827881\n",
            "Batch Number: 3551 Loss: 1.5623416900634766 Time taken: 0.3160891532897949\n",
            "Batch Number: 3552 Loss: 1.5815221071243286 Time taken: 0.3022167682647705\n",
            "Batch Number: 3553 Loss: 1.5762525796890259 Time taken: 0.31522297859191895\n",
            "Batch Number: 3554 Loss: 1.6034811735153198 Time taken: 0.3104586601257324\n",
            "Batch Number: 3555 Loss: 1.5547577142715454 Time taken: 0.3018355369567871\n",
            "Batch Number: 3556 Loss: 1.5449110269546509 Time taken: 0.3172926902770996\n",
            "Batch Number: 3557 Loss: 1.5367002487182617 Time taken: 0.3080742359161377\n",
            "Batch Number: 3558 Loss: 1.523780107498169 Time taken: 0.3096153736114502\n",
            "Batch Number: 3559 Loss: 1.5809396505355835 Time taken: 0.3115267753601074\n",
            "Batch Number: 3560 Loss: 1.598893165588379 Time taken: 0.311537504196167\n",
            "Batch Number: 3561 Loss: 1.5932118892669678 Time taken: 0.30367183685302734\n",
            "Batch Number: 3562 Loss: 1.5806403160095215 Time taken: 0.31960225105285645\n",
            "Batch Number: 3563 Loss: 1.5862114429473877 Time taken: 0.306441068649292\n",
            "Batch Number: 3564 Loss: 1.5512523651123047 Time taken: 0.3018503189086914\n",
            "Batch Number: 3565 Loss: 1.549881100654602 Time taken: 0.30451440811157227\n",
            "Batch Number: 3566 Loss: 1.551651954650879 Time taken: 0.3105788230895996\n",
            "Batch Number: 3567 Loss: 1.553015947341919 Time taken: 0.30465006828308105\n",
            "Batch Number: 3568 Loss: 1.5619120597839355 Time taken: 0.3049814701080322\n",
            "Batch Number: 3569 Loss: 1.6068757772445679 Time taken: 0.313124418258667\n",
            "Batch Number: 3570 Loss: 1.5496982336044312 Time taken: 0.3087601661682129\n",
            "Batch Number: 3571 Loss: 1.5803052186965942 Time taken: 0.3041071891784668\n",
            "Batch Number: 3572 Loss: 1.5955214500427246 Time taken: 0.31666040420532227\n",
            "Batch Number: 3573 Loss: 1.5704436302185059 Time taken: 0.32563281059265137\n",
            "Batch Number: 3574 Loss: 1.5766310691833496 Time taken: 0.3067922592163086\n",
            "Batch Number: 3575 Loss: 1.5645215511322021 Time taken: 0.311077356338501\n",
            "Batch Number: 3576 Loss: 1.5914220809936523 Time taken: 0.3075706958770752\n",
            "Batch Number: 3577 Loss: 1.5957646369934082 Time taken: 0.31603288650512695\n",
            "Batch Number: 3578 Loss: 1.589526891708374 Time taken: 0.3116276264190674\n",
            "Batch Number: 3579 Loss: 1.591342806816101 Time taken: 0.30536508560180664\n",
            "Batch Number: 3580 Loss: 1.5635461807250977 Time taken: 0.30347156524658203\n",
            "Batch Number: 3581 Loss: 1.5779495239257812 Time taken: 0.31015706062316895\n",
            "Batch Number: 3582 Loss: 1.599111795425415 Time taken: 0.3144567012786865\n",
            "Batch Number: 3583 Loss: 1.5600130558013916 Time taken: 0.3063693046569824\n",
            "Batch Number: 3584 Loss: 1.5650676488876343 Time taken: 0.30216240882873535\n",
            "Batch Number: 3585 Loss: 1.5887187719345093 Time taken: 0.31291842460632324\n",
            "Batch Number: 3586 Loss: 1.571340799331665 Time taken: 0.30201148986816406\n",
            "Batch Number: 3587 Loss: 1.5643328428268433 Time taken: 0.31168437004089355\n",
            "Batch Number: 3588 Loss: 1.5717641115188599 Time taken: 0.30681300163269043\n",
            "Batch Number: 3589 Loss: 1.566083312034607 Time taken: 0.307236909866333\n",
            "Batch Number: 3590 Loss: 1.538399577140808 Time taken: 0.29781031608581543\n",
            "Batch Number: 3591 Loss: 1.5293939113616943 Time taken: 0.3121957778930664\n",
            "Batch Number: 3592 Loss: 1.5608530044555664 Time taken: 0.3017408847808838\n",
            "Batch Number: 3593 Loss: 1.5556293725967407 Time taken: 0.31201863288879395\n",
            "Batch Number: 3594 Loss: 1.5541720390319824 Time taken: 0.32076382637023926\n",
            "Batch Number: 3595 Loss: 1.5647218227386475 Time taken: 0.31086158752441406\n",
            "Batch Number: 3596 Loss: 1.5712816715240479 Time taken: 0.30875372886657715\n",
            "Batch Number: 3597 Loss: 1.557794213294983 Time taken: 0.29753994941711426\n",
            "Batch Number: 3598 Loss: 1.599012017250061 Time taken: 0.30962443351745605\n",
            "Batch Number: 3599 Loss: 1.5438002347946167 Time taken: 0.3090975284576416\n",
            "Batch Number: 3600 Loss: 1.583283543586731 Time taken: 0.300609827041626\n",
            "took 1159.072467327118 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "batch_nr = 0\n",
        "for batch_data in dataset:\n",
        "      batch_start = time.time()\n",
        "      batch_nr = batch_nr+1\n",
        "      batch_loss = rnn_sequence(batch_data)\n",
        "      batch_stop = time.time()\n",
        "#      rnn_try(batch_data)\n",
        "      print(\"Batch Number: {} Loss: {} Time taken: {}\".format(batch_nr,batch_loss,batch_stop-batch_start))\n",
        "#      if not steps % 100:\n",
        "#          train_acc_metric(lbl_batch, logits)\n",
        "#          acc = train_acc_metric.result()\n",
        "#          print(\"Loss: {} Accuracy: {}\".format(loss, acc))\n",
        "#          train_acc_metric.reset_states()\n",
        "\n",
        "stop = time.time()\n",
        "print(\"took {} seconds\\n\".format(stop-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vxcfCot09uHm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### DONT RUN THIS BLOCK\n",
        "\n",
        "\n",
        "\n",
        "for dat in dataset.take(1):\n",
        "  h_t = tf.zeros([tf.shape(dat)[0],n_h])\n",
        "  for time_step in tf.range(tf.shape(dat)[1]-1):\n",
        "    data = tf.one_hot(dat[:,time_step], vocab_size)\n",
        "    #print(\"----------------------\")\n",
        "    #print(data[0,:])\n",
        "\n",
        "    a = (tf.matmul(data,w_xh))+ (tf.matmul(h_t, w_hh)) + b_h\n",
        "    #print(tf.shape(a))\n",
        "    h_t = tf.nn.tanh(a)\n",
        "    logits = (tf.matmul(h_t, w_ho)) + b_o\n",
        "    logits = tf.convert_to_tensor(logits)\n",
        "    output = tf.nn.softmax(logits)\n",
        "    #print((output[0]))\n",
        "\n",
        "sampled_indices = tf.random.categorical(output, num_samples= 1)\n",
        "print(sampled_indices)\n",
        "#ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "#ind_to_ch(sampled_indices)\n",
        "gen = map(sampled_indices, ind_to_ch)\n",
        "print(list(gen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "colab_type": "code",
        "id": "v89jmuiuSX4K",
        "outputId": "7c1c7381-31f6-496b-816c-1c830e9ff65c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 5, 6]\n",
            "[6]\n"
          ]
        }
      ],
      "source": [
        "characters = [0,5,6]\n",
        "print(characters)\n",
        "\n",
        "print(characters[-1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "ybQkfhNevCCd",
        "outputId": "b42fa28f-0607-4cc7-fe55-96396ed16fd6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this accy and not is he go\n",
            "did spin and blowd you, and matoce the sunving Turth's forsers;\n",
            "Ween, we she set hus awh be codved alaush.\n",
            "\n",
            "CUSTIL:\n",
            "I say find away from I would you thought you cume,\n",
            "The prentable sufter of up you?\n",
            "\n",
            "istapp'd is, she reled both, for a rate more in his headt,\n",
            "Which, be a valiant I duly ewelton to him? I Franch\n",
            "And bare that leave the retlabied to my sun\n",
            "This rause a greet caper to make thy wars in trilld I before discilen,\n",
            "The ispecient crieve what he be a kingly.\n",
            "\n",
            "AUS:\n",
            "Cret'l I ming it, on Wand here\n",
            "Ap courtious dain.\n",
            "A servent of men he wall be eved extect is hend, within the ceptardont. Thy trausation,\n",
            "If I maulted before thee, lieces,\n",
            "Then all then you their mideh courtest.\n",
            "\n",
            "PoIN\n",
            "AS:\n",
            "Dround I besuting and a bestution of my fiese Who own diemned:\n",
            "Why, and come thy hawning to fortune love;\n",
            "Thou wilt, my I'll from things, you shouse.\n",
            "\n",
            "VIOLA:\n",
            "Nay, there's fallfore thee, Palabur in the formones hear:\n",
            "O, for the unthated in this enes.\n",
            "\n",
            "BUSTIA:\n",
            "O, this ho! who change of him Erent gots our sounted, sir.\n",
            "\n",
            "RRIANO:\n",
            "Des, an heavon we seal too lagg'll and there arvity,\n",
            "But there's to you there are not make thee for their tommob.\n",
            "\n",
            "KING LEAR:\n",
            "Have you him extorm did stoming\n",
            "wit! the duke of I favour that her do stay for gold's\n",
            "against this wind of sturnt not\n",
            "strone blood tempes of madien\n",
            "Doft their will thou do thow my!\n",
            "And though me dance did merivio shopled high,\n",
            "Blowd ansture and which yeurse?\n",
            "\n",
            "Clown:\n",
            "Now, good! Volt thou appotet, and it in the cawers then fear that are from thy hand\n",
            "with a thine I join and stame's criefut\n",
            "That not you have comest enened in Frencher\n",
            "He, who dren my shament of my gentranb him?\n",
            "Nur, mad, sir: that bedoth'd by the rather 'twixt and enrighted,\n",
            "Which curst Perusane! Of a un a veat for them, ont then\n",
            "budging disperces 'tis goo, to\n",
            "I love you.\n",
            "\n",
            "SIR TOBY ANICH:\n",
            "This own old so conjoy mented:\n",
            "The Gatter not. Be need, and that make\n",
            "Will you as you with me:\n",
            "The truat that afterety to be feal;\n",
            "Thou have hers well when it suppland thus upen now of mine, how he greature of my men ang nothing, my sont.\n",
            "\n",
            "MISTRESS QUICKLY:\n",
            "Sil, Pelicant, and you as there's to thesest,\n",
            "She misted procued enamos of mistress Quint,\n",
            "Let ever to then.\n",
            "\n",
            "for there excellant, and make his ptiel\n",
            "Fold by the aintilication?\n",
            "\n",
            "KING PHILAT:\n",
            "\n",
            "Siffow makes the sile\n",
            "What so much thy hans?\n",
            "\n",
            "KING PHILIP:\n",
            "I sir I do meannt to my soul, and then talk is drong him,\n",
            "Pood Sord,\n",
            "Come neither yer;\n",
            "And so long in my dimnger of the hamment up was:\n",
            "How can a blood and\n"
          ]
        }
      ],
      "source": [
        "\n",
        "characters_to_generate = 2500\n",
        "\n",
        "h_pred = tf.zeros([1,n_h])\n",
        "\n",
        "character = [0]\n",
        "\n",
        "text = []\n",
        "for current_character in range(characters_to_generate):\n",
        "    x_pred = tf.one_hot(character[-1:],vocab_size)\n",
        "    h_pred = tf.nn.tanh(tf.matmul(x_pred,w_xh) + tf.matmul(h_pred,w_hh) + b_h)\n",
        "    logits = tf.matmul(h_pred,w_ho) + b_o\n",
        "\n",
        "    preds = tf.nn.softmax(logits)\n",
        "    preds = preds.numpy()[0]\n",
        "    choice = np.random.choice(vocab_size, p = preds)\n",
        "    #print(preds.shape)\n",
        "    #predictions = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    #print(predictions)\n",
        "    character.append(choice)\n",
        "    start_char = ind_to_ch[choice]\n",
        "    #print(input_eval)\n",
        "    text.append(start_char)\n",
        "\n",
        "\n",
        "#print(text)  \n",
        "print(''.join(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LEbUmejDEDjf"
      },
      "source": [
        "#### **Using np.random.choice**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "colab_type": "code",
        "id": "TVTjlD5wXAWb",
        "outputId": "86a9cf5c-9e47-47c1-d2f5-98efc24e9404",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lchess\n",
            "Some nent\n",
            "break an ey;\n",
            "Yor's quest of England, thyoria?\n",
            "\n",
            "VIANA:\n",
            "To, now he or fill I dame:\n",
            "I'l\n"
          ]
        }
      ],
      "source": [
        "num_generate = 100\n",
        "#from prepare_data import chs_to_inds\n",
        "\n",
        "#start_char = '<S>'\n",
        "#input_eval = chs_to_inds(start_char,vocab)\n",
        "#print(input_eval)\n",
        "#input_eval = tf.one_hot(input_eval, vocab_size)\n",
        "#print(input_eval)\n",
        "\n",
        "text_generated = []\n",
        "#temp = 1.5\n",
        "h_t = tf.zeros([1,n_h])\n",
        "choice = [0]\n",
        "\n",
        "for i in range(num_generate):\n",
        "  x_t = tf.one_hot(choice[-1:],depth = vocab_size)\n",
        "  a = (tf.matmul(x_t,w_xh))+ (tf.matmul(h_t, w_hh)) + b_h\n",
        "  #print(type(a))\n",
        "  h_t = tf.nn.tanh(a)\n",
        "  logits = (tf.matmul(h_t, w_ho)) + b_o\n",
        "  #print(logits)\n",
        "  predictions = tf.nn.softmax(logits)\n",
        "  #predictions = predictions[0,:]\n",
        "  # predictions = tf.squeeze(predictions, 0)\n",
        "  #print(\"----------------------\"*3)\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions)\n",
        "  #print(predictions.numpy().shape)\n",
        "  #predictions = predictions / temp\n",
        "\n",
        "  ## The predictions is a tensor \n",
        "  predictions = predictions.numpy()[0]\n",
        "  #predictions = np.array(predictions)\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions)\n",
        "  char_choice = np.random.choice(vocab_size, p = predictions)\n",
        "  #print(char_choice)\n",
        "  choice.append(char_choice)\n",
        "  #predictions = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "  #print(predictions)\n",
        "  start_char = ind_to_ch[char_choice]\n",
        "  #print(start_char)\n",
        "  #print(input_eval)\n",
        "  text_generated.append(start_char)\n",
        "  \n",
        "print(start_char + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mxF-9NR8ENrR"
      },
      "source": [
        "#### **Using np.argmax**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "colab_type": "code",
        "id": "nC2wRxK0C0_V",
        "outputId": "be0961fd-395b-44a2-acc9-fc34ff2b5ea3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "s the best of the country shall be so much and my lord,\n",
            "The sense of my lord, and the common the country shall be so much and my lord,\n",
            "The sense of my lord, and the common the country shall be so much and my lord,\n",
            "The sense of my lord, and the common the country shall be so much and my lord,\n",
            "The sense of my lord, and the common the country shall be so much and my lord,\n",
            "The sense of my lord, and the common the country shall be so much and my lord,\n",
            "The sense of my lord, and the common the country s\n"
          ]
        }
      ],
      "source": [
        "num_generate = 500\n",
        "#from prepare_data import chs_to_inds\n",
        "\n",
        "#start_char = '<S>'\n",
        "#input_eval = chs_to_inds(start_char,vocab)\n",
        "#print(input_eval)\n",
        "#input_eval = tf.one_hot(input_eval, vocab_size)\n",
        "#print(input_eval)\n",
        "\n",
        "text_generated = []\n",
        "#temp = 1.5\n",
        "h_t = tf.zeros([1,n_h])\n",
        "choice = [0]\n",
        "\n",
        "for i in range(num_generate):\n",
        "  x_t = tf.one_hot(choice[-1:],depth = vocab_size)\n",
        "  a = (tf.matmul(x_t,w_xh))+ (tf.matmul(h_t, w_hh)) + b_h\n",
        "  #print(type(a))\n",
        "  h_t = tf.nn.tanh(a)\n",
        "  logits = (tf.matmul(h_t, w_ho)) + b_o\n",
        "  #print(logits)\n",
        "  predictions = tf.nn.softmax(logits)\n",
        "  #predictions = predictions[0,:]\n",
        "  # predictions = tf.squeeze(predictions, 0)\n",
        "  #print(\"----------------------\"*3)\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions.numpy().shape)\n",
        "  #predictions = predictions / temp\n",
        "  predictions = predictions.numpy()[0]\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions)\n",
        "  char_choice = np.argmax(predictions)\n",
        "  #print(char_choice)\n",
        "  #char_choice = np.random.choice(vocab_size, p = predictions)\n",
        "  #print(char_choice)\n",
        "  choice.append(char_choice)\n",
        "  #predictions = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "  #print(predictions)\n",
        "  start_char = ind_to_ch[char_choice]\n",
        "  #print(start_char)\n",
        "  #print(input_eval)\n",
        "  text_generated.append(start_char)\n",
        "  \n",
        "print(start_char + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3R5QlHO3D6dM",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "IDL_Assignment5",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

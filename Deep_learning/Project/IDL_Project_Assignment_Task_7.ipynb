{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2wtSADyaQ3SF"
      },
      "source": [
        "# **IDL Project Assignment Task 7: What did you learn?**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4eZ72qX8ssWf"
      },
      "source": [
        "**Questions to be solved**\n",
        "\n",
        "\n",
        "1.   Which findings were just showing what you already expected?\n",
        "2.   Were there any results which you find surprising or interesting?\n",
        "3.   Which tasks were easy for you to solve?\n",
        "4.   Which tasks were difficult and what would you have needed to solve them (better)?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vyzCNWS2tOSo"
      },
      "source": [
        "## **Observations of Amar Shivaram**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qhBR_qxl4Ew7"
      },
      "source": [
        "I did Task 3 - Transfer Learning and Task 6 - BERT\n",
        "\n",
        "1.   Which findings were just showing what you already expected?\n",
        "\n",
        "    The portion in transfer learning subtask 3 where we freeze all layers yielded the output like how i thought it would be. As logically i felt it will fare poorly. \n",
        "\n",
        "2.   Were there any results which you find surprising or interesting?\n",
        "\n",
        "    The output of BERT where we classify sentences together was actually interesting. As previously having worked with many ML techniques none gave the power in this detail. So i was surprised at the power of BERT. \n",
        "\n",
        "3.   Which tasks were easy for you to solve?\n",
        "\n",
        "    The tasks in Transfer learning were easy to solve as the method to solve was common for all tasks except for minor tweaks. \n",
        "\n",
        "4.   Which tasks were difficult and what would you have needed to solve them (better)?\n",
        "\n",
        "    The preprocessing in BERT took me some time as I had to figure the appropriate and exact way to do. Moreover to come up with a method to train the BERT took a bit of time to find the exact function call to use. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sWVkp3Z1tOa9"
      },
      "source": [
        "## **Observations of Manish Bhandari**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BgJpY4Js4Fbw"
      },
      "source": [
        "I did task 4: Introspection and task 5: NTM\n",
        "\n",
        "1.   Which findings were just showing what you already expected?\n",
        "    \n",
        "    Gradient based saliency map showed the expected results and so did Bahdanau's attention. I'd expected saliency maps to hightlight the important areas and attention weights to attend proper token and it worked as expected.\n",
        "\n",
        "2.   Were there any results which you find surprising or interesting?\n",
        "\n",
        "    I expected luong's multiplicative attention to work on similar lines with Bahdanau's attention with same amount of training. However, the former requires more amount of training to give proper results which was bit suprising.\n",
        "    Also, it was interesting to see how a random image can be optimised to correspond to a particular class in introspection task.\n",
        "\n",
        "3.   Which tasks were easy for you to solve?\n",
        "\n",
        "    Both the tasks were moderately easy give the instructions and the tensorflow tutorials\n",
        "\n",
        "4.   Which tasks were difficult and what would you have needed to solve them (better)?\n",
        "\n",
        "    I think, somehow, my implementation of dot product and Luong's attention is not entirely correct. Mathematically, they seem to be correct and I've implemented them according to what was available in slides and online but it doesn't give the same results as Bahdanau. Maybe a heads up regarding that would have be helpful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rt8kkX9YtOiQ"
      },
      "source": [
        "## **Observations of Aishwarya Jauhari**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aD2w-i2b4F-8"
      },
      "source": [
        "I did Task 1- Optimizers and Task 2- Regularizers\n",
        "1.   Which findings were just showing what you already expected?\n",
        "\n",
        "     In task 1, Adam performed better than SGD as expected. In task 2, it can be seen from the graphs plotted that without regularizers, the test accuracy is quite low and training accuracy is quite high, the gap between the training and test accuracy curves is high showing that the model has overfit, After adding regularizer(Dropout), the gap reduces i.e. training accuracy also decreases and the test accuracy also increases. This was the expected result.\n",
        "2.   Were there any results which you find surprising or interesting?\n",
        " \n",
        " In task 2, trying L1 and L2 regularizers didnot fetch the expected results. The model has over fit even after applying L1/L2 regularizer.\n",
        "\n",
        "3.   Which tasks were easy for you to solve?\n",
        "\n",
        "     Both tasks were easy, followed the tensorflow tutorials.\n",
        "4.   Which tasks were difficult and what would you have needed to solve them (better)?\n",
        "\n",
        "     In task 2, I was expecting L1/L2 regularizer to perform well, but the model overfit even after using the regularizer. Probably, comparison of my results with my peers would have helped me to solve them better.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IDL Project Assignment Task 7.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

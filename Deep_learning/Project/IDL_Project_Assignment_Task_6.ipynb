{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2wtSADyaQ3SF"
      },
      "source": [
        "# **IDL Project Assignment Task 6: BERT fine-tuning for classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b3fVxBd5HocV"
      },
      "source": [
        "## **Assigning Tensorflow version and importing the libraries required for the tasks**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "bBKuyxx184PF",
        "outputId": "e6aeb969-c902-48b2-94dd-4531112206b9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tf-models-official\n",
            "  Using cached https://files.pythonhosted.org/packages/5b/33/91e5e90e3e96292717245d3fe87eb3b35b07c8a2113f2da7f482040facdb/tf_models_official-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (7.0.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.8.3)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.5.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (4.4.0.42)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (2.1.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.1.91)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (3.2.2)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (5.4.8)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.4.1)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.21.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.5.8)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.9.0)\n",
            "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.7.12)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.3.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.0.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.18.5)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.29.21)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-official) (2.7.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.2.1->tf-models-official) (0.1.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (3.12.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (0.3.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (0.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (4.41.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (20.2.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (0.24.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (1.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-official) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-official) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-official) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-official) (1.2.0)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (0.4.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (2020.6.20)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.0.1)\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (0.0.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (1.24.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (0.35.1)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (2.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (3.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (1.32.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (2.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (1.6.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.17.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->tf-models-official) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->tf-models-official) (50.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->tf-models-official) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->tf-models-official) (2.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-official) (1.52.0)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-official) (1.16.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (1.7.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (3.1.0)\n",
            "Installing collected packages: tf-models-official\n",
            "Successfully installed tf-models-official-2.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tf-models-official\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "3g1v3YHPFgiV",
        "outputId": "603b1191-f6f4-488a-970e-d331d92d99e7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "from official.modeling import tf_utils\n",
        "from official import nlp\n",
        "from official.nlp import bert\n",
        "\n",
        "# Load the required submodules\n",
        "import official.nlp.optimization\n",
        "import official.nlp.bert.bert_models\n",
        "import official.nlp.bert.configs\n",
        "import official.nlp.bert.run_classifier\n",
        "import official.nlp.bert.tokenization\n",
        "import official.nlp.data.classifier_data_lib\n",
        "import official.nlp.modeling.losses\n",
        "import official.nlp.modeling.models\n",
        "import official.nlp.modeling.networks\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VIrcPHFKmI_b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "os.getcwd()\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/IDL /IDL Project\") \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kyg91gqYteCb"
      },
      "source": [
        "## **Importing dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "colab_type": "code",
        "id": "dp98Wg_xmPSC",
        "outputId": "a533a833-7fed-41d7-aea9-66bc19556194",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset glue/cola/1.0.0 (download: 368.14 KiB, generated: Unknown size, total: 368.14 KiB) to /root/tensorflow_datasets/glue/cola/1.0.0...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shuffling and writing examples to /root/tensorflow_datasets/glue/cola/1.0.0.incomplete532WPK/glue-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/glue/cola/1.0.0.incomplete532WPK/glue-validation.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/glue/cola/1.0.0.incomplete532WPK/glue-test.tfrecord\n",
            "\u001b[1mDataset glue downloaded and prepared to /root/tensorflow_datasets/glue/cola/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "glue, info = tfds.load('glue/cola', with_info=True,\n",
        "                       # It's small, load the whole dataset\n",
        "                       batch_size=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "ujiWZYbMyld1",
        "outputId": "fe7bd3ad-209d-4edb-aa14-f568396cc3de",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['test', 'train', 'validation']"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(glue.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "colab_type": "code",
        "id": "AdxkNO7vylhN",
        "outputId": "edc694e6-9477-408c-b4d1-6bc81a0faea1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FeaturesDict({\n",
              "    'idx': tf.int32,\n",
              "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
              "    'sentence': Text(shape=(), dtype=tf.string),\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "info.features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "YUsXy3H_7RKR",
        "outputId": "a2864e94-7f3d-44de-d5df-64d175689f18",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['unacceptable', 'acceptable']"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "info.features['label'].names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "nvp-3xbD7Tnm",
        "outputId": "5997e8a6-19b6-4a8a-dfab-856d4a8da94f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "idx      : 1680\n",
            "label    : 1\n",
            "sentence : b'It is this hat that it is certain that he was wearing.'\n"
          ]
        }
      ],
      "source": [
        "glue_train = glue['train']\n",
        "\n",
        "for key, value in glue_train.items():\n",
        "  print(f\"{key:9s}: {value[0].numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CaKUCTVTF7I0"
      },
      "source": [
        "## **Preprocessing the text data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "L-yaXi0K7YEv",
        "outputId": "683304b4-36b6-4616-8e9c-734e2578efa5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['bert_config.json',\n",
              " 'bert_model.ckpt.data-00000-of-00001',\n",
              " 'bert_model.ckpt.index',\n",
              " 'vocab.txt']"
            ]
          },
          "execution_count": 9,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
        "tf.io.gfile.listdir(gs_folder_bert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "u1RiFV2ajFZA",
        "outputId": "c6228c16-0fe2-4d84-b42c-6db1b2bf323b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 30522\n"
          ]
        }
      ],
      "source": [
        "tokenizer = bert.tokenization.FullTokenizer(\n",
        "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n",
        "     do_lower_case=True)\n",
        "\n",
        "print(\"Vocab size:\", len(tokenizer.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "TONdu1H2pVch",
        "outputId": "ffc76c42-2ffa-4bea-fdd4-e03b905008ec",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[101]"
            ]
          },
          "execution_count": 11,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_tokens_to_ids(['[CLS]'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "10wVBd31H3BQ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def encode_sentence(s, tokenizer):\n",
        "   tokens = list(tokenizer.tokenize(s))\n",
        "   tokens.append('[SEP]')\n",
        "   return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def bert_encode(glue_dict, tokenizer):\n",
        "  num_examples = len(glue_dict[\"sentence\"])\n",
        "  \n",
        "  sentence = tf.ragged.constant([encode_sentence(s, tokenizer) for s in np.array(glue_dict[\"sentence\"])])\n",
        "  \n",
        "\n",
        "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence.shape[0]\n",
        "  input_word_ids = tf.concat([cls, sentence], axis=-1)\n",
        "\n",
        "  input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
        "\n",
        "  type_cls = tf.zeros_like(cls)\n",
        "  type_s = tf.ones_like(sentence)\n",
        "  input_type_ids = tf.concat([type_cls, type_s], axis=-1).to_tensor()\n",
        "\n",
        "  inputs = {\n",
        "      'input_word_ids': input_word_ids.to_tensor(),\n",
        "      'input_mask': input_mask,\n",
        "      'input_type_ids': input_type_ids}\n",
        "\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "O7HRgAYjH3Lz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "glue_train = bert_encode(glue['train'], tokenizer)\n",
        "glue_train_labels = glue['train']['label']\n",
        "\n",
        "glue_validation = bert_encode(glue['validation'], tokenizer)\n",
        "glue_validation_labels = glue['validation']['label']\n",
        "\n",
        "glue_test = bert_encode(glue['test'], tokenizer)\n",
        "glue_test_labels  = glue['test']['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "deHAADCoH3Rx",
        "outputId": "03108874-e8ce-4e8e-948f-f3a24c019b51",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_word_ids  shape: (8551, 47)\n",
            "input_mask      shape: (8551, 47)\n",
            "input_type_ids  shape: (8551, 47)\n",
            "glue_train_labels shape: (8551,)\n"
          ]
        }
      ],
      "source": [
        "for key, value in glue_train.items():\n",
        "  print(f'{key:15s} shape: {value.shape}')\n",
        "\n",
        "print(f'glue_train_labels shape: {glue_train_labels.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3Tb36t63FwvS"
      },
      "source": [
        "## **Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "wpEsj1aAH3Uj",
        "outputId": "42c73834-8cdc-4dd7-e98b-56bfd6e1e05c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'attention_probs_dropout_prob': 0.1,\n",
              " 'hidden_act': 'gelu',\n",
              " 'hidden_dropout_prob': 0.1,\n",
              " 'hidden_size': 768,\n",
              " 'initializer_range': 0.02,\n",
              " 'intermediate_size': 3072,\n",
              " 'max_position_embeddings': 512,\n",
              " 'num_attention_heads': 12,\n",
              " 'num_hidden_layers': 12,\n",
              " 'type_vocab_size': 2,\n",
              " 'vocab_size': 30522}"
            ]
          },
          "execution_count": 15,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\n",
        "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n",
        "\n",
        "bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
        "\n",
        "config_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jfHi52_CH3Xj",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "_, bert_encoder = bert.bert_models.classifier_model(bert_config, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "nXHcddqxG8sY",
        "outputId": "0cac75c5-0b60-4d54-fb21-256e8867a915",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7feeaf624e80>"
            ]
          },
          "execution_count": 17,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint = tf.train.Checkpoint(model=bert_encoder)\n",
        "checkpoint.restore(os.path.join(gs_folder_bert, 'bert_model.ckpt')).assert_consumed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mpD3Ca_xG8vT",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Set up epochs and steps\n",
        "epochs = 3\n",
        "batch_size = 32\n",
        "eval_batch_size = 32\n",
        "\n",
        "train_data_size = len(glue_train_labels)\n",
        "steps_per_epoch = int(train_data_size / batch_size)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
        "\n",
        "# creates an optimizer with learning rate schedule\n",
        "optimizer = nlp.optimization.create_optimizer(\n",
        "    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "ijb0diaxNIsF",
        "outputId": "79080ddc-1445-4cd3-8526-a422c9c8d116",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'activation': <function official.modeling.activations.gelu.gelu>,\n",
              " 'attention_dropout_rate': 0.1,\n",
              " 'dropout_rate': 0.1,\n",
              " 'hidden_size': 768,\n",
              " 'initializer': <tensorflow.python.keras.initializers.initializers_v2.TruncatedNormal at 0x7feeaf601b00>,\n",
              " 'intermediate_size': 3072,\n",
              " 'max_sequence_length': 512,\n",
              " 'num_attention_heads': 12,\n",
              " 'num_layers': 12,\n",
              " 'type_vocab_size': 2,\n",
              " 'vocab_size': 30522}"
            ]
          },
          "execution_count": 19,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer_config = config_dict.copy()\n",
        "\n",
        "# You need to rename a few fields to make this work:\n",
        "transformer_config['attention_dropout_rate'] = transformer_config.pop('attention_probs_dropout_prob')\n",
        "transformer_config['activation'] = tf_utils.get_activation(transformer_config.pop('hidden_act'))\n",
        "transformer_config['dropout_rate'] = transformer_config.pop('hidden_dropout_prob')\n",
        "transformer_config['initializer'] = tf.keras.initializers.TruncatedNormal(\n",
        "          stddev=transformer_config.pop('initializer_range'))\n",
        "transformer_config['max_sequence_length'] = transformer_config.pop('max_position_embeddings')\n",
        "transformer_config['num_layers'] = transformer_config.pop('num_hidden_layers')\n",
        "\n",
        "transformer_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Z5Dmr3_6G8yA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "manual_classifier = nlp.modeling.models.BertClassifier(\n",
        "        bert_encoder,\n",
        "        num_classes=2,\n",
        "        dropout_rate=transformer_config['dropout_rate'],\n",
        "        initializer=tf.keras.initializers.TruncatedNormal(\n",
        "          stddev=bert_config.initializer_range))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "colab_type": "code",
        "id": "cvofXDV3G83I",
        "outputId": "ceb1ab02-3d8b-4a4f-cf2f-e6f2daa33100",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "268/268 [==============================] - 113s 422ms/step - loss: 0.5277 - accuracy: 0.7369 - val_loss: 0.4316 - val_accuracy: 0.8073\n",
            "Epoch 2/3\n",
            "268/268 [==============================] - 112s 418ms/step - loss: 0.3274 - accuracy: 0.8693 - val_loss: 0.4265 - val_accuracy: 0.8245\n",
            "Epoch 3/3\n",
            "268/268 [==============================] - 112s 418ms/step - loss: 0.2117 - accuracy: 0.9246 - val_loss: 0.4877 - val_accuracy: 0.8217\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7feeae1e44e0>"
            ]
          },
          "execution_count": 21,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "manual_classifier.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=metrics)\n",
        "\n",
        "manual_classifier.fit(\n",
        "      glue_train, glue_train_labels,\n",
        "      validation_data=(glue_validation, glue_validation_labels),\n",
        "      batch_size=32,\n",
        "      epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wcc58go-G852",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "my_examples = bert_encode(\n",
        "    glue_dict = {\n",
        "        'sentence':[\n",
        "            'The rain in Spain falls mainly on the plain.',\n",
        "            'Look I fine tuned BERT.',\n",
        "            'It rains regularly in Germany',\n",
        "            'Mumbai has easy raining in every year.',\n",
        "            'I had an intusion that I am done the task right but I was not wrong.']\n",
        "    },\n",
        "    tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "Mj4fYr7bG88Z",
        "outputId": "f21c705f-6756-4453-ec8b-3ba136cb46b8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 0])"
            ]
          },
          "execution_count": 27,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = manual_classifier(my_examples, training=False)\n",
        "\n",
        "result = tf.argmax(result,axis=1).numpy()\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "jmGlX-2rH3aM",
        "outputId": "aacd2961-fa2f-4190-b45e-a379dc8d95e9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['acceptable', 'unacceptable', 'acceptable', 'unacceptable',\n",
              "       'unacceptable'], dtype='<U12')"
            ]
          },
          "execution_count": 28,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(info.features['label'].names)[result]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "colab_type": "code",
        "id": "aujDsnnQMI7q",
        "outputId": "39f0ccbe-d749-40a5-d4e4-67801a3fa19a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
          ]
        }
      ],
      "source": [
        "export_dir='./saved_model'\n",
        "tf.saved_model.save(manual_classifier, export_dir=export_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lq6Ad_6ob2u0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GNlz4_LUIrx1"
      },
      "source": [
        "## Questions\n",
        "\n",
        "\n",
        "\n",
        "*   **What is the tutorial classifying when using the GLUE MRPC data set?**\n",
        "The tutorial which uses the GLUE MRPC data set points out that whether the sentences in pairs are semantically equivalent or not. \n",
        "\n",
        "*   **In addition to the input itself, the tutorial feeds two binary tensors for input mask and input type to the model. Is this necessary for the data set single sentence classification?**\n",
        "Technically when doing a single sentence classification input type is not required. But the API call requires all 3 data. So all 3 are fed into it. \n",
        "\n",
        "\n",
        "*   **How does the tokenization in BERT differ from the one in the previous Task 5?**\n",
        "In the previous task we encode and decode ourselves. Here we used an already existing tokenized file.\n",
        "\n",
        "*   **What is a [CLS] token and what is it used for?**\n",
        "The [CLS] token is to point out that its a classification problem. So each sentence is padded with CLS in the starting. \n",
        "\n",
        "\n",
        "*   **Which part of the BERT encoding is used for the classification?Does your answer match the output shape of the encoder?**\n",
        "The tokenised encoding is used for the classification problem. Yes it matches the output shape. \n",
        "The output of BERT encoding shape -> n,768 is used for classification. \n",
        "\n",
        "\n",
        "*   **Are the BERT encoder weights also fine-tuned to the task?**\n",
        "The encoder weights are finetuned. Thats why we restore the checkpoint and do the task. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vKDPLiTKLjY5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "IDL Project Assignment Task 6.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

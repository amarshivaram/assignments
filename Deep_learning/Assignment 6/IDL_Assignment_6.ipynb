{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2wtSADyaQ3SF"
      },
      "source": [
        "# **IDL Assignment 6 - More Realistic Language Modeling & Recurrent Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b3fVxBd5HocV"
      },
      "source": [
        "## **Assigning Tensorflow version and importing the libraries required for the tasks**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "colab_type": "code",
        "id": "3g1v3YHPFgiV",
        "outputId": "50e9a2e8-6e4d-4e90-edc4-e720a31eea78",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "execution_count": 1,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VIrcPHFKmI_b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "os.getcwd()\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/IDL /IDL Assignments/Assignment helper files\") \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_kRcu5S4Uu46"
      },
      "source": [
        "## **Fixed Length Sequenes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CtZrgAErV832"
      },
      "source": [
        "### **Preprocess the text data** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "zoHuC3T6UXaZ",
        "outputId": "2a4f99cc-b317-4dd3-8c38-048c4f2b0892",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-05-29 11:00:47.117101: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Split input into 22981 sequences...\n",
            "Serialized 100 sequences...\n",
            "Serialized 200 sequences...\n",
            "Serialized 300 sequences...\n",
            "Serialized 400 sequences...\n",
            "Serialized 500 sequences...\n",
            "Serialized 600 sequences...\n",
            "Serialized 700 sequences...\n",
            "Serialized 800 sequences...\n",
            "Serialized 900 sequences...\n",
            "Serialized 1000 sequences...\n",
            "Serialized 1100 sequences...\n",
            "Serialized 1200 sequences...\n",
            "Serialized 1300 sequences...\n",
            "Serialized 1400 sequences...\n",
            "Serialized 1500 sequences...\n",
            "Serialized 1600 sequences...\n",
            "Serialized 1700 sequences...\n",
            "Serialized 1800 sequences...\n",
            "Serialized 1900 sequences...\n",
            "Serialized 2000 sequences...\n",
            "Serialized 2100 sequences...\n",
            "Serialized 2200 sequences...\n",
            "Serialized 2300 sequences...\n",
            "Serialized 2400 sequences...\n",
            "Serialized 2500 sequences...\n",
            "Serialized 2600 sequences...\n",
            "Serialized 2700 sequences...\n",
            "Serialized 2800 sequences...\n",
            "Serialized 2900 sequences...\n",
            "Serialized 3000 sequences...\n",
            "Serialized 3100 sequences...\n",
            "Serialized 3200 sequences...\n",
            "Serialized 3300 sequences...\n",
            "Serialized 3400 sequences...\n",
            "Serialized 3500 sequences...\n",
            "Serialized 3600 sequences...\n",
            "Serialized 3700 sequences...\n",
            "Serialized 3800 sequences...\n",
            "Serialized 3900 sequences...\n",
            "Serialized 4000 sequences...\n",
            "Serialized 4100 sequences...\n",
            "Serialized 4200 sequences...\n",
            "Serialized 4300 sequences...\n",
            "Serialized 4400 sequences...\n",
            "Serialized 4500 sequences...\n",
            "Serialized 4600 sequences...\n",
            "Serialized 4700 sequences...\n",
            "Serialized 4800 sequences...\n",
            "Serialized 4900 sequences...\n",
            "Serialized 5000 sequences...\n",
            "Serialized 5100 sequences...\n",
            "Serialized 5200 sequences...\n",
            "Serialized 5300 sequences...\n",
            "Serialized 5400 sequences...\n",
            "Serialized 5500 sequences...\n",
            "Serialized 5600 sequences...\n",
            "Serialized 5700 sequences...\n",
            "Serialized 5800 sequences...\n",
            "Serialized 5900 sequences...\n",
            "Serialized 6000 sequences...\n",
            "Serialized 6100 sequences...\n",
            "Serialized 6200 sequences...\n",
            "Serialized 6300 sequences...\n",
            "Serialized 6400 sequences...\n",
            "Serialized 6500 sequences...\n",
            "Serialized 6600 sequences...\n",
            "Serialized 6700 sequences...\n",
            "Serialized 6800 sequences...\n",
            "Serialized 6900 sequences...\n",
            "Serialized 7000 sequences...\n",
            "Serialized 7100 sequences...\n",
            "Serialized 7200 sequences...\n",
            "Serialized 7300 sequences...\n",
            "Serialized 7400 sequences...\n",
            "Serialized 7500 sequences...\n",
            "Serialized 7600 sequences...\n",
            "Serialized 7700 sequences...\n",
            "Serialized 7800 sequences...\n",
            "Serialized 7900 sequences...\n",
            "Serialized 8000 sequences...\n",
            "Serialized 8100 sequences...\n",
            "Serialized 8200 sequences...\n",
            "Serialized 8300 sequences...\n",
            "Serialized 8400 sequences...\n",
            "Serialized 8500 sequences...\n",
            "Serialized 8600 sequences...\n",
            "Serialized 8700 sequences...\n",
            "Serialized 8800 sequences...\n",
            "Serialized 8900 sequences...\n",
            "Serialized 9000 sequences...\n",
            "Serialized 9100 sequences...\n",
            "Serialized 9200 sequences...\n",
            "Serialized 9300 sequences...\n",
            "Serialized 9400 sequences...\n",
            "Serialized 9500 sequences...\n",
            "Serialized 9600 sequences...\n",
            "Serialized 9700 sequences...\n",
            "Serialized 9800 sequences...\n",
            "Serialized 9900 sequences...\n",
            "Serialized 10000 sequences...\n",
            "Serialized 10100 sequences...\n",
            "Serialized 10200 sequences...\n",
            "Serialized 10300 sequences...\n",
            "Serialized 10400 sequences...\n",
            "Serialized 10500 sequences...\n",
            "Serialized 10600 sequences...\n",
            "Serialized 10700 sequences...\n",
            "Serialized 10800 sequences...\n",
            "Serialized 10900 sequences...\n",
            "Serialized 11000 sequences...\n",
            "Serialized 11100 sequences...\n",
            "Serialized 11200 sequences...\n",
            "Serialized 11300 sequences...\n",
            "Serialized 11400 sequences...\n",
            "Serialized 11500 sequences...\n",
            "Serialized 11600 sequences...\n",
            "Serialized 11700 sequences...\n",
            "Serialized 11800 sequences...\n",
            "Serialized 11900 sequences...\n",
            "Serialized 12000 sequences...\n",
            "Serialized 12100 sequences...\n",
            "Serialized 12200 sequences...\n",
            "Serialized 12300 sequences...\n",
            "Serialized 12400 sequences...\n",
            "Serialized 12500 sequences...\n",
            "Serialized 12600 sequences...\n",
            "Serialized 12700 sequences...\n",
            "Serialized 12800 sequences...\n",
            "Serialized 12900 sequences...\n",
            "Serialized 13000 sequences...\n",
            "Serialized 13100 sequences...\n",
            "Serialized 13200 sequences...\n",
            "Serialized 13300 sequences...\n",
            "Serialized 13400 sequences...\n",
            "Serialized 13500 sequences...\n",
            "Serialized 13600 sequences...\n",
            "Serialized 13700 sequences...\n",
            "Serialized 13800 sequences...\n",
            "Serialized 13900 sequences...\n",
            "Serialized 14000 sequences...\n",
            "Serialized 14100 sequences...\n",
            "Serialized 14200 sequences...\n",
            "Serialized 14300 sequences...\n",
            "Serialized 14400 sequences...\n",
            "Serialized 14500 sequences...\n",
            "Serialized 14600 sequences...\n",
            "Serialized 14700 sequences...\n",
            "Serialized 14800 sequences...\n",
            "Serialized 14900 sequences...\n",
            "Serialized 15000 sequences...\n",
            "Serialized 15100 sequences...\n",
            "Serialized 15200 sequences...\n",
            "Serialized 15300 sequences...\n",
            "Serialized 15400 sequences...\n",
            "Serialized 15500 sequences...\n",
            "Serialized 15600 sequences...\n",
            "Serialized 15700 sequences...\n",
            "Serialized 15800 sequences...\n",
            "Serialized 15900 sequences...\n",
            "Serialized 16000 sequences...\n",
            "Serialized 16100 sequences...\n",
            "Serialized 16200 sequences...\n",
            "Serialized 16300 sequences...\n",
            "Serialized 16400 sequences...\n",
            "Serialized 16500 sequences...\n",
            "Serialized 16600 sequences...\n",
            "Serialized 16700 sequences...\n",
            "Serialized 16800 sequences...\n",
            "Serialized 16900 sequences...\n",
            "Serialized 17000 sequences...\n",
            "Serialized 17100 sequences...\n",
            "Serialized 17200 sequences...\n",
            "Serialized 17300 sequences...\n",
            "Serialized 17400 sequences...\n",
            "Serialized 17500 sequences...\n",
            "Serialized 17600 sequences...\n",
            "Serialized 17700 sequences...\n",
            "Serialized 17800 sequences...\n",
            "Serialized 17900 sequences...\n",
            "Serialized 18000 sequences...\n",
            "Serialized 18100 sequences...\n",
            "Serialized 18200 sequences...\n",
            "Serialized 18300 sequences...\n",
            "Serialized 18400 sequences...\n",
            "Serialized 18500 sequences...\n",
            "Serialized 18600 sequences...\n",
            "Serialized 18700 sequences...\n",
            "Serialized 18800 sequences...\n",
            "Serialized 18900 sequences...\n",
            "Serialized 19000 sequences...\n",
            "Serialized 19100 sequences...\n",
            "Serialized 19200 sequences...\n",
            "Serialized 19300 sequences...\n",
            "Serialized 19400 sequences...\n",
            "Serialized 19500 sequences...\n",
            "Serialized 19600 sequences...\n",
            "Serialized 19700 sequences...\n",
            "Serialized 19800 sequences...\n",
            "Serialized 19900 sequences...\n",
            "Serialized 20000 sequences...\n",
            "Serialized 20100 sequences...\n",
            "Serialized 20200 sequences...\n",
            "Serialized 20300 sequences...\n",
            "Serialized 20400 sequences...\n",
            "Serialized 20500 sequences...\n",
            "Serialized 20600 sequences...\n",
            "Serialized 20700 sequences...\n",
            "Serialized 20800 sequences...\n",
            "Serialized 20900 sequences...\n",
            "Serialized 21000 sequences...\n",
            "Serialized 21100 sequences...\n",
            "Serialized 21200 sequences...\n",
            "Serialized 21300 sequences...\n",
            "Serialized 21400 sequences...\n",
            "Serialized 21500 sequences...\n",
            "Serialized 21600 sequences...\n",
            "Serialized 21700 sequences...\n",
            "Serialized 21800 sequences...\n",
            "Serialized 21900 sequences...\n",
            "Serialized 22000 sequences...\n",
            "Serialized 22100 sequences...\n",
            "Serialized 22200 sequences...\n",
            "Serialized 22300 sequences...\n",
            "Serialized 22400 sequences...\n",
            "Serialized 22500 sequences...\n",
            "Serialized 22600 sequences...\n",
            "Serialized 22700 sequences...\n",
            "Serialized 22800 sequences...\n",
            "Serialized 22900 sequences...\n"
          ]
        }
      ],
      "source": [
        "!python prepare_data.py shakespeare_input.txt skp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TuThM3SqW-zX"
      },
      "source": [
        "**total serialized seq is 22981**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SlitZe4kwCE7"
      },
      "source": [
        "**Loading the data from skp.tfrecords and skp_vocab**\n",
        "\n",
        "The files mentioned are the output obtained after running the program *prepare_data.py* for the Shakespeare data. These files are loaded as data using tf.data and create a vocabulary dictionary \n",
        "\n",
        "**Note:** The vocab contains elements as dict with (key,val) as (character, index). Reverse mapping is done and stored as ind_to_ch which has (key,val) as (index,character)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "colab_type": "code",
        "id": "qZ9hhvAKWLbQ",
        "outputId": "af7b9a7b-9c9d-483e-de18-feac897747ac",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'p': 1, 'C': 2, 'f': 3, 'w': 4, 'h': 5, ']': 6, 't': 7, '\\n': 8, 'j': 9, 'e': 10, 'B': 11, 'o': 12, 'd': 13, \"'\": 14, '!': 15, 'O': 16, 'H': 17, 'U': 18, 'P': 19, 'M': 20, 'T': 21, '.': 22, '?': 23, '&': 24, 'Z': 25, 'A': 26, 'X': 27, ' ': 28, 'W': 29, 'q': 30, 'l': 31, 'J': 32, 'b': 33, '$': 34, 'D': 35, 'E': 36, 'F': 37, 'a': 38, '-': 39, ':': 40, 'I': 41, 'G': 42, 'c': 43, 'N': 44, 'Q': 45, 'V': 46, 'm': 47, ',': 48, 'L': 49, 'n': 50, 'R': 51, 'K': 52, ';': 53, 'g': 54, 'r': 55, 's': 56, 'Y': 57, 'x': 58, 'v': 59, 'S': 60, 'z': 61, 'u': 62, 'k': 63, 'i': 64, '3': 65, '[': 66, 'y': 67, '<S>': 0}\n",
            "68\n",
            "Indices to char\n",
            "{1: 'p', 2: 'C', 3: 'f', 4: 'w', 5: 'h', 6: ']', 7: 't', 8: '\\n', 9: 'j', 10: 'e', 11: 'B', 12: 'o', 13: 'd', 14: \"'\", 15: '!', 16: 'O', 17: 'H', 18: 'U', 19: 'P', 20: 'M', 21: 'T', 22: '.', 23: '?', 24: '&', 25: 'Z', 26: 'A', 27: 'X', 28: ' ', 29: 'W', 30: 'q', 31: 'l', 32: 'J', 33: 'b', 34: '$', 35: 'D', 36: 'E', 37: 'F', 38: 'a', 39: '-', 40: ':', 41: 'I', 42: 'G', 43: 'c', 44: 'N', 45: 'Q', 46: 'V', 47: 'm', 48: ',', 49: 'L', 50: 'n', 51: 'R', 52: 'K', 53: ';', 54: 'g', 55: 'r', 56: 's', 57: 'Y', 58: 'x', 59: 'v', 60: 'S', 61: 'z', 62: 'u', 63: 'k', 64: 'i', 65: '3', 66: '[', 67: 'y', 0: '<S>'}\n"
          ]
        }
      ],
      "source": [
        "from prepare_data import parse_seq\n",
        "import pickle\n",
        "\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data = tf.data.TFRecordDataset(\"skp.tfrecords\")\n",
        "\n",
        "# this maps a parser function that properly interprets the bytes over the dataset\n",
        "# (with fixed sequence length 200)\n",
        "# if you change the sequence length in preprocessing you also need to change it here\n",
        "data = data.map(lambda x: parse_seq(x, 200))\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab = pickle.load(open(\"skp_vocab\", mode=\"rb\"))\n",
        "vocab_size = len(vocab)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "\n",
        "print(vocab)\n",
        "print(vocab_size)\n",
        "\n",
        "print(\"Indices to char\")\n",
        "print(ind_to_ch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "lNQDmDUfTx2Z",
        "outputId": "f4d55c28-0364-4384-b558-59fe7fe9d871",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<MapDataset shapes: (200,), types: tf.int32>"
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LBWdH7SWPHxl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def create_input_target(data):\n",
        "    input_data = data[:-1]\n",
        "    target_data = data[1:]\n",
        "    return input_data, target_data\n",
        "\n",
        "\n",
        "data = data.map(create_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "colab_type": "code",
        "id": "WIcDfaSUQDj6",
        "outputId": "fd9776b5-6d77-4772-aefa-2a561ccd8193",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([  2 199], shape=(2,), dtype=int32)\n",
            "<class 'tuple'>\n",
            "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([  2, 199], dtype=int32)>\n",
            "(<tf.Tensor: shape=(199,), dtype=int32, numpy=\n",
            "array([ 0, 37, 64, 55, 56,  7, 28,  2, 64,  7, 64, 61, 10, 50, 40,  8, 11,\n",
            "       10,  3, 12, 55, 10, 28,  4, 10, 28,  1, 55, 12, 43, 10, 10, 13, 28,\n",
            "       38, 50, 67, 28,  3, 62, 55,  7,  5, 10, 55, 48, 28,  5, 10, 38, 55,\n",
            "       28, 47, 10, 28, 56,  1, 10, 38, 63, 22,  8,  8, 26, 31, 31, 40,  8,\n",
            "       60,  1, 10, 38, 63, 48, 28, 56,  1, 10, 38, 63, 22,  8,  8, 37, 64,\n",
            "       55, 56,  7, 28,  2, 64,  7, 64, 61, 10, 50, 40,  8, 57, 12, 62, 28,\n",
            "       38, 55, 10, 28, 38, 31, 31, 28, 55, 10, 56, 12, 31, 59, 10, 13, 28,\n",
            "       55, 38,  7,  5, 10, 55, 28,  7, 12, 28, 13, 64, 10, 28,  7,  5, 38,\n",
            "       50, 28,  7, 12, 28,  3, 38, 47, 64, 56,  5, 23,  8,  8, 26, 31, 31,\n",
            "       40,  8, 51, 10, 56, 12, 31, 59, 10, 13, 22, 28, 55, 10, 56, 12, 31,\n",
            "       59, 10, 13, 22,  8,  8, 37, 64, 55, 56,  7, 28,  2, 64,  7, 64, 61,\n",
            "       10, 50, 40,  8, 37, 64, 55, 56,  7, 48, 28, 67], dtype=int32)>, <tf.Tensor: shape=(199,), dtype=int32, numpy=\n",
            "array([37, 64, 55, 56,  7, 28,  2, 64,  7, 64, 61, 10, 50, 40,  8, 11, 10,\n",
            "        3, 12, 55, 10, 28,  4, 10, 28,  1, 55, 12, 43, 10, 10, 13, 28, 38,\n",
            "       50, 67, 28,  3, 62, 55,  7,  5, 10, 55, 48, 28,  5, 10, 38, 55, 28,\n",
            "       47, 10, 28, 56,  1, 10, 38, 63, 22,  8,  8, 26, 31, 31, 40,  8, 60,\n",
            "        1, 10, 38, 63, 48, 28, 56,  1, 10, 38, 63, 22,  8,  8, 37, 64, 55,\n",
            "       56,  7, 28,  2, 64,  7, 64, 61, 10, 50, 40,  8, 57, 12, 62, 28, 38,\n",
            "       55, 10, 28, 38, 31, 31, 28, 55, 10, 56, 12, 31, 59, 10, 13, 28, 55,\n",
            "       38,  7,  5, 10, 55, 28,  7, 12, 28, 13, 64, 10, 28,  7,  5, 38, 50,\n",
            "       28,  7, 12, 28,  3, 38, 47, 64, 56,  5, 23,  8,  8, 26, 31, 31, 40,\n",
            "        8, 51, 10, 56, 12, 31, 59, 10, 13, 22, 28, 55, 10, 56, 12, 31, 59,\n",
            "       10, 13, 22,  8,  8, 37, 64, 55, 56,  7, 28,  2, 64,  7, 64, 61, 10,\n",
            "       50, 40,  8, 37, 64, 55, 56,  7, 48, 28, 67, 12], dtype=int32)>)\n"
          ]
        }
      ],
      "source": [
        "for x in data.take(1):\n",
        "  print(tf.shape(x))\n",
        "  print(type(x))\n",
        "  print(repr(tf.shape(x)))\n",
        "  print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b-A6TAtOMqDu"
      },
      "source": [
        "### **Batching data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rFRLy9TYEeKQ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## Declare the sizes of batch, shuffle and repeat\n",
        "\n",
        "SHUFFLE_SIZE = 1000\n",
        "BATCH_SIZE = 128\n",
        "REPEAT_TIMES = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "H5JAavDtD_cu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def batch_shuffle_repeat(data):\n",
        "\n",
        "\n",
        "    data = data.shuffle(SHUFFLE_SIZE)\n",
        "    data = data.padded_batch(BATCH_SIZE, padded_shapes=None,drop_remainder=True)   \n",
        "    # data = data.repeat(REPEAT_TIMES)\n",
        "\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wyyEwmin_pDP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "dataset = batch_shuffle_repeat(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "colab_type": "code",
        "id": "SpGmPlMCpGiX",
        "outputId": "9f8685d1-ebca-4b7a-f130-b4c1c2b5d16c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(128, shape=(), dtype=int32)\n",
            "tf.Tensor(199, shape=(), dtype=int32)\n",
            "<class 'tuple'>\n",
            "<tf.Tensor: shape=(), dtype=int32, numpy=199>\n",
            "(<tf.Tensor: shape=(128, 199), dtype=int32, numpy=\n",
            "array([[ 0, 12, 13, ..., 13, 28, 33],\n",
            "       [ 0, 10, 55, ..., 28, 38,  7],\n",
            "       [ 0, 28,  2, ..., 56, 62, 55],\n",
            "       ...,\n",
            "       [ 0, 13, 10, ..., 28,  1, 38],\n",
            "       [ 0,  7,  5, ..., 13, 28, 54],\n",
            "       [ 0, 50, 54, ..., 26, 56, 28]], dtype=int32)>, <tf.Tensor: shape=(128, 199), dtype=int32, numpy=\n",
            "array([[12, 13, 67, ..., 28, 33, 10],\n",
            "       [10, 55, 28, ..., 38,  7, 28],\n",
            "       [28,  2, 12, ..., 62, 55, 10],\n",
            "       ...,\n",
            "       [13, 10, 28, ...,  1, 38, 64],\n",
            "       [ 7,  5, 10, ..., 28, 54, 12],\n",
            "       [50, 54, 28, ..., 56, 28,  5]], dtype=int32)>)\n"
          ]
        }
      ],
      "source": [
        "for x in dataset.take(1):\n",
        "  print(tf.shape(x)[0])\n",
        "  print(tf.shape(x)[1])\n",
        "\n",
        "  print(tf.shape(x)[2])\n",
        "\n",
        "  print(type(x))\n",
        "  print(repr(tf.shape(x)[2]))\n",
        "  print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3wrLAxYhMzDW"
      },
      "source": [
        "### **Model creation using Keras and printing text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "U-QbgpEJgl_n",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# The embedding dimension\n",
        "embedding_dim = 199\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lVOUybDxgmIn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8VZ2Hn8ouRko",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "colab_type": "code",
        "id": "C5ge-ycC1HnU",
        "outputId": "c99c4dd0-5c51-4e4e-e4d6-9bb4c26d6a05",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (128, None, 199)          13532     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (128, None, 512)          1095168   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (128, None, 68)           34884     \n",
            "=================================================================\n",
            "Total params: 1,143,584\n",
            "Trainable params: 1,143,584\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IIOJHsaOudNF",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "opt = tf.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ml3qbbgUwrbY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# stereotypical train-step-with-function-annotation\n",
        "\n",
        "@tf.function\n",
        "def train_step(input_batch, target_batch):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        logits = model(input_batch,training=True)\n",
        "        loss = loss_fn(target_batch, logits)\n",
        "        # print(loss)\n",
        "\n",
        "    varis = model.trainable_variables\n",
        "    grads = tape.gradient(loss, varis)\n",
        "    opt.apply_gradients(zip(grads, varis))\n",
        "\n",
        "    return loss, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "Ct4vNBRQudKp",
        "outputId": "f2870efc-d007-405e-c391-70b2a630e234",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Start of epoch 1\n",
            "Epoch: 1 Batch Number: 1 Loss: 4.219892501831055 Time taken: 3.700148582458496\n",
            "Epoch: 1 Batch Number: 2 Loss: 4.197140693664551 Time taken: 0.21980023384094238\n",
            "Epoch: 1 Batch Number: 3 Loss: 4.168467044830322 Time taken: 0.20259809494018555\n",
            "Epoch: 1 Batch Number: 4 Loss: 4.126499652862549 Time taken: 0.21307063102722168\n",
            "Epoch: 1 Batch Number: 5 Loss: 4.038734436035156 Time taken: 0.200700044631958\n",
            "Epoch: 1 Batch Number: 6 Loss: 3.822652578353882 Time taken: 0.2007887363433838\n",
            "Epoch: 1 Batch Number: 7 Loss: 4.333465576171875 Time taken: 0.20063257217407227\n",
            "Epoch: 1 Batch Number: 8 Loss: 3.560145616531372 Time taken: 0.2050924301147461\n",
            "Epoch: 1 Batch Number: 9 Loss: 3.7140934467315674 Time taken: 0.2062973976135254\n",
            "Epoch: 1 Batch Number: 10 Loss: 3.753002166748047 Time taken: 0.20309877395629883\n",
            "Epoch: 1 Batch Number: 11 Loss: 3.758662462234497 Time taken: 0.2037043571472168\n",
            "Epoch: 1 Batch Number: 12 Loss: 3.734156608581543 Time taken: 0.19968175888061523\n",
            "Epoch: 1 Batch Number: 13 Loss: 3.6841580867767334 Time taken: 0.19988799095153809\n",
            "Epoch: 1 Batch Number: 14 Loss: 3.636868953704834 Time taken: 0.20967507362365723\n",
            "Epoch: 1 Batch Number: 15 Loss: 3.5856237411499023 Time taken: 0.20015168190002441\n",
            "Epoch: 1 Batch Number: 16 Loss: 3.515702247619629 Time taken: 0.20619964599609375\n",
            "Epoch: 1 Batch Number: 17 Loss: 3.4479851722717285 Time taken: 0.20052146911621094\n",
            "Epoch: 1 Batch Number: 18 Loss: 3.421708345413208 Time taken: 0.20093774795532227\n",
            "Epoch: 1 Batch Number: 19 Loss: 3.4458487033843994 Time taken: 0.21007275581359863\n",
            "Epoch: 1 Batch Number: 20 Loss: 3.4251973628997803 Time taken: 0.2073807716369629\n",
            "Epoch: 1 Batch Number: 21 Loss: 3.4004387855529785 Time taken: 0.20256900787353516\n",
            "Epoch: 1 Batch Number: 22 Loss: 3.3404881954193115 Time taken: 0.20453786849975586\n",
            "Epoch: 1 Batch Number: 23 Loss: 3.310206413269043 Time taken: 0.20975232124328613\n",
            "Epoch: 1 Batch Number: 24 Loss: 3.2594072818756104 Time taken: 0.20085930824279785\n",
            "Epoch: 1 Batch Number: 25 Loss: 3.220364570617676 Time taken: 0.20211052894592285\n",
            "Epoch: 1 Batch Number: 26 Loss: 3.2126874923706055 Time taken: 0.20025205612182617\n",
            "Epoch: 1 Batch Number: 27 Loss: 3.1934010982513428 Time taken: 0.20261025428771973\n",
            "Epoch: 1 Batch Number: 28 Loss: 3.191751718521118 Time taken: 0.21452021598815918\n",
            "Epoch: 1 Batch Number: 29 Loss: 3.195920467376709 Time taken: 0.20190024375915527\n",
            "Epoch: 1 Batch Number: 30 Loss: 3.191944122314453 Time taken: 0.19852232933044434\n",
            "Epoch: 1 Batch Number: 31 Loss: 3.2046420574188232 Time taken: 0.20122146606445312\n",
            "Epoch: 1 Batch Number: 32 Loss: 3.203688144683838 Time taken: 0.2023029327392578\n",
            "Epoch: 1 Batch Number: 33 Loss: 3.1944053173065186 Time taken: 0.22116374969482422\n",
            "Epoch: 1 Batch Number: 34 Loss: 3.1687512397766113 Time taken: 0.20082712173461914\n",
            "Epoch: 1 Batch Number: 35 Loss: 3.1580114364624023 Time taken: 0.2059621810913086\n",
            "Epoch: 1 Batch Number: 36 Loss: 3.1599669456481934 Time taken: 0.2007896900177002\n",
            "Epoch: 1 Batch Number: 37 Loss: 3.116121292114258 Time taken: 0.20218443870544434\n",
            "Epoch: 1 Batch Number: 38 Loss: 3.1153790950775146 Time taken: 0.20543789863586426\n",
            "Epoch: 1 Batch Number: 39 Loss: 3.109265089035034 Time taken: 0.20241379737854004\n",
            "Epoch: 1 Batch Number: 40 Loss: 3.084416151046753 Time taken: 0.20150423049926758\n",
            "Epoch: 1 Batch Number: 41 Loss: 3.060940742492676 Time taken: 0.20044374465942383\n",
            "Epoch: 1 Batch Number: 42 Loss: 3.0915815830230713 Time taken: 0.2108147144317627\n",
            "Epoch: 1 Batch Number: 43 Loss: 3.0655744075775146 Time taken: 0.21041488647460938\n",
            "Epoch: 1 Batch Number: 44 Loss: 3.030646324157715 Time taken: 0.19827914237976074\n",
            "Epoch: 1 Batch Number: 45 Loss: 3.013460874557495 Time taken: 0.19997954368591309\n",
            "Epoch: 1 Batch Number: 46 Loss: 3.0161871910095215 Time taken: 0.20102214813232422\n",
            "Epoch: 1 Batch Number: 47 Loss: 2.982346534729004 Time taken: 0.20958733558654785\n",
            "Epoch: 1 Batch Number: 48 Loss: 2.9851059913635254 Time taken: 0.20778346061706543\n",
            "Epoch: 1 Batch Number: 49 Loss: 2.963412046432495 Time taken: 0.20497632026672363\n",
            "Epoch: 1 Batch Number: 50 Loss: 2.962000846862793 Time taken: 0.20373773574829102\n",
            "Epoch: 1 Batch Number: 51 Loss: 2.952232837677002 Time taken: 0.20406794548034668\n",
            "Epoch: 1 Batch Number: 52 Loss: 2.931091547012329 Time taken: 0.20782732963562012\n",
            "Epoch: 1 Batch Number: 53 Loss: 2.9341912269592285 Time taken: 0.20270729064941406\n",
            "Epoch: 1 Batch Number: 54 Loss: 2.9045956134796143 Time taken: 0.2054274082183838\n",
            "Epoch: 1 Batch Number: 55 Loss: 2.896864414215088 Time taken: 0.2008802890777588\n",
            "Epoch: 1 Batch Number: 56 Loss: 2.866513967514038 Time taken: 0.20160841941833496\n",
            "Epoch: 1 Batch Number: 57 Loss: 2.873072385787964 Time taken: 0.21234488487243652\n",
            "Epoch: 1 Batch Number: 58 Loss: 2.817004919052124 Time taken: 0.20006251335144043\n",
            "Epoch: 1 Batch Number: 59 Loss: 2.8020715713500977 Time taken: 0.20074248313903809\n",
            "Epoch: 1 Batch Number: 60 Loss: 2.7878940105438232 Time taken: 0.19988727569580078\n",
            "Epoch: 1 Batch Number: 61 Loss: 2.7685415744781494 Time taken: 0.20044636726379395\n",
            "Epoch: 1 Batch Number: 62 Loss: 2.747495651245117 Time taken: 0.20403599739074707\n",
            "Epoch: 1 Batch Number: 63 Loss: 2.718519926071167 Time taken: 0.2009899616241455\n",
            "Epoch: 1 Batch Number: 64 Loss: 2.7169275283813477 Time taken: 0.20247626304626465\n",
            "Epoch: 1 Batch Number: 65 Loss: 2.7014102935791016 Time taken: 0.19786643981933594\n",
            "Epoch: 1 Batch Number: 66 Loss: 2.6870837211608887 Time taken: 0.20550894737243652\n",
            "Epoch: 1 Batch Number: 67 Loss: 2.6627707481384277 Time taken: 0.2052750587463379\n",
            "Epoch: 1 Batch Number: 68 Loss: 2.666994094848633 Time taken: 0.20030975341796875\n",
            "Epoch: 1 Batch Number: 69 Loss: 2.6521029472351074 Time taken: 0.20088696479797363\n",
            "Epoch: 1 Batch Number: 70 Loss: 2.6266214847564697 Time taken: 0.20303821563720703\n",
            "Epoch: 1 Batch Number: 71 Loss: 2.6376330852508545 Time taken: 0.21015429496765137\n",
            "Epoch: 1 Batch Number: 72 Loss: 2.630095958709717 Time taken: 0.20208191871643066\n",
            "Epoch: 1 Batch Number: 73 Loss: 2.623814821243286 Time taken: 0.2019948959350586\n",
            "Epoch: 1 Batch Number: 74 Loss: 2.610445499420166 Time taken: 0.20355916023254395\n",
            "Epoch: 1 Batch Number: 75 Loss: 2.6059017181396484 Time taken: 0.20515108108520508\n",
            "Epoch: 1 Batch Number: 76 Loss: 2.588441848754883 Time taken: 0.20047950744628906\n",
            "Epoch: 1 Batch Number: 77 Loss: 2.6019394397735596 Time taken: 0.20357608795166016\n",
            "Epoch: 1 Batch Number: 78 Loss: 2.5617752075195312 Time taken: 0.20378971099853516\n",
            "Epoch: 1 Batch Number: 79 Loss: 2.563727855682373 Time taken: 0.20110034942626953\n",
            "Epoch: 1 Batch Number: 80 Loss: 2.554215669631958 Time taken: 0.20436859130859375\n",
            "Epoch: 1 Batch Number: 81 Loss: 2.5477523803710938 Time taken: 0.2027420997619629\n",
            "Epoch: 1 Batch Number: 82 Loss: 2.534597635269165 Time taken: 0.20953607559204102\n",
            "Epoch: 1 Batch Number: 83 Loss: 2.523286819458008 Time taken: 0.19727444648742676\n",
            "Epoch: 1 Batch Number: 84 Loss: 2.5220730304718018 Time taken: 0.20480060577392578\n",
            "Epoch: 1 Batch Number: 85 Loss: 2.521174907684326 Time taken: 0.20215964317321777\n",
            "Epoch: 1 Batch Number: 86 Loss: 2.5113329887390137 Time taken: 0.21193671226501465\n",
            "Epoch: 1 Batch Number: 87 Loss: 2.511237144470215 Time taken: 0.19815635681152344\n",
            "Epoch: 1 Batch Number: 88 Loss: 2.501485586166382 Time taken: 0.19585728645324707\n",
            "Epoch: 1 Batch Number: 89 Loss: 2.48919939994812 Time taken: 0.20032095909118652\n",
            "Epoch: 1 Batch Number: 90 Loss: 2.4924542903900146 Time taken: 0.20049238204956055\n",
            "Epoch: 1 Batch Number: 91 Loss: 2.4922351837158203 Time taken: 0.20669960975646973\n",
            "Epoch: 1 Batch Number: 92 Loss: 2.4904515743255615 Time taken: 0.1958463191986084\n",
            "Epoch: 1 Batch Number: 93 Loss: 2.4857635498046875 Time taken: 0.2013695240020752\n",
            "Epoch: 1 Batch Number: 94 Loss: 2.4738082885742188 Time taken: 0.2003920078277588\n",
            "Epoch: 1 Batch Number: 95 Loss: 2.4959123134613037 Time taken: 0.20299124717712402\n",
            "Epoch: 1 Batch Number: 96 Loss: 2.458436965942383 Time taken: 0.2014765739440918\n",
            "Epoch: 1 Batch Number: 97 Loss: 2.466505527496338 Time taken: 0.20278310775756836\n",
            "Epoch: 1 Batch Number: 98 Loss: 2.446774959564209 Time taken: 0.20196032524108887\n",
            "Epoch: 1 Batch Number: 99 Loss: 2.441323757171631 Time taken: 0.19935131072998047\n",
            "Epoch: 1 Batch Number: 100 Loss: 2.4470410346984863 Time taken: 0.20318031311035156\n",
            "Epoch: 1 Batch Number: 101 Loss: 2.431346893310547 Time taken: 0.20233440399169922\n",
            "Epoch: 1 Batch Number: 102 Loss: 2.4265975952148438 Time taken: 0.2036755084991455\n",
            "Epoch: 1 Batch Number: 103 Loss: 2.421459436416626 Time taken: 0.20021748542785645\n",
            "Epoch: 1 Batch Number: 104 Loss: 2.412156820297241 Time taken: 0.20448613166809082\n",
            "Epoch: 1 Batch Number: 105 Loss: 2.40889835357666 Time taken: 0.2031078338623047\n",
            "Epoch: 1 Batch Number: 106 Loss: 2.426091432571411 Time taken: 0.2016124725341797\n",
            "Epoch: 1 Batch Number: 107 Loss: 2.3954336643218994 Time taken: 0.2012465000152588\n",
            "Epoch: 1 Batch Number: 108 Loss: 2.4118645191192627 Time taken: 0.20039129257202148\n",
            "Epoch: 1 Batch Number: 109 Loss: 2.3942582607269287 Time taken: 0.21143007278442383\n",
            "Epoch: 1 Batch Number: 110 Loss: 2.397381067276001 Time taken: 0.20913290977478027\n",
            "Epoch: 1 Batch Number: 111 Loss: 2.3873040676116943 Time taken: 0.2080075740814209\n",
            "Epoch: 1 Batch Number: 112 Loss: 2.4031760692596436 Time taken: 0.2082815170288086\n",
            "Epoch: 1 Batch Number: 113 Loss: 2.415726661682129 Time taken: 0.20113182067871094\n",
            "Epoch: 1 Batch Number: 114 Loss: 2.3901357650756836 Time taken: 0.20155620574951172\n",
            "Epoch: 1 Batch Number: 115 Loss: 2.3962013721466064 Time taken: 0.20438647270202637\n",
            "Epoch: 1 Batch Number: 116 Loss: 2.3869850635528564 Time taken: 0.20775580406188965\n",
            "Epoch: 1 Batch Number: 117 Loss: 2.3993582725524902 Time taken: 0.20240306854248047\n",
            "Epoch: 1 Batch Number: 118 Loss: 2.3717215061187744 Time taken: 0.1999983787536621\n",
            "Epoch: 1 Batch Number: 119 Loss: 2.3778395652770996 Time taken: 0.20414423942565918\n",
            "Epoch: 1 Batch Number: 120 Loss: 2.3753819465637207 Time taken: 0.19926023483276367\n",
            "Epoch: 1 Batch Number: 121 Loss: 2.375277042388916 Time taken: 0.20703434944152832\n",
            "Epoch: 1 Batch Number: 122 Loss: 2.365445852279663 Time taken: 0.20057439804077148\n",
            "Epoch: 1 Batch Number: 123 Loss: 2.3725552558898926 Time taken: 0.1981520652770996\n",
            "Epoch: 1 Batch Number: 124 Loss: 2.3476979732513428 Time taken: 0.2032783031463623\n",
            "Epoch: 1 Batch Number: 125 Loss: 2.358459234237671 Time taken: 0.2030932903289795\n",
            "Epoch: 1 Batch Number: 126 Loss: 2.367464542388916 Time taken: 0.20656919479370117\n",
            "Epoch: 1 Batch Number: 127 Loss: 2.3614985942840576 Time taken: 0.20077180862426758\n",
            "Epoch: 1 Batch Number: 128 Loss: 2.368712902069092 Time taken: 0.20043706893920898\n",
            "Epoch: 1 Batch Number: 129 Loss: 2.3449621200561523 Time taken: 0.2006673812866211\n",
            "Epoch: 1 Batch Number: 130 Loss: 2.346207857131958 Time taken: 0.19895577430725098\n",
            "Epoch: 1 Batch Number: 131 Loss: 2.3606574535369873 Time taken: 0.20143866539001465\n",
            "Epoch: 1 Batch Number: 132 Loss: 2.348686456680298 Time taken: 0.20158028602600098\n",
            "Epoch: 1 Batch Number: 133 Loss: 2.3787126541137695 Time taken: 0.20113015174865723\n",
            "Epoch: 1 Batch Number: 134 Loss: 2.3630642890930176 Time taken: 0.20174169540405273\n",
            "Epoch: 1 Batch Number: 135 Loss: 2.3574657440185547 Time taken: 0.20324349403381348\n",
            "Epoch: 1 Batch Number: 136 Loss: 2.358633279800415 Time taken: 0.20076251029968262\n",
            "Epoch: 1 Batch Number: 137 Loss: 2.340167284011841 Time taken: 0.20273137092590332\n",
            "Epoch: 1 Batch Number: 138 Loss: 2.3527963161468506 Time taken: 0.2066798210144043\n",
            "Epoch: 1 Batch Number: 139 Loss: 2.3430793285369873 Time taken: 0.20681023597717285\n",
            "Epoch: 1 Batch Number: 140 Loss: 2.3399658203125 Time taken: 0.20817780494689941\n",
            "Epoch: 1 Batch Number: 141 Loss: 2.342909574508667 Time taken: 0.20181727409362793\n",
            "Epoch: 1 Batch Number: 142 Loss: 2.3520379066467285 Time taken: 0.2034904956817627\n",
            "Epoch: 1 Batch Number: 143 Loss: 2.3326117992401123 Time taken: 0.20101714134216309\n",
            "Epoch: 1 Batch Number: 144 Loss: 2.3229291439056396 Time taken: 0.2072305679321289\n",
            "Epoch: 1 Batch Number: 145 Loss: 2.333301067352295 Time taken: 0.20580792427062988\n",
            "Epoch: 1 Batch Number: 146 Loss: 2.3178212642669678 Time taken: 0.20638012886047363\n",
            "Epoch: 1 Batch Number: 147 Loss: 2.329435348510742 Time taken: 0.21379852294921875\n",
            "Epoch: 1 Batch Number: 148 Loss: 2.3172967433929443 Time taken: 0.20033478736877441\n",
            "Epoch: 1 Batch Number: 149 Loss: 2.300785541534424 Time taken: 0.20279574394226074\n",
            "Epoch: 1 Batch Number: 150 Loss: 2.31282901763916 Time taken: 0.20473313331604004\n",
            "Epoch: 1 Batch Number: 151 Loss: 2.3174805641174316 Time taken: 0.2000741958618164\n",
            "Epoch: 1 Batch Number: 152 Loss: 2.330859899520874 Time taken: 0.19990086555480957\n",
            "Epoch: 1 Batch Number: 153 Loss: 2.311298370361328 Time taken: 0.1999506950378418\n",
            "Epoch: 1 Batch Number: 154 Loss: 2.3133156299591064 Time taken: 0.20104742050170898\n",
            "Epoch: 1 Batch Number: 155 Loss: 2.322547674179077 Time taken: 0.20525717735290527\n",
            "Epoch: 1 Batch Number: 156 Loss: 2.3053481578826904 Time taken: 0.19856905937194824\n",
            "Epoch: 1 Batch Number: 157 Loss: 2.285996913909912 Time taken: 0.20269513130187988\n",
            "Epoch: 1 Batch Number: 158 Loss: 2.2946808338165283 Time taken: 0.20045733451843262\n",
            "Epoch: 1 Batch Number: 159 Loss: 2.3085575103759766 Time taken: 0.20486783981323242\n",
            "Epoch: 1 Batch Number: 160 Loss: 2.2958483695983887 Time taken: 0.2055513858795166\n",
            "Epoch: 1 Batch Number: 161 Loss: 2.2976856231689453 Time taken: 0.2039642333984375\n",
            "Epoch: 1 Batch Number: 162 Loss: 2.284863233566284 Time taken: 0.1999814510345459\n",
            "Epoch: 1 Batch Number: 163 Loss: 2.298884391784668 Time taken: 0.199676513671875\n",
            "Epoch: 1 Batch Number: 164 Loss: 2.2801051139831543 Time taken: 0.20615100860595703\n",
            "Epoch: 1 Batch Number: 165 Loss: 2.281956911087036 Time taken: 0.20170021057128906\n",
            "Epoch: 1 Batch Number: 166 Loss: 2.2799909114837646 Time taken: 0.2022709846496582\n",
            "Epoch: 1 Batch Number: 167 Loss: 2.295562267303467 Time taken: 0.20078468322753906\n",
            "Epoch: 1 Batch Number: 168 Loss: 2.263956069946289 Time taken: 0.2080061435699463\n",
            "Epoch: 1 Batch Number: 169 Loss: 2.2423298358917236 Time taken: 0.2022104263305664\n",
            "Epoch: 1 Batch Number: 170 Loss: 2.262009620666504 Time taken: 0.19923090934753418\n",
            "Epoch: 1 Batch Number: 171 Loss: 2.2703893184661865 Time taken: 0.20081257820129395\n",
            "Epoch: 1 Batch Number: 172 Loss: 2.2413158416748047 Time taken: 0.2015688419342041\n",
            "Epoch: 1 Batch Number: 173 Loss: 2.2449536323547363 Time taken: 0.2060999870300293\n",
            "Epoch: 1 Batch Number: 174 Loss: 2.246248245239258 Time taken: 0.19806408882141113\n",
            "Epoch: 1 Batch Number: 175 Loss: 2.2580316066741943 Time taken: 0.20012593269348145\n",
            "Epoch: 1 Batch Number: 176 Loss: 2.2363128662109375 Time taken: 0.20343971252441406\n",
            "Epoch: 1 Batch Number: 177 Loss: 2.2462995052337646 Time taken: 0.20322489738464355\n",
            "Epoch: 1 Batch Number: 178 Loss: 2.2312228679656982 Time taken: 0.2069847583770752\n",
            "Epoch: 1 Batch Number: 179 Loss: 2.2397234439849854 Time taken: 0.20222091674804688\n",
            "==========================================================================================\n",
            "Start of epoch 2\n",
            "Epoch: 2 Batch Number: 1 Loss: 2.233219861984253 Time taken: 0.1974649429321289\n",
            "Epoch: 2 Batch Number: 2 Loss: 2.2349605560302734 Time taken: 0.19830536842346191\n",
            "Epoch: 2 Batch Number: 3 Loss: 2.2236554622650146 Time taken: 0.2006692886352539\n",
            "Epoch: 2 Batch Number: 4 Loss: 2.2291698455810547 Time taken: 0.2012779712677002\n",
            "Epoch: 2 Batch Number: 5 Loss: 2.2284252643585205 Time taken: 0.20021843910217285\n",
            "Epoch: 2 Batch Number: 6 Loss: 2.220836639404297 Time taken: 0.203749418258667\n",
            "Epoch: 2 Batch Number: 7 Loss: 2.219228744506836 Time taken: 0.20267033576965332\n",
            "Epoch: 2 Batch Number: 8 Loss: 2.226663827896118 Time taken: 0.1999363899230957\n",
            "Epoch: 2 Batch Number: 9 Loss: 2.2332701683044434 Time taken: 0.20056605339050293\n",
            "Epoch: 2 Batch Number: 10 Loss: 2.228001594543457 Time taken: 0.2117772102355957\n",
            "Epoch: 2 Batch Number: 11 Loss: 2.2233705520629883 Time taken: 0.20451998710632324\n",
            "Epoch: 2 Batch Number: 12 Loss: 2.2175745964050293 Time taken: 0.19949102401733398\n",
            "Epoch: 2 Batch Number: 13 Loss: 2.218188524246216 Time taken: 0.2064986228942871\n",
            "Epoch: 2 Batch Number: 14 Loss: 2.2194180488586426 Time taken: 0.20159292221069336\n",
            "Epoch: 2 Batch Number: 15 Loss: 2.213691234588623 Time taken: 0.21416044235229492\n",
            "Epoch: 2 Batch Number: 16 Loss: 2.1927783489227295 Time taken: 0.2017965316772461\n",
            "Epoch: 2 Batch Number: 17 Loss: 2.194159984588623 Time taken: 0.21468710899353027\n",
            "Epoch: 2 Batch Number: 18 Loss: 2.2042205333709717 Time taken: 0.22884249687194824\n",
            "Epoch: 2 Batch Number: 19 Loss: 2.1868584156036377 Time taken: 0.20053410530090332\n",
            "Epoch: 2 Batch Number: 20 Loss: 2.1756341457366943 Time taken: 0.2006685733795166\n",
            "Epoch: 2 Batch Number: 21 Loss: 2.1772570610046387 Time taken: 0.20560050010681152\n",
            "Epoch: 2 Batch Number: 22 Loss: 2.1731371879577637 Time taken: 0.20050930976867676\n",
            "Epoch: 2 Batch Number: 23 Loss: 2.2027621269226074 Time taken: 0.19859099388122559\n",
            "Epoch: 2 Batch Number: 24 Loss: 2.1746630668640137 Time taken: 0.20584917068481445\n",
            "Epoch: 2 Batch Number: 25 Loss: 2.1783010959625244 Time taken: 0.2045278549194336\n",
            "Epoch: 2 Batch Number: 26 Loss: 2.18636417388916 Time taken: 0.2039947509765625\n",
            "Epoch: 2 Batch Number: 27 Loss: 2.1652679443359375 Time taken: 0.21309542655944824\n",
            "Epoch: 2 Batch Number: 28 Loss: 2.180230140686035 Time taken: 0.20424532890319824\n",
            "Epoch: 2 Batch Number: 29 Loss: 2.1930203437805176 Time taken: 0.20171904563903809\n",
            "Epoch: 2 Batch Number: 30 Loss: 2.166550397872925 Time taken: 0.20581960678100586\n",
            "Epoch: 2 Batch Number: 31 Loss: 2.1685726642608643 Time taken: 0.2031402587890625\n",
            "Epoch: 2 Batch Number: 32 Loss: 2.1544768810272217 Time taken: 0.21130061149597168\n",
            "Epoch: 2 Batch Number: 33 Loss: 2.1602089405059814 Time taken: 0.1999039649963379\n",
            "Epoch: 2 Batch Number: 34 Loss: 2.1884281635284424 Time taken: 0.20297789573669434\n",
            "Epoch: 2 Batch Number: 35 Loss: 2.147352457046509 Time taken: 0.20750975608825684\n",
            "Epoch: 2 Batch Number: 36 Loss: 2.1315701007843018 Time taken: 0.20026040077209473\n",
            "Epoch: 2 Batch Number: 37 Loss: 2.1559531688690186 Time taken: 0.2083568572998047\n",
            "Epoch: 2 Batch Number: 38 Loss: 2.161187171936035 Time taken: 0.20064902305603027\n",
            "Epoch: 2 Batch Number: 39 Loss: 2.1481995582580566 Time taken: 0.21298575401306152\n",
            "Epoch: 2 Batch Number: 40 Loss: 2.14756441116333 Time taken: 0.20122790336608887\n",
            "Epoch: 2 Batch Number: 41 Loss: 2.14186954498291 Time taken: 0.2015688419342041\n",
            "Epoch: 2 Batch Number: 42 Loss: 2.130455732345581 Time taken: 0.20166730880737305\n",
            "Epoch: 2 Batch Number: 43 Loss: 2.1611056327819824 Time taken: 0.2038402557373047\n",
            "Epoch: 2 Batch Number: 44 Loss: 2.138841390609741 Time taken: 0.20228314399719238\n",
            "Epoch: 2 Batch Number: 45 Loss: 2.116203784942627 Time taken: 0.2010183334350586\n",
            "Epoch: 2 Batch Number: 46 Loss: 2.1317172050476074 Time taken: 0.2071056365966797\n",
            "Epoch: 2 Batch Number: 47 Loss: 2.1114425659179688 Time taken: 0.20395278930664062\n",
            "Epoch: 2 Batch Number: 48 Loss: 2.144886016845703 Time taken: 0.20250749588012695\n",
            "Epoch: 2 Batch Number: 49 Loss: 2.115138292312622 Time taken: 0.203535795211792\n",
            "Epoch: 2 Batch Number: 50 Loss: 2.1538877487182617 Time taken: 0.201416015625\n",
            "Epoch: 2 Batch Number: 51 Loss: 2.135442018508911 Time taken: 0.21027708053588867\n",
            "Epoch: 2 Batch Number: 52 Loss: 2.1368913650512695 Time taken: 0.20035481452941895\n",
            "Epoch: 2 Batch Number: 53 Loss: 2.139390707015991 Time taken: 0.2058565616607666\n",
            "Epoch: 2 Batch Number: 54 Loss: 2.1441595554351807 Time taken: 0.20173430442810059\n",
            "Epoch: 2 Batch Number: 55 Loss: 2.137958288192749 Time taken: 0.20079684257507324\n",
            "Epoch: 2 Batch Number: 56 Loss: 2.112677574157715 Time taken: 0.20728588104248047\n",
            "Epoch: 2 Batch Number: 57 Loss: 2.129255771636963 Time taken: 0.20372653007507324\n",
            "Epoch: 2 Batch Number: 58 Loss: 2.1165289878845215 Time taken: 0.2014482021331787\n",
            "Epoch: 2 Batch Number: 59 Loss: 2.1049485206604004 Time taken: 0.2017207145690918\n",
            "Epoch: 2 Batch Number: 60 Loss: 2.0964486598968506 Time taken: 0.20229840278625488\n",
            "Epoch: 2 Batch Number: 61 Loss: 2.082376003265381 Time taken: 0.2049875259399414\n",
            "Epoch: 2 Batch Number: 62 Loss: 2.0987484455108643 Time taken: 0.2038896083831787\n",
            "Epoch: 2 Batch Number: 63 Loss: 2.0833401679992676 Time taken: 0.20768070220947266\n",
            "Epoch: 2 Batch Number: 64 Loss: 2.0855116844177246 Time taken: 0.19968175888061523\n",
            "Epoch: 2 Batch Number: 65 Loss: 2.073748826980591 Time taken: 0.20073175430297852\n",
            "Epoch: 2 Batch Number: 66 Loss: 2.1113698482513428 Time taken: 0.207061767578125\n",
            "Epoch: 2 Batch Number: 67 Loss: 2.096259832382202 Time taken: 0.203263521194458\n",
            "Epoch: 2 Batch Number: 68 Loss: 2.1004276275634766 Time taken: 0.21579194068908691\n",
            "Epoch: 2 Batch Number: 69 Loss: 2.0759830474853516 Time taken: 0.20098066329956055\n",
            "Epoch: 2 Batch Number: 70 Loss: 2.061289072036743 Time taken: 0.2062225341796875\n",
            "Epoch: 2 Batch Number: 71 Loss: 2.0954346656799316 Time taken: 0.19972014427185059\n",
            "Epoch: 2 Batch Number: 72 Loss: 2.0954971313476562 Time taken: 0.20012450218200684\n",
            "Epoch: 2 Batch Number: 73 Loss: 2.075181484222412 Time taken: 0.20653986930847168\n",
            "Epoch: 2 Batch Number: 74 Loss: 2.0949254035949707 Time taken: 0.1983647346496582\n",
            "Epoch: 2 Batch Number: 75 Loss: 2.0804331302642822 Time taken: 0.20047616958618164\n",
            "Epoch: 2 Batch Number: 76 Loss: 2.0740277767181396 Time taken: 0.2069847583770752\n",
            "Epoch: 2 Batch Number: 77 Loss: 2.0672411918640137 Time taken: 0.20141839981079102\n",
            "Epoch: 2 Batch Number: 78 Loss: 2.065106153488159 Time taken: 0.21560406684875488\n",
            "Epoch: 2 Batch Number: 79 Loss: 2.0547780990600586 Time taken: 0.20045018196105957\n",
            "Epoch: 2 Batch Number: 80 Loss: 2.062711477279663 Time taken: 0.20810699462890625\n",
            "Epoch: 2 Batch Number: 81 Loss: 2.0679750442504883 Time taken: 0.20196771621704102\n",
            "Epoch: 2 Batch Number: 82 Loss: 2.0558202266693115 Time taken: 0.20158815383911133\n",
            "Epoch: 2 Batch Number: 83 Loss: 2.055542230606079 Time taken: 0.20133113861083984\n",
            "Epoch: 2 Batch Number: 84 Loss: 2.046414375305176 Time taken: 0.2021644115447998\n",
            "Epoch: 2 Batch Number: 85 Loss: 2.0617446899414062 Time taken: 0.20362615585327148\n",
            "Epoch: 2 Batch Number: 86 Loss: 2.044466018676758 Time taken: 0.20371723175048828\n",
            "Epoch: 2 Batch Number: 87 Loss: 2.0498251914978027 Time taken: 0.2067263126373291\n",
            "Epoch: 2 Batch Number: 88 Loss: 2.0242457389831543 Time taken: 0.20273637771606445\n",
            "Epoch: 2 Batch Number: 89 Loss: 2.0457632541656494 Time taken: 0.20325970649719238\n",
            "Epoch: 2 Batch Number: 90 Loss: 2.0263047218322754 Time taken: 0.20357179641723633\n",
            "Epoch: 2 Batch Number: 91 Loss: 2.042400360107422 Time taken: 0.19974303245544434\n",
            "Epoch: 2 Batch Number: 92 Loss: 2.0626425743103027 Time taken: 0.21102690696716309\n",
            "Epoch: 2 Batch Number: 93 Loss: 2.0553574562072754 Time taken: 0.1994020938873291\n",
            "Epoch: 2 Batch Number: 94 Loss: 2.042970657348633 Time taken: 0.20469260215759277\n",
            "Epoch: 2 Batch Number: 95 Loss: 2.0706121921539307 Time taken: 0.20040130615234375\n",
            "Epoch: 2 Batch Number: 96 Loss: 2.0427260398864746 Time taken: 0.20151925086975098\n",
            "Epoch: 2 Batch Number: 97 Loss: 2.04746150970459 Time taken: 0.20383882522583008\n",
            "Epoch: 2 Batch Number: 98 Loss: 2.051671266555786 Time taken: 0.20117592811584473\n",
            "Epoch: 2 Batch Number: 99 Loss: 2.0237374305725098 Time taken: 0.20039749145507812\n",
            "Epoch: 2 Batch Number: 100 Loss: 2.0289065837860107 Time taken: 0.20282244682312012\n",
            "Epoch: 2 Batch Number: 101 Loss: 2.0483152866363525 Time taken: 0.20333313941955566\n",
            "Epoch: 2 Batch Number: 102 Loss: 2.0165786743164062 Time taken: 0.19937729835510254\n",
            "Epoch: 2 Batch Number: 103 Loss: 2.044827699661255 Time taken: 0.20275068283081055\n",
            "Epoch: 2 Batch Number: 104 Loss: 2.029630661010742 Time taken: 0.20255398750305176\n",
            "Epoch: 2 Batch Number: 105 Loss: 2.0267679691314697 Time taken: 0.21051502227783203\n",
            "Epoch: 2 Batch Number: 106 Loss: 2.031019926071167 Time taken: 0.20398426055908203\n",
            "Epoch: 2 Batch Number: 107 Loss: 2.0156970024108887 Time taken: 0.21060657501220703\n",
            "Epoch: 2 Batch Number: 108 Loss: 2.028691291809082 Time taken: 0.20604324340820312\n",
            "Epoch: 2 Batch Number: 109 Loss: 2.023836135864258 Time taken: 0.20699429512023926\n",
            "Epoch: 2 Batch Number: 110 Loss: 2.039212226867676 Time taken: 0.1997213363647461\n",
            "Epoch: 2 Batch Number: 111 Loss: 2.028825044631958 Time taken: 0.2028946876525879\n",
            "Epoch: 2 Batch Number: 112 Loss: 2.0347821712493896 Time taken: 0.20323657989501953\n",
            "Epoch: 2 Batch Number: 113 Loss: 2.0135879516601562 Time taken: 0.2020702362060547\n",
            "Epoch: 2 Batch Number: 114 Loss: 2.0315608978271484 Time taken: 0.20705628395080566\n",
            "Epoch: 2 Batch Number: 115 Loss: 2.0240862369537354 Time taken: 0.20249319076538086\n",
            "Epoch: 2 Batch Number: 116 Loss: 2.0352084636688232 Time taken: 0.2013869285583496\n",
            "Epoch: 2 Batch Number: 117 Loss: 2.009760856628418 Time taken: 0.1998753547668457\n",
            "Epoch: 2 Batch Number: 118 Loss: 1.9874752759933472 Time taken: 0.20250773429870605\n",
            "Epoch: 2 Batch Number: 119 Loss: 2.016545057296753 Time taken: 0.20328664779663086\n",
            "Epoch: 2 Batch Number: 120 Loss: 2.018857955932617 Time taken: 0.20284247398376465\n",
            "Epoch: 2 Batch Number: 121 Loss: 2.005990743637085 Time taken: 0.20322465896606445\n",
            "Epoch: 2 Batch Number: 122 Loss: 1.9901562929153442 Time taken: 0.20003008842468262\n",
            "Epoch: 2 Batch Number: 123 Loss: 1.9980852603912354 Time taken: 0.1997835636138916\n",
            "Epoch: 2 Batch Number: 124 Loss: 2.0143611431121826 Time taken: 0.20548772811889648\n",
            "Epoch: 2 Batch Number: 125 Loss: 2.0087172985076904 Time taken: 0.20234107971191406\n",
            "Epoch: 2 Batch Number: 126 Loss: 1.9989811182022095 Time taken: 0.2066183090209961\n",
            "Epoch: 2 Batch Number: 127 Loss: 1.9867357015609741 Time taken: 0.20076870918273926\n",
            "Epoch: 2 Batch Number: 128 Loss: 1.9897223711013794 Time taken: 0.1985950469970703\n",
            "Epoch: 2 Batch Number: 129 Loss: 1.970564365386963 Time taken: 0.215118408203125\n",
            "Epoch: 2 Batch Number: 130 Loss: 1.9793380498886108 Time taken: 0.20116496086120605\n",
            "Epoch: 2 Batch Number: 131 Loss: 1.986975908279419 Time taken: 0.2051069736480713\n",
            "Epoch: 2 Batch Number: 132 Loss: 2.0019125938415527 Time taken: 0.20048093795776367\n",
            "Epoch: 2 Batch Number: 133 Loss: 2.0018160343170166 Time taken: 0.20200276374816895\n",
            "Epoch: 2 Batch Number: 134 Loss: 2.0167624950408936 Time taken: 0.2002861499786377\n",
            "Epoch: 2 Batch Number: 135 Loss: 1.9963253736495972 Time taken: 0.20645856857299805\n",
            "Epoch: 2 Batch Number: 136 Loss: 1.995345115661621 Time taken: 0.20211195945739746\n",
            "Epoch: 2 Batch Number: 137 Loss: 1.982893943786621 Time taken: 0.20906591415405273\n",
            "Epoch: 2 Batch Number: 138 Loss: 1.9870604276657104 Time taken: 0.2011406421661377\n",
            "Epoch: 2 Batch Number: 139 Loss: 1.9840164184570312 Time taken: 0.20079565048217773\n",
            "Epoch: 2 Batch Number: 140 Loss: 1.9931727647781372 Time taken: 0.20466017723083496\n",
            "Epoch: 2 Batch Number: 141 Loss: 1.9592162370681763 Time taken: 0.21439623832702637\n",
            "Epoch: 2 Batch Number: 142 Loss: 1.9907950162887573 Time taken: 0.20258593559265137\n",
            "Epoch: 2 Batch Number: 143 Loss: 1.9743247032165527 Time taken: 0.20804262161254883\n",
            "Epoch: 2 Batch Number: 144 Loss: 1.9772642850875854 Time taken: 0.20303893089294434\n",
            "Epoch: 2 Batch Number: 145 Loss: 1.96471107006073 Time taken: 0.20354342460632324\n",
            "Epoch: 2 Batch Number: 146 Loss: 1.9864721298217773 Time taken: 0.20322561264038086\n",
            "Epoch: 2 Batch Number: 147 Loss: 1.9692248106002808 Time taken: 0.1990349292755127\n",
            "Epoch: 2 Batch Number: 148 Loss: 1.9649314880371094 Time taken: 0.21561360359191895\n",
            "Epoch: 2 Batch Number: 149 Loss: 1.9515513181686401 Time taken: 0.20261383056640625\n",
            "Epoch: 2 Batch Number: 150 Loss: 1.9679619073867798 Time taken: 0.21289610862731934\n",
            "Epoch: 2 Batch Number: 151 Loss: 1.9496346712112427 Time taken: 0.20660948753356934\n",
            "Epoch: 2 Batch Number: 152 Loss: 1.9657117128372192 Time taken: 0.2007763385772705\n",
            "Epoch: 2 Batch Number: 153 Loss: 1.9603331089019775 Time taken: 0.20140600204467773\n",
            "Epoch: 2 Batch Number: 154 Loss: 1.950821876525879 Time taken: 0.19963860511779785\n",
            "Epoch: 2 Batch Number: 155 Loss: 1.9385461807250977 Time taken: 0.2038571834564209\n",
            "Epoch: 2 Batch Number: 156 Loss: 1.9580998420715332 Time taken: 0.20014572143554688\n",
            "Epoch: 2 Batch Number: 157 Loss: 1.9658206701278687 Time taken: 0.20289158821105957\n",
            "Epoch: 2 Batch Number: 158 Loss: 1.9607278108596802 Time taken: 0.20612096786499023\n",
            "Epoch: 2 Batch Number: 159 Loss: 1.9546085596084595 Time taken: 0.20247149467468262\n",
            "Epoch: 2 Batch Number: 160 Loss: 1.945627212524414 Time taken: 0.21158933639526367\n",
            "Epoch: 2 Batch Number: 161 Loss: 1.9648683071136475 Time taken: 0.1984260082244873\n",
            "Epoch: 2 Batch Number: 162 Loss: 1.9291954040527344 Time taken: 0.21197748184204102\n",
            "Epoch: 2 Batch Number: 163 Loss: 1.930869221687317 Time taken: 0.20288729667663574\n",
            "Epoch: 2 Batch Number: 164 Loss: 1.9338690042495728 Time taken: 0.20344948768615723\n",
            "Epoch: 2 Batch Number: 165 Loss: 1.9304989576339722 Time taken: 0.21451735496520996\n",
            "Epoch: 2 Batch Number: 166 Loss: 1.9421552419662476 Time taken: 0.2032470703125\n",
            "Epoch: 2 Batch Number: 167 Loss: 1.9394607543945312 Time taken: 0.20759105682373047\n",
            "Epoch: 2 Batch Number: 168 Loss: 1.9334139823913574 Time taken: 0.20903754234313965\n",
            "Epoch: 2 Batch Number: 169 Loss: 1.9285448789596558 Time taken: 0.20499634742736816\n",
            "Epoch: 2 Batch Number: 170 Loss: 1.9282819032669067 Time taken: 0.20772528648376465\n",
            "Epoch: 2 Batch Number: 171 Loss: 1.9139306545257568 Time taken: 0.20349454879760742\n",
            "Epoch: 2 Batch Number: 172 Loss: 1.9151986837387085 Time taken: 0.20079517364501953\n",
            "Epoch: 2 Batch Number: 173 Loss: 1.9303539991378784 Time taken: 0.20555663108825684\n",
            "Epoch: 2 Batch Number: 174 Loss: 1.9105788469314575 Time taken: 0.20773577690124512\n",
            "Epoch: 2 Batch Number: 175 Loss: 1.929622769355774 Time taken: 0.199493408203125\n",
            "Epoch: 2 Batch Number: 176 Loss: 1.9007128477096558 Time taken: 0.20187139511108398\n",
            "Epoch: 2 Batch Number: 177 Loss: 1.8974604606628418 Time taken: 0.2085111141204834\n",
            "Epoch: 2 Batch Number: 178 Loss: 1.8789256811141968 Time taken: 0.20855259895324707\n",
            "Epoch: 2 Batch Number: 179 Loss: 1.9063496589660645 Time taken: 0.20515727996826172\n",
            "==========================================================================================\n",
            "Start of epoch 3\n",
            "Epoch: 3 Batch Number: 1 Loss: 1.9100677967071533 Time taken: 0.20376181602478027\n",
            "Epoch: 3 Batch Number: 2 Loss: 1.8993208408355713 Time taken: 0.20209932327270508\n",
            "Epoch: 3 Batch Number: 3 Loss: 1.885718822479248 Time taken: 0.21048212051391602\n",
            "Epoch: 3 Batch Number: 4 Loss: 1.889851450920105 Time taken: 0.20078110694885254\n",
            "Epoch: 3 Batch Number: 5 Loss: 1.8719172477722168 Time taken: 0.2100973129272461\n",
            "Epoch: 3 Batch Number: 6 Loss: 1.9017021656036377 Time taken: 0.202254056930542\n",
            "Epoch: 3 Batch Number: 7 Loss: 1.8794279098510742 Time taken: 0.20757818222045898\n",
            "Epoch: 3 Batch Number: 8 Loss: 1.8992122411727905 Time taken: 0.21253371238708496\n",
            "Epoch: 3 Batch Number: 9 Loss: 1.8837326765060425 Time taken: 0.20254087448120117\n",
            "Epoch: 3 Batch Number: 10 Loss: 1.8765007257461548 Time taken: 0.20965838432312012\n",
            "Epoch: 3 Batch Number: 11 Loss: 1.868540644645691 Time taken: 0.20678114891052246\n",
            "Epoch: 3 Batch Number: 12 Loss: 1.8669289350509644 Time taken: 0.20138072967529297\n",
            "Epoch: 3 Batch Number: 13 Loss: 1.886797547340393 Time taken: 0.20711660385131836\n",
            "Epoch: 3 Batch Number: 14 Loss: 1.8693771362304688 Time taken: 0.20052003860473633\n",
            "Epoch: 3 Batch Number: 15 Loss: 1.902988314628601 Time taken: 0.20739364624023438\n",
            "Epoch: 3 Batch Number: 16 Loss: 1.896106243133545 Time taken: 0.19997382164001465\n",
            "Epoch: 3 Batch Number: 17 Loss: 1.8755595684051514 Time taken: 0.20894956588745117\n",
            "Epoch: 3 Batch Number: 18 Loss: 1.8683542013168335 Time taken: 0.20497989654541016\n",
            "Epoch: 3 Batch Number: 19 Loss: 1.8826411962509155 Time taken: 0.20804619789123535\n",
            "Epoch: 3 Batch Number: 20 Loss: 1.8877685070037842 Time taken: 0.2032148838043213\n",
            "Epoch: 3 Batch Number: 21 Loss: 1.8535789251327515 Time taken: 0.20219779014587402\n",
            "Epoch: 3 Batch Number: 22 Loss: 1.868697166442871 Time taken: 0.20676112174987793\n",
            "Epoch: 3 Batch Number: 23 Loss: 1.8568352460861206 Time taken: 0.20114994049072266\n",
            "Epoch: 3 Batch Number: 24 Loss: 1.863124966621399 Time taken: 0.20688366889953613\n",
            "Epoch: 3 Batch Number: 25 Loss: 1.859319806098938 Time taken: 0.20189476013183594\n",
            "Epoch: 3 Batch Number: 26 Loss: 1.8513745069503784 Time taken: 0.20542359352111816\n",
            "Epoch: 3 Batch Number: 27 Loss: 1.8585797548294067 Time taken: 0.20455312728881836\n",
            "Epoch: 3 Batch Number: 28 Loss: 1.8520599603652954 Time taken: 0.20325469970703125\n",
            "Epoch: 3 Batch Number: 29 Loss: 1.8773568868637085 Time taken: 0.20413446426391602\n",
            "Epoch: 3 Batch Number: 30 Loss: 1.8622175455093384 Time taken: 0.20802927017211914\n",
            "Epoch: 3 Batch Number: 31 Loss: 1.8607803583145142 Time taken: 0.20982933044433594\n",
            "Epoch: 3 Batch Number: 32 Loss: 1.8312488794326782 Time taken: 0.20094966888427734\n",
            "Epoch: 3 Batch Number: 33 Loss: 1.8558396100997925 Time taken: 0.20993351936340332\n",
            "Epoch: 3 Batch Number: 34 Loss: 1.866205096244812 Time taken: 0.20221281051635742\n",
            "Epoch: 3 Batch Number: 35 Loss: 1.8516474962234497 Time taken: 0.20531654357910156\n",
            "Epoch: 3 Batch Number: 36 Loss: 1.8484127521514893 Time taken: 0.206071138381958\n",
            "Epoch: 3 Batch Number: 37 Loss: 1.850620150566101 Time taken: 0.20611095428466797\n",
            "Epoch: 3 Batch Number: 38 Loss: 1.862238883972168 Time taken: 0.2061295509338379\n",
            "Epoch: 3 Batch Number: 39 Loss: 1.856588363647461 Time taken: 0.22364282608032227\n",
            "Epoch: 3 Batch Number: 40 Loss: 1.8615449666976929 Time taken: 0.20537090301513672\n",
            "Epoch: 3 Batch Number: 41 Loss: 1.8291659355163574 Time taken: 0.20456433296203613\n",
            "Epoch: 3 Batch Number: 42 Loss: 1.859357476234436 Time taken: 0.19907188415527344\n",
            "Epoch: 3 Batch Number: 43 Loss: 1.8598761558532715 Time taken: 0.20638275146484375\n",
            "Epoch: 3 Batch Number: 44 Loss: 1.8309108018875122 Time taken: 0.2009749412536621\n",
            "Epoch: 3 Batch Number: 45 Loss: 1.82318913936615 Time taken: 0.212388277053833\n",
            "Epoch: 3 Batch Number: 46 Loss: 1.812382698059082 Time taken: 0.20154595375061035\n",
            "Epoch: 3 Batch Number: 47 Loss: 1.8416543006896973 Time taken: 0.20165777206420898\n",
            "Epoch: 3 Batch Number: 48 Loss: 1.83118736743927 Time taken: 0.20157408714294434\n",
            "Epoch: 3 Batch Number: 49 Loss: 1.8305151462554932 Time taken: 0.20204949378967285\n",
            "Epoch: 3 Batch Number: 50 Loss: 1.829833984375 Time taken: 0.20753741264343262\n",
            "Epoch: 3 Batch Number: 51 Loss: 1.8612875938415527 Time taken: 0.2022387981414795\n",
            "Epoch: 3 Batch Number: 52 Loss: 1.840844750404358 Time taken: 0.21330499649047852\n",
            "Epoch: 3 Batch Number: 53 Loss: 1.859886884689331 Time taken: 0.21086478233337402\n",
            "Epoch: 3 Batch Number: 54 Loss: 1.855002522468567 Time taken: 0.2032308578491211\n",
            "Epoch: 3 Batch Number: 55 Loss: 1.8590888977050781 Time taken: 0.20499396324157715\n",
            "Epoch: 3 Batch Number: 56 Loss: 1.8424201011657715 Time taken: 0.1985023021697998\n",
            "Epoch: 3 Batch Number: 57 Loss: 1.8277591466903687 Time taken: 0.20366406440734863\n",
            "Epoch: 3 Batch Number: 58 Loss: 1.8259086608886719 Time taken: 0.20840764045715332\n",
            "Epoch: 3 Batch Number: 59 Loss: 1.8437570333480835 Time taken: 0.2077169418334961\n",
            "Epoch: 3 Batch Number: 60 Loss: 1.8309326171875 Time taken: 0.20433759689331055\n",
            "Epoch: 3 Batch Number: 61 Loss: 1.8002612590789795 Time taken: 0.2017199993133545\n",
            "Epoch: 3 Batch Number: 62 Loss: 1.83150315284729 Time taken: 0.20200419425964355\n",
            "Epoch: 3 Batch Number: 63 Loss: 1.7926359176635742 Time taken: 0.21077680587768555\n",
            "Epoch: 3 Batch Number: 64 Loss: 1.8012036085128784 Time taken: 0.2044382095336914\n",
            "Epoch: 3 Batch Number: 65 Loss: 1.7886005640029907 Time taken: 0.20281100273132324\n",
            "Epoch: 3 Batch Number: 66 Loss: 1.7944037914276123 Time taken: 0.2002887725830078\n",
            "Epoch: 3 Batch Number: 67 Loss: 1.8162798881530762 Time taken: 0.2058572769165039\n",
            "Epoch: 3 Batch Number: 68 Loss: 1.8050367832183838 Time taken: 0.20583534240722656\n",
            "Epoch: 3 Batch Number: 69 Loss: 1.8065673112869263 Time taken: 0.2046804428100586\n",
            "Epoch: 3 Batch Number: 70 Loss: 1.7907500267028809 Time taken: 0.20163726806640625\n",
            "Epoch: 3 Batch Number: 71 Loss: 1.8067193031311035 Time taken: 0.20400524139404297\n",
            "Epoch: 3 Batch Number: 72 Loss: 1.8158808946609497 Time taken: 0.20226120948791504\n",
            "Epoch: 3 Batch Number: 73 Loss: 1.8280774354934692 Time taken: 0.20186424255371094\n",
            "Epoch: 3 Batch Number: 74 Loss: 1.820811152458191 Time taken: 0.2066938877105713\n",
            "Epoch: 3 Batch Number: 75 Loss: 1.8082714080810547 Time taken: 0.20124173164367676\n",
            "Epoch: 3 Batch Number: 76 Loss: 1.8188074827194214 Time taken: 0.1997826099395752\n",
            "Epoch: 3 Batch Number: 77 Loss: 1.8021512031555176 Time taken: 0.20032954216003418\n",
            "Epoch: 3 Batch Number: 78 Loss: 1.8006789684295654 Time taken: 0.1990492343902588\n",
            "Epoch: 3 Batch Number: 79 Loss: 1.805366039276123 Time taken: 0.20784902572631836\n",
            "Epoch: 3 Batch Number: 80 Loss: 1.806913137435913 Time taken: 0.1992015838623047\n",
            "Epoch: 3 Batch Number: 81 Loss: 1.774457335472107 Time taken: 0.20134449005126953\n",
            "Epoch: 3 Batch Number: 82 Loss: 1.7836956977844238 Time taken: 0.20618820190429688\n",
            "Epoch: 3 Batch Number: 83 Loss: 1.7685296535491943 Time taken: 0.2049105167388916\n",
            "Epoch: 3 Batch Number: 84 Loss: 1.7876806259155273 Time taken: 0.1991281509399414\n",
            "Epoch: 3 Batch Number: 85 Loss: 1.7804721593856812 Time taken: 0.20322108268737793\n",
            "Epoch: 3 Batch Number: 86 Loss: 1.7749594449996948 Time taken: 0.20754551887512207\n",
            "Epoch: 3 Batch Number: 87 Loss: 1.7814205884933472 Time taken: 0.20508909225463867\n",
            "Epoch: 3 Batch Number: 88 Loss: 1.7595937252044678 Time taken: 0.20866107940673828\n",
            "Epoch: 3 Batch Number: 89 Loss: 1.7810653448104858 Time taken: 0.20615267753601074\n",
            "Epoch: 3 Batch Number: 90 Loss: 1.7748498916625977 Time taken: 0.20110225677490234\n",
            "Epoch: 3 Batch Number: 91 Loss: 1.7715466022491455 Time taken: 0.20020484924316406\n",
            "Epoch: 3 Batch Number: 92 Loss: 1.7856295108795166 Time taken: 0.2005624771118164\n",
            "Epoch: 3 Batch Number: 93 Loss: 1.8113223314285278 Time taken: 0.20328903198242188\n",
            "Epoch: 3 Batch Number: 94 Loss: 1.8128200769424438 Time taken: 0.20152688026428223\n",
            "Epoch: 3 Batch Number: 95 Loss: 1.8056188821792603 Time taken: 0.20957279205322266\n",
            "Epoch: 3 Batch Number: 96 Loss: 1.7935705184936523 Time taken: 0.19945335388183594\n",
            "Epoch: 3 Batch Number: 97 Loss: 1.8186930418014526 Time taken: 0.2094104290008545\n",
            "Epoch: 3 Batch Number: 98 Loss: 1.8097037076950073 Time taken: 0.2024400234222412\n",
            "Epoch: 3 Batch Number: 99 Loss: 1.8116060495376587 Time taken: 0.20144295692443848\n",
            "Epoch: 3 Batch Number: 100 Loss: 1.779318928718567 Time taken: 0.20188522338867188\n",
            "Epoch: 3 Batch Number: 101 Loss: 1.7708147764205933 Time taken: 0.20137763023376465\n",
            "Epoch: 3 Batch Number: 102 Loss: 1.7975815534591675 Time taken: 0.20767760276794434\n",
            "Epoch: 3 Batch Number: 103 Loss: 1.788728952407837 Time taken: 0.20952272415161133\n",
            "Epoch: 3 Batch Number: 104 Loss: 1.8130964040756226 Time taken: 0.20631098747253418\n",
            "Epoch: 3 Batch Number: 105 Loss: 1.8196330070495605 Time taken: 0.19957590103149414\n",
            "Epoch: 3 Batch Number: 106 Loss: 1.783405065536499 Time taken: 0.2032327651977539\n",
            "Epoch: 3 Batch Number: 107 Loss: 1.805216908454895 Time taken: 0.20532488822937012\n",
            "Epoch: 3 Batch Number: 108 Loss: 1.7923295497894287 Time taken: 0.2048799991607666\n",
            "Epoch: 3 Batch Number: 109 Loss: 1.8023991584777832 Time taken: 0.202545166015625\n",
            "Epoch: 3 Batch Number: 110 Loss: 1.8055189847946167 Time taken: 0.20389914512634277\n",
            "Epoch: 3 Batch Number: 111 Loss: 1.835943341255188 Time taken: 0.20122885704040527\n",
            "Epoch: 3 Batch Number: 112 Loss: 1.8461575508117676 Time taken: 0.20065999031066895\n",
            "Epoch: 3 Batch Number: 113 Loss: 1.8092130422592163 Time taken: 0.20200371742248535\n",
            "Epoch: 3 Batch Number: 114 Loss: 1.8026411533355713 Time taken: 0.1993255615234375\n",
            "Epoch: 3 Batch Number: 115 Loss: 1.789691686630249 Time taken: 0.20131778717041016\n",
            "Epoch: 3 Batch Number: 116 Loss: 1.7679753303527832 Time taken: 0.20567989349365234\n",
            "Epoch: 3 Batch Number: 117 Loss: 1.7724320888519287 Time taken: 0.20630335807800293\n",
            "Epoch: 3 Batch Number: 118 Loss: 1.7691445350646973 Time taken: 0.20029616355895996\n",
            "Epoch: 3 Batch Number: 119 Loss: 1.7888216972351074 Time taken: 0.20280694961547852\n",
            "Epoch: 3 Batch Number: 120 Loss: 1.7714369297027588 Time taken: 0.20242857933044434\n",
            "Epoch: 3 Batch Number: 121 Loss: 1.7931153774261475 Time taken: 0.21543145179748535\n",
            "Epoch: 3 Batch Number: 122 Loss: 1.772222638130188 Time taken: 0.20680928230285645\n",
            "Epoch: 3 Batch Number: 123 Loss: 1.8032269477844238 Time taken: 0.21216797828674316\n",
            "Epoch: 3 Batch Number: 124 Loss: 1.771417498588562 Time taken: 0.20452213287353516\n",
            "Epoch: 3 Batch Number: 125 Loss: 1.7694318294525146 Time taken: 0.2012639045715332\n",
            "Epoch: 3 Batch Number: 126 Loss: 1.7778443098068237 Time taken: 0.1978130340576172\n",
            "Epoch: 3 Batch Number: 127 Loss: 1.7497583627700806 Time taken: 0.20626544952392578\n",
            "Epoch: 3 Batch Number: 128 Loss: 1.7576699256896973 Time taken: 0.19963836669921875\n",
            "Epoch: 3 Batch Number: 129 Loss: 1.7644402980804443 Time taken: 0.20321989059448242\n",
            "Epoch: 3 Batch Number: 130 Loss: 1.751686453819275 Time taken: 0.20182061195373535\n",
            "Epoch: 3 Batch Number: 131 Loss: 1.7650071382522583 Time taken: 0.2118091583251953\n",
            "Epoch: 3 Batch Number: 132 Loss: 1.756483793258667 Time taken: 0.2045881748199463\n",
            "Epoch: 3 Batch Number: 133 Loss: 1.7692095041275024 Time taken: 0.2069389820098877\n",
            "Epoch: 3 Batch Number: 134 Loss: 1.7974528074264526 Time taken: 0.19927406311035156\n",
            "Epoch: 3 Batch Number: 135 Loss: 1.7901157140731812 Time taken: 0.20428943634033203\n",
            "Epoch: 3 Batch Number: 136 Loss: 1.7904138565063477 Time taken: 0.2130420207977295\n",
            "Epoch: 3 Batch Number: 137 Loss: 1.7742937803268433 Time taken: 0.20228052139282227\n",
            "Epoch: 3 Batch Number: 138 Loss: 1.7544020414352417 Time taken: 0.20348644256591797\n",
            "Epoch: 3 Batch Number: 139 Loss: 1.7631851434707642 Time taken: 0.20024800300598145\n",
            "Epoch: 3 Batch Number: 140 Loss: 1.752995491027832 Time taken: 0.20633530616760254\n",
            "Epoch: 3 Batch Number: 141 Loss: 1.758619785308838 Time taken: 0.20099687576293945\n",
            "Epoch: 3 Batch Number: 142 Loss: 1.7475758790969849 Time taken: 0.2008054256439209\n",
            "Epoch: 3 Batch Number: 143 Loss: 1.7982773780822754 Time taken: 0.20488953590393066\n",
            "Epoch: 3 Batch Number: 144 Loss: 1.7743523120880127 Time taken: 0.20159387588500977\n",
            "Epoch: 3 Batch Number: 145 Loss: 1.7518095970153809 Time taken: 0.20020484924316406\n",
            "Epoch: 3 Batch Number: 146 Loss: 1.778288722038269 Time taken: 0.2039785385131836\n",
            "Epoch: 3 Batch Number: 147 Loss: 1.7483258247375488 Time taken: 0.20079421997070312\n",
            "Epoch: 3 Batch Number: 148 Loss: 1.761951208114624 Time taken: 0.2078723907470703\n",
            "Epoch: 3 Batch Number: 149 Loss: 1.7389365434646606 Time taken: 0.19862771034240723\n",
            "Epoch: 3 Batch Number: 150 Loss: 1.7429026365280151 Time taken: 0.20116662979125977\n",
            "Epoch: 3 Batch Number: 151 Loss: 1.7456625699996948 Time taken: 0.20411920547485352\n",
            "Epoch: 3 Batch Number: 152 Loss: 1.734281301498413 Time taken: 0.2048630714416504\n",
            "Epoch: 3 Batch Number: 153 Loss: 1.727536916732788 Time taken: 0.20144057273864746\n",
            "Epoch: 3 Batch Number: 154 Loss: 1.7244489192962646 Time taken: 0.19945788383483887\n",
            "Epoch: 3 Batch Number: 155 Loss: 1.729058861732483 Time taken: 0.20636820793151855\n",
            "Epoch: 3 Batch Number: 156 Loss: 1.727314829826355 Time taken: 0.20785140991210938\n",
            "Epoch: 3 Batch Number: 157 Loss: 1.7399013042449951 Time taken: 0.19975996017456055\n",
            "Epoch: 3 Batch Number: 158 Loss: 1.774167776107788 Time taken: 0.20246148109436035\n",
            "Epoch: 3 Batch Number: 159 Loss: 1.7448080778121948 Time taken: 0.20094060897827148\n",
            "Epoch: 3 Batch Number: 160 Loss: 1.7542954683303833 Time taken: 0.20880889892578125\n",
            "Epoch: 3 Batch Number: 161 Loss: 1.7382663488388062 Time taken: 0.20416522026062012\n",
            "Epoch: 3 Batch Number: 162 Loss: 1.732798457145691 Time taken: 0.2031116485595703\n",
            "Epoch: 3 Batch Number: 163 Loss: 1.7090234756469727 Time taken: 0.20124244689941406\n",
            "Epoch: 3 Batch Number: 164 Loss: 1.7370519638061523 Time taken: 0.1990213394165039\n",
            "Epoch: 3 Batch Number: 165 Loss: 1.7494269609451294 Time taken: 0.2066512107849121\n",
            "Epoch: 3 Batch Number: 166 Loss: 1.741512417793274 Time taken: 0.19920086860656738\n",
            "Epoch: 3 Batch Number: 167 Loss: 1.7347739934921265 Time taken: 0.2031855583190918\n",
            "Epoch: 3 Batch Number: 168 Loss: 1.7194528579711914 Time taken: 0.20073437690734863\n",
            "Epoch: 3 Batch Number: 169 Loss: 1.7332051992416382 Time taken: 0.2043001651763916\n",
            "Epoch: 3 Batch Number: 170 Loss: 1.7104151248931885 Time taken: 0.21135735511779785\n",
            "Epoch: 3 Batch Number: 171 Loss: 1.7251169681549072 Time taken: 0.2073061466217041\n",
            "Epoch: 3 Batch Number: 172 Loss: 1.7016798257827759 Time taken: 0.20126795768737793\n",
            "Epoch: 3 Batch Number: 173 Loss: 1.7006467580795288 Time taken: 0.19937682151794434\n",
            "Epoch: 3 Batch Number: 174 Loss: 1.7101291418075562 Time taken: 0.20173406600952148\n",
            "Epoch: 3 Batch Number: 175 Loss: 1.7151025533676147 Time taken: 0.20911169052124023\n",
            "Epoch: 3 Batch Number: 176 Loss: 1.6983283758163452 Time taken: 0.19772648811340332\n",
            "Epoch: 3 Batch Number: 177 Loss: 1.6944931745529175 Time taken: 0.20118999481201172\n",
            "Epoch: 3 Batch Number: 178 Loss: 1.6775625944137573 Time taken: 0.19739222526550293\n",
            "Epoch: 3 Batch Number: 179 Loss: 1.6793538331985474 Time taken: 0.2035679817199707\n",
            "==========================================================================================\n",
            "Start of epoch 4\n",
            "Epoch: 4 Batch Number: 1 Loss: 1.6991469860076904 Time taken: 0.20904207229614258\n",
            "Epoch: 4 Batch Number: 2 Loss: 1.7024186849594116 Time taken: 0.20012903213500977\n",
            "Epoch: 4 Batch Number: 3 Loss: 1.6647467613220215 Time taken: 0.19669127464294434\n",
            "Epoch: 4 Batch Number: 4 Loss: 1.6925123929977417 Time taken: 0.20224380493164062\n",
            "Epoch: 4 Batch Number: 5 Loss: 1.676768183708191 Time taken: 0.21189427375793457\n",
            "Epoch: 4 Batch Number: 6 Loss: 1.6902631521224976 Time taken: 0.20766568183898926\n",
            "Epoch: 4 Batch Number: 7 Loss: 1.6867401599884033 Time taken: 0.2055344581604004\n",
            "Epoch: 4 Batch Number: 8 Loss: 1.684007167816162 Time taken: 0.19873309135437012\n",
            "Epoch: 4 Batch Number: 9 Loss: 1.6904278993606567 Time taken: 0.2141249179840088\n",
            "Epoch: 4 Batch Number: 10 Loss: 1.6822540760040283 Time taken: 0.20910143852233887\n",
            "Epoch: 4 Batch Number: 11 Loss: 1.696536898612976 Time taken: 0.20291686058044434\n",
            "Epoch: 4 Batch Number: 12 Loss: 1.6759625673294067 Time taken: 0.20134329795837402\n",
            "Epoch: 4 Batch Number: 13 Loss: 1.6994621753692627 Time taken: 0.20224833488464355\n",
            "Epoch: 4 Batch Number: 14 Loss: 1.6985609531402588 Time taken: 0.20418405532836914\n",
            "Epoch: 4 Batch Number: 15 Loss: 1.702109456062317 Time taken: 0.20590925216674805\n",
            "Epoch: 4 Batch Number: 16 Loss: 1.6793819665908813 Time taken: 0.20121240615844727\n",
            "Epoch: 4 Batch Number: 17 Loss: 1.7049376964569092 Time taken: 0.20212292671203613\n",
            "Epoch: 4 Batch Number: 18 Loss: 1.6834760904312134 Time taken: 0.20030879974365234\n",
            "Epoch: 4 Batch Number: 19 Loss: 1.6874679327011108 Time taken: 0.20206928253173828\n",
            "Epoch: 4 Batch Number: 20 Loss: 1.663262963294983 Time taken: 0.21043133735656738\n",
            "Epoch: 4 Batch Number: 21 Loss: 1.6625210046768188 Time taken: 0.20031142234802246\n",
            "Epoch: 4 Batch Number: 22 Loss: 1.65741765499115 Time taken: 0.20010828971862793\n",
            "Epoch: 4 Batch Number: 23 Loss: 1.6717536449432373 Time taken: 0.20521044731140137\n",
            "Epoch: 4 Batch Number: 24 Loss: 1.683185338973999 Time taken: 0.19946980476379395\n",
            "Epoch: 4 Batch Number: 25 Loss: 1.6832419633865356 Time taken: 0.2057814598083496\n",
            "Epoch: 4 Batch Number: 26 Loss: 1.6801481246948242 Time taken: 0.20164704322814941\n",
            "Epoch: 4 Batch Number: 27 Loss: 1.670453429222107 Time taken: 0.2025752067565918\n",
            "Epoch: 4 Batch Number: 28 Loss: 1.673048496246338 Time taken: 0.20040273666381836\n",
            "Epoch: 4 Batch Number: 29 Loss: 1.6757214069366455 Time taken: 0.19974613189697266\n",
            "Epoch: 4 Batch Number: 30 Loss: 1.6625274419784546 Time taken: 0.20767927169799805\n",
            "Epoch: 4 Batch Number: 31 Loss: 1.6705995798110962 Time taken: 0.20102143287658691\n",
            "Epoch: 4 Batch Number: 32 Loss: 1.6632798910140991 Time taken: 0.20330309867858887\n",
            "Epoch: 4 Batch Number: 33 Loss: 1.6795315742492676 Time taken: 0.20466184616088867\n",
            "Epoch: 4 Batch Number: 34 Loss: 1.6763330698013306 Time taken: 0.20335865020751953\n",
            "Epoch: 4 Batch Number: 35 Loss: 1.666326880455017 Time taken: 0.19927692413330078\n",
            "Epoch: 4 Batch Number: 36 Loss: 1.689686894416809 Time taken: 0.20046353340148926\n",
            "Epoch: 4 Batch Number: 37 Loss: 1.6702296733856201 Time taken: 0.19890785217285156\n",
            "Epoch: 4 Batch Number: 38 Loss: 1.6885504722595215 Time taken: 0.20243096351623535\n",
            "Epoch: 4 Batch Number: 39 Loss: 1.7040157318115234 Time taken: 0.20232534408569336\n",
            "Epoch: 4 Batch Number: 40 Loss: 1.6771315336227417 Time taken: 0.20010709762573242\n",
            "Epoch: 4 Batch Number: 41 Loss: 1.6876527070999146 Time taken: 0.21069622039794922\n",
            "Epoch: 4 Batch Number: 42 Loss: 1.6846493482589722 Time taken: 0.20109128952026367\n",
            "Epoch: 4 Batch Number: 43 Loss: 1.682428002357483 Time taken: 0.20235848426818848\n",
            "Epoch: 4 Batch Number: 44 Loss: 1.6968255043029785 Time taken: 0.20948219299316406\n",
            "Epoch: 4 Batch Number: 45 Loss: 1.6860020160675049 Time taken: 0.20308709144592285\n",
            "Epoch: 4 Batch Number: 46 Loss: 1.6552971601486206 Time taken: 0.20314478874206543\n",
            "Epoch: 4 Batch Number: 47 Loss: 1.6661981344223022 Time taken: 0.19954252243041992\n",
            "Epoch: 4 Batch Number: 48 Loss: 1.6343941688537598 Time taken: 0.20068097114562988\n",
            "Epoch: 4 Batch Number: 49 Loss: 1.653019905090332 Time taken: 0.2073965072631836\n",
            "Epoch: 4 Batch Number: 50 Loss: 1.6897412538528442 Time taken: 0.2017688751220703\n",
            "Epoch: 4 Batch Number: 51 Loss: 1.671521544456482 Time taken: 0.2004852294921875\n",
            "Epoch: 4 Batch Number: 52 Loss: 1.6925045251846313 Time taken: 0.20351386070251465\n",
            "Epoch: 4 Batch Number: 53 Loss: 1.6791236400604248 Time taken: 0.20736289024353027\n",
            "Epoch: 4 Batch Number: 54 Loss: 1.693538784980774 Time taken: 0.22301292419433594\n",
            "Epoch: 4 Batch Number: 55 Loss: 1.6604810953140259 Time taken: 0.19950103759765625\n",
            "Epoch: 4 Batch Number: 56 Loss: 1.6532118320465088 Time taken: 0.20789742469787598\n",
            "Epoch: 4 Batch Number: 57 Loss: 1.659868836402893 Time taken: 0.20105195045471191\n",
            "Epoch: 4 Batch Number: 58 Loss: 1.6621440649032593 Time taken: 0.20037484169006348\n",
            "Epoch: 4 Batch Number: 59 Loss: 1.6696878671646118 Time taken: 0.2091684341430664\n",
            "Epoch: 4 Batch Number: 60 Loss: 1.6542143821716309 Time taken: 0.19892144203186035\n",
            "Epoch: 4 Batch Number: 61 Loss: 1.6203078031539917 Time taken: 0.19580674171447754\n",
            "Epoch: 4 Batch Number: 62 Loss: 1.6552739143371582 Time taken: 0.19976139068603516\n",
            "Epoch: 4 Batch Number: 63 Loss: 1.649614930152893 Time taken: 0.20331406593322754\n",
            "Epoch: 4 Batch Number: 64 Loss: 1.6427247524261475 Time taken: 0.2064511775970459\n",
            "Epoch: 4 Batch Number: 65 Loss: 1.652910828590393 Time taken: 0.20238661766052246\n",
            "Epoch: 4 Batch Number: 66 Loss: 1.6400634050369263 Time taken: 0.21376490592956543\n",
            "Epoch: 4 Batch Number: 67 Loss: 1.6603922843933105 Time taken: 0.20436644554138184\n",
            "Epoch: 4 Batch Number: 68 Loss: 1.652634859085083 Time taken: 0.21099591255187988\n",
            "Epoch: 4 Batch Number: 69 Loss: 1.637195110321045 Time taken: 0.20359587669372559\n",
            "Epoch: 4 Batch Number: 70 Loss: 1.6476333141326904 Time taken: 0.20850419998168945\n",
            "Epoch: 4 Batch Number: 71 Loss: 1.6603163480758667 Time taken: 0.20941591262817383\n",
            "Epoch: 4 Batch Number: 72 Loss: 1.6741007566452026 Time taken: 0.2028639316558838\n",
            "Epoch: 4 Batch Number: 73 Loss: 1.6845654249191284 Time taken: 0.2042834758758545\n",
            "Epoch: 4 Batch Number: 74 Loss: 1.6815147399902344 Time taken: 0.20176100730895996\n",
            "Epoch: 4 Batch Number: 75 Loss: 1.654198408126831 Time taken: 0.2127981185913086\n",
            "Epoch: 4 Batch Number: 76 Loss: 1.6682558059692383 Time taken: 0.20390748977661133\n",
            "Epoch: 4 Batch Number: 77 Loss: 1.6333622932434082 Time taken: 0.20438694953918457\n",
            "Epoch: 4 Batch Number: 78 Loss: 1.6548315286636353 Time taken: 0.21321725845336914\n",
            "Epoch: 4 Batch Number: 79 Loss: 1.6310051679611206 Time taken: 0.20756745338439941\n",
            "Epoch: 4 Batch Number: 80 Loss: 1.6204553842544556 Time taken: 0.20476007461547852\n",
            "Epoch: 4 Batch Number: 81 Loss: 1.6221675872802734 Time taken: 0.2053697109222412\n",
            "Epoch: 4 Batch Number: 82 Loss: 1.6453304290771484 Time taken: 0.20174050331115723\n",
            "Epoch: 4 Batch Number: 83 Loss: 1.6240918636322021 Time taken: 0.22053837776184082\n",
            "Epoch: 4 Batch Number: 84 Loss: 1.621682047843933 Time taken: 0.2019968032836914\n",
            "Epoch: 4 Batch Number: 85 Loss: 1.6225166320800781 Time taken: 0.2128124237060547\n",
            "Epoch: 4 Batch Number: 86 Loss: 1.608716607093811 Time taken: 0.20660877227783203\n",
            "Epoch: 4 Batch Number: 87 Loss: 1.5991655588150024 Time taken: 0.21185016632080078\n",
            "Epoch: 4 Batch Number: 88 Loss: 1.616553544998169 Time taken: 0.2135920524597168\n",
            "Epoch: 4 Batch Number: 89 Loss: 1.5978280305862427 Time taken: 0.21032238006591797\n",
            "Epoch: 4 Batch Number: 90 Loss: 1.6108843088150024 Time taken: 0.20407414436340332\n",
            "Epoch: 4 Batch Number: 91 Loss: 1.6133626699447632 Time taken: 0.20342659950256348\n",
            "Epoch: 4 Batch Number: 92 Loss: 1.6578015089035034 Time taken: 0.20806622505187988\n",
            "Epoch: 4 Batch Number: 93 Loss: 1.639906644821167 Time taken: 0.20479321479797363\n",
            "Epoch: 4 Batch Number: 94 Loss: 1.6343328952789307 Time taken: 0.2057180404663086\n",
            "Epoch: 4 Batch Number: 95 Loss: 1.6642789840698242 Time taken: 0.20239877700805664\n",
            "Epoch: 4 Batch Number: 96 Loss: 1.651879906654358 Time taken: 0.2088170051574707\n",
            "Epoch: 4 Batch Number: 97 Loss: 1.6509370803833008 Time taken: 0.20990920066833496\n",
            "Epoch: 4 Batch Number: 98 Loss: 1.6625937223434448 Time taken: 0.21024751663208008\n",
            "Epoch: 4 Batch Number: 99 Loss: 1.638556718826294 Time taken: 0.2064979076385498\n",
            "Epoch: 4 Batch Number: 100 Loss: 1.6366770267486572 Time taken: 0.2018139362335205\n",
            "Epoch: 4 Batch Number: 101 Loss: 1.6426194906234741 Time taken: 0.20645356178283691\n",
            "Epoch: 4 Batch Number: 102 Loss: 1.635677456855774 Time taken: 0.20839548110961914\n",
            "Epoch: 4 Batch Number: 103 Loss: 1.6609734296798706 Time taken: 0.2079005241394043\n",
            "Epoch: 4 Batch Number: 104 Loss: 1.6445955038070679 Time taken: 0.20077085494995117\n",
            "Epoch: 4 Batch Number: 105 Loss: 1.661522388458252 Time taken: 0.20851397514343262\n",
            "Epoch: 4 Batch Number: 106 Loss: 1.6674680709838867 Time taken: 0.20553016662597656\n",
            "Epoch: 4 Batch Number: 107 Loss: 1.6821787357330322 Time taken: 0.21038556098937988\n",
            "Epoch: 4 Batch Number: 108 Loss: 1.66499924659729 Time taken: 0.21326065063476562\n",
            "Epoch: 4 Batch Number: 109 Loss: 1.6638835668563843 Time taken: 0.21140241622924805\n",
            "Epoch: 4 Batch Number: 110 Loss: 1.6866601705551147 Time taken: 0.2023172378540039\n",
            "Epoch: 4 Batch Number: 111 Loss: 1.6927189826965332 Time taken: 0.2090291976928711\n",
            "Epoch: 4 Batch Number: 112 Loss: 1.6878432035446167 Time taken: 0.20139431953430176\n",
            "Epoch: 4 Batch Number: 113 Loss: 1.6532018184661865 Time taken: 0.2120823860168457\n",
            "Epoch: 4 Batch Number: 114 Loss: 1.6872680187225342 Time taken: 0.20317864418029785\n",
            "Epoch: 4 Batch Number: 115 Loss: 1.6434804201126099 Time taken: 0.20126914978027344\n",
            "Epoch: 4 Batch Number: 116 Loss: 1.6442781686782837 Time taken: 0.20639491081237793\n",
            "Epoch: 4 Batch Number: 117 Loss: 1.6380573511123657 Time taken: 0.20319294929504395\n",
            "Epoch: 4 Batch Number: 118 Loss: 1.631966233253479 Time taken: 0.204392671585083\n",
            "Epoch: 4 Batch Number: 119 Loss: 1.6635829210281372 Time taken: 0.20066404342651367\n",
            "Epoch: 4 Batch Number: 120 Loss: 1.6451534032821655 Time taken: 0.20316720008850098\n",
            "Epoch: 4 Batch Number: 121 Loss: 1.6401227712631226 Time taken: 0.20894169807434082\n",
            "Epoch: 4 Batch Number: 122 Loss: 1.6519787311553955 Time taken: 0.20482134819030762\n",
            "Epoch: 4 Batch Number: 123 Loss: 1.639780879020691 Time taken: 0.20116209983825684\n",
            "Epoch: 4 Batch Number: 124 Loss: 1.6225740909576416 Time taken: 0.19949746131896973\n",
            "Epoch: 4 Batch Number: 125 Loss: 1.6323530673980713 Time taken: 0.2011868953704834\n",
            "Epoch: 4 Batch Number: 126 Loss: 1.6192903518676758 Time taken: 0.20123624801635742\n",
            "Epoch: 4 Batch Number: 127 Loss: 1.6202616691589355 Time taken: 0.2005758285522461\n",
            "Epoch: 4 Batch Number: 128 Loss: 1.6440067291259766 Time taken: 0.20184707641601562\n",
            "Epoch: 4 Batch Number: 129 Loss: 1.6261231899261475 Time taken: 0.20173430442810059\n",
            "Epoch: 4 Batch Number: 130 Loss: 1.6423410177230835 Time taken: 0.20151400566101074\n",
            "Epoch: 4 Batch Number: 131 Loss: 1.6419227123260498 Time taken: 0.2089824676513672\n",
            "Epoch: 4 Batch Number: 132 Loss: 1.6381727457046509 Time taken: 0.20417118072509766\n",
            "Epoch: 4 Batch Number: 133 Loss: 1.6296720504760742 Time taken: 0.19945216178894043\n",
            "Epoch: 4 Batch Number: 134 Loss: 1.633560299873352 Time taken: 0.1999979019165039\n",
            "Epoch: 4 Batch Number: 135 Loss: 1.6183360815048218 Time taken: 0.20551514625549316\n",
            "Epoch: 4 Batch Number: 136 Loss: 1.6216665506362915 Time taken: 0.21018218994140625\n",
            "Epoch: 4 Batch Number: 137 Loss: 1.6031858921051025 Time taken: 0.20937561988830566\n",
            "Epoch: 4 Batch Number: 138 Loss: 1.6164556741714478 Time taken: 0.19959473609924316\n",
            "Epoch: 4 Batch Number: 139 Loss: 1.6380289793014526 Time taken: 0.20349788665771484\n",
            "Epoch: 4 Batch Number: 140 Loss: 1.6435166597366333 Time taken: 0.21195292472839355\n",
            "Epoch: 4 Batch Number: 141 Loss: 1.6072767972946167 Time taken: 0.19823503494262695\n",
            "Epoch: 4 Batch Number: 142 Loss: 1.6293362379074097 Time taken: 0.19928193092346191\n",
            "Epoch: 4 Batch Number: 143 Loss: 1.67034113407135 Time taken: 0.20016241073608398\n",
            "Epoch: 4 Batch Number: 144 Loss: 1.6219332218170166 Time taken: 0.2051551342010498\n",
            "Epoch: 4 Batch Number: 145 Loss: 1.6318107843399048 Time taken: 0.2235872745513916\n",
            "Epoch: 4 Batch Number: 146 Loss: 1.6221389770507812 Time taken: 0.2036275863647461\n",
            "Epoch: 4 Batch Number: 147 Loss: 1.614240050315857 Time taken: 0.20087122917175293\n",
            "Epoch: 4 Batch Number: 148 Loss: 1.6101309061050415 Time taken: 0.20085453987121582\n",
            "Epoch: 4 Batch Number: 149 Loss: 1.6155966520309448 Time taken: 0.20228266716003418\n",
            "Epoch: 4 Batch Number: 150 Loss: 1.6056809425354004 Time taken: 0.20822572708129883\n",
            "Epoch: 4 Batch Number: 151 Loss: 1.6037890911102295 Time taken: 0.19914507865905762\n",
            "Epoch: 4 Batch Number: 152 Loss: 1.6078855991363525 Time taken: 0.20006871223449707\n",
            "Epoch: 4 Batch Number: 153 Loss: 1.6108938455581665 Time taken: 0.20517826080322266\n",
            "Epoch: 4 Batch Number: 154 Loss: 1.624706745147705 Time taken: 0.20480942726135254\n",
            "Epoch: 4 Batch Number: 155 Loss: 1.5947893857955933 Time taken: 0.20540642738342285\n",
            "Epoch: 4 Batch Number: 156 Loss: 1.6293220520019531 Time taken: 0.2030632495880127\n",
            "Epoch: 4 Batch Number: 157 Loss: 1.6235007047653198 Time taken: 0.1996142864227295\n",
            "Epoch: 4 Batch Number: 158 Loss: 1.6055705547332764 Time taken: 0.21313905715942383\n",
            "Epoch: 4 Batch Number: 159 Loss: 1.6104373931884766 Time taken: 0.20809602737426758\n",
            "Epoch: 4 Batch Number: 160 Loss: 1.6125789880752563 Time taken: 0.21201014518737793\n",
            "Epoch: 4 Batch Number: 161 Loss: 1.614872932434082 Time taken: 0.20495891571044922\n",
            "Epoch: 4 Batch Number: 162 Loss: 1.613511562347412 Time taken: 0.20620465278625488\n",
            "Epoch: 4 Batch Number: 163 Loss: 1.6144143342971802 Time taken: 0.19937944412231445\n",
            "Epoch: 4 Batch Number: 164 Loss: 1.5900338888168335 Time taken: 0.2152235507965088\n",
            "Epoch: 4 Batch Number: 165 Loss: 1.600008249282837 Time taken: 0.20322918891906738\n",
            "Epoch: 4 Batch Number: 166 Loss: 1.6204816102981567 Time taken: 0.20785260200500488\n",
            "Epoch: 4 Batch Number: 167 Loss: 1.5956099033355713 Time taken: 0.20229148864746094\n",
            "Epoch: 4 Batch Number: 168 Loss: 1.6108895540237427 Time taken: 0.20462536811828613\n",
            "Epoch: 4 Batch Number: 169 Loss: 1.5785454511642456 Time taken: 0.20772743225097656\n",
            "Epoch: 4 Batch Number: 170 Loss: 1.5871511697769165 Time taken: 0.2023770809173584\n",
            "Epoch: 4 Batch Number: 171 Loss: 1.5952321290969849 Time taken: 0.20383095741271973\n",
            "Epoch: 4 Batch Number: 172 Loss: 1.5812009572982788 Time taken: 0.20065999031066895\n",
            "Epoch: 4 Batch Number: 173 Loss: 1.589793086051941 Time taken: 0.20052075386047363\n",
            "Epoch: 4 Batch Number: 174 Loss: 1.5897098779678345 Time taken: 0.2084341049194336\n",
            "Epoch: 4 Batch Number: 175 Loss: 1.593004822731018 Time taken: 0.19948148727416992\n",
            "Epoch: 4 Batch Number: 176 Loss: 1.580396056175232 Time taken: 0.20090866088867188\n",
            "Epoch: 4 Batch Number: 177 Loss: 1.5706475973129272 Time taken: 0.20218276977539062\n",
            "Epoch: 4 Batch Number: 178 Loss: 1.5754388570785522 Time taken: 0.20382142066955566\n",
            "Epoch: 4 Batch Number: 179 Loss: 1.566798210144043 Time taken: 0.20923876762390137\n",
            "==========================================================================================\n",
            "Start of epoch 5\n",
            "Epoch: 5 Batch Number: 1 Loss: 1.5600967407226562 Time taken: 0.19733309745788574\n",
            "Epoch: 5 Batch Number: 2 Loss: 1.5808132886886597 Time taken: 0.19945979118347168\n",
            "Epoch: 5 Batch Number: 3 Loss: 1.539089322090149 Time taken: 0.20733952522277832\n",
            "Epoch: 5 Batch Number: 4 Loss: 1.5672886371612549 Time taken: 0.20342326164245605\n",
            "Epoch: 5 Batch Number: 5 Loss: 1.5412366390228271 Time taken: 0.21665239334106445\n",
            "Epoch: 5 Batch Number: 6 Loss: 1.5403223037719727 Time taken: 0.20113134384155273\n",
            "Epoch: 5 Batch Number: 7 Loss: 1.561418056488037 Time taken: 0.20008182525634766\n",
            "Epoch: 5 Batch Number: 8 Loss: 1.560931921005249 Time taken: 0.20097017288208008\n",
            "Epoch: 5 Batch Number: 9 Loss: 1.5625818967819214 Time taken: 0.20522475242614746\n",
            "Epoch: 5 Batch Number: 10 Loss: 1.5390747785568237 Time taken: 0.20732545852661133\n",
            "Epoch: 5 Batch Number: 11 Loss: 1.560746431350708 Time taken: 0.19919443130493164\n",
            "Epoch: 5 Batch Number: 12 Loss: 1.5606759786605835 Time taken: 0.20576190948486328\n",
            "Epoch: 5 Batch Number: 13 Loss: 1.5663893222808838 Time taken: 0.20223689079284668\n",
            "Epoch: 5 Batch Number: 14 Loss: 1.6089916229248047 Time taken: 0.20801591873168945\n",
            "Epoch: 5 Batch Number: 15 Loss: 1.583673357963562 Time taken: 0.2044210433959961\n",
            "Epoch: 5 Batch Number: 16 Loss: 1.580551028251648 Time taken: 0.20777630805969238\n",
            "Epoch: 5 Batch Number: 17 Loss: 1.5491797924041748 Time taken: 0.1994941234588623\n",
            "Epoch: 5 Batch Number: 18 Loss: 1.5696932077407837 Time taken: 0.19860410690307617\n",
            "Epoch: 5 Batch Number: 19 Loss: 1.5667340755462646 Time taken: 0.20200872421264648\n",
            "Epoch: 5 Batch Number: 20 Loss: 1.5613185167312622 Time taken: 0.20039582252502441\n",
            "Epoch: 5 Batch Number: 21 Loss: 1.554919958114624 Time taken: 0.20364975929260254\n",
            "Epoch: 5 Batch Number: 22 Loss: 1.537845492362976 Time taken: 0.19950342178344727\n",
            "Epoch: 5 Batch Number: 23 Loss: 1.5640044212341309 Time taken: 0.20470738410949707\n",
            "Epoch: 5 Batch Number: 24 Loss: 1.5489369630813599 Time taken: 0.20104742050170898\n",
            "Epoch: 5 Batch Number: 25 Loss: 1.5562059879302979 Time taken: 0.19829010963439941\n",
            "Epoch: 5 Batch Number: 26 Loss: 1.5764509439468384 Time taken: 0.2011713981628418\n",
            "Epoch: 5 Batch Number: 27 Loss: 1.5615134239196777 Time taken: 0.20530152320861816\n",
            "Epoch: 5 Batch Number: 28 Loss: 1.5611437559127808 Time taken: 0.2129828929901123\n",
            "Epoch: 5 Batch Number: 29 Loss: 1.5626193284988403 Time taken: 0.2228381633758545\n",
            "Epoch: 5 Batch Number: 30 Loss: 1.5342450141906738 Time taken: 0.20592570304870605\n",
            "Epoch: 5 Batch Number: 31 Loss: 1.5382230281829834 Time taken: 0.20057344436645508\n",
            "Epoch: 5 Batch Number: 32 Loss: 1.566176176071167 Time taken: 0.20008206367492676\n",
            "Epoch: 5 Batch Number: 33 Loss: 1.5641628503799438 Time taken: 0.21098995208740234\n",
            "Epoch: 5 Batch Number: 34 Loss: 1.5633819103240967 Time taken: 0.20349621772766113\n",
            "Epoch: 5 Batch Number: 35 Loss: 1.5464413166046143 Time taken: 0.20003557205200195\n",
            "Epoch: 5 Batch Number: 36 Loss: 1.5528202056884766 Time taken: 0.20139741897583008\n",
            "Epoch: 5 Batch Number: 37 Loss: 1.5848158597946167 Time taken: 0.1990506649017334\n",
            "Epoch: 5 Batch Number: 38 Loss: 1.5831108093261719 Time taken: 0.20279479026794434\n",
            "Epoch: 5 Batch Number: 39 Loss: 1.6056932210922241 Time taken: 0.2088909149169922\n",
            "Epoch: 5 Batch Number: 40 Loss: 1.5855525732040405 Time taken: 0.2008986473083496\n",
            "Epoch: 5 Batch Number: 41 Loss: 1.5851413011550903 Time taken: 0.20250749588012695\n",
            "Epoch: 5 Batch Number: 42 Loss: 1.5721886157989502 Time taken: 0.20694303512573242\n",
            "Epoch: 5 Batch Number: 43 Loss: 1.566153883934021 Time taken: 0.20572662353515625\n",
            "Epoch: 5 Batch Number: 44 Loss: 1.5619598627090454 Time taken: 0.2036280632019043\n",
            "Epoch: 5 Batch Number: 45 Loss: 1.5700503587722778 Time taken: 0.20170903205871582\n",
            "Epoch: 5 Batch Number: 46 Loss: 1.5522937774658203 Time taken: 0.20075440406799316\n",
            "Epoch: 5 Batch Number: 47 Loss: 1.564501404762268 Time taken: 0.20251059532165527\n",
            "Epoch: 5 Batch Number: 48 Loss: 1.5412709712982178 Time taken: 0.2030496597290039\n",
            "Epoch: 5 Batch Number: 49 Loss: 1.548222541809082 Time taken: 0.21356582641601562\n",
            "Epoch: 5 Batch Number: 50 Loss: 1.5283331871032715 Time taken: 0.2001817226409912\n",
            "Epoch: 5 Batch Number: 51 Loss: 1.5544415712356567 Time taken: 0.20135903358459473\n",
            "Epoch: 5 Batch Number: 52 Loss: 1.5830497741699219 Time taken: 0.20517373085021973\n",
            "Epoch: 5 Batch Number: 53 Loss: 1.5609087944030762 Time taken: 0.20890426635742188\n",
            "Epoch: 5 Batch Number: 54 Loss: 1.5626498460769653 Time taken: 0.20640110969543457\n",
            "Epoch: 5 Batch Number: 55 Loss: 1.5753014087677002 Time taken: 0.19936418533325195\n",
            "Epoch: 5 Batch Number: 56 Loss: 1.56219482421875 Time taken: 0.20102357864379883\n",
            "Epoch: 5 Batch Number: 57 Loss: 1.5369311571121216 Time taken: 0.20658612251281738\n",
            "Epoch: 5 Batch Number: 58 Loss: 1.53621244430542 Time taken: 0.20430850982666016\n",
            "Epoch: 5 Batch Number: 59 Loss: 1.538100004196167 Time taken: 0.20867562294006348\n",
            "Epoch: 5 Batch Number: 60 Loss: 1.5497300624847412 Time taken: 0.20352649688720703\n",
            "Epoch: 5 Batch Number: 61 Loss: 1.542740821838379 Time taken: 0.2005300521850586\n",
            "Epoch: 5 Batch Number: 62 Loss: 1.5488109588623047 Time taken: 0.21040773391723633\n",
            "Epoch: 5 Batch Number: 63 Loss: 1.5460150241851807 Time taken: 0.2024672031402588\n",
            "Epoch: 5 Batch Number: 64 Loss: 1.5423903465270996 Time taken: 0.20003628730773926\n",
            "Epoch: 5 Batch Number: 65 Loss: 1.550309419631958 Time taken: 0.20694518089294434\n",
            "Epoch: 5 Batch Number: 66 Loss: 1.521405816078186 Time taken: 0.2034168243408203\n",
            "Epoch: 5 Batch Number: 67 Loss: 1.5599067211151123 Time taken: 0.20133328437805176\n",
            "Epoch: 5 Batch Number: 68 Loss: 1.5630863904953003 Time taken: 0.20637202262878418\n",
            "Epoch: 5 Batch Number: 69 Loss: 1.5624619722366333 Time taken: 0.20479869842529297\n",
            "Epoch: 5 Batch Number: 70 Loss: 1.5596367120742798 Time taken: 0.19992589950561523\n",
            "Epoch: 5 Batch Number: 71 Loss: 1.561553955078125 Time taken: 0.20831608772277832\n",
            "Epoch: 5 Batch Number: 72 Loss: 1.5676944255828857 Time taken: 0.20033931732177734\n",
            "Epoch: 5 Batch Number: 73 Loss: 1.5450711250305176 Time taken: 0.21398019790649414\n",
            "Epoch: 5 Batch Number: 74 Loss: 1.5876046419143677 Time taken: 0.20107030868530273\n",
            "Epoch: 5 Batch Number: 75 Loss: 1.5446372032165527 Time taken: 0.20079755783081055\n",
            "Epoch: 5 Batch Number: 76 Loss: 1.549754023551941 Time taken: 0.20937466621398926\n",
            "Epoch: 5 Batch Number: 77 Loss: 1.5389022827148438 Time taken: 0.20438599586486816\n",
            "Epoch: 5 Batch Number: 78 Loss: 1.5529420375823975 Time taken: 0.21678948402404785\n",
            "Epoch: 5 Batch Number: 79 Loss: 1.5414965152740479 Time taken: 0.20665860176086426\n",
            "Epoch: 5 Batch Number: 80 Loss: 1.5441381931304932 Time taken: 0.19979238510131836\n",
            "Epoch: 5 Batch Number: 81 Loss: 1.529646873474121 Time taken: 0.20643138885498047\n",
            "Epoch: 5 Batch Number: 82 Loss: 1.5459084510803223 Time taken: 0.2020115852355957\n",
            "Epoch: 5 Batch Number: 83 Loss: 1.508599877357483 Time taken: 0.2025148868560791\n",
            "Epoch: 5 Batch Number: 84 Loss: 1.5234646797180176 Time taken: 0.20379996299743652\n",
            "Epoch: 5 Batch Number: 85 Loss: 1.5131175518035889 Time taken: 0.2053391933441162\n",
            "Epoch: 5 Batch Number: 86 Loss: 1.517226219177246 Time taken: 0.21281170845031738\n",
            "Epoch: 5 Batch Number: 87 Loss: 1.5201319456100464 Time taken: 0.21512222290039062\n",
            "Epoch: 5 Batch Number: 88 Loss: 1.5376523733139038 Time taken: 0.20644474029541016\n",
            "Epoch: 5 Batch Number: 89 Loss: 1.5002597570419312 Time taken: 0.20340704917907715\n",
            "Epoch: 5 Batch Number: 90 Loss: 1.510535478591919 Time taken: 0.20441627502441406\n",
            "Epoch: 5 Batch Number: 91 Loss: 1.5298312902450562 Time taken: 0.20113635063171387\n",
            "Epoch: 5 Batch Number: 92 Loss: 1.5370478630065918 Time taken: 0.19837236404418945\n",
            "Epoch: 5 Batch Number: 93 Loss: 1.5625256299972534 Time taken: 0.2011866569519043\n",
            "Epoch: 5 Batch Number: 94 Loss: 1.5563510656356812 Time taken: 0.19996380805969238\n",
            "Epoch: 5 Batch Number: 95 Loss: 1.570811152458191 Time taken: 0.20741748809814453\n",
            "Epoch: 5 Batch Number: 96 Loss: 1.5537762641906738 Time taken: 0.20022034645080566\n",
            "Epoch: 5 Batch Number: 97 Loss: 1.6002168655395508 Time taken: 0.21113967895507812\n",
            "Epoch: 5 Batch Number: 98 Loss: 1.5806424617767334 Time taken: 0.20074725151062012\n",
            "Epoch: 5 Batch Number: 99 Loss: 1.5648542642593384 Time taken: 0.20140862464904785\n",
            "Epoch: 5 Batch Number: 100 Loss: 1.5477043390274048 Time taken: 0.20604586601257324\n",
            "Epoch: 5 Batch Number: 101 Loss: 1.567910075187683 Time taken: 0.2007138729095459\n",
            "Epoch: 5 Batch Number: 102 Loss: 1.5530861616134644 Time taken: 0.2122640609741211\n",
            "Epoch: 5 Batch Number: 103 Loss: 1.5287857055664062 Time taken: 0.20521140098571777\n",
            "Epoch: 5 Batch Number: 104 Loss: 1.583085060119629 Time taken: 0.2010190486907959\n",
            "Epoch: 5 Batch Number: 105 Loss: 1.5424402952194214 Time taken: 0.2027139663696289\n",
            "Epoch: 5 Batch Number: 106 Loss: 1.5700669288635254 Time taken: 0.20296549797058105\n",
            "Epoch: 5 Batch Number: 107 Loss: 1.578751564025879 Time taken: 0.2024679183959961\n",
            "Epoch: 5 Batch Number: 108 Loss: 1.5523433685302734 Time taken: 0.2032017707824707\n",
            "Epoch: 5 Batch Number: 109 Loss: 1.6015815734863281 Time taken: 0.20067763328552246\n",
            "Epoch: 5 Batch Number: 110 Loss: 1.579986572265625 Time taken: 0.20450496673583984\n",
            "Epoch: 5 Batch Number: 111 Loss: 1.5950837135314941 Time taken: 0.20279693603515625\n",
            "Epoch: 5 Batch Number: 112 Loss: 1.5757147073745728 Time taken: 0.20209550857543945\n",
            "Epoch: 5 Batch Number: 113 Loss: 1.5989514589309692 Time taken: 0.2080979347229004\n",
            "Epoch: 5 Batch Number: 114 Loss: 1.5680992603302002 Time taken: 0.20501422882080078\n",
            "Epoch: 5 Batch Number: 115 Loss: 1.5774589776992798 Time taken: 0.2028179168701172\n",
            "Epoch: 5 Batch Number: 116 Loss: 1.5353708267211914 Time taken: 0.20347166061401367\n",
            "Epoch: 5 Batch Number: 117 Loss: 1.5289766788482666 Time taken: 0.2040548324584961\n",
            "Epoch: 5 Batch Number: 118 Loss: 1.542549729347229 Time taken: 0.20248627662658691\n",
            "Epoch: 5 Batch Number: 119 Loss: 1.555994987487793 Time taken: 0.2031557559967041\n",
            "Epoch: 5 Batch Number: 120 Loss: 1.5784826278686523 Time taken: 0.20170140266418457\n",
            "Epoch: 5 Batch Number: 121 Loss: 1.5429141521453857 Time taken: 0.20596599578857422\n",
            "Epoch: 5 Batch Number: 122 Loss: 1.5162208080291748 Time taken: 0.20119118690490723\n",
            "Epoch: 5 Batch Number: 123 Loss: 1.5598266124725342 Time taken: 0.20547008514404297\n",
            "Epoch: 5 Batch Number: 124 Loss: 1.5392820835113525 Time taken: 0.20240521430969238\n",
            "Epoch: 5 Batch Number: 125 Loss: 1.526382565498352 Time taken: 0.20229029655456543\n",
            "Epoch: 5 Batch Number: 126 Loss: 1.5278682708740234 Time taken: 0.2101120948791504\n",
            "Epoch: 5 Batch Number: 127 Loss: 1.5446773767471313 Time taken: 0.20073533058166504\n",
            "Epoch: 5 Batch Number: 128 Loss: 1.5194752216339111 Time taken: 0.20555663108825684\n",
            "Epoch: 5 Batch Number: 129 Loss: 1.5454304218292236 Time taken: 0.21705913543701172\n",
            "Epoch: 5 Batch Number: 130 Loss: 1.5336003303527832 Time taken: 0.20267963409423828\n",
            "Epoch: 5 Batch Number: 131 Loss: 1.54389488697052 Time taken: 0.20639586448669434\n",
            "Epoch: 5 Batch Number: 132 Loss: 1.542331576347351 Time taken: 0.20409584045410156\n",
            "Epoch: 5 Batch Number: 133 Loss: 1.5277326107025146 Time taken: 0.20476031303405762\n",
            "Epoch: 5 Batch Number: 134 Loss: 1.573653221130371 Time taken: 0.20520830154418945\n",
            "Epoch: 5 Batch Number: 135 Loss: 1.5190842151641846 Time taken: 0.20009064674377441\n",
            "Epoch: 5 Batch Number: 136 Loss: 1.5323163270950317 Time taken: 0.20451569557189941\n",
            "Epoch: 5 Batch Number: 137 Loss: 1.5232186317443848 Time taken: 0.20585942268371582\n",
            "Epoch: 5 Batch Number: 138 Loss: 1.520698070526123 Time taken: 0.20665931701660156\n",
            "Epoch: 5 Batch Number: 139 Loss: 1.5143851041793823 Time taken: 0.19780325889587402\n",
            "Epoch: 5 Batch Number: 140 Loss: 1.5394121408462524 Time taken: 0.20048904418945312\n",
            "Epoch: 5 Batch Number: 141 Loss: 1.5519764423370361 Time taken: 0.20330572128295898\n",
            "Epoch: 5 Batch Number: 142 Loss: 1.5593137741088867 Time taken: 0.19959330558776855\n",
            "Epoch: 5 Batch Number: 143 Loss: 1.572470784187317 Time taken: 0.20006632804870605\n",
            "Epoch: 5 Batch Number: 144 Loss: 1.524416208267212 Time taken: 0.2012026309967041\n",
            "Epoch: 5 Batch Number: 145 Loss: 1.550601840019226 Time taken: 0.20532965660095215\n",
            "Epoch: 5 Batch Number: 146 Loss: 1.5432631969451904 Time taken: 0.20340466499328613\n",
            "Epoch: 5 Batch Number: 147 Loss: 1.5383230447769165 Time taken: 0.2022864818572998\n",
            "Epoch: 5 Batch Number: 148 Loss: 1.5340242385864258 Time taken: 0.2000904083251953\n",
            "Epoch: 5 Batch Number: 149 Loss: 1.5148862600326538 Time taken: 0.20383000373840332\n",
            "Epoch: 5 Batch Number: 150 Loss: 1.5300064086914062 Time taken: 0.19994330406188965\n",
            "Epoch: 5 Batch Number: 151 Loss: 1.5154950618743896 Time taken: 0.20025038719177246\n",
            "Epoch: 5 Batch Number: 152 Loss: 1.5310126543045044 Time taken: 0.20221519470214844\n",
            "Epoch: 5 Batch Number: 153 Loss: 1.5331295728683472 Time taken: 0.2121109962463379\n",
            "Epoch: 5 Batch Number: 154 Loss: 1.5154688358306885 Time taken: 0.20487523078918457\n",
            "Epoch: 5 Batch Number: 155 Loss: 1.5088565349578857 Time taken: 0.20246219635009766\n",
            "Epoch: 5 Batch Number: 156 Loss: 1.5328394174575806 Time taken: 0.20180034637451172\n",
            "Epoch: 5 Batch Number: 157 Loss: 1.5754956007003784 Time taken: 0.20315885543823242\n",
            "Epoch: 5 Batch Number: 158 Loss: 1.5355803966522217 Time taken: 0.20466899871826172\n",
            "Epoch: 5 Batch Number: 159 Loss: 1.5325905084609985 Time taken: 0.20527052879333496\n",
            "Epoch: 5 Batch Number: 160 Loss: 1.531873106956482 Time taken: 0.20689082145690918\n",
            "Epoch: 5 Batch Number: 161 Loss: 1.5268046855926514 Time taken: 0.20071721076965332\n",
            "Epoch: 5 Batch Number: 162 Loss: 1.5347400903701782 Time taken: 0.19801902770996094\n",
            "Epoch: 5 Batch Number: 163 Loss: 1.5251679420471191 Time taken: 0.20451855659484863\n",
            "Epoch: 5 Batch Number: 164 Loss: 1.5389447212219238 Time taken: 0.20037150382995605\n",
            "Epoch: 5 Batch Number: 165 Loss: 1.5149526596069336 Time taken: 0.20508575439453125\n",
            "Epoch: 5 Batch Number: 166 Loss: 1.5117515325546265 Time taken: 0.20595192909240723\n",
            "Epoch: 5 Batch Number: 167 Loss: 1.513899803161621 Time taken: 0.20460963249206543\n",
            "Epoch: 5 Batch Number: 168 Loss: 1.5070853233337402 Time taken: 0.200730562210083\n",
            "Epoch: 5 Batch Number: 169 Loss: 1.4990507364273071 Time taken: 0.20156455039978027\n",
            "Epoch: 5 Batch Number: 170 Loss: 1.52396559715271 Time taken: 0.21591877937316895\n",
            "Epoch: 5 Batch Number: 171 Loss: 1.517578125 Time taken: 0.21161437034606934\n",
            "Epoch: 5 Batch Number: 172 Loss: 1.4926073551177979 Time taken: 0.20617389678955078\n",
            "Epoch: 5 Batch Number: 173 Loss: 1.5284831523895264 Time taken: 0.20659446716308594\n",
            "Epoch: 5 Batch Number: 174 Loss: 1.478914499282837 Time taken: 0.20674395561218262\n",
            "Epoch: 5 Batch Number: 175 Loss: 1.5058070421218872 Time taken: 0.2062516212463379\n",
            "Epoch: 5 Batch Number: 176 Loss: 1.502337098121643 Time taken: 0.20364093780517578\n",
            "Epoch: 5 Batch Number: 177 Loss: 1.481312870979309 Time taken: 0.19750571250915527\n",
            "Epoch: 5 Batch Number: 178 Loss: 1.4972176551818848 Time taken: 0.21088957786560059\n",
            "Epoch: 5 Batch Number: 179 Loss: 1.4911011457443237 Time taken: 0.1992499828338623\n",
            "==========================================================================================\n",
            "Start of epoch 6\n",
            "Epoch: 6 Batch Number: 1 Loss: 1.4966812133789062 Time taken: 0.20082449913024902\n",
            "Epoch: 6 Batch Number: 2 Loss: 1.477099895477295 Time taken: 0.2007465362548828\n",
            "Epoch: 6 Batch Number: 3 Loss: 1.4718537330627441 Time taken: 0.20445489883422852\n",
            "Epoch: 6 Batch Number: 4 Loss: 1.4872689247131348 Time taken: 0.19848060607910156\n",
            "Epoch: 6 Batch Number: 5 Loss: 1.4614070653915405 Time taken: 0.2073061466217041\n",
            "Epoch: 6 Batch Number: 6 Loss: 1.4544689655303955 Time taken: 0.20104551315307617\n",
            "Epoch: 6 Batch Number: 7 Loss: 1.466845154762268 Time taken: 0.20027446746826172\n",
            "Epoch: 6 Batch Number: 8 Loss: 1.4870365858078003 Time taken: 0.21035122871398926\n",
            "Epoch: 6 Batch Number: 9 Loss: 1.4854079484939575 Time taken: 0.1989285945892334\n",
            "Epoch: 6 Batch Number: 10 Loss: 1.4662818908691406 Time taken: 0.20862412452697754\n",
            "Epoch: 6 Batch Number: 11 Loss: 1.4801645278930664 Time taken: 0.20836210250854492\n",
            "Epoch: 6 Batch Number: 12 Loss: 1.487432599067688 Time taken: 0.21282196044921875\n",
            "Epoch: 6 Batch Number: 13 Loss: 1.48775315284729 Time taken: 0.20516490936279297\n",
            "Epoch: 6 Batch Number: 14 Loss: 1.5030112266540527 Time taken: 0.20232558250427246\n",
            "Epoch: 6 Batch Number: 15 Loss: 1.5028786659240723 Time taken: 0.21990585327148438\n",
            "Epoch: 6 Batch Number: 16 Loss: 1.4994481801986694 Time taken: 0.20312094688415527\n",
            "Epoch: 6 Batch Number: 17 Loss: 1.4724599123001099 Time taken: 0.2035233974456787\n",
            "Epoch: 6 Batch Number: 18 Loss: 1.4841943979263306 Time taken: 0.20205044746398926\n",
            "Epoch: 6 Batch Number: 19 Loss: 1.4732874631881714 Time taken: 0.20798587799072266\n",
            "Epoch: 6 Batch Number: 20 Loss: 1.455003261566162 Time taken: 0.20197200775146484\n",
            "Epoch: 6 Batch Number: 21 Loss: 1.4815524816513062 Time taken: 0.2042236328125\n",
            "Epoch: 6 Batch Number: 22 Loss: 1.4894087314605713 Time taken: 0.20103168487548828\n",
            "Epoch: 6 Batch Number: 23 Loss: 1.4719232320785522 Time taken: 0.20306682586669922\n",
            "Epoch: 6 Batch Number: 24 Loss: 1.483721137046814 Time taken: 0.20208311080932617\n",
            "Epoch: 6 Batch Number: 25 Loss: 1.5111448764801025 Time taken: 0.2026073932647705\n",
            "Epoch: 6 Batch Number: 26 Loss: 1.4976742267608643 Time taken: 0.2043318748474121\n",
            "Epoch: 6 Batch Number: 27 Loss: 1.48224675655365 Time taken: 0.20395159721374512\n",
            "Epoch: 6 Batch Number: 28 Loss: 1.4870338439941406 Time taken: 0.2077021598815918\n",
            "Epoch: 6 Batch Number: 29 Loss: 1.4721214771270752 Time taken: 0.20864391326904297\n",
            "Epoch: 6 Batch Number: 30 Loss: 1.4930311441421509 Time taken: 0.21279454231262207\n",
            "Epoch: 6 Batch Number: 31 Loss: 1.4620884656906128 Time taken: 0.2003467082977295\n",
            "Epoch: 6 Batch Number: 32 Loss: 1.47214674949646 Time taken: 0.20371294021606445\n",
            "Epoch: 6 Batch Number: 33 Loss: 1.4862415790557861 Time taken: 0.2002720832824707\n",
            "Epoch: 6 Batch Number: 34 Loss: 1.5062497854232788 Time taken: 0.21261000633239746\n",
            "Epoch: 6 Batch Number: 35 Loss: 1.4741954803466797 Time taken: 0.20526123046875\n",
            "Epoch: 6 Batch Number: 36 Loss: 1.484154462814331 Time taken: 0.2085132598876953\n",
            "Epoch: 6 Batch Number: 37 Loss: 1.4833447933197021 Time taken: 0.20180797576904297\n",
            "Epoch: 6 Batch Number: 38 Loss: 1.5107964277267456 Time taken: 0.19945311546325684\n",
            "Epoch: 6 Batch Number: 39 Loss: 1.5344040393829346 Time taken: 0.21089911460876465\n",
            "Epoch: 6 Batch Number: 40 Loss: 1.513264536857605 Time taken: 0.20365214347839355\n",
            "Epoch: 6 Batch Number: 41 Loss: 1.486651062965393 Time taken: 0.20357608795166016\n",
            "Epoch: 6 Batch Number: 42 Loss: 1.4913479089736938 Time taken: 0.20108389854431152\n",
            "Epoch: 6 Batch Number: 43 Loss: 1.4690582752227783 Time taken: 0.20049285888671875\n",
            "Epoch: 6 Batch Number: 44 Loss: 1.4693909883499146 Time taken: 0.20677542686462402\n",
            "Epoch: 6 Batch Number: 45 Loss: 1.5035748481750488 Time taken: 0.2063901424407959\n",
            "Epoch: 6 Batch Number: 46 Loss: 1.4762845039367676 Time taken: 0.20268630981445312\n",
            "Epoch: 6 Batch Number: 47 Loss: 1.4823826551437378 Time taken: 0.2002103328704834\n",
            "Epoch: 6 Batch Number: 48 Loss: 1.4587074518203735 Time taken: 0.20582962036132812\n",
            "Epoch: 6 Batch Number: 49 Loss: 1.5013129711151123 Time taken: 0.19990062713623047\n",
            "Epoch: 6 Batch Number: 50 Loss: 1.4610402584075928 Time taken: 0.20402312278747559\n",
            "Epoch: 6 Batch Number: 51 Loss: 1.4980149269104004 Time taken: 0.20163512229919434\n",
            "Epoch: 6 Batch Number: 52 Loss: 1.4884263277053833 Time taken: 0.20057988166809082\n",
            "Epoch: 6 Batch Number: 53 Loss: 1.4949383735656738 Time taken: 0.20247960090637207\n",
            "Epoch: 6 Batch Number: 54 Loss: 1.4893053770065308 Time taken: 0.19889211654663086\n",
            "Epoch: 6 Batch Number: 55 Loss: 1.465916633605957 Time taken: 0.20390987396240234\n",
            "Epoch: 6 Batch Number: 56 Loss: 1.4589506387710571 Time taken: 0.20103049278259277\n",
            "Epoch: 6 Batch Number: 57 Loss: 1.478402018547058 Time taken: 0.20539569854736328\n",
            "Epoch: 6 Batch Number: 58 Loss: 1.4724277257919312 Time taken: 0.20131444931030273\n",
            "Epoch: 6 Batch Number: 59 Loss: 1.4854533672332764 Time taken: 0.20479679107666016\n",
            "Epoch: 6 Batch Number: 60 Loss: 1.4761065244674683 Time taken: 0.20293974876403809\n",
            "Epoch: 6 Batch Number: 61 Loss: 1.4625228643417358 Time taken: 0.20141077041625977\n",
            "Epoch: 6 Batch Number: 62 Loss: 1.4880805015563965 Time taken: 0.1991877555847168\n",
            "Epoch: 6 Batch Number: 63 Loss: 1.4605133533477783 Time taken: 0.2085878849029541\n",
            "Epoch: 6 Batch Number: 64 Loss: 1.4727771282196045 Time taken: 0.20316338539123535\n",
            "Epoch: 6 Batch Number: 65 Loss: 1.4907875061035156 Time taken: 0.20205974578857422\n",
            "Epoch: 6 Batch Number: 66 Loss: 1.5044386386871338 Time taken: 0.2012042999267578\n",
            "Epoch: 6 Batch Number: 67 Loss: 1.470811367034912 Time taken: 0.20072364807128906\n",
            "Epoch: 6 Batch Number: 68 Loss: 1.4829485416412354 Time taken: 0.20488715171813965\n",
            "Epoch: 6 Batch Number: 69 Loss: 1.4907571077346802 Time taken: 0.2032911777496338\n",
            "Epoch: 6 Batch Number: 70 Loss: 1.4836992025375366 Time taken: 0.20418524742126465\n",
            "Epoch: 6 Batch Number: 71 Loss: 1.4807813167572021 Time taken: 0.1996173858642578\n",
            "Epoch: 6 Batch Number: 72 Loss: 1.5007319450378418 Time taken: 0.2012031078338623\n",
            "Epoch: 6 Batch Number: 73 Loss: 1.473634958267212 Time taken: 0.2054905891418457\n",
            "Epoch: 6 Batch Number: 74 Loss: 1.510257601737976 Time taken: 0.21191000938415527\n",
            "Epoch: 6 Batch Number: 75 Loss: 1.4791529178619385 Time taken: 0.20331168174743652\n",
            "Epoch: 6 Batch Number: 76 Loss: 1.49226975440979 Time taken: 0.2009437084197998\n",
            "Epoch: 6 Batch Number: 77 Loss: 1.4774792194366455 Time taken: 0.20798134803771973\n",
            "Epoch: 6 Batch Number: 78 Loss: 1.4737588167190552 Time taken: 0.21106314659118652\n",
            "Epoch: 6 Batch Number: 79 Loss: 1.4994388818740845 Time taken: 0.20028018951416016\n",
            "Epoch: 6 Batch Number: 80 Loss: 1.481116771697998 Time taken: 0.19971084594726562\n",
            "Epoch: 6 Batch Number: 81 Loss: 1.4725608825683594 Time taken: 0.20392847061157227\n",
            "Epoch: 6 Batch Number: 82 Loss: 1.4657280445098877 Time taken: 0.20394253730773926\n",
            "Epoch: 6 Batch Number: 83 Loss: 1.4542957544326782 Time taken: 0.20268464088439941\n",
            "Epoch: 6 Batch Number: 84 Loss: 1.4598983526229858 Time taken: 0.20385122299194336\n",
            "Epoch: 6 Batch Number: 85 Loss: 1.4520032405853271 Time taken: 0.19843840599060059\n",
            "Epoch: 6 Batch Number: 86 Loss: 1.4327130317687988 Time taken: 0.20327019691467285\n",
            "Epoch: 6 Batch Number: 87 Loss: 1.414900541305542 Time taken: 0.2042398452758789\n",
            "Epoch: 6 Batch Number: 88 Loss: 1.452317714691162 Time taken: 0.20387744903564453\n",
            "Epoch: 6 Batch Number: 89 Loss: 1.4323654174804688 Time taken: 0.2005143165588379\n",
            "Epoch: 6 Batch Number: 90 Loss: 1.4652096033096313 Time taken: 0.2001504898071289\n",
            "Epoch: 6 Batch Number: 91 Loss: 1.485029697418213 Time taken: 0.20107769966125488\n",
            "Epoch: 6 Batch Number: 92 Loss: 1.4851425886154175 Time taken: 0.20125603675842285\n",
            "Epoch: 6 Batch Number: 93 Loss: 1.4712611436843872 Time taken: 0.19964098930358887\n",
            "Epoch: 6 Batch Number: 94 Loss: 1.4849905967712402 Time taken: 0.20134544372558594\n",
            "Epoch: 6 Batch Number: 95 Loss: 1.5056260824203491 Time taken: 0.20017385482788086\n",
            "Epoch: 6 Batch Number: 96 Loss: 1.502704381942749 Time taken: 0.2024669647216797\n",
            "Epoch: 6 Batch Number: 97 Loss: 1.4941209554672241 Time taken: 0.20013904571533203\n",
            "Epoch: 6 Batch Number: 98 Loss: 1.4774067401885986 Time taken: 0.2033240795135498\n",
            "Epoch: 6 Batch Number: 99 Loss: 1.492782711982727 Time taken: 0.20259666442871094\n",
            "Epoch: 6 Batch Number: 100 Loss: 1.4990811347961426 Time taken: 0.20083236694335938\n",
            "Epoch: 6 Batch Number: 101 Loss: 1.4883071184158325 Time taken: 0.20402240753173828\n",
            "Epoch: 6 Batch Number: 102 Loss: 1.4988130331039429 Time taken: 0.20315980911254883\n",
            "Epoch: 6 Batch Number: 103 Loss: 1.4692293405532837 Time taken: 0.20099258422851562\n",
            "Epoch: 6 Batch Number: 104 Loss: 1.478007435798645 Time taken: 0.20348358154296875\n",
            "Epoch: 6 Batch Number: 105 Loss: 1.4926178455352783 Time taken: 0.20256638526916504\n",
            "Epoch: 6 Batch Number: 106 Loss: 1.4989511966705322 Time taken: 0.1997356414794922\n",
            "Epoch: 6 Batch Number: 107 Loss: 1.5202388763427734 Time taken: 0.2066664695739746\n",
            "Epoch: 6 Batch Number: 108 Loss: 1.5330007076263428 Time taken: 0.20008063316345215\n",
            "Epoch: 6 Batch Number: 109 Loss: 1.5111764669418335 Time taken: 0.20770788192749023\n",
            "Epoch: 6 Batch Number: 110 Loss: 1.5163120031356812 Time taken: 0.20319175720214844\n",
            "Epoch: 6 Batch Number: 111 Loss: 1.5324052572250366 Time taken: 0.2019333839416504\n",
            "Epoch: 6 Batch Number: 112 Loss: 1.514904260635376 Time taken: 0.19997787475585938\n",
            "Epoch: 6 Batch Number: 113 Loss: 1.5029054880142212 Time taken: 0.2108933925628662\n",
            "Epoch: 6 Batch Number: 114 Loss: 1.5180519819259644 Time taken: 0.20015764236450195\n",
            "Epoch: 6 Batch Number: 115 Loss: 1.500029444694519 Time taken: 0.20071125030517578\n",
            "Epoch: 6 Batch Number: 116 Loss: 1.485825538635254 Time taken: 0.20261120796203613\n",
            "Epoch: 6 Batch Number: 117 Loss: 1.4992133378982544 Time taken: 0.20886874198913574\n",
            "Epoch: 6 Batch Number: 118 Loss: 1.4957472085952759 Time taken: 0.20109224319458008\n",
            "Epoch: 6 Batch Number: 119 Loss: 1.480200171470642 Time taken: 0.1977529525756836\n",
            "Epoch: 6 Batch Number: 120 Loss: 1.4796197414398193 Time taken: 0.20107078552246094\n",
            "Epoch: 6 Batch Number: 121 Loss: 1.4997533559799194 Time taken: 0.19980788230895996\n",
            "Epoch: 6 Batch Number: 122 Loss: 1.480815052986145 Time taken: 0.2068643569946289\n",
            "Epoch: 6 Batch Number: 123 Loss: 1.492817759513855 Time taken: 0.20345568656921387\n",
            "Epoch: 6 Batch Number: 124 Loss: 1.46285080909729 Time taken: 0.20403623580932617\n",
            "Epoch: 6 Batch Number: 125 Loss: 1.4878945350646973 Time taken: 0.20235276222229004\n",
            "Epoch: 6 Batch Number: 126 Loss: 1.4774399995803833 Time taken: 0.20508670806884766\n",
            "Epoch: 6 Batch Number: 127 Loss: 1.4819287061691284 Time taken: 0.20409250259399414\n",
            "Epoch: 6 Batch Number: 128 Loss: 1.4700005054473877 Time taken: 0.20172119140625\n",
            "Epoch: 6 Batch Number: 129 Loss: 1.4619064331054688 Time taken: 0.20082759857177734\n",
            "Epoch: 6 Batch Number: 130 Loss: 1.4667614698410034 Time taken: 0.20211124420166016\n",
            "Epoch: 6 Batch Number: 131 Loss: 1.4845893383026123 Time taken: 0.19940614700317383\n",
            "Epoch: 6 Batch Number: 132 Loss: 1.4767922163009644 Time taken: 0.20057201385498047\n",
            "Epoch: 6 Batch Number: 133 Loss: 1.4588466882705688 Time taken: 0.19898343086242676\n",
            "Epoch: 6 Batch Number: 134 Loss: 1.4672355651855469 Time taken: 0.20599722862243652\n",
            "Epoch: 6 Batch Number: 135 Loss: 1.476493239402771 Time taken: 0.20089125633239746\n",
            "Epoch: 6 Batch Number: 136 Loss: 1.4445371627807617 Time taken: 0.20271039009094238\n",
            "Epoch: 6 Batch Number: 137 Loss: 1.4557740688323975 Time taken: 0.21059894561767578\n",
            "Epoch: 6 Batch Number: 138 Loss: 1.4263321161270142 Time taken: 0.2006692886352539\n",
            "Epoch: 6 Batch Number: 139 Loss: 1.4685395956039429 Time taken: 0.20271992683410645\n",
            "Epoch: 6 Batch Number: 140 Loss: 1.4877827167510986 Time taken: 0.20113420486450195\n",
            "Epoch: 6 Batch Number: 141 Loss: 1.4968867301940918 Time taken: 0.20704984664916992\n",
            "Epoch: 6 Batch Number: 142 Loss: 1.4727199077606201 Time taken: 0.19985461235046387\n",
            "Epoch: 6 Batch Number: 143 Loss: 1.5395234823226929 Time taken: 0.20095014572143555\n",
            "Epoch: 6 Batch Number: 144 Loss: 1.5055252313613892 Time taken: 0.20701217651367188\n",
            "Epoch: 6 Batch Number: 145 Loss: 1.483572006225586 Time taken: 0.20280003547668457\n",
            "Epoch: 6 Batch Number: 146 Loss: 1.4721988439559937 Time taken: 0.20734620094299316\n",
            "Epoch: 6 Batch Number: 147 Loss: 1.4675281047821045 Time taken: 0.1986677646636963\n",
            "Epoch: 6 Batch Number: 148 Loss: 1.4741847515106201 Time taken: 0.2016279697418213\n",
            "Epoch: 6 Batch Number: 149 Loss: 1.4711769819259644 Time taken: 0.20128846168518066\n",
            "Epoch: 6 Batch Number: 150 Loss: 1.4638991355895996 Time taken: 0.20224285125732422\n",
            "Epoch: 6 Batch Number: 151 Loss: 1.4925357103347778 Time taken: 0.20519518852233887\n",
            "Epoch: 6 Batch Number: 152 Loss: 1.4821301698684692 Time taken: 0.20078134536743164\n",
            "Epoch: 6 Batch Number: 153 Loss: 1.4777270555496216 Time taken: 0.20194315910339355\n",
            "Epoch: 6 Batch Number: 154 Loss: 1.4733229875564575 Time taken: 0.19751524925231934\n",
            "Epoch: 6 Batch Number: 155 Loss: 1.4526078701019287 Time taken: 0.21518325805664062\n",
            "Epoch: 6 Batch Number: 156 Loss: 1.4596155881881714 Time taken: 0.2075188159942627\n",
            "Epoch: 6 Batch Number: 157 Loss: 1.4773772954940796 Time taken: 0.20297718048095703\n",
            "Epoch: 6 Batch Number: 158 Loss: 1.4824031591415405 Time taken: 0.20967531204223633\n",
            "Epoch: 6 Batch Number: 159 Loss: 1.469369888305664 Time taken: 0.200331449508667\n",
            "Epoch: 6 Batch Number: 160 Loss: 1.478232979774475 Time taken: 0.20138120651245117\n",
            "Epoch: 6 Batch Number: 161 Loss: 1.4554730653762817 Time taken: 0.20600628852844238\n",
            "Epoch: 6 Batch Number: 162 Loss: 1.4564844369888306 Time taken: 0.20192670822143555\n",
            "Epoch: 6 Batch Number: 163 Loss: 1.4548183679580688 Time taken: 0.2000432014465332\n",
            "Epoch: 6 Batch Number: 164 Loss: 1.4838491678237915 Time taken: 0.20240330696105957\n",
            "Epoch: 6 Batch Number: 165 Loss: 1.4761351346969604 Time taken: 0.20363688468933105\n",
            "Epoch: 6 Batch Number: 166 Loss: 1.4753541946411133 Time taken: 0.2014756202697754\n",
            "Epoch: 6 Batch Number: 167 Loss: 1.4630712270736694 Time taken: 0.2082352638244629\n",
            "Epoch: 6 Batch Number: 168 Loss: 1.461754560470581 Time taken: 0.21368694305419922\n",
            "Epoch: 6 Batch Number: 169 Loss: 1.4536235332489014 Time taken: 0.20203304290771484\n",
            "Epoch: 6 Batch Number: 170 Loss: 1.4716026782989502 Time taken: 0.20082807540893555\n",
            "Epoch: 6 Batch Number: 171 Loss: 1.4729652404785156 Time taken: 0.20047473907470703\n",
            "Epoch: 6 Batch Number: 172 Loss: 1.4474376440048218 Time taken: 0.2195580005645752\n",
            "Epoch: 6 Batch Number: 173 Loss: 1.4477977752685547 Time taken: 0.20116138458251953\n",
            "Epoch: 6 Batch Number: 174 Loss: 1.4480819702148438 Time taken: 0.19917988777160645\n",
            "Epoch: 6 Batch Number: 175 Loss: 1.4295425415039062 Time taken: 0.21033072471618652\n",
            "Epoch: 6 Batch Number: 176 Loss: 1.447960615158081 Time taken: 0.20537114143371582\n",
            "Epoch: 6 Batch Number: 177 Loss: 1.4474390745162964 Time taken: 0.20289301872253418\n",
            "Epoch: 6 Batch Number: 178 Loss: 1.4399487972259521 Time taken: 0.20375299453735352\n",
            "Epoch: 6 Batch Number: 179 Loss: 1.4337940216064453 Time taken: 0.19942736625671387\n",
            "==========================================================================================\n",
            "Start of epoch 7\n",
            "Epoch: 7 Batch Number: 1 Loss: 1.4358241558074951 Time taken: 0.20154809951782227\n",
            "Epoch: 7 Batch Number: 2 Loss: 1.4220452308654785 Time taken: 0.20777177810668945\n",
            "Epoch: 7 Batch Number: 3 Loss: 1.4128175973892212 Time taken: 0.20196747779846191\n",
            "Epoch: 7 Batch Number: 4 Loss: 1.411028265953064 Time taken: 0.20541048049926758\n",
            "Epoch: 7 Batch Number: 5 Loss: 1.4125715494155884 Time taken: 0.20629167556762695\n",
            "Epoch: 7 Batch Number: 6 Loss: 1.4209654331207275 Time taken: 0.21006989479064941\n",
            "Epoch: 7 Batch Number: 7 Loss: 1.413382887840271 Time taken: 0.2035379409790039\n",
            "Epoch: 7 Batch Number: 8 Loss: 1.432750940322876 Time taken: 0.2004547119140625\n",
            "Epoch: 7 Batch Number: 9 Loss: 1.411258578300476 Time taken: 0.205061674118042\n",
            "Epoch: 7 Batch Number: 10 Loss: 1.4182456731796265 Time taken: 0.20071840286254883\n",
            "Epoch: 7 Batch Number: 11 Loss: 1.4323281049728394 Time taken: 0.20959067344665527\n",
            "Epoch: 7 Batch Number: 12 Loss: 1.4297218322753906 Time taken: 0.20698881149291992\n",
            "Epoch: 7 Batch Number: 13 Loss: 1.4383143186569214 Time taken: 0.20086121559143066\n",
            "Epoch: 7 Batch Number: 14 Loss: 1.4490303993225098 Time taken: 0.20818877220153809\n",
            "Epoch: 7 Batch Number: 15 Loss: 1.4443387985229492 Time taken: 0.20874547958374023\n",
            "Epoch: 7 Batch Number: 16 Loss: 1.4265555143356323 Time taken: 0.20328974723815918\n",
            "Epoch: 7 Batch Number: 17 Loss: 1.4469423294067383 Time taken: 0.20315122604370117\n",
            "Epoch: 7 Batch Number: 18 Loss: 1.413582444190979 Time taken: 0.20352792739868164\n",
            "Epoch: 7 Batch Number: 19 Loss: 1.4231600761413574 Time taken: 0.20735979080200195\n",
            "Epoch: 7 Batch Number: 20 Loss: 1.417594313621521 Time taken: 0.2064659595489502\n",
            "Epoch: 7 Batch Number: 21 Loss: 1.3942880630493164 Time taken: 0.20138072967529297\n",
            "Epoch: 7 Batch Number: 22 Loss: 1.433429479598999 Time taken: 0.2000112533569336\n",
            "Epoch: 7 Batch Number: 23 Loss: 1.440045714378357 Time taken: 0.19815444946289062\n",
            "Epoch: 7 Batch Number: 24 Loss: 1.4326214790344238 Time taken: 0.1999800205230713\n",
            "Epoch: 7 Batch Number: 25 Loss: 1.4643168449401855 Time taken: 0.20761513710021973\n",
            "Epoch: 7 Batch Number: 26 Loss: 1.436968445777893 Time taken: 0.20180010795593262\n",
            "Epoch: 7 Batch Number: 27 Loss: 1.4335060119628906 Time taken: 0.20174074172973633\n",
            "Epoch: 7 Batch Number: 28 Loss: 1.4272364377975464 Time taken: 0.19887733459472656\n",
            "Epoch: 7 Batch Number: 29 Loss: 1.4422940015792847 Time taken: 0.1962118148803711\n",
            "Epoch: 7 Batch Number: 30 Loss: 1.4120210409164429 Time taken: 0.21084213256835938\n",
            "Epoch: 7 Batch Number: 31 Loss: 1.4072973728179932 Time taken: 0.20201659202575684\n",
            "Epoch: 7 Batch Number: 32 Loss: 1.4269399642944336 Time taken: 0.20073986053466797\n",
            "Epoch: 7 Batch Number: 33 Loss: 1.4202553033828735 Time taken: 0.20221805572509766\n",
            "Epoch: 7 Batch Number: 34 Loss: 1.461069941520691 Time taken: 0.20301389694213867\n",
            "Epoch: 7 Batch Number: 35 Loss: 1.4234141111373901 Time taken: 0.20195960998535156\n",
            "Epoch: 7 Batch Number: 36 Loss: 1.4345762729644775 Time taken: 0.21426653861999512\n",
            "Epoch: 7 Batch Number: 37 Loss: 1.4571669101715088 Time taken: 0.20185136795043945\n",
            "Epoch: 7 Batch Number: 38 Loss: 1.4379551410675049 Time taken: 0.2068617343902588\n",
            "Epoch: 7 Batch Number: 39 Loss: 1.4505900144577026 Time taken: 0.20108985900878906\n",
            "Epoch: 7 Batch Number: 40 Loss: 1.4449838399887085 Time taken: 0.21559572219848633\n",
            "Epoch: 7 Batch Number: 41 Loss: 1.417462706565857 Time taken: 0.20597290992736816\n",
            "Epoch: 7 Batch Number: 42 Loss: 1.4621127843856812 Time taken: 0.20284271240234375\n",
            "Epoch: 7 Batch Number: 43 Loss: 1.4475023746490479 Time taken: 0.2014474868774414\n",
            "Epoch: 7 Batch Number: 44 Loss: 1.4307383298873901 Time taken: 0.20256543159484863\n",
            "Epoch: 7 Batch Number: 45 Loss: 1.4164848327636719 Time taken: 0.2088932991027832\n",
            "Epoch: 7 Batch Number: 46 Loss: 1.4320316314697266 Time taken: 0.20177531242370605\n",
            "Epoch: 7 Batch Number: 47 Loss: 1.4461071491241455 Time taken: 0.20043420791625977\n",
            "Epoch: 7 Batch Number: 48 Loss: 1.421202540397644 Time taken: 0.2006843090057373\n",
            "Epoch: 7 Batch Number: 49 Loss: 1.421142578125 Time taken: 0.2015695571899414\n",
            "Epoch: 7 Batch Number: 50 Loss: 1.4492809772491455 Time taken: 0.19982624053955078\n",
            "Epoch: 7 Batch Number: 51 Loss: 1.4174388647079468 Time taken: 0.20542383193969727\n",
            "Epoch: 7 Batch Number: 52 Loss: 1.4235378503799438 Time taken: 0.19925189018249512\n",
            "Epoch: 7 Batch Number: 53 Loss: 1.4479992389678955 Time taken: 0.20300579071044922\n",
            "Epoch: 7 Batch Number: 54 Loss: 1.4117183685302734 Time taken: 0.1998424530029297\n",
            "Epoch: 7 Batch Number: 55 Loss: 1.4461357593536377 Time taken: 0.20505762100219727\n",
            "Epoch: 7 Batch Number: 56 Loss: 1.4281980991363525 Time taken: 0.2072298526763916\n",
            "Epoch: 7 Batch Number: 57 Loss: 1.4095094203948975 Time taken: 0.20025968551635742\n",
            "Epoch: 7 Batch Number: 58 Loss: 1.422536849975586 Time taken: 0.20122194290161133\n",
            "Epoch: 7 Batch Number: 59 Loss: 1.4205514192581177 Time taken: 0.2032625675201416\n",
            "Epoch: 7 Batch Number: 60 Loss: 1.401906132698059 Time taken: 0.202195405960083\n",
            "Epoch: 7 Batch Number: 61 Loss: 1.4165682792663574 Time taken: 0.20316433906555176\n",
            "Epoch: 7 Batch Number: 62 Loss: 1.416471242904663 Time taken: 0.20241951942443848\n",
            "Epoch: 7 Batch Number: 63 Loss: 1.4213974475860596 Time taken: 0.20007538795471191\n",
            "Epoch: 7 Batch Number: 64 Loss: 1.3987447023391724 Time taken: 0.22040915489196777\n",
            "Epoch: 7 Batch Number: 65 Loss: 1.4381974935531616 Time taken: 0.2562429904937744\n",
            "Epoch: 7 Batch Number: 66 Loss: 1.4312299489974976 Time taken: 0.2021327018737793\n",
            "Epoch: 7 Batch Number: 67 Loss: 1.4477026462554932 Time taken: 0.20086407661437988\n",
            "Epoch: 7 Batch Number: 68 Loss: 1.4516249895095825 Time taken: 0.20050549507141113\n",
            "Epoch: 7 Batch Number: 69 Loss: 1.4236444234848022 Time taken: 0.21533751487731934\n",
            "Epoch: 7 Batch Number: 70 Loss: 1.4501984119415283 Time taken: 0.19997358322143555\n",
            "Epoch: 7 Batch Number: 71 Loss: 1.4458414316177368 Time taken: 0.2035229206085205\n",
            "Epoch: 7 Batch Number: 72 Loss: 1.4266492128372192 Time taken: 0.20105671882629395\n",
            "Epoch: 7 Batch Number: 73 Loss: 1.4445325136184692 Time taken: 0.20530271530151367\n",
            "Epoch: 7 Batch Number: 74 Loss: 1.4573349952697754 Time taken: 0.20511627197265625\n",
            "Epoch: 7 Batch Number: 75 Loss: 1.4640843868255615 Time taken: 0.20012950897216797\n",
            "Epoch: 7 Batch Number: 76 Loss: 1.4374839067459106 Time taken: 0.20456504821777344\n",
            "Epoch: 7 Batch Number: 77 Loss: 1.4548732042312622 Time taken: 0.20271086692810059\n",
            "Epoch: 7 Batch Number: 78 Loss: 1.415602445602417 Time taken: 0.2058558464050293\n",
            "Epoch: 7 Batch Number: 79 Loss: 1.4037904739379883 Time taken: 0.20313191413879395\n",
            "Epoch: 7 Batch Number: 80 Loss: 1.4218446016311646 Time taken: 0.19710397720336914\n",
            "Epoch: 7 Batch Number: 81 Loss: 1.4217464923858643 Time taken: 0.20134902000427246\n",
            "Epoch: 7 Batch Number: 82 Loss: 1.38771390914917 Time taken: 0.2011547088623047\n",
            "Epoch: 7 Batch Number: 83 Loss: 1.41100013256073 Time taken: 0.20401668548583984\n",
            "Epoch: 7 Batch Number: 84 Loss: 1.3972753286361694 Time taken: 0.20698976516723633\n",
            "Epoch: 7 Batch Number: 85 Loss: 1.408939242362976 Time taken: 0.2158036231994629\n",
            "Epoch: 7 Batch Number: 86 Loss: 1.4004617929458618 Time taken: 0.2067403793334961\n",
            "Epoch: 7 Batch Number: 87 Loss: 1.415160059928894 Time taken: 0.19988441467285156\n",
            "Epoch: 7 Batch Number: 88 Loss: 1.4159321784973145 Time taken: 0.20389699935913086\n",
            "Epoch: 7 Batch Number: 89 Loss: 1.399579405784607 Time taken: 0.20024538040161133\n",
            "Epoch: 7 Batch Number: 90 Loss: 1.4204460382461548 Time taken: 0.19972896575927734\n",
            "Epoch: 7 Batch Number: 91 Loss: 1.4204165935516357 Time taken: 0.20571684837341309\n",
            "Epoch: 7 Batch Number: 92 Loss: 1.4391169548034668 Time taken: 0.20319080352783203\n",
            "Epoch: 7 Batch Number: 93 Loss: 1.4084800481796265 Time taken: 0.2055952548980713\n",
            "Epoch: 7 Batch Number: 94 Loss: 1.4342331886291504 Time taken: 0.20014452934265137\n",
            "Epoch: 7 Batch Number: 95 Loss: 1.4369655847549438 Time taken: 0.20493245124816895\n",
            "Epoch: 7 Batch Number: 96 Loss: 1.4417473077774048 Time taken: 0.2020564079284668\n",
            "Epoch: 7 Batch Number: 97 Loss: 1.4562801122665405 Time taken: 0.20043158531188965\n",
            "Epoch: 7 Batch Number: 98 Loss: 1.4493533372879028 Time taken: 0.21891546249389648\n",
            "Epoch: 7 Batch Number: 99 Loss: 1.4457674026489258 Time taken: 0.2013401985168457\n",
            "Epoch: 7 Batch Number: 100 Loss: 1.4377131462097168 Time taken: 0.20802760124206543\n",
            "Epoch: 7 Batch Number: 101 Loss: 1.4576631784439087 Time taken: 0.20273613929748535\n",
            "Epoch: 7 Batch Number: 102 Loss: 1.4635004997253418 Time taken: 0.20540165901184082\n",
            "Epoch: 7 Batch Number: 103 Loss: 1.4316270351409912 Time taken: 0.21044158935546875\n",
            "Epoch: 7 Batch Number: 104 Loss: 1.4507262706756592 Time taken: 0.2053050994873047\n",
            "Epoch: 7 Batch Number: 105 Loss: 1.457686424255371 Time taken: 0.20113396644592285\n",
            "Epoch: 7 Batch Number: 106 Loss: 1.4730417728424072 Time taken: 0.2002415657043457\n",
            "Epoch: 7 Batch Number: 107 Loss: 1.450458288192749 Time taken: 0.2012479305267334\n",
            "Epoch: 7 Batch Number: 108 Loss: 1.4636470079421997 Time taken: 0.20509552955627441\n",
            "Epoch: 7 Batch Number: 109 Loss: 1.4685337543487549 Time taken: 0.20123815536499023\n",
            "Epoch: 7 Batch Number: 110 Loss: 1.4547765254974365 Time taken: 0.1995539665222168\n",
            "Epoch: 7 Batch Number: 111 Loss: 1.4878959655761719 Time taken: 0.21012330055236816\n",
            "Epoch: 7 Batch Number: 112 Loss: 1.4502462148666382 Time taken: 0.20324397087097168\n",
            "Epoch: 7 Batch Number: 113 Loss: 1.455084204673767 Time taken: 0.2048664093017578\n",
            "Epoch: 7 Batch Number: 114 Loss: 1.4595717191696167 Time taken: 0.20757412910461426\n",
            "Epoch: 7 Batch Number: 115 Loss: 1.442750096321106 Time taken: 0.19978976249694824\n",
            "Epoch: 7 Batch Number: 116 Loss: 1.453694224357605 Time taken: 0.20087885856628418\n",
            "Epoch: 7 Batch Number: 117 Loss: 1.447724461555481 Time taken: 0.20502257347106934\n",
            "Epoch: 7 Batch Number: 118 Loss: 1.4348561763763428 Time taken: 0.21827054023742676\n",
            "Epoch: 7 Batch Number: 119 Loss: 1.4466972351074219 Time taken: 0.20283889770507812\n",
            "Epoch: 7 Batch Number: 120 Loss: 1.4242606163024902 Time taken: 0.20826363563537598\n",
            "Epoch: 7 Batch Number: 121 Loss: 1.4382020235061646 Time taken: 0.196821928024292\n",
            "Epoch: 7 Batch Number: 122 Loss: 1.4461820125579834 Time taken: 0.2062537670135498\n",
            "Epoch: 7 Batch Number: 123 Loss: 1.4237210750579834 Time taken: 0.2032783031463623\n",
            "Epoch: 7 Batch Number: 124 Loss: 1.4486794471740723 Time taken: 0.1991736888885498\n",
            "Epoch: 7 Batch Number: 125 Loss: 1.3967608213424683 Time taken: 0.19888019561767578\n",
            "Epoch: 7 Batch Number: 126 Loss: 1.447331428527832 Time taken: 0.2003927230834961\n",
            "Epoch: 7 Batch Number: 127 Loss: 1.4442154169082642 Time taken: 0.21002721786499023\n",
            "Epoch: 7 Batch Number: 128 Loss: 1.426019549369812 Time taken: 0.1999366283416748\n",
            "Epoch: 7 Batch Number: 129 Loss: 1.4113807678222656 Time taken: 0.1983187198638916\n",
            "Epoch: 7 Batch Number: 130 Loss: 1.42876398563385 Time taken: 0.200575590133667\n",
            "Epoch: 7 Batch Number: 131 Loss: 1.4332573413848877 Time taken: 0.20230937004089355\n",
            "Epoch: 7 Batch Number: 132 Loss: 1.4442706108093262 Time taken: 0.2186436653137207\n",
            "Epoch: 7 Batch Number: 133 Loss: 1.4280592203140259 Time taken: 0.20293259620666504\n",
            "Epoch: 7 Batch Number: 134 Loss: 1.452980875968933 Time taken: 0.19983696937561035\n",
            "Epoch: 7 Batch Number: 135 Loss: 1.442368745803833 Time taken: 0.2027890682220459\n",
            "Epoch: 7 Batch Number: 136 Loss: 1.3975353240966797 Time taken: 0.20275354385375977\n",
            "Epoch: 7 Batch Number: 137 Loss: 1.4082978963851929 Time taken: 0.20562291145324707\n",
            "Epoch: 7 Batch Number: 138 Loss: 1.4115345478057861 Time taken: 0.20103073120117188\n",
            "Epoch: 7 Batch Number: 139 Loss: 1.4289498329162598 Time taken: 0.201371431350708\n",
            "Epoch: 7 Batch Number: 140 Loss: 1.4551856517791748 Time taken: 0.20881962776184082\n",
            "Epoch: 7 Batch Number: 141 Loss: 1.4536843299865723 Time taken: 0.19936251640319824\n",
            "Epoch: 7 Batch Number: 142 Loss: 1.428479552268982 Time taken: 0.20681142807006836\n",
            "Epoch: 7 Batch Number: 143 Loss: 1.4600287675857544 Time taken: 0.20351314544677734\n",
            "Epoch: 7 Batch Number: 144 Loss: 1.4387160539627075 Time taken: 0.20102691650390625\n",
            "Epoch: 7 Batch Number: 145 Loss: 1.4718728065490723 Time taken: 0.2035515308380127\n",
            "Epoch: 7 Batch Number: 146 Loss: 1.4288601875305176 Time taken: 0.2021470069885254\n",
            "Epoch: 7 Batch Number: 147 Loss: 1.411480188369751 Time taken: 0.20292997360229492\n",
            "Epoch: 7 Batch Number: 148 Loss: 1.4363629817962646 Time taken: 0.20270657539367676\n",
            "Epoch: 7 Batch Number: 149 Loss: 1.4327702522277832 Time taken: 0.20465922355651855\n",
            "Epoch: 7 Batch Number: 150 Loss: 1.4231219291687012 Time taken: 0.19977140426635742\n",
            "Epoch: 7 Batch Number: 151 Loss: 1.4497840404510498 Time taken: 0.19911408424377441\n",
            "Epoch: 7 Batch Number: 152 Loss: 1.4341875314712524 Time taken: 0.20095133781433105\n",
            "Epoch: 7 Batch Number: 153 Loss: 1.4237070083618164 Time taken: 0.20215773582458496\n",
            "Epoch: 7 Batch Number: 154 Loss: 1.4286623001098633 Time taken: 0.20163226127624512\n",
            "Epoch: 7 Batch Number: 155 Loss: 1.4113726615905762 Time taken: 0.2017526626586914\n",
            "Epoch: 7 Batch Number: 156 Loss: 1.4556444883346558 Time taken: 0.20598959922790527\n",
            "Epoch: 7 Batch Number: 157 Loss: 1.420213222503662 Time taken: 0.20615887641906738\n",
            "Epoch: 7 Batch Number: 158 Loss: 1.4446932077407837 Time taken: 0.1994929313659668\n",
            "Epoch: 7 Batch Number: 159 Loss: 1.4467592239379883 Time taken: 0.19794297218322754\n",
            "Epoch: 7 Batch Number: 160 Loss: 1.4205232858657837 Time taken: 0.19910430908203125\n",
            "Epoch: 7 Batch Number: 161 Loss: 1.4161591529846191 Time taken: 0.20392441749572754\n",
            "Epoch: 7 Batch Number: 162 Loss: 1.4189453125 Time taken: 0.20049715042114258\n",
            "Epoch: 7 Batch Number: 163 Loss: 1.4187419414520264 Time taken: 0.20373058319091797\n",
            "Epoch: 7 Batch Number: 164 Loss: 1.4092246294021606 Time taken: 0.20135712623596191\n",
            "Epoch: 7 Batch Number: 165 Loss: 1.4164422750473022 Time taken: 0.19988155364990234\n",
            "Epoch: 7 Batch Number: 166 Loss: 1.4234634637832642 Time taken: 0.207366943359375\n",
            "Epoch: 7 Batch Number: 167 Loss: 1.4098495244979858 Time taken: 0.20119714736938477\n",
            "Epoch: 7 Batch Number: 168 Loss: 1.4313381910324097 Time taken: 0.21674251556396484\n",
            "Epoch: 7 Batch Number: 169 Loss: 1.4148815870285034 Time taken: 0.20111823081970215\n",
            "Epoch: 7 Batch Number: 170 Loss: 1.4102071523666382 Time taken: 0.20094776153564453\n",
            "Epoch: 7 Batch Number: 171 Loss: 1.4095908403396606 Time taken: 0.21168279647827148\n",
            "Epoch: 7 Batch Number: 172 Loss: 1.3883641958236694 Time taken: 0.20090699195861816\n",
            "Epoch: 7 Batch Number: 173 Loss: 1.4054961204528809 Time taken: 0.19899344444274902\n",
            "Epoch: 7 Batch Number: 174 Loss: 1.4126088619232178 Time taken: 0.20427155494689941\n",
            "Epoch: 7 Batch Number: 175 Loss: 1.4206819534301758 Time taken: 0.19843029975891113\n",
            "Epoch: 7 Batch Number: 176 Loss: 1.4211057424545288 Time taken: 0.21114516258239746\n",
            "Epoch: 7 Batch Number: 177 Loss: 1.4136990308761597 Time taken: 0.20528912544250488\n",
            "Epoch: 7 Batch Number: 178 Loss: 1.3911470174789429 Time taken: 0.20542383193969727\n",
            "Epoch: 7 Batch Number: 179 Loss: 1.4080638885498047 Time taken: 0.20113420486450195\n",
            "==========================================================================================\n",
            "Start of epoch 8\n",
            "Epoch: 8 Batch Number: 1 Loss: 1.4064500331878662 Time taken: 0.20611095428466797\n",
            "Epoch: 8 Batch Number: 2 Loss: 1.3840408325195312 Time taken: 0.20700454711914062\n",
            "Epoch: 8 Batch Number: 3 Loss: 1.3774912357330322 Time taken: 0.20022940635681152\n",
            "Epoch: 8 Batch Number: 4 Loss: 1.3690614700317383 Time taken: 0.20801424980163574\n",
            "Epoch: 8 Batch Number: 5 Loss: 1.3560748100280762 Time taken: 0.20227265357971191\n",
            "Epoch: 8 Batch Number: 6 Loss: 1.37339186668396 Time taken: 0.212446928024292\n",
            "Epoch: 8 Batch Number: 7 Loss: 1.3802536725997925 Time taken: 0.19835472106933594\n",
            "Epoch: 8 Batch Number: 8 Loss: 1.382573127746582 Time taken: 0.20233821868896484\n",
            "Epoch: 8 Batch Number: 9 Loss: 1.3665305376052856 Time taken: 0.2025737762451172\n",
            "Epoch: 8 Batch Number: 10 Loss: 1.3928529024124146 Time taken: 0.20110583305358887\n",
            "Epoch: 8 Batch Number: 11 Loss: 1.36928391456604 Time taken: 0.20902371406555176\n",
            "Epoch: 8 Batch Number: 12 Loss: 1.3765361309051514 Time taken: 0.19851994514465332\n",
            "Epoch: 8 Batch Number: 13 Loss: 1.3941656351089478 Time taken: 0.2010660171508789\n",
            "Epoch: 8 Batch Number: 14 Loss: 1.4281086921691895 Time taken: 0.20167303085327148\n",
            "Epoch: 8 Batch Number: 15 Loss: 1.404349684715271 Time taken: 0.20551013946533203\n",
            "Epoch: 8 Batch Number: 16 Loss: 1.3782891035079956 Time taken: 0.2017507553100586\n",
            "Epoch: 8 Batch Number: 17 Loss: 1.3796519041061401 Time taken: 0.20357298851013184\n",
            "Epoch: 8 Batch Number: 18 Loss: 1.4010647535324097 Time taken: 0.20354509353637695\n",
            "Epoch: 8 Batch Number: 19 Loss: 1.3889975547790527 Time taken: 0.20255208015441895\n",
            "Epoch: 8 Batch Number: 20 Loss: 1.387563943862915 Time taken: 0.19977879524230957\n",
            "Epoch: 8 Batch Number: 21 Loss: 1.3777387142181396 Time taken: 0.20565414428710938\n",
            "Epoch: 8 Batch Number: 22 Loss: 1.3789564371109009 Time taken: 0.200714111328125\n",
            "Epoch: 8 Batch Number: 23 Loss: 1.3730186223983765 Time taken: 0.2031700611114502\n",
            "Epoch: 8 Batch Number: 24 Loss: 1.3959189653396606 Time taken: 0.20293498039245605\n",
            "Epoch: 8 Batch Number: 25 Loss: 1.4187893867492676 Time taken: 0.20230937004089355\n",
            "Epoch: 8 Batch Number: 26 Loss: 1.4232594966888428 Time taken: 0.20247554779052734\n",
            "Epoch: 8 Batch Number: 27 Loss: 1.3998359441757202 Time taken: 0.2022993564605713\n",
            "Epoch: 8 Batch Number: 28 Loss: 1.4087570905685425 Time taken: 0.1979231834411621\n",
            "Epoch: 8 Batch Number: 29 Loss: 1.3884893655776978 Time taken: 0.1982712745666504\n",
            "Epoch: 8 Batch Number: 30 Loss: 1.4031411409378052 Time taken: 0.20462727546691895\n",
            "Epoch: 8 Batch Number: 31 Loss: 1.3860900402069092 Time taken: 0.2076704502105713\n",
            "Epoch: 8 Batch Number: 32 Loss: 1.3850750923156738 Time taken: 0.20264911651611328\n",
            "Epoch: 8 Batch Number: 33 Loss: 1.3888731002807617 Time taken: 0.20303773880004883\n",
            "Epoch: 8 Batch Number: 34 Loss: 1.3916407823562622 Time taken: 0.20081853866577148\n",
            "Epoch: 8 Batch Number: 35 Loss: 1.3997620344161987 Time taken: 0.19945621490478516\n",
            "Epoch: 8 Batch Number: 36 Loss: 1.4029541015625 Time taken: 0.1993885040283203\n",
            "Epoch: 8 Batch Number: 37 Loss: 1.3902294635772705 Time taken: 0.1992027759552002\n",
            "Epoch: 8 Batch Number: 38 Loss: 1.4501152038574219 Time taken: 0.2016160488128662\n",
            "Epoch: 8 Batch Number: 39 Loss: 1.409734845161438 Time taken: 0.21525049209594727\n",
            "Epoch: 8 Batch Number: 40 Loss: 1.3957796096801758 Time taken: 0.2057664394378662\n",
            "Epoch: 8 Batch Number: 41 Loss: 1.4106714725494385 Time taken: 0.2026057243347168\n",
            "Epoch: 8 Batch Number: 42 Loss: 1.406657338142395 Time taken: 0.20500874519348145\n",
            "Epoch: 8 Batch Number: 43 Loss: 1.387982726097107 Time taken: 0.20064187049865723\n",
            "Epoch: 8 Batch Number: 44 Loss: 1.392182469367981 Time taken: 0.20121026039123535\n",
            "Epoch: 8 Batch Number: 45 Loss: 1.383618950843811 Time taken: 0.20428085327148438\n",
            "Epoch: 8 Batch Number: 46 Loss: 1.4000794887542725 Time taken: 0.20073413848876953\n",
            "Epoch: 8 Batch Number: 47 Loss: 1.3975679874420166 Time taken: 0.20370030403137207\n",
            "Epoch: 8 Batch Number: 48 Loss: 1.3879897594451904 Time taken: 0.20262837409973145\n",
            "Epoch: 8 Batch Number: 49 Loss: 1.3900508880615234 Time taken: 0.20538663864135742\n",
            "Epoch: 8 Batch Number: 50 Loss: 1.3877520561218262 Time taken: 0.20395493507385254\n",
            "Epoch: 8 Batch Number: 51 Loss: 1.3798432350158691 Time taken: 0.20924639701843262\n",
            "Epoch: 8 Batch Number: 52 Loss: 1.3925368785858154 Time taken: 0.20499420166015625\n",
            "Epoch: 8 Batch Number: 53 Loss: 1.4021265506744385 Time taken: 0.20304465293884277\n",
            "Epoch: 8 Batch Number: 54 Loss: 1.4329689741134644 Time taken: 0.2002115249633789\n",
            "Epoch: 8 Batch Number: 55 Loss: 1.3887498378753662 Time taken: 0.1996915340423584\n",
            "Epoch: 8 Batch Number: 56 Loss: 1.4041475057601929 Time taken: 0.2006075382232666\n",
            "Epoch: 8 Batch Number: 57 Loss: 1.3952609300613403 Time taken: 0.20244574546813965\n",
            "Epoch: 8 Batch Number: 58 Loss: 1.388786792755127 Time taken: 0.20097136497497559\n",
            "Epoch: 8 Batch Number: 59 Loss: 1.3806688785552979 Time taken: 0.20246195793151855\n",
            "Epoch: 8 Batch Number: 60 Loss: 1.3696562051773071 Time taken: 0.20600295066833496\n",
            "Epoch: 8 Batch Number: 61 Loss: 1.3775711059570312 Time taken: 0.2031114101409912\n",
            "Epoch: 8 Batch Number: 62 Loss: 1.4059003591537476 Time taken: 0.20343422889709473\n",
            "Epoch: 8 Batch Number: 63 Loss: 1.3803671598434448 Time taken: 0.19980359077453613\n",
            "Epoch: 8 Batch Number: 64 Loss: 1.3768935203552246 Time taken: 0.2002866268157959\n",
            "Epoch: 8 Batch Number: 65 Loss: 1.3874843120574951 Time taken: 0.21010160446166992\n",
            "Epoch: 8 Batch Number: 66 Loss: 1.4025920629501343 Time taken: 0.20077013969421387\n",
            "Epoch: 8 Batch Number: 67 Loss: 1.4057985544204712 Time taken: 0.19859766960144043\n",
            "Epoch: 8 Batch Number: 68 Loss: 1.419898509979248 Time taken: 0.20014619827270508\n",
            "Epoch: 8 Batch Number: 69 Loss: 1.3739687204360962 Time taken: 0.2031998634338379\n",
            "Epoch: 8 Batch Number: 70 Loss: 1.386176586151123 Time taken: 0.20836186408996582\n",
            "Epoch: 8 Batch Number: 71 Loss: 1.4236421585083008 Time taken: 0.20244073867797852\n",
            "Epoch: 8 Batch Number: 72 Loss: 1.3874790668487549 Time taken: 0.199859619140625\n",
            "Epoch: 8 Batch Number: 73 Loss: 1.3986480236053467 Time taken: 0.20186901092529297\n",
            "Epoch: 8 Batch Number: 74 Loss: 1.4059913158416748 Time taken: 0.20986366271972656\n",
            "Epoch: 8 Batch Number: 75 Loss: 1.3973312377929688 Time taken: 0.20149803161621094\n",
            "Epoch: 8 Batch Number: 76 Loss: 1.3865019083023071 Time taken: 0.19886016845703125\n",
            "Epoch: 8 Batch Number: 77 Loss: 1.4019827842712402 Time taken: 0.199676513671875\n",
            "Epoch: 8 Batch Number: 78 Loss: 1.3783340454101562 Time taken: 0.2008984088897705\n",
            "Epoch: 8 Batch Number: 79 Loss: 1.3771164417266846 Time taken: 0.19881105422973633\n",
            "Epoch: 8 Batch Number: 80 Loss: 1.3657011985778809 Time taken: 0.19973349571228027\n",
            "Epoch: 8 Batch Number: 81 Loss: 1.3983250856399536 Time taken: 0.20029282569885254\n",
            "Epoch: 8 Batch Number: 82 Loss: 1.3736281394958496 Time taken: 0.20165491104125977\n",
            "Epoch: 8 Batch Number: 83 Loss: 1.3825808763504028 Time taken: 0.20505571365356445\n",
            "Epoch: 8 Batch Number: 84 Loss: 1.3636168241500854 Time taken: 0.19823360443115234\n",
            "Epoch: 8 Batch Number: 85 Loss: 1.3762962818145752 Time taken: 0.20583343505859375\n",
            "Epoch: 8 Batch Number: 86 Loss: 1.3890502452850342 Time taken: 0.20795536041259766\n",
            "Epoch: 8 Batch Number: 87 Loss: 1.3781566619873047 Time taken: 0.1985316276550293\n",
            "Epoch: 8 Batch Number: 88 Loss: 1.371550440788269 Time taken: 0.1996138095855713\n",
            "Epoch: 8 Batch Number: 89 Loss: 1.3619983196258545 Time taken: 0.2003941535949707\n",
            "Epoch: 8 Batch Number: 90 Loss: 1.3726719617843628 Time taken: 0.20044445991516113\n",
            "Epoch: 8 Batch Number: 91 Loss: 1.415027379989624 Time taken: 0.20203900337219238\n",
            "Epoch: 8 Batch Number: 92 Loss: 1.4126253128051758 Time taken: 0.2088625431060791\n",
            "Epoch: 8 Batch Number: 93 Loss: 1.3965561389923096 Time taken: 0.2000899314880371\n",
            "Epoch: 8 Batch Number: 94 Loss: 1.3972581624984741 Time taken: 0.2062532901763916\n",
            "Epoch: 8 Batch Number: 95 Loss: 1.3994193077087402 Time taken: 0.2038283348083496\n",
            "Epoch: 8 Batch Number: 96 Loss: 1.4239915609359741 Time taken: 0.20171761512756348\n",
            "Epoch: 8 Batch Number: 97 Loss: 1.4193828105926514 Time taken: 0.20090460777282715\n",
            "Epoch: 8 Batch Number: 98 Loss: 1.4424852132797241 Time taken: 0.2012772560119629\n",
            "Epoch: 8 Batch Number: 99 Loss: 1.3933703899383545 Time taken: 0.20081758499145508\n",
            "Epoch: 8 Batch Number: 100 Loss: 1.37577223777771 Time taken: 0.20097923278808594\n",
            "Epoch: 8 Batch Number: 101 Loss: 1.389044165611267 Time taken: 0.19493961334228516\n",
            "Epoch: 8 Batch Number: 102 Loss: 1.4216173887252808 Time taken: 0.1976149082183838\n",
            "Epoch: 8 Batch Number: 103 Loss: 1.4190049171447754 Time taken: 0.20185422897338867\n",
            "Epoch: 8 Batch Number: 104 Loss: 1.4045026302337646 Time taken: 0.2061612606048584\n",
            "Epoch: 8 Batch Number: 105 Loss: 1.4076526165008545 Time taken: 0.2010343074798584\n",
            "Epoch: 8 Batch Number: 106 Loss: 1.4193772077560425 Time taken: 0.20074987411499023\n",
            "Epoch: 8 Batch Number: 107 Loss: 1.4304306507110596 Time taken: 0.20131969451904297\n",
            "Epoch: 8 Batch Number: 108 Loss: 1.416490912437439 Time taken: 0.20533084869384766\n",
            "Epoch: 8 Batch Number: 109 Loss: 1.4319770336151123 Time taken: 0.2091813087463379\n",
            "Epoch: 8 Batch Number: 110 Loss: 1.4547367095947266 Time taken: 0.19936490058898926\n",
            "Epoch: 8 Batch Number: 111 Loss: 1.4460866451263428 Time taken: 0.20186686515808105\n",
            "Epoch: 8 Batch Number: 112 Loss: 1.448675513267517 Time taken: 0.20554232597351074\n",
            "Epoch: 8 Batch Number: 113 Loss: 1.443894624710083 Time taken: 0.2001805305480957\n",
            "Epoch: 8 Batch Number: 114 Loss: 1.4229998588562012 Time taken: 0.21002674102783203\n",
            "Epoch: 8 Batch Number: 115 Loss: 1.434165358543396 Time taken: 0.20245003700256348\n",
            "Epoch: 8 Batch Number: 116 Loss: 1.4256055355072021 Time taken: 0.20484018325805664\n",
            "Epoch: 8 Batch Number: 117 Loss: 1.3851903676986694 Time taken: 0.20458626747131348\n",
            "Epoch: 8 Batch Number: 118 Loss: 1.3920241594314575 Time taken: 0.20580792427062988\n",
            "Epoch: 8 Batch Number: 119 Loss: 1.4252822399139404 Time taken: 0.20151996612548828\n",
            "Epoch: 8 Batch Number: 120 Loss: 1.3694411516189575 Time taken: 0.20171618461608887\n",
            "Epoch: 8 Batch Number: 121 Loss: 1.4150056838989258 Time taken: 0.20220232009887695\n",
            "Epoch: 8 Batch Number: 122 Loss: 1.3881654739379883 Time taken: 0.20261335372924805\n",
            "Epoch: 8 Batch Number: 123 Loss: 1.4173814058303833 Time taken: 0.1997087001800537\n",
            "Epoch: 8 Batch Number: 124 Loss: 1.4026950597763062 Time taken: 0.2004101276397705\n",
            "Epoch: 8 Batch Number: 125 Loss: 1.3812534809112549 Time taken: 0.20320367813110352\n",
            "Epoch: 8 Batch Number: 126 Loss: 1.394321322441101 Time taken: 0.20151090621948242\n",
            "Epoch: 8 Batch Number: 127 Loss: 1.3965803384780884 Time taken: 0.20367193222045898\n",
            "Epoch: 8 Batch Number: 128 Loss: 1.403424620628357 Time taken: 0.21181178092956543\n",
            "Epoch: 8 Batch Number: 129 Loss: 1.3963472843170166 Time taken: 0.19951272010803223\n",
            "Epoch: 8 Batch Number: 130 Loss: 1.3782962560653687 Time taken: 0.20433330535888672\n",
            "Epoch: 8 Batch Number: 131 Loss: 1.3988702297210693 Time taken: 0.19989466667175293\n",
            "Epoch: 8 Batch Number: 132 Loss: 1.3939874172210693 Time taken: 0.20051193237304688\n",
            "Epoch: 8 Batch Number: 133 Loss: 1.4081687927246094 Time taken: 0.20055603981018066\n",
            "Epoch: 8 Batch Number: 134 Loss: 1.4083659648895264 Time taken: 0.20211529731750488\n",
            "Epoch: 8 Batch Number: 135 Loss: 1.383928894996643 Time taken: 0.20143747329711914\n",
            "Epoch: 8 Batch Number: 136 Loss: 1.383721113204956 Time taken: 0.20072031021118164\n",
            "Epoch: 8 Batch Number: 137 Loss: 1.3671773672103882 Time taken: 0.20308613777160645\n",
            "Epoch: 8 Batch Number: 138 Loss: 1.3856360912322998 Time taken: 0.20120930671691895\n",
            "Epoch: 8 Batch Number: 139 Loss: 1.3918123245239258 Time taken: 0.20016741752624512\n",
            "Epoch: 8 Batch Number: 140 Loss: 1.4043290615081787 Time taken: 0.2061920166015625\n",
            "Epoch: 8 Batch Number: 141 Loss: 1.3712210655212402 Time taken: 0.20158934593200684\n",
            "Epoch: 8 Batch Number: 142 Loss: 1.4443949460983276 Time taken: 0.20939111709594727\n",
            "Epoch: 8 Batch Number: 143 Loss: 1.4245188236236572 Time taken: 0.20127177238464355\n",
            "Epoch: 8 Batch Number: 144 Loss: 1.3985799551010132 Time taken: 0.1999218463897705\n",
            "Epoch: 8 Batch Number: 145 Loss: 1.456470012664795 Time taken: 0.20184612274169922\n",
            "Epoch: 8 Batch Number: 146 Loss: 1.3820880651474 Time taken: 0.20100975036621094\n",
            "Epoch: 8 Batch Number: 147 Loss: 1.4098832607269287 Time taken: 0.20368289947509766\n",
            "Epoch: 8 Batch Number: 148 Loss: 1.3998842239379883 Time taken: 0.20354127883911133\n",
            "Epoch: 8 Batch Number: 149 Loss: 1.3920073509216309 Time taken: 0.20206475257873535\n",
            "Epoch: 8 Batch Number: 150 Loss: 1.3923366069793701 Time taken: 0.20287370681762695\n",
            "Epoch: 8 Batch Number: 151 Loss: 1.3791221380233765 Time taken: 0.2045276165008545\n",
            "Epoch: 8 Batch Number: 152 Loss: 1.3842862844467163 Time taken: 0.20126771926879883\n",
            "Epoch: 8 Batch Number: 153 Loss: 1.3734638690948486 Time taken: 0.21074843406677246\n",
            "Epoch: 8 Batch Number: 154 Loss: 1.397436499595642 Time taken: 0.202653169631958\n",
            "Epoch: 8 Batch Number: 155 Loss: 1.3841434717178345 Time taken: 0.19737029075622559\n",
            "Epoch: 8 Batch Number: 156 Loss: 1.3988534212112427 Time taken: 0.20148205757141113\n",
            "Epoch: 8 Batch Number: 157 Loss: 1.4036486148834229 Time taken: 0.20538091659545898\n",
            "Epoch: 8 Batch Number: 158 Loss: 1.4076275825500488 Time taken: 0.210158109664917\n",
            "Epoch: 8 Batch Number: 159 Loss: 1.3910458087921143 Time taken: 0.20124101638793945\n",
            "Epoch: 8 Batch Number: 160 Loss: 1.395471215248108 Time taken: 0.20236921310424805\n",
            "Epoch: 8 Batch Number: 161 Loss: 1.4005897045135498 Time taken: 0.1986095905303955\n",
            "Epoch: 8 Batch Number: 162 Loss: 1.4117456674575806 Time taken: 0.19960427284240723\n",
            "Epoch: 8 Batch Number: 163 Loss: 1.3956447839736938 Time taken: 0.20215749740600586\n",
            "Epoch: 8 Batch Number: 164 Loss: 1.408313512802124 Time taken: 0.20097613334655762\n",
            "Epoch: 8 Batch Number: 165 Loss: 1.3960347175598145 Time taken: 0.2019178867340088\n",
            "Epoch: 8 Batch Number: 166 Loss: 1.388839602470398 Time taken: 0.2067413330078125\n",
            "Epoch: 8 Batch Number: 167 Loss: 1.3854944705963135 Time taken: 0.20156097412109375\n",
            "Epoch: 8 Batch Number: 168 Loss: 1.3976455926895142 Time taken: 0.19977760314941406\n",
            "Epoch: 8 Batch Number: 169 Loss: 1.406791090965271 Time taken: 0.20009064674377441\n",
            "Epoch: 8 Batch Number: 170 Loss: 1.3772202730178833 Time taken: 0.203627347946167\n",
            "Epoch: 8 Batch Number: 171 Loss: 1.3787128925323486 Time taken: 0.21172142028808594\n",
            "Epoch: 8 Batch Number: 172 Loss: 1.3821576833724976 Time taken: 0.2092905044555664\n",
            "Epoch: 8 Batch Number: 173 Loss: 1.3796439170837402 Time taken: 0.20231914520263672\n",
            "Epoch: 8 Batch Number: 174 Loss: 1.399530291557312 Time taken: 0.20004653930664062\n",
            "Epoch: 8 Batch Number: 175 Loss: 1.3769583702087402 Time taken: 0.2073802947998047\n",
            "Epoch: 8 Batch Number: 176 Loss: 1.3658536672592163 Time taken: 0.2046985626220703\n",
            "Epoch: 8 Batch Number: 177 Loss: 1.364798903465271 Time taken: 0.20542240142822266\n",
            "Epoch: 8 Batch Number: 178 Loss: 1.3722270727157593 Time taken: 0.20315074920654297\n",
            "Epoch: 8 Batch Number: 179 Loss: 1.361358642578125 Time taken: 0.20347213745117188\n",
            "==========================================================================================\n",
            "Start of epoch 9\n",
            "Epoch: 9 Batch Number: 1 Loss: 1.3680880069732666 Time taken: 0.20243382453918457\n",
            "Epoch: 9 Batch Number: 2 Loss: 1.3599553108215332 Time taken: 0.20259571075439453\n",
            "Epoch: 9 Batch Number: 3 Loss: 1.361382007598877 Time taken: 0.2088794708251953\n",
            "Epoch: 9 Batch Number: 4 Loss: 1.3576520681381226 Time taken: 0.20638084411621094\n",
            "Epoch: 9 Batch Number: 5 Loss: 1.3202320337295532 Time taken: 0.19961142539978027\n",
            "Epoch: 9 Batch Number: 6 Loss: 1.3356966972351074 Time taken: 0.20235705375671387\n",
            "Epoch: 9 Batch Number: 7 Loss: 1.3382607698440552 Time taken: 0.1996610164642334\n",
            "Epoch: 9 Batch Number: 8 Loss: 1.3567825555801392 Time taken: 0.21477198600769043\n",
            "Epoch: 9 Batch Number: 9 Loss: 1.3443591594696045 Time taken: 0.2014627456665039\n",
            "Epoch: 9 Batch Number: 10 Loss: 1.3300588130950928 Time taken: 0.19858551025390625\n",
            "Epoch: 9 Batch Number: 11 Loss: 1.344016432762146 Time taken: 0.20410370826721191\n",
            "Epoch: 9 Batch Number: 12 Loss: 1.3760162591934204 Time taken: 0.20550799369812012\n",
            "Epoch: 9 Batch Number: 13 Loss: 1.3625104427337646 Time taken: 0.20612812042236328\n",
            "Epoch: 9 Batch Number: 14 Loss: 1.3913888931274414 Time taken: 0.20257782936096191\n",
            "Epoch: 9 Batch Number: 15 Loss: 1.3680671453475952 Time taken: 0.20166444778442383\n",
            "Epoch: 9 Batch Number: 16 Loss: 1.3718924522399902 Time taken: 0.20571541786193848\n",
            "Epoch: 9 Batch Number: 17 Loss: 1.3726246356964111 Time taken: 0.19857072830200195\n",
            "Epoch: 9 Batch Number: 18 Loss: 1.3493181467056274 Time taken: 0.19902944564819336\n",
            "Epoch: 9 Batch Number: 19 Loss: 1.3276557922363281 Time taken: 0.20415735244750977\n",
            "Epoch: 9 Batch Number: 20 Loss: 1.3491249084472656 Time taken: 0.20421934127807617\n",
            "Epoch: 9 Batch Number: 21 Loss: 1.351030945777893 Time taken: 0.20581293106079102\n",
            "Epoch: 9 Batch Number: 22 Loss: 1.3818809986114502 Time taken: 0.2054610252380371\n",
            "Epoch: 9 Batch Number: 23 Loss: 1.3734210729599 Time taken: 0.20077109336853027\n",
            "Epoch: 9 Batch Number: 24 Loss: 1.3958595991134644 Time taken: 0.19947576522827148\n",
            "Epoch: 9 Batch Number: 25 Loss: 1.3778513669967651 Time taken: 0.2052304744720459\n",
            "Epoch: 9 Batch Number: 26 Loss: 1.353201985359192 Time taken: 0.20024776458740234\n",
            "Epoch: 9 Batch Number: 27 Loss: 1.3891535997390747 Time taken: 0.21344566345214844\n",
            "Epoch: 9 Batch Number: 28 Loss: 1.3688805103302002 Time taken: 0.20112156867980957\n",
            "Epoch: 9 Batch Number: 29 Loss: 1.372691035270691 Time taken: 0.20064926147460938\n",
            "Epoch: 9 Batch Number: 30 Loss: 1.3692893981933594 Time taken: 0.20215749740600586\n",
            "Epoch: 9 Batch Number: 31 Loss: 1.3483285903930664 Time taken: 0.20007872581481934\n",
            "Epoch: 9 Batch Number: 32 Loss: 1.3487972021102905 Time taken: 0.20249319076538086\n",
            "Epoch: 9 Batch Number: 33 Loss: 1.359075903892517 Time taken: 0.2000865936279297\n",
            "Epoch: 9 Batch Number: 34 Loss: 1.3745267391204834 Time taken: 0.20084428787231445\n",
            "Epoch: 9 Batch Number: 35 Loss: 1.4049090147018433 Time taken: 0.20257115364074707\n",
            "Epoch: 9 Batch Number: 36 Loss: 1.3653671741485596 Time taken: 0.20451641082763672\n",
            "Epoch: 9 Batch Number: 37 Loss: 1.3640719652175903 Time taken: 0.2026817798614502\n",
            "Epoch: 9 Batch Number: 38 Loss: 1.3795865774154663 Time taken: 0.2027113437652588\n",
            "Epoch: 9 Batch Number: 39 Loss: 1.373237133026123 Time taken: 0.20354866981506348\n",
            "Epoch: 9 Batch Number: 40 Loss: 1.3546816110610962 Time taken: 0.21575450897216797\n",
            "Epoch: 9 Batch Number: 41 Loss: 1.3869569301605225 Time taken: 0.2004380226135254\n",
            "Epoch: 9 Batch Number: 42 Loss: 1.366832971572876 Time taken: 0.2061450481414795\n",
            "Epoch: 9 Batch Number: 43 Loss: 1.3499221801757812 Time taken: 0.20067858695983887\n",
            "Epoch: 9 Batch Number: 44 Loss: 1.3377689123153687 Time taken: 0.20492839813232422\n",
            "Epoch: 9 Batch Number: 45 Loss: 1.3802186250686646 Time taken: 0.2023906707763672\n",
            "Epoch: 9 Batch Number: 46 Loss: 1.3635832071304321 Time taken: 0.20432806015014648\n",
            "Epoch: 9 Batch Number: 47 Loss: 1.3597984313964844 Time taken: 0.20608115196228027\n",
            "Epoch: 9 Batch Number: 48 Loss: 1.3772989511489868 Time taken: 0.20103716850280762\n",
            "Epoch: 9 Batch Number: 49 Loss: 1.3563876152038574 Time taken: 0.20506024360656738\n",
            "Epoch: 9 Batch Number: 50 Loss: 1.388382911682129 Time taken: 0.19903206825256348\n",
            "Epoch: 9 Batch Number: 51 Loss: 1.3665574789047241 Time taken: 0.2107686996459961\n",
            "Epoch: 9 Batch Number: 52 Loss: 1.3749384880065918 Time taken: 0.1987166404724121\n",
            "Epoch: 9 Batch Number: 53 Loss: 1.4022216796875 Time taken: 0.20125269889831543\n",
            "Epoch: 9 Batch Number: 54 Loss: 1.3668973445892334 Time taken: 0.19937467575073242\n",
            "Epoch: 9 Batch Number: 55 Loss: 1.337355375289917 Time taken: 0.1992511749267578\n",
            "Epoch: 9 Batch Number: 56 Loss: 1.3595921993255615 Time taken: 0.20039772987365723\n",
            "Epoch: 9 Batch Number: 57 Loss: 1.384686827659607 Time taken: 0.19969964027404785\n",
            "Epoch: 9 Batch Number: 58 Loss: 1.3781934976577759 Time taken: 0.2035541534423828\n",
            "Epoch: 9 Batch Number: 59 Loss: 1.3582282066345215 Time taken: 0.20453953742980957\n",
            "Epoch: 9 Batch Number: 60 Loss: 1.363014578819275 Time taken: 0.19833159446716309\n",
            "Epoch: 9 Batch Number: 61 Loss: 1.3589236736297607 Time taken: 0.21183443069458008\n",
            "Epoch: 9 Batch Number: 62 Loss: 1.3912895917892456 Time taken: 0.20048975944519043\n",
            "Epoch: 9 Batch Number: 63 Loss: 1.344101905822754 Time taken: 0.20372939109802246\n",
            "Epoch: 9 Batch Number: 64 Loss: 1.3382459878921509 Time taken: 0.20012211799621582\n",
            "Epoch: 9 Batch Number: 65 Loss: 1.3606027364730835 Time taken: 0.1993083953857422\n",
            "Epoch: 9 Batch Number: 66 Loss: 1.370283603668213 Time taken: 0.20123052597045898\n",
            "Epoch: 9 Batch Number: 67 Loss: 1.3719491958618164 Time taken: 0.2053225040435791\n",
            "Epoch: 9 Batch Number: 68 Loss: 1.3645042181015015 Time taken: 0.19908642768859863\n",
            "Epoch: 9 Batch Number: 69 Loss: 1.3753689527511597 Time taken: 0.21068382263183594\n",
            "Epoch: 9 Batch Number: 70 Loss: 1.3423666954040527 Time taken: 0.19664359092712402\n",
            "Epoch: 9 Batch Number: 71 Loss: 1.362813115119934 Time taken: 0.2034742832183838\n",
            "Epoch: 9 Batch Number: 72 Loss: 1.3807612657546997 Time taken: 0.20093894004821777\n",
            "Epoch: 9 Batch Number: 73 Loss: 1.3802318572998047 Time taken: 0.20128583908081055\n",
            "Epoch: 9 Batch Number: 74 Loss: 1.374174952507019 Time taken: 0.20682930946350098\n",
            "Epoch: 9 Batch Number: 75 Loss: 1.3653112649917603 Time taken: 0.20529389381408691\n",
            "Epoch: 9 Batch Number: 76 Loss: 1.3750989437103271 Time taken: 0.2044391632080078\n",
            "Epoch: 9 Batch Number: 77 Loss: 1.384732961654663 Time taken: 0.20207786560058594\n",
            "Epoch: 9 Batch Number: 78 Loss: 1.365004301071167 Time taken: 0.2064220905303955\n",
            "Epoch: 9 Batch Number: 79 Loss: 1.3713197708129883 Time taken: 0.1992943286895752\n",
            "Epoch: 9 Batch Number: 80 Loss: 1.343572974205017 Time taken: 0.1978459358215332\n",
            "Epoch: 9 Batch Number: 81 Loss: 1.3417303562164307 Time taken: 0.19920682907104492\n",
            "Epoch: 9 Batch Number: 82 Loss: 1.353263258934021 Time taken: 0.1985926628112793\n",
            "Epoch: 9 Batch Number: 83 Loss: 1.356616735458374 Time taken: 0.2031543254852295\n",
            "Epoch: 9 Batch Number: 84 Loss: 1.3618943691253662 Time taken: 0.19870638847351074\n",
            "Epoch: 9 Batch Number: 85 Loss: 1.3536717891693115 Time taken: 0.20071983337402344\n",
            "Epoch: 9 Batch Number: 86 Loss: 1.365805745124817 Time taken: 0.20670223236083984\n",
            "Epoch: 9 Batch Number: 87 Loss: 1.332784652709961 Time taken: 0.20120859146118164\n",
            "Epoch: 9 Batch Number: 88 Loss: 1.3395609855651855 Time taken: 0.20819830894470215\n",
            "Epoch: 9 Batch Number: 89 Loss: 1.3591382503509521 Time taken: 0.20032644271850586\n",
            "Epoch: 9 Batch Number: 90 Loss: 1.3484177589416504 Time taken: 0.198136568069458\n",
            "Epoch: 9 Batch Number: 91 Loss: 1.3326705694198608 Time taken: 0.20645809173583984\n",
            "Epoch: 9 Batch Number: 92 Loss: 1.36527681350708 Time taken: 0.2012312412261963\n",
            "Epoch: 9 Batch Number: 93 Loss: 1.3879890441894531 Time taken: 0.20572781562805176\n",
            "Epoch: 9 Batch Number: 94 Loss: 1.3659977912902832 Time taken: 0.20291900634765625\n",
            "Epoch: 9 Batch Number: 95 Loss: 1.382893681526184 Time taken: 0.19727683067321777\n",
            "Epoch: 9 Batch Number: 96 Loss: 1.3870831727981567 Time taken: 0.20021915435791016\n",
            "Epoch: 9 Batch Number: 97 Loss: 1.3941905498504639 Time taken: 0.20174646377563477\n",
            "Epoch: 9 Batch Number: 98 Loss: 1.387776255607605 Time taken: 0.20049571990966797\n",
            "Epoch: 9 Batch Number: 99 Loss: 1.3576089143753052 Time taken: 0.20487022399902344\n",
            "Epoch: 9 Batch Number: 100 Loss: 1.38057541847229 Time taken: 0.20404434204101562\n",
            "Epoch: 9 Batch Number: 101 Loss: 1.3750441074371338 Time taken: 0.20142722129821777\n",
            "Epoch: 9 Batch Number: 102 Loss: 1.374491810798645 Time taken: 0.20044589042663574\n",
            "Epoch: 9 Batch Number: 103 Loss: 1.3809751272201538 Time taken: 0.20137405395507812\n",
            "Epoch: 9 Batch Number: 104 Loss: 1.36962890625 Time taken: 0.2046527862548828\n",
            "Epoch: 9 Batch Number: 105 Loss: 1.4050121307373047 Time taken: 0.20176100730895996\n",
            "Epoch: 9 Batch Number: 106 Loss: 1.4257104396820068 Time taken: 0.20136642456054688\n",
            "Epoch: 9 Batch Number: 107 Loss: 1.3915966749191284 Time taken: 0.20099759101867676\n",
            "Epoch: 9 Batch Number: 108 Loss: 1.398982048034668 Time taken: 0.2102952003479004\n",
            "Epoch: 9 Batch Number: 109 Loss: 1.425246000289917 Time taken: 0.20293736457824707\n",
            "Epoch: 9 Batch Number: 110 Loss: 1.4088928699493408 Time taken: 0.20424103736877441\n",
            "Epoch: 9 Batch Number: 111 Loss: 1.3997604846954346 Time taken: 0.20146465301513672\n",
            "Epoch: 9 Batch Number: 112 Loss: 1.4106712341308594 Time taken: 0.21982598304748535\n",
            "Epoch: 9 Batch Number: 113 Loss: 1.3973119258880615 Time taken: 0.20376801490783691\n",
            "Epoch: 9 Batch Number: 114 Loss: 1.3992149829864502 Time taken: 0.2064669132232666\n",
            "Epoch: 9 Batch Number: 115 Loss: 1.3987758159637451 Time taken: 0.20753240585327148\n",
            "Epoch: 9 Batch Number: 116 Loss: 1.3790321350097656 Time taken: 0.19901752471923828\n",
            "Epoch: 9 Batch Number: 117 Loss: 1.3755861520767212 Time taken: 0.19993281364440918\n",
            "Epoch: 9 Batch Number: 118 Loss: 1.3842593431472778 Time taken: 0.20122909545898438\n",
            "Epoch: 9 Batch Number: 119 Loss: 1.3734177350997925 Time taken: 0.20110702514648438\n",
            "Epoch: 9 Batch Number: 120 Loss: 1.3770054578781128 Time taken: 0.2074110507965088\n",
            "Epoch: 9 Batch Number: 121 Loss: 1.3906786441802979 Time taken: 0.2029128074645996\n",
            "Epoch: 9 Batch Number: 122 Loss: 1.367902398109436 Time taken: 0.21006011962890625\n",
            "Epoch: 9 Batch Number: 123 Loss: 1.3733781576156616 Time taken: 0.2019972801208496\n",
            "Epoch: 9 Batch Number: 124 Loss: 1.3858648538589478 Time taken: 0.20375394821166992\n",
            "Epoch: 9 Batch Number: 125 Loss: 1.368430256843567 Time taken: 0.21174836158752441\n",
            "Epoch: 9 Batch Number: 126 Loss: 1.3670899868011475 Time taken: 0.20246577262878418\n",
            "Epoch: 9 Batch Number: 127 Loss: 1.3937346935272217 Time taken: 0.20860624313354492\n",
            "Epoch: 9 Batch Number: 128 Loss: 1.3678615093231201 Time taken: 0.1995713710784912\n",
            "Epoch: 9 Batch Number: 129 Loss: 1.379868984222412 Time taken: 0.2037806510925293\n",
            "Epoch: 9 Batch Number: 130 Loss: 1.371097445487976 Time taken: 0.2009563446044922\n",
            "Epoch: 9 Batch Number: 131 Loss: 1.3564872741699219 Time taken: 0.20421695709228516\n",
            "Epoch: 9 Batch Number: 132 Loss: 1.3886359930038452 Time taken: 0.20605206489562988\n",
            "Epoch: 9 Batch Number: 133 Loss: 1.3800939321517944 Time taken: 0.20455121994018555\n",
            "Epoch: 9 Batch Number: 134 Loss: 1.3738077878952026 Time taken: 0.20373964309692383\n",
            "Epoch: 9 Batch Number: 135 Loss: 1.3537784814834595 Time taken: 0.20402240753173828\n",
            "Epoch: 9 Batch Number: 136 Loss: 1.3440381288528442 Time taken: 0.20388197898864746\n",
            "Epoch: 9 Batch Number: 137 Loss: 1.3357957601547241 Time taken: 0.1997971534729004\n",
            "Epoch: 9 Batch Number: 138 Loss: 1.3491418361663818 Time taken: 0.20055055618286133\n",
            "Epoch: 9 Batch Number: 139 Loss: 1.33799409866333 Time taken: 0.20128178596496582\n",
            "Epoch: 9 Batch Number: 140 Loss: 1.3920502662658691 Time taken: 0.20098567008972168\n",
            "Epoch: 9 Batch Number: 141 Loss: 1.4096083641052246 Time taken: 0.19902706146240234\n",
            "Epoch: 9 Batch Number: 142 Loss: 1.395976185798645 Time taken: 0.199568510055542\n",
            "Epoch: 9 Batch Number: 143 Loss: 1.3848469257354736 Time taken: 0.2001497745513916\n",
            "Epoch: 9 Batch Number: 144 Loss: 1.3823456764221191 Time taken: 0.2063298225402832\n",
            "Epoch: 9 Batch Number: 145 Loss: 1.3776628971099854 Time taken: 0.2026066780090332\n",
            "Epoch: 9 Batch Number: 146 Loss: 1.376477599143982 Time taken: 0.20206403732299805\n",
            "Epoch: 9 Batch Number: 147 Loss: 1.362370491027832 Time taken: 0.1998271942138672\n",
            "Epoch: 9 Batch Number: 148 Loss: 1.374131679534912 Time taken: 0.20095133781433105\n",
            "Epoch: 9 Batch Number: 149 Loss: 1.3637219667434692 Time taken: 0.20995235443115234\n",
            "Epoch: 9 Batch Number: 150 Loss: 1.374374270439148 Time taken: 0.20035743713378906\n",
            "Epoch: 9 Batch Number: 151 Loss: 1.3642407655715942 Time taken: 0.20210623741149902\n",
            "Epoch: 9 Batch Number: 152 Loss: 1.3583204746246338 Time taken: 0.2006525993347168\n",
            "Epoch: 9 Batch Number: 153 Loss: 1.3484405279159546 Time taken: 0.20151996612548828\n",
            "Epoch: 9 Batch Number: 154 Loss: 1.3548743724822998 Time taken: 0.20286083221435547\n",
            "Epoch: 9 Batch Number: 155 Loss: 1.403664469718933 Time taken: 0.20041704177856445\n",
            "Epoch: 9 Batch Number: 156 Loss: 1.362870693206787 Time taken: 0.20625066757202148\n",
            "Epoch: 9 Batch Number: 157 Loss: 1.3678193092346191 Time taken: 0.20551657676696777\n",
            "Epoch: 9 Batch Number: 158 Loss: 1.38239586353302 Time taken: 0.19981718063354492\n",
            "Epoch: 9 Batch Number: 159 Loss: 1.3707300424575806 Time taken: 0.2028331756591797\n",
            "Epoch: 9 Batch Number: 160 Loss: 1.349902629852295 Time taken: 0.20060205459594727\n",
            "Epoch: 9 Batch Number: 161 Loss: 1.3845689296722412 Time taken: 0.21251797676086426\n",
            "Epoch: 9 Batch Number: 162 Loss: 1.3719491958618164 Time taken: 0.20963454246520996\n",
            "Epoch: 9 Batch Number: 163 Loss: 1.3725420236587524 Time taken: 0.20537710189819336\n",
            "Epoch: 9 Batch Number: 164 Loss: 1.3817123174667358 Time taken: 0.21451044082641602\n",
            "Epoch: 9 Batch Number: 165 Loss: 1.3719182014465332 Time taken: 0.20102763175964355\n",
            "Epoch: 9 Batch Number: 166 Loss: 1.3563698530197144 Time taken: 0.1993105411529541\n",
            "Epoch: 9 Batch Number: 167 Loss: 1.3671270608901978 Time taken: 0.19729280471801758\n",
            "Epoch: 9 Batch Number: 168 Loss: 1.3747098445892334 Time taken: 0.201063871383667\n",
            "Epoch: 9 Batch Number: 169 Loss: 1.3499921560287476 Time taken: 0.1983489990234375\n",
            "Epoch: 9 Batch Number: 170 Loss: 1.377364158630371 Time taken: 0.2024853229522705\n",
            "Epoch: 9 Batch Number: 171 Loss: 1.3838735818862915 Time taken: 0.20339632034301758\n",
            "Epoch: 9 Batch Number: 172 Loss: 1.3782802820205688 Time taken: 0.1992332935333252\n",
            "Epoch: 9 Batch Number: 173 Loss: 1.3549209833145142 Time taken: 0.19948267936706543\n",
            "Epoch: 9 Batch Number: 174 Loss: 1.3550386428833008 Time taken: 0.2054734230041504\n",
            "Epoch: 9 Batch Number: 175 Loss: 1.3501224517822266 Time taken: 0.1998000144958496\n",
            "Epoch: 9 Batch Number: 176 Loss: 1.358663558959961 Time taken: 0.20053601264953613\n",
            "Epoch: 9 Batch Number: 177 Loss: 1.3592143058776855 Time taken: 0.20028305053710938\n",
            "Epoch: 9 Batch Number: 178 Loss: 1.3507801294326782 Time taken: 0.20554852485656738\n",
            "Epoch: 9 Batch Number: 179 Loss: 1.3258142471313477 Time taken: 0.19835281372070312\n",
            "==========================================================================================\n",
            "Start of epoch 10\n",
            "Epoch: 10 Batch Number: 1 Loss: 1.3394845724105835 Time taken: 0.1978895664215088\n",
            "Epoch: 10 Batch Number: 2 Loss: 1.320638656616211 Time taken: 0.19983744621276855\n",
            "Epoch: 10 Batch Number: 3 Loss: 1.3374382257461548 Time taken: 0.202667236328125\n",
            "Epoch: 10 Batch Number: 4 Loss: 1.3291070461273193 Time taken: 0.20824003219604492\n",
            "Epoch: 10 Batch Number: 5 Loss: 1.3207837343215942 Time taken: 0.20432329177856445\n",
            "Epoch: 10 Batch Number: 6 Loss: 1.326025128364563 Time taken: 0.20328950881958008\n",
            "Epoch: 10 Batch Number: 7 Loss: 1.3406068086624146 Time taken: 0.1986064910888672\n",
            "Epoch: 10 Batch Number: 8 Loss: 1.3294745683670044 Time taken: 0.19899988174438477\n",
            "Epoch: 10 Batch Number: 9 Loss: 1.329628348350525 Time taken: 0.20633578300476074\n",
            "Epoch: 10 Batch Number: 10 Loss: 1.3022263050079346 Time taken: 0.20308136940002441\n",
            "Epoch: 10 Batch Number: 11 Loss: 1.3359735012054443 Time taken: 0.20566654205322266\n",
            "Epoch: 10 Batch Number: 12 Loss: 1.3392592668533325 Time taken: 0.2005918025970459\n",
            "Epoch: 10 Batch Number: 13 Loss: 1.35664963722229 Time taken: 0.20180320739746094\n",
            "Epoch: 10 Batch Number: 14 Loss: 1.3312028646469116 Time taken: 0.20378947257995605\n",
            "Epoch: 10 Batch Number: 15 Loss: 1.365760087966919 Time taken: 0.19933700561523438\n",
            "Epoch: 10 Batch Number: 16 Loss: 1.3288934230804443 Time taken: 0.20277047157287598\n",
            "Epoch: 10 Batch Number: 17 Loss: 1.3183550834655762 Time taken: 0.2053356170654297\n",
            "Epoch: 10 Batch Number: 18 Loss: 1.3443983793258667 Time taken: 0.20002961158752441\n",
            "Epoch: 10 Batch Number: 19 Loss: 1.3275659084320068 Time taken: 0.20608258247375488\n",
            "Epoch: 10 Batch Number: 20 Loss: 1.2944661378860474 Time taken: 0.19949626922607422\n",
            "Epoch: 10 Batch Number: 21 Loss: 1.3546240329742432 Time taken: 0.20130157470703125\n",
            "Epoch: 10 Batch Number: 22 Loss: 1.3262420892715454 Time taken: 0.2020435333251953\n",
            "Epoch: 10 Batch Number: 23 Loss: 1.334060788154602 Time taken: 0.20060324668884277\n",
            "Epoch: 10 Batch Number: 24 Loss: 1.3515839576721191 Time taken: 0.20402145385742188\n",
            "Epoch: 10 Batch Number: 25 Loss: 1.3708125352859497 Time taken: 0.20308184623718262\n",
            "Epoch: 10 Batch Number: 26 Loss: 1.3564897775650024 Time taken: 0.19992661476135254\n",
            "Epoch: 10 Batch Number: 27 Loss: 1.3511343002319336 Time taken: 0.20063209533691406\n",
            "Epoch: 10 Batch Number: 28 Loss: 1.3468998670578003 Time taken: 0.2066645622253418\n",
            "Epoch: 10 Batch Number: 29 Loss: 1.3606700897216797 Time taken: 0.20621466636657715\n",
            "Epoch: 10 Batch Number: 30 Loss: 1.3425545692443848 Time taken: 0.20258378982543945\n",
            "Epoch: 10 Batch Number: 31 Loss: 1.3330038785934448 Time taken: 0.1989893913269043\n",
            "Epoch: 10 Batch Number: 32 Loss: 1.342212200164795 Time taken: 0.20116329193115234\n",
            "Epoch: 10 Batch Number: 33 Loss: 1.3339558839797974 Time taken: 0.19972968101501465\n",
            "Epoch: 10 Batch Number: 34 Loss: 1.3653720617294312 Time taken: 0.2002887725830078\n",
            "Epoch: 10 Batch Number: 35 Loss: 1.3376274108886719 Time taken: 0.19808483123779297\n",
            "Epoch: 10 Batch Number: 36 Loss: 1.3227097988128662 Time taken: 0.19728851318359375\n",
            "Epoch: 10 Batch Number: 37 Loss: 1.3784773349761963 Time taken: 0.2042231559753418\n",
            "Epoch: 10 Batch Number: 38 Loss: 1.3612704277038574 Time taken: 0.20254278182983398\n",
            "Epoch: 10 Batch Number: 39 Loss: 1.3517985343933105 Time taken: 0.2013087272644043\n",
            "Epoch: 10 Batch Number: 40 Loss: 1.3517216444015503 Time taken: 0.2049849033355713\n",
            "Epoch: 10 Batch Number: 41 Loss: 1.3478111028671265 Time taken: 0.20120692253112793\n",
            "Epoch: 10 Batch Number: 42 Loss: 1.367209792137146 Time taken: 0.20291948318481445\n",
            "Epoch: 10 Batch Number: 43 Loss: 1.358138084411621 Time taken: 0.20223689079284668\n",
            "Epoch: 10 Batch Number: 44 Loss: 1.346378207206726 Time taken: 0.20253872871398926\n",
            "Epoch: 10 Batch Number: 45 Loss: 1.351170539855957 Time taken: 0.20551133155822754\n",
            "Epoch: 10 Batch Number: 46 Loss: 1.3511115312576294 Time taken: 0.20102596282958984\n",
            "Epoch: 10 Batch Number: 47 Loss: 1.367022156715393 Time taken: 0.20104503631591797\n",
            "Epoch: 10 Batch Number: 48 Loss: 1.358146071434021 Time taken: 0.20638632774353027\n",
            "Epoch: 10 Batch Number: 49 Loss: 1.3423418998718262 Time taken: 0.20175790786743164\n",
            "Epoch: 10 Batch Number: 50 Loss: 1.362920880317688 Time taken: 0.20309996604919434\n",
            "Epoch: 10 Batch Number: 51 Loss: 1.3522518873214722 Time taken: 0.20208525657653809\n",
            "Epoch: 10 Batch Number: 52 Loss: 1.3234277963638306 Time taken: 0.20728468894958496\n",
            "Epoch: 10 Batch Number: 53 Loss: 1.3590307235717773 Time taken: 0.20656466484069824\n",
            "Epoch: 10 Batch Number: 54 Loss: 1.3582823276519775 Time taken: 0.20075106620788574\n",
            "Epoch: 10 Batch Number: 55 Loss: 1.3207134008407593 Time taken: 0.2003345489501953\n",
            "Epoch: 10 Batch Number: 56 Loss: 1.3473488092422485 Time taken: 0.20046329498291016\n",
            "Epoch: 10 Batch Number: 57 Loss: 1.3351013660430908 Time taken: 0.196610689163208\n",
            "Epoch: 10 Batch Number: 58 Loss: 1.3530839681625366 Time taken: 0.19995379447937012\n",
            "Epoch: 10 Batch Number: 59 Loss: 1.3538657426834106 Time taken: 0.20103716850280762\n",
            "Epoch: 10 Batch Number: 60 Loss: 1.3282057046890259 Time taken: 0.20795083045959473\n",
            "Epoch: 10 Batch Number: 61 Loss: 1.3407875299453735 Time taken: 0.19951391220092773\n",
            "Epoch: 10 Batch Number: 62 Loss: 1.3290746212005615 Time taken: 0.1997377872467041\n",
            "Epoch: 10 Batch Number: 63 Loss: 1.3238047361373901 Time taken: 0.20748019218444824\n",
            "Epoch: 10 Batch Number: 64 Loss: 1.3229327201843262 Time taken: 0.19886517524719238\n",
            "Epoch: 10 Batch Number: 65 Loss: 1.334252119064331 Time taken: 0.2034597396850586\n",
            "Epoch: 10 Batch Number: 66 Loss: 1.3530349731445312 Time taken: 0.19977641105651855\n",
            "Epoch: 10 Batch Number: 67 Loss: 1.3581254482269287 Time taken: 0.19929075241088867\n",
            "Epoch: 10 Batch Number: 68 Loss: 1.3528660535812378 Time taken: 0.20732665061950684\n",
            "Epoch: 10 Batch Number: 69 Loss: 1.3246099948883057 Time taken: 0.2159266471862793\n",
            "Epoch: 10 Batch Number: 70 Loss: 1.3305959701538086 Time taken: 0.2160043716430664\n",
            "Epoch: 10 Batch Number: 71 Loss: 1.3545114994049072 Time taken: 0.2002701759338379\n",
            "Epoch: 10 Batch Number: 72 Loss: 1.3507863283157349 Time taken: 0.20236706733703613\n",
            "Epoch: 10 Batch Number: 73 Loss: 1.3688231706619263 Time taken: 0.20288562774658203\n",
            "Epoch: 10 Batch Number: 74 Loss: 1.339770793914795 Time taken: 0.19948482513427734\n",
            "Epoch: 10 Batch Number: 75 Loss: 1.3449265956878662 Time taken: 0.20269227027893066\n",
            "Epoch: 10 Batch Number: 76 Loss: 1.3452427387237549 Time taken: 0.20074200630187988\n",
            "Epoch: 10 Batch Number: 77 Loss: 1.3273729085922241 Time taken: 0.1996152400970459\n",
            "Epoch: 10 Batch Number: 78 Loss: 1.3359962701797485 Time taken: 0.20073533058166504\n",
            "Epoch: 10 Batch Number: 79 Loss: 1.3418588638305664 Time taken: 0.20014262199401855\n",
            "Epoch: 10 Batch Number: 80 Loss: 1.3375662565231323 Time taken: 0.20318913459777832\n",
            "Epoch: 10 Batch Number: 81 Loss: 1.3208998441696167 Time taken: 0.200209379196167\n",
            "Epoch: 10 Batch Number: 82 Loss: 1.336474895477295 Time taken: 0.203538179397583\n",
            "Epoch: 10 Batch Number: 83 Loss: 1.3345959186553955 Time taken: 0.20155954360961914\n",
            "Epoch: 10 Batch Number: 84 Loss: 1.3370695114135742 Time taken: 0.20027852058410645\n",
            "Epoch: 10 Batch Number: 85 Loss: 1.338426947593689 Time taken: 0.2029259204864502\n",
            "Epoch: 10 Batch Number: 86 Loss: 1.3261266946792603 Time taken: 0.1982898712158203\n",
            "Epoch: 10 Batch Number: 87 Loss: 1.324139952659607 Time taken: 0.20263957977294922\n",
            "Epoch: 10 Batch Number: 88 Loss: 1.345809817314148 Time taken: 0.20111322402954102\n",
            "Epoch: 10 Batch Number: 89 Loss: 1.3219009637832642 Time taken: 0.20702695846557617\n",
            "Epoch: 10 Batch Number: 90 Loss: 1.309153437614441 Time taken: 0.20119833946228027\n",
            "Epoch: 10 Batch Number: 91 Loss: 1.3437403440475464 Time taken: 0.20108652114868164\n",
            "Epoch: 10 Batch Number: 92 Loss: 1.351210117340088 Time taken: 0.2042827606201172\n",
            "Epoch: 10 Batch Number: 93 Loss: 1.357799768447876 Time taken: 0.20373892784118652\n",
            "Epoch: 10 Batch Number: 94 Loss: 1.349050521850586 Time taken: 0.20745158195495605\n",
            "Epoch: 10 Batch Number: 95 Loss: 1.357457160949707 Time taken: 0.2006378173828125\n",
            "Epoch: 10 Batch Number: 96 Loss: 1.3585058450698853 Time taken: 0.20063042640686035\n",
            "Epoch: 10 Batch Number: 97 Loss: 1.3635162115097046 Time taken: 0.20046758651733398\n",
            "Epoch: 10 Batch Number: 98 Loss: 1.3524346351623535 Time taken: 0.20076847076416016\n",
            "Epoch: 10 Batch Number: 99 Loss: 1.3700469732284546 Time taken: 0.20421123504638672\n",
            "Epoch: 10 Batch Number: 100 Loss: 1.3418846130371094 Time taken: 0.20160388946533203\n",
            "Epoch: 10 Batch Number: 101 Loss: 1.3494161367416382 Time taken: 0.20500826835632324\n",
            "Epoch: 10 Batch Number: 102 Loss: 1.3501276969909668 Time taken: 0.2053225040435791\n",
            "Epoch: 10 Batch Number: 103 Loss: 1.3749383687973022 Time taken: 0.20329523086547852\n",
            "Epoch: 10 Batch Number: 104 Loss: 1.366585373878479 Time taken: 0.1975095272064209\n",
            "Epoch: 10 Batch Number: 105 Loss: 1.3899548053741455 Time taken: 0.2008039951324463\n",
            "Epoch: 10 Batch Number: 106 Loss: 1.3722141981124878 Time taken: 0.21094107627868652\n",
            "Epoch: 10 Batch Number: 107 Loss: 1.360284447669983 Time taken: 0.20427751541137695\n",
            "Epoch: 10 Batch Number: 108 Loss: 1.4013437032699585 Time taken: 0.20214176177978516\n",
            "Epoch: 10 Batch Number: 109 Loss: 1.3861762285232544 Time taken: 0.2081446647644043\n",
            "Epoch: 10 Batch Number: 110 Loss: 1.4212565422058105 Time taken: 0.20117974281311035\n",
            "Epoch: 10 Batch Number: 111 Loss: 1.3964523077011108 Time taken: 0.20353126525878906\n",
            "Epoch: 10 Batch Number: 112 Loss: 1.3970218896865845 Time taken: 0.20670485496520996\n",
            "Epoch: 10 Batch Number: 113 Loss: 1.3403022289276123 Time taken: 0.20084786415100098\n",
            "Epoch: 10 Batch Number: 114 Loss: 1.351899266242981 Time taken: 0.20044422149658203\n",
            "Epoch: 10 Batch Number: 115 Loss: 1.3508851528167725 Time taken: 0.20300793647766113\n",
            "Epoch: 10 Batch Number: 116 Loss: 1.3734742403030396 Time taken: 0.20050454139709473\n",
            "Epoch: 10 Batch Number: 117 Loss: 1.356094479560852 Time taken: 0.20090985298156738\n",
            "Epoch: 10 Batch Number: 118 Loss: 1.3611167669296265 Time taken: 0.20287799835205078\n",
            "Epoch: 10 Batch Number: 119 Loss: 1.357245922088623 Time taken: 0.19973540306091309\n",
            "Epoch: 10 Batch Number: 120 Loss: 1.3593469858169556 Time taken: 0.20058345794677734\n",
            "Epoch: 10 Batch Number: 121 Loss: 1.3456976413726807 Time taken: 0.19901061058044434\n",
            "Epoch: 10 Batch Number: 122 Loss: 1.373205542564392 Time taken: 0.20409679412841797\n",
            "Epoch: 10 Batch Number: 123 Loss: 1.3431895971298218 Time taken: 0.19887447357177734\n",
            "Epoch: 10 Batch Number: 124 Loss: 1.3469650745391846 Time taken: 0.19915342330932617\n",
            "Epoch: 10 Batch Number: 125 Loss: 1.334972858428955 Time taken: 0.1992645263671875\n",
            "Epoch: 10 Batch Number: 126 Loss: 1.3721635341644287 Time taken: 0.20122718811035156\n",
            "Epoch: 10 Batch Number: 127 Loss: 1.3726240396499634 Time taken: 0.20127272605895996\n",
            "Epoch: 10 Batch Number: 128 Loss: 1.3337609767913818 Time taken: 0.20238518714904785\n",
            "Epoch: 10 Batch Number: 129 Loss: 1.349997639656067 Time taken: 0.20195984840393066\n",
            "Epoch: 10 Batch Number: 130 Loss: 1.3645657300949097 Time taken: 0.20139217376708984\n",
            "Epoch: 10 Batch Number: 131 Loss: 1.355881929397583 Time taken: 0.2030029296875\n",
            "Epoch: 10 Batch Number: 132 Loss: 1.3456765413284302 Time taken: 0.20446443557739258\n",
            "Epoch: 10 Batch Number: 133 Loss: 1.3506438732147217 Time taken: 0.20050287246704102\n",
            "Epoch: 10 Batch Number: 134 Loss: 1.3755916357040405 Time taken: 0.20132160186767578\n",
            "Epoch: 10 Batch Number: 135 Loss: 1.3244330883026123 Time taken: 0.20147490501403809\n",
            "Epoch: 10 Batch Number: 136 Loss: 1.3301060199737549 Time taken: 0.2023482322692871\n",
            "Epoch: 10 Batch Number: 137 Loss: 1.3491661548614502 Time taken: 0.20315861701965332\n",
            "Epoch: 10 Batch Number: 138 Loss: 1.31938898563385 Time taken: 0.2028331756591797\n",
            "Epoch: 10 Batch Number: 139 Loss: 1.3211853504180908 Time taken: 0.20067667961120605\n",
            "Epoch: 10 Batch Number: 140 Loss: 1.3524737358093262 Time taken: 0.20060968399047852\n",
            "Epoch: 10 Batch Number: 141 Loss: 1.3390061855316162 Time taken: 0.20175695419311523\n",
            "Epoch: 10 Batch Number: 142 Loss: 1.3822433948516846 Time taken: 0.200120210647583\n",
            "Epoch: 10 Batch Number: 143 Loss: 1.3497414588928223 Time taken: 0.1996746063232422\n",
            "Epoch: 10 Batch Number: 144 Loss: 1.4057430028915405 Time taken: 0.1996006965637207\n",
            "Epoch: 10 Batch Number: 145 Loss: 1.3794819116592407 Time taken: 0.19800305366516113\n",
            "Epoch: 10 Batch Number: 146 Loss: 1.33707594871521 Time taken: 0.20282530784606934\n",
            "Epoch: 10 Batch Number: 147 Loss: 1.363783597946167 Time taken: 0.20299029350280762\n",
            "Epoch: 10 Batch Number: 148 Loss: 1.34050714969635 Time taken: 0.20299935340881348\n",
            "Epoch: 10 Batch Number: 149 Loss: 1.3329473733901978 Time taken: 0.19827055931091309\n",
            "Epoch: 10 Batch Number: 150 Loss: 1.3182958364486694 Time taken: 0.2041463851928711\n",
            "Epoch: 10 Batch Number: 151 Loss: 1.365360140800476 Time taken: 0.2157750129699707\n",
            "Epoch: 10 Batch Number: 152 Loss: 1.360394835472107 Time taken: 0.2009882926940918\n",
            "Epoch: 10 Batch Number: 153 Loss: 1.377500295639038 Time taken: 0.19642019271850586\n",
            "Epoch: 10 Batch Number: 154 Loss: 1.3712400197982788 Time taken: 0.20346736907958984\n",
            "Epoch: 10 Batch Number: 155 Loss: 1.3746052980422974 Time taken: 0.1990973949432373\n",
            "Epoch: 10 Batch Number: 156 Loss: 1.3444541692733765 Time taken: 0.20126605033874512\n",
            "Epoch: 10 Batch Number: 157 Loss: 1.34221613407135 Time taken: 0.20296454429626465\n",
            "Epoch: 10 Batch Number: 158 Loss: 1.3563045263290405 Time taken: 0.19961333274841309\n",
            "Epoch: 10 Batch Number: 159 Loss: 1.3540462255477905 Time taken: 0.20006203651428223\n",
            "Epoch: 10 Batch Number: 160 Loss: 1.3331964015960693 Time taken: 0.20476508140563965\n",
            "Epoch: 10 Batch Number: 161 Loss: 1.3563419580459595 Time taken: 0.20308804512023926\n",
            "Epoch: 10 Batch Number: 162 Loss: 1.351546287536621 Time taken: 0.20016956329345703\n",
            "Epoch: 10 Batch Number: 163 Loss: 1.3465039730072021 Time taken: 0.20082402229309082\n",
            "Epoch: 10 Batch Number: 164 Loss: 1.3457896709442139 Time taken: 0.20285558700561523\n",
            "Epoch: 10 Batch Number: 165 Loss: 1.3513195514678955 Time taken: 0.19913125038146973\n",
            "Epoch: 10 Batch Number: 166 Loss: 1.3531434535980225 Time taken: 0.20416927337646484\n",
            "Epoch: 10 Batch Number: 167 Loss: 1.3273268938064575 Time taken: 0.19940400123596191\n",
            "Epoch: 10 Batch Number: 168 Loss: 1.3583959341049194 Time taken: 0.20396828651428223\n",
            "Epoch: 10 Batch Number: 169 Loss: 1.3405896425247192 Time taken: 0.2001512050628662\n",
            "Epoch: 10 Batch Number: 170 Loss: 1.3524101972579956 Time taken: 0.20623540878295898\n",
            "Epoch: 10 Batch Number: 171 Loss: 1.3400477170944214 Time taken: 0.21109962463378906\n",
            "Epoch: 10 Batch Number: 172 Loss: 1.321137547492981 Time taken: 0.2005901336669922\n",
            "Epoch: 10 Batch Number: 173 Loss: 1.348600149154663 Time taken: 0.19732952117919922\n",
            "Epoch: 10 Batch Number: 174 Loss: 1.3309968709945679 Time taken: 0.1975867748260498\n",
            "Epoch: 10 Batch Number: 175 Loss: 1.3496711254119873 Time taken: 0.20308804512023926\n",
            "Epoch: 10 Batch Number: 176 Loss: 1.3288629055023193 Time taken: 0.2065122127532959\n",
            "Epoch: 10 Batch Number: 177 Loss: 1.3088921308517456 Time taken: 0.20295143127441406\n",
            "Epoch: 10 Batch Number: 178 Loss: 1.3364564180374146 Time taken: 0.20018863677978516\n",
            "Epoch: 10 Batch Number: 179 Loss: 1.329564094543457 Time taken: 0.2005786895751953\n",
            "==========================================================================================\n",
            "Start of epoch 11\n",
            "Epoch: 11 Batch Number: 1 Loss: 1.325114369392395 Time taken: 0.22658658027648926\n",
            "Epoch: 11 Batch Number: 2 Loss: 1.3108552694320679 Time taken: 0.20266509056091309\n",
            "Epoch: 11 Batch Number: 3 Loss: 1.306830883026123 Time taken: 0.19973373413085938\n",
            "Epoch: 11 Batch Number: 4 Loss: 1.331102967262268 Time taken: 0.2018420696258545\n",
            "Epoch: 11 Batch Number: 5 Loss: 1.270681619644165 Time taken: 0.20105648040771484\n",
            "Epoch: 11 Batch Number: 6 Loss: 1.3136073350906372 Time taken: 0.21083545684814453\n",
            "Epoch: 11 Batch Number: 7 Loss: 1.2874408960342407 Time taken: 0.20764923095703125\n",
            "Epoch: 11 Batch Number: 8 Loss: 1.3047460317611694 Time taken: 0.20551681518554688\n",
            "Epoch: 11 Batch Number: 9 Loss: 1.3134759664535522 Time taken: 0.20166587829589844\n",
            "Epoch: 11 Batch Number: 10 Loss: 1.3010317087173462 Time taken: 0.19737744331359863\n",
            "Epoch: 11 Batch Number: 11 Loss: 1.324696660041809 Time taken: 0.21299386024475098\n",
            "Epoch: 11 Batch Number: 12 Loss: 1.3218135833740234 Time taken: 0.20354199409484863\n",
            "Epoch: 11 Batch Number: 13 Loss: 1.318168044090271 Time taken: 0.20079827308654785\n",
            "Epoch: 11 Batch Number: 14 Loss: 1.3298720121383667 Time taken: 0.20197558403015137\n",
            "Epoch: 11 Batch Number: 15 Loss: 1.3397536277770996 Time taken: 0.20435571670532227\n",
            "Epoch: 11 Batch Number: 16 Loss: 1.3228727579116821 Time taken: 0.20453834533691406\n",
            "Epoch: 11 Batch Number: 17 Loss: 1.3267039060592651 Time taken: 0.20342183113098145\n",
            "Epoch: 11 Batch Number: 18 Loss: 1.3218492269515991 Time taken: 0.20448827743530273\n",
            "Epoch: 11 Batch Number: 19 Loss: 1.2973108291625977 Time taken: 0.20017743110656738\n",
            "Epoch: 11 Batch Number: 20 Loss: 1.311879277229309 Time taken: 0.20164966583251953\n",
            "Epoch: 11 Batch Number: 21 Loss: 1.309521198272705 Time taken: 0.19932174682617188\n",
            "Epoch: 11 Batch Number: 22 Loss: 1.3077431917190552 Time taken: 0.2018415927886963\n",
            "Epoch: 11 Batch Number: 23 Loss: 1.326563835144043 Time taken: 0.20014643669128418\n",
            "Epoch: 11 Batch Number: 24 Loss: 1.3174610137939453 Time taken: 0.20045828819274902\n",
            "Epoch: 11 Batch Number: 25 Loss: 1.3476589918136597 Time taken: 0.20090842247009277\n",
            "Epoch: 11 Batch Number: 26 Loss: 1.3353148698806763 Time taken: 0.19987845420837402\n",
            "Epoch: 11 Batch Number: 27 Loss: 1.3295514583587646 Time taken: 0.2003927230834961\n",
            "Epoch: 11 Batch Number: 28 Loss: 1.3363583087921143 Time taken: 0.2019023895263672\n",
            "Epoch: 11 Batch Number: 29 Loss: 1.3215913772583008 Time taken: 0.2000870704650879\n",
            "Epoch: 11 Batch Number: 30 Loss: 1.3351199626922607 Time taken: 0.20822882652282715\n",
            "Epoch: 11 Batch Number: 31 Loss: 1.316674828529358 Time taken: 0.20462632179260254\n",
            "Epoch: 11 Batch Number: 32 Loss: 1.2976276874542236 Time taken: 0.20073366165161133\n",
            "Epoch: 11 Batch Number: 33 Loss: 1.3369488716125488 Time taken: 0.1998898983001709\n",
            "Epoch: 11 Batch Number: 34 Loss: 1.329864501953125 Time taken: 0.20061230659484863\n",
            "Epoch: 11 Batch Number: 35 Loss: 1.3393511772155762 Time taken: 0.20642542839050293\n",
            "Epoch: 11 Batch Number: 36 Loss: 1.3353480100631714 Time taken: 0.20218539237976074\n",
            "Epoch: 11 Batch Number: 37 Loss: 1.3269994258880615 Time taken: 0.1992807388305664\n",
            "Epoch: 11 Batch Number: 38 Loss: 1.3626405000686646 Time taken: 0.20077180862426758\n",
            "Epoch: 11 Batch Number: 39 Loss: 1.335960865020752 Time taken: 0.20100998878479004\n",
            "Epoch: 11 Batch Number: 40 Loss: 1.3345584869384766 Time taken: 0.20371174812316895\n",
            "Epoch: 11 Batch Number: 41 Loss: 1.324598789215088 Time taken: 0.19968724250793457\n",
            "Epoch: 11 Batch Number: 42 Loss: 1.3131238222122192 Time taken: 0.19934463500976562\n",
            "Epoch: 11 Batch Number: 43 Loss: 1.323115348815918 Time taken: 0.19987201690673828\n",
            "Epoch: 11 Batch Number: 44 Loss: 1.3329161405563354 Time taken: 0.20443105697631836\n",
            "Epoch: 11 Batch Number: 45 Loss: 1.340287446975708 Time taken: 0.2179117202758789\n",
            "Epoch: 11 Batch Number: 46 Loss: 1.311020016670227 Time taken: 0.20407462120056152\n",
            "Epoch: 11 Batch Number: 47 Loss: 1.3186060190200806 Time taken: 0.19994735717773438\n",
            "Epoch: 11 Batch Number: 48 Loss: 1.3188600540161133 Time taken: 0.1980600357055664\n",
            "Epoch: 11 Batch Number: 49 Loss: 1.3337440490722656 Time taken: 0.20026659965515137\n",
            "Epoch: 11 Batch Number: 50 Loss: 1.3578416109085083 Time taken: 0.2078533172607422\n",
            "Epoch: 11 Batch Number: 51 Loss: 1.299149513244629 Time taken: 0.21577882766723633\n",
            "Epoch: 11 Batch Number: 52 Loss: 1.3443642854690552 Time taken: 0.201401948928833\n",
            "Epoch: 11 Batch Number: 53 Loss: 1.3092949390411377 Time taken: 0.19762849807739258\n",
            "Epoch: 11 Batch Number: 54 Loss: 1.3275703191757202 Time taken: 0.2043464183807373\n",
            "Epoch: 11 Batch Number: 55 Loss: 1.3103625774383545 Time taken: 0.20879340171813965\n",
            "Epoch: 11 Batch Number: 56 Loss: 1.324940800666809 Time taken: 0.20311880111694336\n",
            "Epoch: 11 Batch Number: 57 Loss: 1.333764910697937 Time taken: 0.20011043548583984\n",
            "Epoch: 11 Batch Number: 58 Loss: 1.3141154050827026 Time taken: 0.20493125915527344\n",
            "Epoch: 11 Batch Number: 59 Loss: 1.3407546281814575 Time taken: 0.20122313499450684\n",
            "Epoch: 11 Batch Number: 60 Loss: 1.3217650651931763 Time taken: 0.21543550491333008\n",
            "Epoch: 11 Batch Number: 61 Loss: 1.33544921875 Time taken: 0.2015371322631836\n",
            "Epoch: 11 Batch Number: 62 Loss: 1.3121423721313477 Time taken: 0.1996290683746338\n",
            "Epoch: 11 Batch Number: 63 Loss: 1.333051323890686 Time taken: 0.20345187187194824\n",
            "Epoch: 11 Batch Number: 64 Loss: 1.3143565654754639 Time taken: 0.20201373100280762\n",
            "Epoch: 11 Batch Number: 65 Loss: 1.325209617614746 Time taken: 0.207960844039917\n",
            "Epoch: 11 Batch Number: 66 Loss: 1.3232910633087158 Time taken: 0.20296049118041992\n",
            "Epoch: 11 Batch Number: 67 Loss: 1.3086729049682617 Time taken: 0.20250368118286133\n",
            "Epoch: 11 Batch Number: 68 Loss: 1.343787431716919 Time taken: 0.20400500297546387\n",
            "Epoch: 11 Batch Number: 69 Loss: 1.3162617683410645 Time taken: 0.2107832431793213\n",
            "Epoch: 11 Batch Number: 70 Loss: 1.3424694538116455 Time taken: 0.20280814170837402\n",
            "Epoch: 11 Batch Number: 71 Loss: 1.33332097530365 Time taken: 0.2021329402923584\n",
            "Epoch: 11 Batch Number: 72 Loss: 1.3325750827789307 Time taken: 0.20299458503723145\n",
            "Epoch: 11 Batch Number: 73 Loss: 1.3461992740631104 Time taken: 0.20638060569763184\n",
            "Epoch: 11 Batch Number: 74 Loss: 1.3331817388534546 Time taken: 0.20329976081848145\n",
            "Epoch: 11 Batch Number: 75 Loss: 1.3298357725143433 Time taken: 0.2026822566986084\n",
            "Epoch: 11 Batch Number: 76 Loss: 1.3165838718414307 Time taken: 0.19858980178833008\n",
            "Epoch: 11 Batch Number: 77 Loss: 1.3224622011184692 Time taken: 0.20040202140808105\n",
            "Epoch: 11 Batch Number: 78 Loss: 1.329727292060852 Time taken: 0.20203685760498047\n",
            "Epoch: 11 Batch Number: 79 Loss: 1.337104320526123 Time taken: 0.21004652976989746\n",
            "Epoch: 11 Batch Number: 80 Loss: 1.3337037563323975 Time taken: 0.20008635520935059\n",
            "Epoch: 11 Batch Number: 81 Loss: 1.3289951086044312 Time taken: 0.20235037803649902\n",
            "Epoch: 11 Batch Number: 82 Loss: 1.3196784257888794 Time taken: 0.20188021659851074\n",
            "Epoch: 11 Batch Number: 83 Loss: 1.2977042198181152 Time taken: 0.20338988304138184\n",
            "Epoch: 11 Batch Number: 84 Loss: 1.3131153583526611 Time taken: 0.20560288429260254\n",
            "Epoch: 11 Batch Number: 85 Loss: 1.3021464347839355 Time taken: 0.19943451881408691\n",
            "Epoch: 11 Batch Number: 86 Loss: 1.3124399185180664 Time taken: 0.19925379753112793\n",
            "Epoch: 11 Batch Number: 87 Loss: 1.300665259361267 Time taken: 0.20174312591552734\n",
            "Epoch: 11 Batch Number: 88 Loss: 1.3135430812835693 Time taken: 0.2019047737121582\n",
            "Epoch: 11 Batch Number: 89 Loss: 1.3057445287704468 Time taken: 0.2096390724182129\n",
            "Epoch: 11 Batch Number: 90 Loss: 1.309792399406433 Time taken: 0.19906377792358398\n",
            "Epoch: 11 Batch Number: 91 Loss: 1.31534743309021 Time taken: 0.2037208080291748\n",
            "Epoch: 11 Batch Number: 92 Loss: 1.3242459297180176 Time taken: 0.20684480667114258\n",
            "Epoch: 11 Batch Number: 93 Loss: 1.3286552429199219 Time taken: 0.1986556053161621\n",
            "Epoch: 11 Batch Number: 94 Loss: 1.3338285684585571 Time taken: 0.20892643928527832\n",
            "Epoch: 11 Batch Number: 95 Loss: 1.3364269733428955 Time taken: 0.20003271102905273\n",
            "Epoch: 11 Batch Number: 96 Loss: 1.33847975730896 Time taken: 0.20674991607666016\n",
            "Epoch: 11 Batch Number: 97 Loss: 1.3347551822662354 Time taken: 0.20052433013916016\n",
            "Epoch: 11 Batch Number: 98 Loss: 1.3416792154312134 Time taken: 0.20380115509033203\n",
            "Epoch: 11 Batch Number: 99 Loss: 1.334649682044983 Time taken: 0.200087308883667\n",
            "Epoch: 11 Batch Number: 100 Loss: 1.3422915935516357 Time taken: 0.20073175430297852\n",
            "Epoch: 11 Batch Number: 101 Loss: 1.3513392210006714 Time taken: 0.20117831230163574\n",
            "Epoch: 11 Batch Number: 102 Loss: 1.355362057685852 Time taken: 0.2078232765197754\n",
            "Epoch: 11 Batch Number: 103 Loss: 1.3610498905181885 Time taken: 0.2021944522857666\n",
            "Epoch: 11 Batch Number: 104 Loss: 1.3481310606002808 Time taken: 0.200791597366333\n",
            "Epoch: 11 Batch Number: 105 Loss: 1.369107961654663 Time taken: 0.19978785514831543\n",
            "Epoch: 11 Batch Number: 106 Loss: 1.348827838897705 Time taken: 0.20338988304138184\n",
            "Epoch: 11 Batch Number: 107 Loss: 1.3509923219680786 Time taken: 0.2013392448425293\n",
            "Epoch: 11 Batch Number: 108 Loss: 1.3659790754318237 Time taken: 0.20514893531799316\n",
            "Epoch: 11 Batch Number: 109 Loss: 1.326468825340271 Time taken: 0.20392251014709473\n",
            "Epoch: 11 Batch Number: 110 Loss: 1.376726508140564 Time taken: 0.19843006134033203\n",
            "Epoch: 11 Batch Number: 111 Loss: 1.3648476600646973 Time taken: 0.20849823951721191\n",
            "Epoch: 11 Batch Number: 112 Loss: 1.366736888885498 Time taken: 0.2010514736175537\n",
            "Epoch: 11 Batch Number: 113 Loss: 1.362857699394226 Time taken: 0.201735258102417\n",
            "Epoch: 11 Batch Number: 114 Loss: 1.38846755027771 Time taken: 0.20685124397277832\n",
            "Epoch: 11 Batch Number: 115 Loss: 1.3514078855514526 Time taken: 0.20154666900634766\n",
            "Epoch: 11 Batch Number: 116 Loss: 1.3458197116851807 Time taken: 0.2023165225982666\n",
            "Epoch: 11 Batch Number: 117 Loss: 1.3535263538360596 Time taken: 0.20027542114257812\n",
            "Epoch: 11 Batch Number: 118 Loss: 1.3310235738754272 Time taken: 0.2081007957458496\n",
            "Epoch: 11 Batch Number: 119 Loss: 1.3376704454421997 Time taken: 0.2098526954650879\n",
            "Epoch: 11 Batch Number: 120 Loss: 1.3496838808059692 Time taken: 0.20305204391479492\n",
            "Epoch: 11 Batch Number: 121 Loss: 1.3285598754882812 Time taken: 0.2053360939025879\n",
            "Epoch: 11 Batch Number: 122 Loss: 1.3413255214691162 Time taken: 0.2030634880065918\n",
            "Epoch: 11 Batch Number: 123 Loss: 1.34034264087677 Time taken: 0.21207642555236816\n",
            "Epoch: 11 Batch Number: 124 Loss: 1.3094135522842407 Time taken: 0.2066636085510254\n",
            "Epoch: 11 Batch Number: 125 Loss: 1.348128318786621 Time taken: 0.2096109390258789\n",
            "Epoch: 11 Batch Number: 126 Loss: 1.3424897193908691 Time taken: 0.2000563144683838\n",
            "Epoch: 11 Batch Number: 127 Loss: 1.3165879249572754 Time taken: 0.20405912399291992\n",
            "Epoch: 11 Batch Number: 128 Loss: 1.347611665725708 Time taken: 0.21367263793945312\n",
            "Epoch: 11 Batch Number: 129 Loss: 1.325094223022461 Time taken: 0.1995077133178711\n",
            "Epoch: 11 Batch Number: 130 Loss: 1.354332447052002 Time taken: 0.20278000831604004\n",
            "Epoch: 11 Batch Number: 131 Loss: 1.3199907541275024 Time taken: 0.2000713348388672\n",
            "Epoch: 11 Batch Number: 132 Loss: 1.3583793640136719 Time taken: 0.20795893669128418\n",
            "Epoch: 11 Batch Number: 133 Loss: 1.3172000646591187 Time taken: 0.19752979278564453\n",
            "Epoch: 11 Batch Number: 134 Loss: 1.3151705265045166 Time taken: 0.20177245140075684\n",
            "Epoch: 11 Batch Number: 135 Loss: 1.320420503616333 Time taken: 0.20287013053894043\n",
            "Epoch: 11 Batch Number: 136 Loss: 1.3362352848052979 Time taken: 0.2031106948852539\n",
            "Epoch: 11 Batch Number: 137 Loss: 1.303783893585205 Time taken: 0.20159363746643066\n",
            "Epoch: 11 Batch Number: 138 Loss: 1.3517498970031738 Time taken: 0.20367121696472168\n",
            "Epoch: 11 Batch Number: 139 Loss: 1.3355903625488281 Time taken: 0.20053958892822266\n",
            "Epoch: 11 Batch Number: 140 Loss: 1.3418327569961548 Time taken: 0.20098137855529785\n",
            "Epoch: 11 Batch Number: 141 Loss: 1.3158756494522095 Time taken: 0.21045994758605957\n",
            "Epoch: 11 Batch Number: 142 Loss: 1.3914273977279663 Time taken: 0.20308303833007812\n",
            "Epoch: 11 Batch Number: 143 Loss: 1.3715146780014038 Time taken: 0.20276212692260742\n",
            "Epoch: 11 Batch Number: 144 Loss: 1.3411009311676025 Time taken: 0.2036914825439453\n",
            "Epoch: 11 Batch Number: 145 Loss: 1.338700294494629 Time taken: 0.19861721992492676\n",
            "Epoch: 11 Batch Number: 146 Loss: 1.3563263416290283 Time taken: 0.2077333927154541\n",
            "Epoch: 11 Batch Number: 147 Loss: 1.3416475057601929 Time taken: 0.20088911056518555\n",
            "Epoch: 11 Batch Number: 148 Loss: 1.3528155088424683 Time taken: 0.20336604118347168\n",
            "Epoch: 11 Batch Number: 149 Loss: 1.3358956575393677 Time taken: 0.20451664924621582\n",
            "Epoch: 11 Batch Number: 150 Loss: 1.315045952796936 Time taken: 0.19971132278442383\n",
            "Epoch: 11 Batch Number: 151 Loss: 1.3186633586883545 Time taken: 0.22314119338989258\n",
            "Epoch: 11 Batch Number: 152 Loss: 1.3190215826034546 Time taken: 0.20705413818359375\n",
            "Epoch: 11 Batch Number: 153 Loss: 1.3314614295959473 Time taken: 0.20308971405029297\n",
            "Epoch: 11 Batch Number: 154 Loss: 1.3339380025863647 Time taken: 0.20460224151611328\n",
            "Epoch: 11 Batch Number: 155 Loss: 1.3078962564468384 Time taken: 0.20332741737365723\n",
            "Epoch: 11 Batch Number: 156 Loss: 1.3306200504302979 Time taken: 0.20450973510742188\n",
            "Epoch: 11 Batch Number: 157 Loss: 1.3471943140029907 Time taken: 0.20679688453674316\n",
            "Epoch: 11 Batch Number: 158 Loss: 1.3163292407989502 Time taken: 0.21153688430786133\n",
            "Epoch: 11 Batch Number: 159 Loss: 1.3497326374053955 Time taken: 0.20149683952331543\n",
            "Epoch: 11 Batch Number: 160 Loss: 1.3416459560394287 Time taken: 0.20237040519714355\n",
            "Epoch: 11 Batch Number: 161 Loss: 1.3523865938186646 Time taken: 0.20465445518493652\n",
            "Epoch: 11 Batch Number: 162 Loss: 1.3452774286270142 Time taken: 0.20168113708496094\n",
            "Epoch: 11 Batch Number: 163 Loss: 1.3319085836410522 Time taken: 0.1960463523864746\n",
            "Epoch: 11 Batch Number: 164 Loss: 1.3136365413665771 Time taken: 0.19845247268676758\n",
            "Epoch: 11 Batch Number: 165 Loss: 1.3070998191833496 Time taken: 0.20732569694519043\n",
            "Epoch: 11 Batch Number: 166 Loss: 1.3305673599243164 Time taken: 0.19923067092895508\n",
            "Epoch: 11 Batch Number: 167 Loss: 1.3344625234603882 Time taken: 0.2015516757965088\n",
            "Epoch: 11 Batch Number: 168 Loss: 1.3491863012313843 Time taken: 0.19982004165649414\n",
            "Epoch: 11 Batch Number: 169 Loss: 1.3254339694976807 Time taken: 0.2021026611328125\n",
            "Epoch: 11 Batch Number: 170 Loss: 1.3363534212112427 Time taken: 0.20462441444396973\n",
            "Epoch: 11 Batch Number: 171 Loss: 1.3188636302947998 Time taken: 0.20016956329345703\n",
            "Epoch: 11 Batch Number: 172 Loss: 1.3119585514068604 Time taken: 0.20424866676330566\n",
            "Epoch: 11 Batch Number: 173 Loss: 1.326859951019287 Time taken: 0.20356488227844238\n",
            "Epoch: 11 Batch Number: 174 Loss: 1.3202803134918213 Time taken: 0.19997596740722656\n",
            "Epoch: 11 Batch Number: 175 Loss: 1.2996398210525513 Time taken: 0.2090132236480713\n",
            "Epoch: 11 Batch Number: 176 Loss: 1.3272144794464111 Time taken: 0.20108866691589355\n",
            "Epoch: 11 Batch Number: 177 Loss: 1.3171018362045288 Time taken: 0.20584392547607422\n",
            "Epoch: 11 Batch Number: 178 Loss: 1.3056068420410156 Time taken: 0.2075033187866211\n",
            "Epoch: 11 Batch Number: 179 Loss: 1.3225836753845215 Time taken: 0.19948148727416992\n",
            "==========================================================================================\n",
            "Start of epoch 12\n",
            "Epoch: 12 Batch Number: 1 Loss: 1.3176318407058716 Time taken: 0.20357012748718262\n",
            "Epoch: 12 Batch Number: 2 Loss: 1.2835413217544556 Time taken: 0.2019953727722168\n",
            "Epoch: 12 Batch Number: 3 Loss: 1.2863450050354004 Time taken: 0.19913983345031738\n",
            "Epoch: 12 Batch Number: 4 Loss: 1.279885172843933 Time taken: 0.2015984058380127\n",
            "Epoch: 12 Batch Number: 5 Loss: 1.2785273790359497 Time taken: 0.2022864818572998\n",
            "Epoch: 12 Batch Number: 6 Loss: 1.3089995384216309 Time taken: 0.19900774955749512\n",
            "Epoch: 12 Batch Number: 7 Loss: 1.2698520421981812 Time taken: 0.20584321022033691\n",
            "Epoch: 12 Batch Number: 8 Loss: 1.2950444221496582 Time taken: 0.20561861991882324\n",
            "Epoch: 12 Batch Number: 9 Loss: 1.2874739170074463 Time taken: 0.19768166542053223\n",
            "Epoch: 12 Batch Number: 10 Loss: 1.2930376529693604 Time taken: 0.20794963836669922\n",
            "Epoch: 12 Batch Number: 11 Loss: 1.2970093488693237 Time taken: 0.19921159744262695\n",
            "Epoch: 12 Batch Number: 12 Loss: 1.3044782876968384 Time taken: 0.2103278636932373\n",
            "Epoch: 12 Batch Number: 13 Loss: 1.3073233366012573 Time taken: 0.20403170585632324\n",
            "Epoch: 12 Batch Number: 14 Loss: 1.3300631046295166 Time taken: 0.20551443099975586\n",
            "Epoch: 12 Batch Number: 15 Loss: 1.2996028661727905 Time taken: 0.20069074630737305\n",
            "Epoch: 12 Batch Number: 16 Loss: 1.3267168998718262 Time taken: 0.20466160774230957\n",
            "Epoch: 12 Batch Number: 17 Loss: 1.3107900619506836 Time taken: 0.20367217063903809\n",
            "Epoch: 12 Batch Number: 18 Loss: 1.2825599908828735 Time taken: 0.20215535163879395\n",
            "Epoch: 12 Batch Number: 19 Loss: 1.3043646812438965 Time taken: 0.20134878158569336\n",
            "Epoch: 12 Batch Number: 20 Loss: 1.2616925239562988 Time taken: 0.20337414741516113\n",
            "Epoch: 12 Batch Number: 21 Loss: 1.3014075756072998 Time taken: 0.19918203353881836\n",
            "Epoch: 12 Batch Number: 22 Loss: 1.3056831359863281 Time taken: 0.20177865028381348\n",
            "Epoch: 12 Batch Number: 23 Loss: 1.3039244413375854 Time taken: 0.20466089248657227\n",
            "Epoch: 12 Batch Number: 24 Loss: 1.322360873222351 Time taken: 0.20487642288208008\n",
            "Epoch: 12 Batch Number: 25 Loss: 1.314827799797058 Time taken: 0.19944524765014648\n",
            "Epoch: 12 Batch Number: 26 Loss: 1.3253552913665771 Time taken: 0.1986865997314453\n",
            "Epoch: 12 Batch Number: 27 Loss: 1.3180440664291382 Time taken: 0.19897150993347168\n",
            "Epoch: 12 Batch Number: 28 Loss: 1.2897192239761353 Time taken: 0.20460748672485352\n",
            "Epoch: 12 Batch Number: 29 Loss: 1.2935165166854858 Time taken: 0.20026946067810059\n",
            "Epoch: 12 Batch Number: 30 Loss: 1.3252167701721191 Time taken: 0.20686984062194824\n",
            "Epoch: 12 Batch Number: 31 Loss: 1.3058379888534546 Time taken: 0.20105814933776855\n",
            "Epoch: 12 Batch Number: 32 Loss: 1.3116507530212402 Time taken: 0.2043914794921875\n",
            "Epoch: 12 Batch Number: 33 Loss: 1.2841367721557617 Time taken: 0.2052907943725586\n",
            "Epoch: 12 Batch Number: 34 Loss: 1.2905027866363525 Time taken: 0.20060348510742188\n",
            "Epoch: 12 Batch Number: 35 Loss: 1.3268963098526 Time taken: 0.2021942138671875\n",
            "Epoch: 12 Batch Number: 36 Loss: 1.3552597761154175 Time taken: 0.21467852592468262\n",
            "Epoch: 12 Batch Number: 37 Loss: 1.33284592628479 Time taken: 0.1998310089111328\n",
            "Epoch: 12 Batch Number: 38 Loss: 1.3349909782409668 Time taken: 0.20032334327697754\n",
            "Epoch: 12 Batch Number: 39 Loss: 1.3221527338027954 Time taken: 0.2016587257385254\n",
            "Epoch: 12 Batch Number: 40 Loss: 1.3131150007247925 Time taken: 0.20077157020568848\n",
            "Epoch: 12 Batch Number: 41 Loss: 1.3378442525863647 Time taken: 0.20041513442993164\n",
            "Epoch: 12 Batch Number: 42 Loss: 1.3062273263931274 Time taken: 0.20499539375305176\n",
            "Epoch: 12 Batch Number: 43 Loss: 1.2953153848648071 Time taken: 0.20126581192016602\n",
            "Epoch: 12 Batch Number: 44 Loss: 1.2972102165222168 Time taken: 0.2062973976135254\n",
            "Epoch: 12 Batch Number: 45 Loss: 1.297677993774414 Time taken: 0.20015740394592285\n",
            "Epoch: 12 Batch Number: 46 Loss: 1.3013556003570557 Time taken: 0.21764254570007324\n",
            "Epoch: 12 Batch Number: 47 Loss: 1.3185008764266968 Time taken: 0.1986398696899414\n",
            "Epoch: 12 Batch Number: 48 Loss: 1.3167955875396729 Time taken: 0.2019033432006836\n",
            "Epoch: 12 Batch Number: 49 Loss: 1.3347456455230713 Time taken: 0.20986247062683105\n",
            "Epoch: 12 Batch Number: 50 Loss: 1.332375407218933 Time taken: 0.20127439498901367\n",
            "Epoch: 12 Batch Number: 51 Loss: 1.3453370332717896 Time taken: 0.20510196685791016\n",
            "Epoch: 12 Batch Number: 52 Loss: 1.31768000125885 Time taken: 0.2022402286529541\n",
            "Epoch: 12 Batch Number: 53 Loss: 1.353533148765564 Time taken: 0.20235323905944824\n",
            "Epoch: 12 Batch Number: 54 Loss: 1.3321901559829712 Time taken: 0.2050342559814453\n",
            "Epoch: 12 Batch Number: 55 Loss: 1.2883498668670654 Time taken: 0.20149016380310059\n",
            "Epoch: 12 Batch Number: 56 Loss: 1.2862902879714966 Time taken: 0.20019793510437012\n",
            "Epoch: 12 Batch Number: 57 Loss: 1.3121248483657837 Time taken: 0.2012794017791748\n",
            "Epoch: 12 Batch Number: 58 Loss: 1.2856558561325073 Time taken: 0.20354485511779785\n",
            "Epoch: 12 Batch Number: 59 Loss: 1.3154622316360474 Time taken: 0.2101147174835205\n",
            "Epoch: 12 Batch Number: 60 Loss: 1.3151637315750122 Time taken: 0.20363497734069824\n",
            "Epoch: 12 Batch Number: 61 Loss: 1.3296763896942139 Time taken: 0.20112204551696777\n",
            "Epoch: 12 Batch Number: 62 Loss: 1.3329434394836426 Time taken: 0.20031976699829102\n",
            "Epoch: 12 Batch Number: 63 Loss: 1.307938575744629 Time taken: 0.2064955234527588\n",
            "Epoch: 12 Batch Number: 64 Loss: 1.2953715324401855 Time taken: 0.19864535331726074\n",
            "Epoch: 12 Batch Number: 65 Loss: 1.2952526807785034 Time taken: 0.2035675048828125\n",
            "Epoch: 12 Batch Number: 66 Loss: 1.3047430515289307 Time taken: 0.20295214653015137\n",
            "Epoch: 12 Batch Number: 67 Loss: 1.3136634826660156 Time taken: 0.20127153396606445\n",
            "Epoch: 12 Batch Number: 68 Loss: 1.3175454139709473 Time taken: 0.20028042793273926\n",
            "Epoch: 12 Batch Number: 69 Loss: 1.3109678030014038 Time taken: 0.20271921157836914\n",
            "Epoch: 12 Batch Number: 70 Loss: 1.3192631006240845 Time taken: 0.2049083709716797\n",
            "Epoch: 12 Batch Number: 71 Loss: 1.3212485313415527 Time taken: 0.2183070182800293\n",
            "Epoch: 12 Batch Number: 72 Loss: 1.3169025182724 Time taken: 0.2004413604736328\n",
            "Epoch: 12 Batch Number: 73 Loss: 1.3170064687728882 Time taken: 0.2009875774383545\n",
            "Epoch: 12 Batch Number: 74 Loss: 1.3159319162368774 Time taken: 0.21135973930358887\n",
            "Epoch: 12 Batch Number: 75 Loss: 1.309611439704895 Time taken: 0.20393610000610352\n",
            "Epoch: 12 Batch Number: 76 Loss: 1.328485131263733 Time taken: 0.2065412998199463\n",
            "Epoch: 12 Batch Number: 77 Loss: 1.301768183708191 Time taken: 0.20119595527648926\n",
            "Epoch: 12 Batch Number: 78 Loss: 1.3212541341781616 Time taken: 0.21037650108337402\n",
            "Epoch: 12 Batch Number: 79 Loss: 1.2771855592727661 Time taken: 0.2116096019744873\n",
            "Epoch: 12 Batch Number: 80 Loss: 1.3273112773895264 Time taken: 0.21076035499572754\n",
            "Epoch: 12 Batch Number: 81 Loss: 1.3031625747680664 Time taken: 0.20064115524291992\n",
            "Epoch: 12 Batch Number: 82 Loss: 1.3044215440750122 Time taken: 0.1997206211090088\n",
            "Epoch: 12 Batch Number: 83 Loss: 1.2913533449172974 Time taken: 0.20029497146606445\n",
            "Epoch: 12 Batch Number: 84 Loss: 1.2900177240371704 Time taken: 0.20238900184631348\n",
            "Epoch: 12 Batch Number: 85 Loss: 1.2924221754074097 Time taken: 0.20761370658874512\n",
            "Epoch: 12 Batch Number: 86 Loss: 1.2863013744354248 Time taken: 0.20032215118408203\n",
            "Epoch: 12 Batch Number: 87 Loss: 1.291740894317627 Time taken: 0.2031245231628418\n",
            "Epoch: 12 Batch Number: 88 Loss: 1.273236870765686 Time taken: 0.2018601894378662\n",
            "Epoch: 12 Batch Number: 89 Loss: 1.295774221420288 Time taken: 0.20370268821716309\n",
            "Epoch: 12 Batch Number: 90 Loss: 1.2772735357284546 Time taken: 0.19947576522827148\n",
            "Epoch: 12 Batch Number: 91 Loss: 1.2986726760864258 Time taken: 0.19887399673461914\n",
            "Epoch: 12 Batch Number: 92 Loss: 1.324697494506836 Time taken: 0.20032024383544922\n",
            "Epoch: 12 Batch Number: 93 Loss: 1.33052659034729 Time taken: 0.19774770736694336\n",
            "Epoch: 12 Batch Number: 94 Loss: 1.30078125 Time taken: 0.20448851585388184\n",
            "Epoch: 12 Batch Number: 95 Loss: 1.3274531364440918 Time taken: 0.19953632354736328\n",
            "Epoch: 12 Batch Number: 96 Loss: 1.3365000486373901 Time taken: 0.20673465728759766\n",
            "Epoch: 12 Batch Number: 97 Loss: 1.3064544200897217 Time taken: 0.20229005813598633\n",
            "Epoch: 12 Batch Number: 98 Loss: 1.3399428129196167 Time taken: 0.20432257652282715\n",
            "Epoch: 12 Batch Number: 99 Loss: 1.3276901245117188 Time taken: 0.20196914672851562\n",
            "Epoch: 12 Batch Number: 100 Loss: 1.3267040252685547 Time taken: 0.2010636329650879\n",
            "Epoch: 12 Batch Number: 101 Loss: 1.3336621522903442 Time taken: 0.20267915725708008\n",
            "Epoch: 12 Batch Number: 102 Loss: 1.3263163566589355 Time taken: 0.20038175582885742\n",
            "Epoch: 12 Batch Number: 103 Loss: 1.3166524171829224 Time taken: 0.2029721736907959\n",
            "Epoch: 12 Batch Number: 104 Loss: 1.3555810451507568 Time taken: 0.20197248458862305\n",
            "Epoch: 12 Batch Number: 105 Loss: 1.3252463340759277 Time taken: 0.20450162887573242\n",
            "Epoch: 12 Batch Number: 106 Loss: 1.346455454826355 Time taken: 0.1987762451171875\n",
            "Epoch: 12 Batch Number: 107 Loss: 1.3633308410644531 Time taken: 0.20373058319091797\n",
            "Epoch: 12 Batch Number: 108 Loss: 1.3552472591400146 Time taken: 0.20311737060546875\n",
            "Epoch: 12 Batch Number: 109 Loss: 1.3358126878738403 Time taken: 0.20268821716308594\n",
            "Epoch: 12 Batch Number: 110 Loss: 1.358163595199585 Time taken: 0.20939397811889648\n",
            "Epoch: 12 Batch Number: 111 Loss: 1.3617547750473022 Time taken: 0.20198273658752441\n",
            "Epoch: 12 Batch Number: 112 Loss: 1.3698632717132568 Time taken: 0.21534419059753418\n",
            "Epoch: 12 Batch Number: 113 Loss: 1.347407341003418 Time taken: 0.20250558853149414\n",
            "Epoch: 12 Batch Number: 114 Loss: 1.366916537284851 Time taken: 0.20295953750610352\n",
            "Epoch: 12 Batch Number: 115 Loss: 1.3161412477493286 Time taken: 0.20589518547058105\n",
            "Epoch: 12 Batch Number: 116 Loss: 1.3236029148101807 Time taken: 0.20380926132202148\n",
            "Epoch: 12 Batch Number: 117 Loss: 1.2949212789535522 Time taken: 0.20262479782104492\n",
            "Epoch: 12 Batch Number: 118 Loss: 1.325720191001892 Time taken: 0.20390677452087402\n",
            "Epoch: 12 Batch Number: 119 Loss: 1.3224278688430786 Time taken: 0.20688962936401367\n",
            "Epoch: 12 Batch Number: 120 Loss: 1.3475914001464844 Time taken: 0.20461130142211914\n",
            "Epoch: 12 Batch Number: 121 Loss: 1.3317046165466309 Time taken: 0.21061372756958008\n",
            "Epoch: 12 Batch Number: 122 Loss: 1.3391000032424927 Time taken: 0.21117877960205078\n",
            "Epoch: 12 Batch Number: 123 Loss: 1.2997065782546997 Time taken: 0.21190571784973145\n",
            "Epoch: 12 Batch Number: 124 Loss: 1.338436245918274 Time taken: 0.20452380180358887\n",
            "Epoch: 12 Batch Number: 125 Loss: 1.3222159147262573 Time taken: 0.2007427215576172\n",
            "Epoch: 12 Batch Number: 126 Loss: 1.3077704906463623 Time taken: 0.2032623291015625\n",
            "Epoch: 12 Batch Number: 127 Loss: 1.3260760307312012 Time taken: 0.2021338939666748\n",
            "Epoch: 12 Batch Number: 128 Loss: 1.3189650774002075 Time taken: 0.20084810256958008\n",
            "Epoch: 12 Batch Number: 129 Loss: 1.3121975660324097 Time taken: 0.20255160331726074\n",
            "Epoch: 12 Batch Number: 130 Loss: 1.3070539236068726 Time taken: 0.20196247100830078\n",
            "Epoch: 12 Batch Number: 131 Loss: 1.3379549980163574 Time taken: 0.20863008499145508\n",
            "Epoch: 12 Batch Number: 132 Loss: 1.3186109066009521 Time taken: 0.2005910873413086\n",
            "Epoch: 12 Batch Number: 133 Loss: 1.3032361268997192 Time taken: 0.21418094635009766\n",
            "Epoch: 12 Batch Number: 134 Loss: 1.3214632272720337 Time taken: 0.2069408893585205\n",
            "Epoch: 12 Batch Number: 135 Loss: 1.3205516338348389 Time taken: 0.20423030853271484\n",
            "Epoch: 12 Batch Number: 136 Loss: 1.3026342391967773 Time taken: 0.2020094394683838\n",
            "Epoch: 12 Batch Number: 137 Loss: 1.2818859815597534 Time taken: 0.20314574241638184\n",
            "Epoch: 12 Batch Number: 138 Loss: 1.2932465076446533 Time taken: 0.20954322814941406\n",
            "Epoch: 12 Batch Number: 139 Loss: 1.3156769275665283 Time taken: 0.2016921043395996\n",
            "Epoch: 12 Batch Number: 140 Loss: 1.3337347507476807 Time taken: 0.20328330993652344\n",
            "Epoch: 12 Batch Number: 141 Loss: 1.3315755128860474 Time taken: 0.20123767852783203\n",
            "Epoch: 12 Batch Number: 142 Loss: 1.3233237266540527 Time taken: 0.20673012733459473\n",
            "Epoch: 12 Batch Number: 143 Loss: 1.3644206523895264 Time taken: 0.2116715908050537\n",
            "Epoch: 12 Batch Number: 144 Loss: 1.3116455078125 Time taken: 0.201552152633667\n",
            "Epoch: 12 Batch Number: 145 Loss: 1.3317896127700806 Time taken: 0.20223164558410645\n",
            "Epoch: 12 Batch Number: 146 Loss: 1.3342690467834473 Time taken: 0.20116925239562988\n",
            "Epoch: 12 Batch Number: 147 Loss: 1.3113956451416016 Time taken: 0.20317339897155762\n",
            "Epoch: 12 Batch Number: 148 Loss: 1.3518421649932861 Time taken: 0.21424078941345215\n",
            "Epoch: 12 Batch Number: 149 Loss: 1.3064062595367432 Time taken: 0.20444369316101074\n",
            "Epoch: 12 Batch Number: 150 Loss: 1.3075752258300781 Time taken: 0.21121501922607422\n",
            "Epoch: 12 Batch Number: 151 Loss: 1.3062962293624878 Time taken: 0.20650911331176758\n",
            "Epoch: 12 Batch Number: 152 Loss: 1.3129183053970337 Time taken: 0.20175766944885254\n",
            "Epoch: 12 Batch Number: 153 Loss: 1.3181437253952026 Time taken: 0.20274901390075684\n",
            "Epoch: 12 Batch Number: 154 Loss: 1.3452860116958618 Time taken: 0.2078111171722412\n",
            "Epoch: 12 Batch Number: 155 Loss: 1.3218986988067627 Time taken: 0.20408201217651367\n",
            "Epoch: 12 Batch Number: 156 Loss: 1.324392557144165 Time taken: 0.2023317813873291\n",
            "Epoch: 12 Batch Number: 157 Loss: 1.3321582078933716 Time taken: 0.20343780517578125\n",
            "Epoch: 12 Batch Number: 158 Loss: 1.3223989009857178 Time taken: 0.20249199867248535\n",
            "Epoch: 12 Batch Number: 159 Loss: 1.3154360055923462 Time taken: 0.2017371654510498\n",
            "Epoch: 12 Batch Number: 160 Loss: 1.3008121252059937 Time taken: 0.1998128890991211\n",
            "Epoch: 12 Batch Number: 161 Loss: 1.3301397562026978 Time taken: 0.19974565505981445\n",
            "Epoch: 12 Batch Number: 162 Loss: 1.3239856958389282 Time taken: 0.20363593101501465\n",
            "Epoch: 12 Batch Number: 163 Loss: 1.3343387842178345 Time taken: 0.19944214820861816\n",
            "Epoch: 12 Batch Number: 164 Loss: 1.31544828414917 Time taken: 0.20025897026062012\n",
            "Epoch: 12 Batch Number: 165 Loss: 1.3188905715942383 Time taken: 0.20057129859924316\n",
            "Epoch: 12 Batch Number: 166 Loss: 1.3202227354049683 Time taken: 0.20268464088439941\n",
            "Epoch: 12 Batch Number: 167 Loss: 1.307260274887085 Time taken: 0.20571208000183105\n",
            "Epoch: 12 Batch Number: 168 Loss: 1.3210817575454712 Time taken: 0.20201635360717773\n",
            "Epoch: 12 Batch Number: 169 Loss: 1.3010469675064087 Time taken: 0.2036731243133545\n",
            "Epoch: 12 Batch Number: 170 Loss: 1.310437560081482 Time taken: 0.20471715927124023\n",
            "Epoch: 12 Batch Number: 171 Loss: 1.3133823871612549 Time taken: 0.20735716819763184\n",
            "Epoch: 12 Batch Number: 172 Loss: 1.298965573310852 Time taken: 0.2076888084411621\n",
            "Epoch: 12 Batch Number: 173 Loss: 1.3029979467391968 Time taken: 0.19806432723999023\n",
            "Epoch: 12 Batch Number: 174 Loss: 1.2923160791397095 Time taken: 0.20402240753173828\n",
            "Epoch: 12 Batch Number: 175 Loss: 1.3094513416290283 Time taken: 0.1977558135986328\n",
            "Epoch: 12 Batch Number: 176 Loss: 1.293602466583252 Time taken: 0.1999962329864502\n",
            "Epoch: 12 Batch Number: 177 Loss: 1.3091884851455688 Time taken: 0.20967316627502441\n",
            "Epoch: 12 Batch Number: 178 Loss: 1.3110326528549194 Time taken: 0.198486328125\n",
            "Epoch: 12 Batch Number: 179 Loss: 1.2738584280014038 Time taken: 0.20611262321472168\n",
            "==========================================================================================\n",
            "Start of epoch 13\n",
            "Epoch: 13 Batch Number: 1 Loss: 1.3033506870269775 Time taken: 0.20153236389160156\n",
            "Epoch: 13 Batch Number: 2 Loss: 1.2906789779663086 Time taken: 0.20012545585632324\n",
            "Epoch: 13 Batch Number: 3 Loss: 1.2738947868347168 Time taken: 0.21143841743469238\n",
            "Epoch: 13 Batch Number: 4 Loss: 1.2574450969696045 Time taken: 0.20117592811584473\n",
            "Epoch: 13 Batch Number: 5 Loss: 1.2709283828735352 Time taken: 0.21021199226379395\n",
            "Epoch: 13 Batch Number: 6 Loss: 1.263440728187561 Time taken: 0.20026588439941406\n",
            "Epoch: 13 Batch Number: 7 Loss: 1.265878677368164 Time taken: 0.20545125007629395\n",
            "Epoch: 13 Batch Number: 8 Loss: 1.2685548067092896 Time taken: 0.20060968399047852\n",
            "Epoch: 13 Batch Number: 9 Loss: 1.2782461643218994 Time taken: 0.20082616806030273\n",
            "Epoch: 13 Batch Number: 10 Loss: 1.2715609073638916 Time taken: 0.20277094841003418\n",
            "Epoch: 13 Batch Number: 11 Loss: 1.2951810359954834 Time taken: 0.20262908935546875\n",
            "Epoch: 13 Batch Number: 12 Loss: 1.2837022542953491 Time taken: 0.19987225532531738\n",
            "Epoch: 13 Batch Number: 13 Loss: 1.2932448387145996 Time taken: 0.19942307472229004\n",
            "Epoch: 13 Batch Number: 14 Loss: 1.3107867240905762 Time taken: 0.20976853370666504\n",
            "Epoch: 13 Batch Number: 15 Loss: 1.30010187625885 Time taken: 0.2013542652130127\n",
            "Epoch: 13 Batch Number: 16 Loss: 1.300186276435852 Time taken: 0.20143556594848633\n",
            "Epoch: 13 Batch Number: 17 Loss: 1.291387915611267 Time taken: 0.20444345474243164\n",
            "Epoch: 13 Batch Number: 18 Loss: 1.288264274597168 Time taken: 0.19893574714660645\n",
            "Epoch: 13 Batch Number: 19 Loss: 1.2883505821228027 Time taken: 0.20148301124572754\n",
            "Epoch: 13 Batch Number: 20 Loss: 1.2725399732589722 Time taken: 0.20219087600708008\n",
            "Epoch: 13 Batch Number: 21 Loss: 1.3045979738235474 Time taken: 0.20011663436889648\n",
            "Epoch: 13 Batch Number: 22 Loss: 1.268791913986206 Time taken: 0.20102167129516602\n",
            "Epoch: 13 Batch Number: 23 Loss: 1.2866365909576416 Time taken: 0.20235109329223633\n",
            "Epoch: 13 Batch Number: 24 Loss: 1.3253322839736938 Time taken: 0.20872926712036133\n",
            "Epoch: 13 Batch Number: 25 Loss: 1.2868139743804932 Time taken: 0.20294404029846191\n",
            "Epoch: 13 Batch Number: 26 Loss: 1.3077583312988281 Time taken: 0.20071911811828613\n",
            "Epoch: 13 Batch Number: 27 Loss: 1.2887738943099976 Time taken: 0.20079922676086426\n",
            "Epoch: 13 Batch Number: 28 Loss: 1.2889831066131592 Time taken: 0.1997673511505127\n",
            "Epoch: 13 Batch Number: 29 Loss: 1.304898977279663 Time taken: 0.20227265357971191\n",
            "Epoch: 13 Batch Number: 30 Loss: 1.2875021696090698 Time taken: 0.19888806343078613\n",
            "Epoch: 13 Batch Number: 31 Loss: 1.300222396850586 Time taken: 0.2012941837310791\n",
            "Epoch: 13 Batch Number: 32 Loss: 1.2984431982040405 Time taken: 0.20708703994750977\n",
            "Epoch: 13 Batch Number: 33 Loss: 1.3197100162506104 Time taken: 0.20163512229919434\n",
            "Epoch: 13 Batch Number: 34 Loss: 1.3055237531661987 Time taken: 0.20705056190490723\n",
            "Epoch: 13 Batch Number: 35 Loss: 1.3064483404159546 Time taken: 0.20207858085632324\n",
            "Epoch: 13 Batch Number: 36 Loss: 1.2863261699676514 Time taken: 0.20545601844787598\n",
            "Epoch: 13 Batch Number: 37 Loss: 1.3047131299972534 Time taken: 0.20563387870788574\n",
            "Epoch: 13 Batch Number: 38 Loss: 1.309465765953064 Time taken: 0.20650625228881836\n",
            "Epoch: 13 Batch Number: 39 Loss: 1.3235034942626953 Time taken: 0.20182156562805176\n",
            "Epoch: 13 Batch Number: 40 Loss: 1.3016940355300903 Time taken: 0.20171070098876953\n",
            "Epoch: 13 Batch Number: 41 Loss: 1.300978183746338 Time taken: 0.20062637329101562\n",
            "Epoch: 13 Batch Number: 42 Loss: 1.2751190662384033 Time taken: 0.22289681434631348\n",
            "Epoch: 13 Batch Number: 43 Loss: 1.308496356010437 Time taken: 0.20122003555297852\n",
            "Epoch: 13 Batch Number: 44 Loss: 1.2979613542556763 Time taken: 0.2001628875732422\n",
            "Epoch: 13 Batch Number: 45 Loss: 1.2936410903930664 Time taken: 0.2069263458251953\n",
            "Epoch: 13 Batch Number: 46 Loss: 1.3011598587036133 Time taken: 0.2013237476348877\n",
            "Epoch: 13 Batch Number: 47 Loss: 1.2937513589859009 Time taken: 0.19919347763061523\n",
            "Epoch: 13 Batch Number: 48 Loss: 1.315755844116211 Time taken: 0.20145821571350098\n",
            "Epoch: 13 Batch Number: 49 Loss: 1.3272510766983032 Time taken: 0.19861316680908203\n",
            "Epoch: 13 Batch Number: 50 Loss: 1.290303349494934 Time taken: 0.20489811897277832\n",
            "Epoch: 13 Batch Number: 51 Loss: 1.3043838739395142 Time taken: 0.20475268363952637\n",
            "Epoch: 13 Batch Number: 52 Loss: 1.3091318607330322 Time taken: 0.19876551628112793\n",
            "Epoch: 13 Batch Number: 53 Loss: 1.3108117580413818 Time taken: 0.210005521774292\n",
            "Epoch: 13 Batch Number: 54 Loss: 1.304933786392212 Time taken: 0.20752835273742676\n",
            "Epoch: 13 Batch Number: 55 Loss: 1.3041236400604248 Time taken: 0.20038986206054688\n",
            "Epoch: 13 Batch Number: 56 Loss: 1.2766621112823486 Time taken: 0.1997065544128418\n",
            "Epoch: 13 Batch Number: 57 Loss: 1.3056553602218628 Time taken: 0.19928836822509766\n",
            "Epoch: 13 Batch Number: 58 Loss: 1.2985303401947021 Time taken: 0.2016305923461914\n",
            "Epoch: 13 Batch Number: 59 Loss: 1.3041516542434692 Time taken: 0.20023107528686523\n",
            "Epoch: 13 Batch Number: 60 Loss: 1.2885454893112183 Time taken: 0.19847655296325684\n",
            "Epoch: 13 Batch Number: 61 Loss: 1.2920455932617188 Time taken: 0.19905567169189453\n",
            "Epoch: 13 Batch Number: 62 Loss: 1.2987483739852905 Time taken: 0.20339226722717285\n",
            "Epoch: 13 Batch Number: 63 Loss: 1.2955107688903809 Time taken: 0.2061152458190918\n",
            "Epoch: 13 Batch Number: 64 Loss: 1.3046257495880127 Time taken: 0.20679640769958496\n",
            "Epoch: 13 Batch Number: 65 Loss: 1.2974436283111572 Time taken: 0.19862961769104004\n",
            "Epoch: 13 Batch Number: 66 Loss: 1.270640254020691 Time taken: 0.2128767967224121\n",
            "Epoch: 13 Batch Number: 67 Loss: 1.3011811971664429 Time taken: 0.20111656188964844\n",
            "Epoch: 13 Batch Number: 68 Loss: 1.3021868467330933 Time taken: 0.20031952857971191\n",
            "Epoch: 13 Batch Number: 69 Loss: 1.3115233182907104 Time taken: 0.1996326446533203\n",
            "Epoch: 13 Batch Number: 70 Loss: 1.322155475616455 Time taken: 0.19957423210144043\n",
            "Epoch: 13 Batch Number: 71 Loss: 1.2958835363388062 Time taken: 0.20601367950439453\n",
            "Epoch: 13 Batch Number: 72 Loss: 1.3362185955047607 Time taken: 0.20545530319213867\n",
            "Epoch: 13 Batch Number: 73 Loss: 1.2922297716140747 Time taken: 0.2059321403503418\n",
            "Epoch: 13 Batch Number: 74 Loss: 1.3154826164245605 Time taken: 0.2026076316833496\n",
            "Epoch: 13 Batch Number: 75 Loss: 1.3056676387786865 Time taken: 0.2041943073272705\n",
            "Epoch: 13 Batch Number: 76 Loss: 1.2902263402938843 Time taken: 0.20862054824829102\n",
            "Epoch: 13 Batch Number: 77 Loss: 1.2939640283584595 Time taken: 0.207780122756958\n",
            "Epoch: 13 Batch Number: 78 Loss: 1.2816399335861206 Time taken: 0.1998288631439209\n",
            "Epoch: 13 Batch Number: 79 Loss: 1.2965140342712402 Time taken: 0.20083880424499512\n",
            "Epoch: 13 Batch Number: 80 Loss: 1.2877352237701416 Time taken: 0.20366120338439941\n",
            "Epoch: 13 Batch Number: 81 Loss: 1.302423357963562 Time taken: 0.2035200595855713\n",
            "Epoch: 13 Batch Number: 82 Loss: 1.287987232208252 Time taken: 0.2014765739440918\n",
            "Epoch: 13 Batch Number: 83 Loss: 1.2929145097732544 Time taken: 0.20553851127624512\n",
            "Epoch: 13 Batch Number: 84 Loss: 1.2681533098220825 Time taken: 0.20400142669677734\n",
            "Epoch: 13 Batch Number: 85 Loss: 1.2573471069335938 Time taken: 0.20472407341003418\n",
            "Epoch: 13 Batch Number: 86 Loss: 1.2703477144241333 Time taken: 0.20008206367492676\n",
            "Epoch: 13 Batch Number: 87 Loss: 1.2821663618087769 Time taken: 0.20907282829284668\n",
            "Epoch: 13 Batch Number: 88 Loss: 1.2860665321350098 Time taken: 0.20093083381652832\n",
            "Epoch: 13 Batch Number: 89 Loss: 1.2902816534042358 Time taken: 0.20118188858032227\n",
            "Epoch: 13 Batch Number: 90 Loss: 1.2917289733886719 Time taken: 0.20265626907348633\n",
            "Epoch: 13 Batch Number: 91 Loss: 1.2979179620742798 Time taken: 0.20093250274658203\n",
            "Epoch: 13 Batch Number: 92 Loss: 1.303742527961731 Time taken: 0.22111105918884277\n",
            "Epoch: 13 Batch Number: 93 Loss: 1.2957699298858643 Time taken: 0.1999201774597168\n",
            "Epoch: 13 Batch Number: 94 Loss: 1.296736240386963 Time taken: 0.20079350471496582\n",
            "Epoch: 13 Batch Number: 95 Loss: 1.3169878721237183 Time taken: 0.199815034866333\n",
            "Epoch: 13 Batch Number: 96 Loss: 1.315533995628357 Time taken: 0.1993858814239502\n",
            "Epoch: 13 Batch Number: 97 Loss: 1.3233458995819092 Time taken: 0.2000265121459961\n",
            "Epoch: 13 Batch Number: 98 Loss: 1.3162355422973633 Time taken: 0.20075392723083496\n",
            "Epoch: 13 Batch Number: 99 Loss: 1.3118126392364502 Time taken: 0.1984100341796875\n",
            "Epoch: 13 Batch Number: 100 Loss: 1.3233942985534668 Time taken: 0.2033524513244629\n",
            "Epoch: 13 Batch Number: 101 Loss: 1.3012428283691406 Time taken: 0.2045457363128662\n",
            "Epoch: 13 Batch Number: 102 Loss: 1.3109198808670044 Time taken: 0.1995677947998047\n",
            "Epoch: 13 Batch Number: 103 Loss: 1.2959070205688477 Time taken: 0.20497870445251465\n",
            "Epoch: 13 Batch Number: 104 Loss: 1.3023027181625366 Time taken: 0.19817495346069336\n",
            "Epoch: 13 Batch Number: 105 Loss: 1.328621745109558 Time taken: 0.20659828186035156\n",
            "Epoch: 13 Batch Number: 106 Loss: 1.327333688735962 Time taken: 0.20199990272521973\n",
            "Epoch: 13 Batch Number: 107 Loss: 1.3315757513046265 Time taken: 0.19916272163391113\n",
            "Epoch: 13 Batch Number: 108 Loss: 1.354732632637024 Time taken: 0.19939970970153809\n",
            "Epoch: 13 Batch Number: 109 Loss: 1.3146976232528687 Time taken: 0.20131611824035645\n",
            "Epoch: 13 Batch Number: 110 Loss: 1.3601124286651611 Time taken: 0.20483708381652832\n",
            "Epoch: 13 Batch Number: 111 Loss: 1.3396862745285034 Time taken: 0.20881414413452148\n",
            "Epoch: 13 Batch Number: 112 Loss: 1.3459755182266235 Time taken: 0.203963041305542\n",
            "Epoch: 13 Batch Number: 113 Loss: 1.3366938829421997 Time taken: 0.1994173526763916\n",
            "Epoch: 13 Batch Number: 114 Loss: 1.3286126852035522 Time taken: 0.19936037063598633\n",
            "Epoch: 13 Batch Number: 115 Loss: 1.3297302722930908 Time taken: 0.20909452438354492\n",
            "Epoch: 13 Batch Number: 116 Loss: 1.3224531412124634 Time taken: 0.20965290069580078\n",
            "Epoch: 13 Batch Number: 117 Loss: 1.2926706075668335 Time taken: 0.20504426956176758\n",
            "Epoch: 13 Batch Number: 118 Loss: 1.310745358467102 Time taken: 0.20106720924377441\n",
            "Epoch: 13 Batch Number: 119 Loss: 1.3143895864486694 Time taken: 0.19834542274475098\n",
            "Epoch: 13 Batch Number: 120 Loss: 1.320859670639038 Time taken: 0.20859766006469727\n",
            "Epoch: 13 Batch Number: 121 Loss: 1.3047988414764404 Time taken: 0.20779681205749512\n",
            "Epoch: 13 Batch Number: 122 Loss: 1.3041691780090332 Time taken: 0.200819730758667\n",
            "Epoch: 13 Batch Number: 123 Loss: 1.3119699954986572 Time taken: 0.19913148880004883\n",
            "Epoch: 13 Batch Number: 124 Loss: 1.2880992889404297 Time taken: 0.20352745056152344\n",
            "Epoch: 13 Batch Number: 125 Loss: 1.3108793497085571 Time taken: 0.20183682441711426\n",
            "Epoch: 13 Batch Number: 126 Loss: 1.3158173561096191 Time taken: 0.20206356048583984\n",
            "Epoch: 13 Batch Number: 127 Loss: 1.2894071340560913 Time taken: 0.19956040382385254\n",
            "Epoch: 13 Batch Number: 128 Loss: 1.297521710395813 Time taken: 0.19909930229187012\n",
            "Epoch: 13 Batch Number: 129 Loss: 1.2900739908218384 Time taken: 0.20064544677734375\n",
            "Epoch: 13 Batch Number: 130 Loss: 1.320063591003418 Time taken: 0.20008277893066406\n",
            "Epoch: 13 Batch Number: 131 Loss: 1.302868366241455 Time taken: 0.19919943809509277\n",
            "Epoch: 13 Batch Number: 132 Loss: 1.3172049522399902 Time taken: 0.19862985610961914\n",
            "Epoch: 13 Batch Number: 133 Loss: 1.2826687097549438 Time taken: 0.19964885711669922\n",
            "Epoch: 13 Batch Number: 134 Loss: 1.2992360591888428 Time taken: 0.20661377906799316\n",
            "Epoch: 13 Batch Number: 135 Loss: 1.2952992916107178 Time taken: 0.2007439136505127\n",
            "Epoch: 13 Batch Number: 136 Loss: 1.2954844236373901 Time taken: 0.2115931510925293\n",
            "Epoch: 13 Batch Number: 137 Loss: 1.2837742567062378 Time taken: 0.20604205131530762\n",
            "Epoch: 13 Batch Number: 138 Loss: 1.2682487964630127 Time taken: 0.20134758949279785\n",
            "Epoch: 13 Batch Number: 139 Loss: 1.3118553161621094 Time taken: 0.2095623016357422\n",
            "Epoch: 13 Batch Number: 140 Loss: 1.3009861707687378 Time taken: 0.20792126655578613\n",
            "Epoch: 13 Batch Number: 141 Loss: 1.330243706703186 Time taken: 0.19959807395935059\n",
            "Epoch: 13 Batch Number: 142 Loss: 1.3679478168487549 Time taken: 0.20868706703186035\n",
            "Epoch: 13 Batch Number: 143 Loss: 1.320359468460083 Time taken: 0.2005453109741211\n",
            "Epoch: 13 Batch Number: 144 Loss: 1.312078833580017 Time taken: 0.20729279518127441\n",
            "Epoch: 13 Batch Number: 145 Loss: 1.300736665725708 Time taken: 0.20724940299987793\n",
            "Epoch: 13 Batch Number: 146 Loss: 1.3270683288574219 Time taken: 0.20497369766235352\n",
            "Epoch: 13 Batch Number: 147 Loss: 1.2821964025497437 Time taken: 0.20070242881774902\n",
            "Epoch: 13 Batch Number: 148 Loss: 1.321054458618164 Time taken: 0.2029411792755127\n",
            "Epoch: 13 Batch Number: 149 Loss: 1.3012149333953857 Time taken: 0.20687341690063477\n",
            "Epoch: 13 Batch Number: 150 Loss: 1.2924612760543823 Time taken: 0.20446276664733887\n",
            "Epoch: 13 Batch Number: 151 Loss: 1.3077644109725952 Time taken: 0.201019287109375\n",
            "Epoch: 13 Batch Number: 152 Loss: 1.282601237297058 Time taken: 0.2004413604736328\n",
            "Epoch: 13 Batch Number: 153 Loss: 1.3032211065292358 Time taken: 0.20066142082214355\n",
            "Epoch: 13 Batch Number: 154 Loss: 1.3015832901000977 Time taken: 0.2109968662261963\n",
            "Epoch: 13 Batch Number: 155 Loss: 1.3084560632705688 Time taken: 0.20123767852783203\n",
            "Epoch: 13 Batch Number: 156 Loss: 1.312315821647644 Time taken: 0.20089340209960938\n",
            "Epoch: 13 Batch Number: 157 Loss: 1.3164645433425903 Time taken: 0.20759940147399902\n",
            "Epoch: 13 Batch Number: 158 Loss: 1.3096673488616943 Time taken: 0.20631074905395508\n",
            "Epoch: 13 Batch Number: 159 Loss: 1.3423418998718262 Time taken: 0.2148730754852295\n",
            "Epoch: 13 Batch Number: 160 Loss: 1.315598726272583 Time taken: 0.1998124122619629\n",
            "Epoch: 13 Batch Number: 161 Loss: 1.2974225282669067 Time taken: 0.20010137557983398\n",
            "Epoch: 13 Batch Number: 162 Loss: 1.3205219507217407 Time taken: 0.20533251762390137\n",
            "Epoch: 13 Batch Number: 163 Loss: 1.3171361684799194 Time taken: 0.20295214653015137\n",
            "Epoch: 13 Batch Number: 164 Loss: 1.322581171989441 Time taken: 0.2076396942138672\n",
            "Epoch: 13 Batch Number: 165 Loss: 1.3113278150558472 Time taken: 0.20931053161621094\n",
            "Epoch: 13 Batch Number: 166 Loss: 1.3008123636245728 Time taken: 0.20008540153503418\n",
            "Epoch: 13 Batch Number: 167 Loss: 1.294137954711914 Time taken: 0.2041170597076416\n",
            "Epoch: 13 Batch Number: 168 Loss: 1.2925965785980225 Time taken: 0.20499324798583984\n",
            "Epoch: 13 Batch Number: 169 Loss: 1.3110127449035645 Time taken: 0.2088325023651123\n",
            "Epoch: 13 Batch Number: 170 Loss: 1.3076316118240356 Time taken: 0.19951200485229492\n",
            "Epoch: 13 Batch Number: 171 Loss: 1.3279056549072266 Time taken: 0.2021169662475586\n",
            "Epoch: 13 Batch Number: 172 Loss: 1.296400547027588 Time taken: 0.20710349082946777\n",
            "Epoch: 13 Batch Number: 173 Loss: 1.2755080461502075 Time taken: 0.20694684982299805\n",
            "Epoch: 13 Batch Number: 174 Loss: 1.2824397087097168 Time taken: 0.20830059051513672\n",
            "Epoch: 13 Batch Number: 175 Loss: 1.2892638444900513 Time taken: 0.20108747482299805\n",
            "Epoch: 13 Batch Number: 176 Loss: 1.2959113121032715 Time taken: 0.19848227500915527\n",
            "Epoch: 13 Batch Number: 177 Loss: 1.2849483489990234 Time taken: 0.20619916915893555\n",
            "Epoch: 13 Batch Number: 178 Loss: 1.2900840044021606 Time taken: 0.2127387523651123\n",
            "Epoch: 13 Batch Number: 179 Loss: 1.2676609754562378 Time taken: 0.2027750015258789\n",
            "==========================================================================================\n",
            "Start of epoch 14\n",
            "Epoch: 14 Batch Number: 1 Loss: 1.3077013492584229 Time taken: 0.2018721103668213\n",
            "Epoch: 14 Batch Number: 2 Loss: 1.2619065046310425 Time taken: 0.20325064659118652\n",
            "Epoch: 14 Batch Number: 3 Loss: 1.2631484270095825 Time taken: 0.205841064453125\n",
            "Epoch: 14 Batch Number: 4 Loss: 1.2508732080459595 Time taken: 0.21007347106933594\n",
            "Epoch: 14 Batch Number: 5 Loss: 1.271376132965088 Time taken: 0.2022087574005127\n",
            "Epoch: 14 Batch Number: 6 Loss: 1.2581932544708252 Time taken: 0.20376276969909668\n",
            "Epoch: 14 Batch Number: 7 Loss: 1.2699158191680908 Time taken: 0.20769214630126953\n",
            "Epoch: 14 Batch Number: 8 Loss: 1.2572227716445923 Time taken: 0.19989919662475586\n",
            "Epoch: 14 Batch Number: 9 Loss: 1.2620197534561157 Time taken: 0.20592498779296875\n",
            "Epoch: 14 Batch Number: 10 Loss: 1.2584301233291626 Time taken: 0.19904470443725586\n",
            "Epoch: 14 Batch Number: 11 Loss: 1.2627862691879272 Time taken: 0.20061755180358887\n",
            "Epoch: 14 Batch Number: 12 Loss: 1.2615169286727905 Time taken: 0.19841217994689941\n",
            "Epoch: 14 Batch Number: 13 Loss: 1.281451940536499 Time taken: 0.20139241218566895\n",
            "Epoch: 14 Batch Number: 14 Loss: 1.2756881713867188 Time taken: 0.20839500427246094\n",
            "Epoch: 14 Batch Number: 15 Loss: 1.272135853767395 Time taken: 0.19926667213439941\n",
            "Epoch: 14 Batch Number: 16 Loss: 1.298134446144104 Time taken: 0.206587553024292\n",
            "Epoch: 14 Batch Number: 17 Loss: 1.2703548669815063 Time taken: 0.20025944709777832\n",
            "Epoch: 14 Batch Number: 18 Loss: 1.2753944396972656 Time taken: 0.2120494842529297\n",
            "Epoch: 14 Batch Number: 19 Loss: 1.2884763479232788 Time taken: 0.20093846321105957\n",
            "Epoch: 14 Batch Number: 20 Loss: 1.2615467309951782 Time taken: 0.2030625343322754\n",
            "Epoch: 14 Batch Number: 21 Loss: 1.2622264623641968 Time taken: 0.2005147933959961\n",
            "Epoch: 14 Batch Number: 22 Loss: 1.2420947551727295 Time taken: 0.20282745361328125\n",
            "Epoch: 14 Batch Number: 23 Loss: 1.2784514427185059 Time taken: 0.2156367301940918\n",
            "Epoch: 14 Batch Number: 24 Loss: 1.2983691692352295 Time taken: 0.20019078254699707\n",
            "Epoch: 14 Batch Number: 25 Loss: 1.2979528903961182 Time taken: 0.19951081275939941\n",
            "Epoch: 14 Batch Number: 26 Loss: 1.309564471244812 Time taken: 0.2033991813659668\n",
            "Epoch: 14 Batch Number: 27 Loss: 1.308361530303955 Time taken: 0.2033545970916748\n",
            "Epoch: 14 Batch Number: 28 Loss: 1.2716615200042725 Time taken: 0.20769500732421875\n",
            "Epoch: 14 Batch Number: 29 Loss: 1.274887204170227 Time taken: 0.20564055442810059\n",
            "Epoch: 14 Batch Number: 30 Loss: 1.3008568286895752 Time taken: 0.20110344886779785\n",
            "Epoch: 14 Batch Number: 31 Loss: 1.2998273372650146 Time taken: 0.19971060752868652\n",
            "Epoch: 14 Batch Number: 32 Loss: 1.2833749055862427 Time taken: 0.20116114616394043\n",
            "Epoch: 14 Batch Number: 33 Loss: 1.280248999595642 Time taken: 0.22067022323608398\n",
            "Epoch: 14 Batch Number: 34 Loss: 1.3042752742767334 Time taken: 0.2083890438079834\n",
            "Epoch: 14 Batch Number: 35 Loss: 1.2949362993240356 Time taken: 0.2024242877960205\n",
            "Epoch: 14 Batch Number: 36 Loss: 1.294355034828186 Time taken: 0.2073984146118164\n",
            "Epoch: 14 Batch Number: 37 Loss: 1.2841722965240479 Time taken: 0.2077469825744629\n",
            "Epoch: 14 Batch Number: 38 Loss: 1.3259544372558594 Time taken: 0.20407629013061523\n",
            "Epoch: 14 Batch Number: 39 Loss: 1.303687572479248 Time taken: 0.20036530494689941\n",
            "Epoch: 14 Batch Number: 40 Loss: 1.3110281229019165 Time taken: 0.2042233943939209\n",
            "Epoch: 14 Batch Number: 41 Loss: 1.3150444030761719 Time taken: 0.2065725326538086\n",
            "Epoch: 14 Batch Number: 42 Loss: 1.2753206491470337 Time taken: 0.20907998085021973\n",
            "Epoch: 14 Batch Number: 43 Loss: 1.292702078819275 Time taken: 0.20038247108459473\n",
            "Epoch: 14 Batch Number: 44 Loss: 1.2810207605361938 Time taken: 0.20376062393188477\n",
            "Epoch: 14 Batch Number: 45 Loss: 1.2774016857147217 Time taken: 0.20956659317016602\n",
            "Epoch: 14 Batch Number: 46 Loss: 1.2712788581848145 Time taken: 0.20033931732177734\n",
            "Epoch: 14 Batch Number: 47 Loss: 1.28046452999115 Time taken: 0.21315860748291016\n",
            "Epoch: 14 Batch Number: 48 Loss: 1.3270460367202759 Time taken: 0.20554614067077637\n",
            "Epoch: 14 Batch Number: 49 Loss: 1.2918747663497925 Time taken: 0.2107563018798828\n",
            "Epoch: 14 Batch Number: 50 Loss: 1.2889131307601929 Time taken: 0.1991124153137207\n",
            "Epoch: 14 Batch Number: 51 Loss: 1.3076813220977783 Time taken: 0.20055937767028809\n",
            "Epoch: 14 Batch Number: 52 Loss: 1.2961724996566772 Time taken: 0.2087390422821045\n",
            "Epoch: 14 Batch Number: 53 Loss: 1.2893744707107544 Time taken: 0.20435690879821777\n",
            "Epoch: 14 Batch Number: 54 Loss: 1.279654860496521 Time taken: 0.20457124710083008\n",
            "Epoch: 14 Batch Number: 55 Loss: 1.275536298751831 Time taken: 0.2017357349395752\n",
            "Epoch: 14 Batch Number: 56 Loss: 1.2972443103790283 Time taken: 0.20101118087768555\n",
            "Epoch: 14 Batch Number: 57 Loss: 1.2979159355163574 Time taken: 0.2189943790435791\n",
            "Epoch: 14 Batch Number: 58 Loss: 1.2873566150665283 Time taken: 0.203826904296875\n",
            "Epoch: 14 Batch Number: 59 Loss: 1.2653063535690308 Time taken: 0.2044057846069336\n",
            "Epoch: 14 Batch Number: 60 Loss: 1.2839934825897217 Time taken: 0.19975805282592773\n",
            "Epoch: 14 Batch Number: 61 Loss: 1.2851039171218872 Time taken: 0.2039356231689453\n",
            "Epoch: 14 Batch Number: 62 Loss: 1.2807788848876953 Time taken: 0.2143113613128662\n",
            "Epoch: 14 Batch Number: 63 Loss: 1.2718549966812134 Time taken: 0.20565557479858398\n",
            "Epoch: 14 Batch Number: 64 Loss: 1.2908871173858643 Time taken: 0.2070770263671875\n",
            "Epoch: 14 Batch Number: 65 Loss: 1.2780907154083252 Time taken: 0.20462536811828613\n",
            "Epoch: 14 Batch Number: 66 Loss: 1.2924665212631226 Time taken: 0.20081281661987305\n",
            "Epoch: 14 Batch Number: 67 Loss: 1.3130468130111694 Time taken: 0.19880270957946777\n",
            "Epoch: 14 Batch Number: 68 Loss: 1.2860972881317139 Time taken: 0.20049118995666504\n",
            "Epoch: 14 Batch Number: 69 Loss: 1.3109526634216309 Time taken: 0.2036421298980713\n",
            "Epoch: 14 Batch Number: 70 Loss: 1.2823902368545532 Time taken: 0.20454668998718262\n",
            "Epoch: 14 Batch Number: 71 Loss: 1.281850814819336 Time taken: 0.20393157005310059\n",
            "Epoch: 14 Batch Number: 72 Loss: 1.284653902053833 Time taken: 0.1999340057373047\n",
            "Epoch: 14 Batch Number: 73 Loss: 1.2993272542953491 Time taken: 0.20245981216430664\n",
            "Epoch: 14 Batch Number: 74 Loss: 1.2955151796340942 Time taken: 0.20060515403747559\n",
            "Epoch: 14 Batch Number: 75 Loss: 1.2794955968856812 Time taken: 0.2013232707977295\n",
            "Epoch: 14 Batch Number: 76 Loss: 1.2746027708053589 Time taken: 0.2095179557800293\n",
            "Epoch: 14 Batch Number: 77 Loss: 1.2592048645019531 Time taken: 0.20061826705932617\n",
            "Epoch: 14 Batch Number: 78 Loss: 1.2742197513580322 Time taken: 0.20064663887023926\n",
            "Epoch: 14 Batch Number: 79 Loss: 1.271228313446045 Time taken: 0.20035052299499512\n",
            "Epoch: 14 Batch Number: 80 Loss: 1.2933324575424194 Time taken: 0.20028281211853027\n",
            "Epoch: 14 Batch Number: 81 Loss: 1.2898564338684082 Time taken: 0.2010955810546875\n",
            "Epoch: 14 Batch Number: 82 Loss: 1.2777856588363647 Time taken: 0.20038652420043945\n",
            "Epoch: 14 Batch Number: 83 Loss: 1.2777891159057617 Time taken: 0.2017226219177246\n",
            "Epoch: 14 Batch Number: 84 Loss: 1.2632468938827515 Time taken: 0.20485496520996094\n",
            "Epoch: 14 Batch Number: 85 Loss: 1.2831662893295288 Time taken: 0.20784354209899902\n",
            "Epoch: 14 Batch Number: 86 Loss: 1.2600109577178955 Time taken: 0.211989164352417\n",
            "Epoch: 14 Batch Number: 87 Loss: 1.270945429801941 Time taken: 0.20052027702331543\n",
            "Epoch: 14 Batch Number: 88 Loss: 1.2747440338134766 Time taken: 0.20026707649230957\n",
            "Epoch: 14 Batch Number: 89 Loss: 1.243268609046936 Time taken: 0.2003939151763916\n",
            "Epoch: 14 Batch Number: 90 Loss: 1.267289400100708 Time taken: 0.20071959495544434\n",
            "Epoch: 14 Batch Number: 91 Loss: 1.275321125984192 Time taken: 0.20766210556030273\n",
            "Epoch: 14 Batch Number: 92 Loss: 1.2945994138717651 Time taken: 0.19958209991455078\n",
            "Epoch: 14 Batch Number: 93 Loss: 1.309158444404602 Time taken: 0.20201539993286133\n",
            "Epoch: 14 Batch Number: 94 Loss: 1.299387812614441 Time taken: 0.20095038414001465\n",
            "Epoch: 14 Batch Number: 95 Loss: 1.284663200378418 Time taken: 0.20041608810424805\n",
            "Epoch: 14 Batch Number: 96 Loss: 1.2884275913238525 Time taken: 0.21543598175048828\n",
            "Epoch: 14 Batch Number: 97 Loss: 1.2917613983154297 Time taken: 0.1978440284729004\n",
            "Epoch: 14 Batch Number: 98 Loss: 1.298081398010254 Time taken: 0.200026273727417\n",
            "Epoch: 14 Batch Number: 99 Loss: 1.3125362396240234 Time taken: 0.20119309425354004\n",
            "Epoch: 14 Batch Number: 100 Loss: 1.2983756065368652 Time taken: 0.20026898384094238\n",
            "Epoch: 14 Batch Number: 101 Loss: 1.2818479537963867 Time taken: 0.21005487442016602\n",
            "Epoch: 14 Batch Number: 102 Loss: 1.3002721071243286 Time taken: 0.20110535621643066\n",
            "Epoch: 14 Batch Number: 103 Loss: 1.2892488241195679 Time taken: 0.19993114471435547\n",
            "Epoch: 14 Batch Number: 104 Loss: 1.2898269891738892 Time taken: 0.19952988624572754\n",
            "Epoch: 14 Batch Number: 105 Loss: 1.3325237035751343 Time taken: 0.20202040672302246\n",
            "Epoch: 14 Batch Number: 106 Loss: 1.3095595836639404 Time taken: 0.20926356315612793\n",
            "Epoch: 14 Batch Number: 107 Loss: 1.3352142572402954 Time taken: 0.19973349571228027\n",
            "Epoch: 14 Batch Number: 108 Loss: 1.316853404045105 Time taken: 0.20099496841430664\n",
            "Epoch: 14 Batch Number: 109 Loss: 1.3418844938278198 Time taken: 0.1993718147277832\n",
            "Epoch: 14 Batch Number: 110 Loss: 1.3409607410430908 Time taken: 0.19953298568725586\n",
            "Epoch: 14 Batch Number: 111 Loss: 1.3226242065429688 Time taken: 0.20783329010009766\n",
            "Epoch: 14 Batch Number: 112 Loss: 1.3296116590499878 Time taken: 0.1977825164794922\n",
            "Epoch: 14 Batch Number: 113 Loss: 1.3272267580032349 Time taken: 0.19378232955932617\n",
            "Epoch: 14 Batch Number: 114 Loss: 1.3479678630828857 Time taken: 0.20070314407348633\n",
            "Epoch: 14 Batch Number: 115 Loss: 1.3047966957092285 Time taken: 0.2002277374267578\n",
            "Epoch: 14 Batch Number: 116 Loss: 1.3208909034729004 Time taken: 0.20572948455810547\n",
            "Epoch: 14 Batch Number: 117 Loss: 1.289958119392395 Time taken: 0.20186448097229004\n",
            "Epoch: 14 Batch Number: 118 Loss: 1.293737530708313 Time taken: 0.20061326026916504\n",
            "Epoch: 14 Batch Number: 119 Loss: 1.287423849105835 Time taken: 0.2010965347290039\n",
            "Epoch: 14 Batch Number: 120 Loss: 1.2989946603775024 Time taken: 0.2023792266845703\n",
            "Epoch: 14 Batch Number: 121 Loss: 1.316698431968689 Time taken: 0.19966626167297363\n",
            "Epoch: 14 Batch Number: 122 Loss: 1.2864187955856323 Time taken: 0.20145583152770996\n",
            "Epoch: 14 Batch Number: 123 Loss: 1.2702754735946655 Time taken: 0.20156097412109375\n",
            "Epoch: 14 Batch Number: 124 Loss: 1.2993382215499878 Time taken: 0.2003476619720459\n",
            "Epoch: 14 Batch Number: 125 Loss: 1.2850053310394287 Time taken: 0.20362305641174316\n",
            "Epoch: 14 Batch Number: 126 Loss: 1.2884055376052856 Time taken: 0.20403075218200684\n",
            "Epoch: 14 Batch Number: 127 Loss: 1.3011999130249023 Time taken: 0.20440196990966797\n",
            "Epoch: 14 Batch Number: 128 Loss: 1.281327724456787 Time taken: 0.20618009567260742\n",
            "Epoch: 14 Batch Number: 129 Loss: 1.285285472869873 Time taken: 0.20239043235778809\n",
            "Epoch: 14 Batch Number: 130 Loss: 1.278037190437317 Time taken: 0.2048804759979248\n",
            "Epoch: 14 Batch Number: 131 Loss: 1.3178720474243164 Time taken: 0.2045586109161377\n",
            "Epoch: 14 Batch Number: 132 Loss: 1.295296549797058 Time taken: 0.19928455352783203\n",
            "Epoch: 14 Batch Number: 133 Loss: 1.2981257438659668 Time taken: 0.20122694969177246\n",
            "Epoch: 14 Batch Number: 134 Loss: 1.3096773624420166 Time taken: 0.20146703720092773\n",
            "Epoch: 14 Batch Number: 135 Loss: 1.2809606790542603 Time taken: 0.20915722846984863\n",
            "Epoch: 14 Batch Number: 136 Loss: 1.2791712284088135 Time taken: 0.19919276237487793\n",
            "Epoch: 14 Batch Number: 137 Loss: 1.2948859930038452 Time taken: 0.20047307014465332\n",
            "Epoch: 14 Batch Number: 138 Loss: 1.2573553323745728 Time taken: 0.1975879669189453\n",
            "Epoch: 14 Batch Number: 139 Loss: 1.304323434829712 Time taken: 0.20029115676879883\n",
            "Epoch: 14 Batch Number: 140 Loss: 1.3168402910232544 Time taken: 0.20457983016967773\n",
            "Epoch: 14 Batch Number: 141 Loss: 1.3336749076843262 Time taken: 0.20257306098937988\n",
            "Epoch: 14 Batch Number: 142 Loss: 1.3189473152160645 Time taken: 0.200270414352417\n",
            "Epoch: 14 Batch Number: 143 Loss: 1.3199059963226318 Time taken: 0.19852042198181152\n",
            "Epoch: 14 Batch Number: 144 Loss: 1.3083034753799438 Time taken: 0.2016594409942627\n",
            "Epoch: 14 Batch Number: 145 Loss: 1.3040921688079834 Time taken: 0.2066023349761963\n",
            "Epoch: 14 Batch Number: 146 Loss: 1.3052170276641846 Time taken: 0.2001190185546875\n",
            "Epoch: 14 Batch Number: 147 Loss: 1.3127683401107788 Time taken: 0.20343947410583496\n",
            "Epoch: 14 Batch Number: 148 Loss: 1.2784569263458252 Time taken: 0.20108413696289062\n",
            "Epoch: 14 Batch Number: 149 Loss: 1.2904080152511597 Time taken: 0.20425176620483398\n",
            "Epoch: 14 Batch Number: 150 Loss: 1.2797437906265259 Time taken: 0.20411062240600586\n",
            "Epoch: 14 Batch Number: 151 Loss: 1.3167463541030884 Time taken: 0.20215344429016113\n",
            "Epoch: 14 Batch Number: 152 Loss: 1.2892100811004639 Time taken: 0.1987171173095703\n",
            "Epoch: 14 Batch Number: 153 Loss: 1.2803359031677246 Time taken: 0.20026826858520508\n",
            "Epoch: 14 Batch Number: 154 Loss: 1.2885860204696655 Time taken: 0.20522356033325195\n",
            "Epoch: 14 Batch Number: 155 Loss: 1.2946220636367798 Time taken: 0.20700693130493164\n",
            "Epoch: 14 Batch Number: 156 Loss: 1.3095488548278809 Time taken: 0.20060420036315918\n",
            "Epoch: 14 Batch Number: 157 Loss: 1.3065168857574463 Time taken: 0.19994473457336426\n",
            "Epoch: 14 Batch Number: 158 Loss: 1.3036317825317383 Time taken: 0.1999187469482422\n",
            "Epoch: 14 Batch Number: 159 Loss: 1.3118512630462646 Time taken: 0.2046961784362793\n",
            "Epoch: 14 Batch Number: 160 Loss: 1.2985560894012451 Time taken: 0.20167231559753418\n",
            "Epoch: 14 Batch Number: 161 Loss: 1.2949851751327515 Time taken: 0.19929218292236328\n",
            "Epoch: 14 Batch Number: 162 Loss: 1.327225685119629 Time taken: 0.21096396446228027\n",
            "Epoch: 14 Batch Number: 163 Loss: 1.2722327709197998 Time taken: 0.20505070686340332\n",
            "Epoch: 14 Batch Number: 164 Loss: 1.2860026359558105 Time taken: 0.20101284980773926\n",
            "Epoch: 14 Batch Number: 165 Loss: 1.30824875831604 Time taken: 0.2014634609222412\n",
            "Epoch: 14 Batch Number: 166 Loss: 1.296127438545227 Time taken: 0.20455479621887207\n",
            "Epoch: 14 Batch Number: 167 Loss: 1.287966012954712 Time taken: 0.20299053192138672\n",
            "Epoch: 14 Batch Number: 168 Loss: 1.2810921669006348 Time taken: 0.2056570053100586\n",
            "Epoch: 14 Batch Number: 169 Loss: 1.266398310661316 Time taken: 0.20358800888061523\n",
            "Epoch: 14 Batch Number: 170 Loss: 1.2845830917358398 Time taken: 0.2011256217956543\n",
            "Epoch: 14 Batch Number: 171 Loss: 1.2943994998931885 Time taken: 0.20362448692321777\n",
            "Epoch: 14 Batch Number: 172 Loss: 1.276012897491455 Time taken: 0.20029687881469727\n",
            "Epoch: 14 Batch Number: 173 Loss: 1.2702863216400146 Time taken: 0.2114124298095703\n",
            "Epoch: 14 Batch Number: 174 Loss: 1.2803586721420288 Time taken: 0.2062065601348877\n",
            "Epoch: 14 Batch Number: 175 Loss: 1.2924851179122925 Time taken: 0.2022995948791504\n",
            "Epoch: 14 Batch Number: 176 Loss: 1.271132469177246 Time taken: 0.19904589653015137\n",
            "Epoch: 14 Batch Number: 177 Loss: 1.260017991065979 Time taken: 0.20632433891296387\n",
            "Epoch: 14 Batch Number: 178 Loss: 1.2914284467697144 Time taken: 0.19736433029174805\n",
            "Epoch: 14 Batch Number: 179 Loss: 1.272934913635254 Time taken: 0.20032930374145508\n",
            "==========================================================================================\n",
            "Start of epoch 15\n",
            "Epoch: 15 Batch Number: 1 Loss: 1.2673488855361938 Time taken: 0.19806957244873047\n",
            "Epoch: 15 Batch Number: 2 Loss: 1.2803605794906616 Time taken: 0.1992027759552002\n",
            "Epoch: 15 Batch Number: 3 Loss: 1.2504314184188843 Time taken: 0.2001338005065918\n",
            "Epoch: 15 Batch Number: 4 Loss: 1.2503923177719116 Time taken: 0.20481252670288086\n",
            "Epoch: 15 Batch Number: 5 Loss: 1.2726722955703735 Time taken: 0.20552921295166016\n",
            "Epoch: 15 Batch Number: 6 Loss: 1.2529308795928955 Time taken: 0.20220279693603516\n",
            "Epoch: 15 Batch Number: 7 Loss: 1.2369334697723389 Time taken: 0.1998286247253418\n",
            "Epoch: 15 Batch Number: 8 Loss: 1.2487502098083496 Time taken: 0.20031285285949707\n",
            "Epoch: 15 Batch Number: 9 Loss: 1.2437375783920288 Time taken: 0.2125546932220459\n",
            "Epoch: 15 Batch Number: 10 Loss: 1.250967025756836 Time taken: 0.20954561233520508\n",
            "Epoch: 15 Batch Number: 11 Loss: 1.25990891456604 Time taken: 0.206024169921875\n",
            "Epoch: 15 Batch Number: 12 Loss: 1.2628103494644165 Time taken: 0.20116806030273438\n",
            "Epoch: 15 Batch Number: 13 Loss: 1.2806257009506226 Time taken: 0.2003037929534912\n",
            "Epoch: 15 Batch Number: 14 Loss: 1.286679983139038 Time taken: 0.20395755767822266\n",
            "Epoch: 15 Batch Number: 15 Loss: 1.3012378215789795 Time taken: 0.20189213752746582\n",
            "Epoch: 15 Batch Number: 16 Loss: 1.2734886407852173 Time taken: 0.20097041130065918\n",
            "Epoch: 15 Batch Number: 17 Loss: 1.275130271911621 Time taken: 0.19982266426086426\n",
            "Epoch: 15 Batch Number: 18 Loss: 1.2580418586730957 Time taken: 0.2049703598022461\n",
            "Epoch: 15 Batch Number: 19 Loss: 1.2486276626586914 Time taken: 0.20164799690246582\n",
            "Epoch: 15 Batch Number: 20 Loss: 1.2251663208007812 Time taken: 0.2008647918701172\n",
            "Epoch: 15 Batch Number: 21 Loss: 1.2493844032287598 Time taken: 0.2049860954284668\n",
            "Epoch: 15 Batch Number: 22 Loss: 1.267372488975525 Time taken: 0.2023298740386963\n",
            "Epoch: 15 Batch Number: 23 Loss: 1.257852554321289 Time taken: 0.1989283561706543\n",
            "Epoch: 15 Batch Number: 24 Loss: 1.2947858572006226 Time taken: 0.20246338844299316\n",
            "Epoch: 15 Batch Number: 25 Loss: 1.2886364459991455 Time taken: 0.20053339004516602\n",
            "Epoch: 15 Batch Number: 26 Loss: 1.2788527011871338 Time taken: 0.20386314392089844\n",
            "Epoch: 15 Batch Number: 27 Loss: 1.2837605476379395 Time taken: 0.20171260833740234\n",
            "Epoch: 15 Batch Number: 28 Loss: 1.2726751565933228 Time taken: 0.2142479419708252\n",
            "Epoch: 15 Batch Number: 29 Loss: 1.2881430387496948 Time taken: 0.20056700706481934\n",
            "Epoch: 15 Batch Number: 30 Loss: 1.2718408107757568 Time taken: 0.20298123359680176\n",
            "Epoch: 15 Batch Number: 31 Loss: 1.295822024345398 Time taken: 0.20583224296569824\n",
            "Epoch: 15 Batch Number: 32 Loss: 1.2647792100906372 Time taken: 0.19980716705322266\n",
            "Epoch: 15 Batch Number: 33 Loss: 1.2375578880310059 Time taken: 0.2026822566986084\n",
            "Epoch: 15 Batch Number: 34 Loss: 1.2753660678863525 Time taken: 0.21071958541870117\n",
            "Epoch: 15 Batch Number: 35 Loss: 1.258443832397461 Time taken: 0.20177292823791504\n",
            "Epoch: 15 Batch Number: 36 Loss: 1.2990403175354004 Time taken: 0.19805312156677246\n",
            "Epoch: 15 Batch Number: 37 Loss: 1.292128562927246 Time taken: 0.19981956481933594\n",
            "Epoch: 15 Batch Number: 38 Loss: 1.2950893640518188 Time taken: 0.19994473457336426\n",
            "Epoch: 15 Batch Number: 39 Loss: 1.280084490776062 Time taken: 0.2033374309539795\n",
            "Epoch: 15 Batch Number: 40 Loss: 1.2979892492294312 Time taken: 0.19968175888061523\n",
            "Epoch: 15 Batch Number: 41 Loss: 1.2980133295059204 Time taken: 0.20963001251220703\n",
            "Epoch: 15 Batch Number: 42 Loss: 1.275351643562317 Time taken: 0.20528936386108398\n",
            "Epoch: 15 Batch Number: 43 Loss: 1.2684117555618286 Time taken: 0.20364713668823242\n",
            "Epoch: 15 Batch Number: 44 Loss: 1.2703192234039307 Time taken: 0.20792055130004883\n",
            "Epoch: 15 Batch Number: 45 Loss: 1.2741838693618774 Time taken: 0.20212292671203613\n",
            "Epoch: 15 Batch Number: 46 Loss: 1.2751458883285522 Time taken: 0.20066118240356445\n",
            "Epoch: 15 Batch Number: 47 Loss: 1.267219066619873 Time taken: 0.2011885643005371\n",
            "Epoch: 15 Batch Number: 48 Loss: 1.3037774562835693 Time taken: 0.19892525672912598\n",
            "Epoch: 15 Batch Number: 49 Loss: 1.3091939687728882 Time taken: 0.20380139350891113\n",
            "Epoch: 15 Batch Number: 50 Loss: 1.2895679473876953 Time taken: 0.20098137855529785\n",
            "Epoch: 15 Batch Number: 51 Loss: 1.3090108633041382 Time taken: 0.2048351764678955\n",
            "Epoch: 15 Batch Number: 52 Loss: 1.2712396383285522 Time taken: 0.21074676513671875\n",
            "Epoch: 15 Batch Number: 53 Loss: 1.28038489818573 Time taken: 0.2015833854675293\n",
            "Epoch: 15 Batch Number: 54 Loss: 1.2752649784088135 Time taken: 0.2008824348449707\n",
            "Epoch: 15 Batch Number: 55 Loss: 1.2665424346923828 Time taken: 0.20347118377685547\n",
            "Epoch: 15 Batch Number: 56 Loss: 1.2814656496047974 Time taken: 0.20204734802246094\n",
            "Epoch: 15 Batch Number: 57 Loss: 1.291336178779602 Time taken: 0.19820690155029297\n",
            "Epoch: 15 Batch Number: 58 Loss: 1.2766194343566895 Time taken: 0.19907855987548828\n",
            "Epoch: 15 Batch Number: 59 Loss: 1.264056921005249 Time taken: 0.2002100944519043\n",
            "Epoch: 15 Batch Number: 60 Loss: 1.2574689388275146 Time taken: 0.20058035850524902\n",
            "Epoch: 15 Batch Number: 61 Loss: 1.275697946548462 Time taken: 0.20365691184997559\n",
            "Epoch: 15 Batch Number: 62 Loss: 1.259935975074768 Time taken: 0.21194243431091309\n",
            "Epoch: 15 Batch Number: 63 Loss: 1.2449445724487305 Time taken: 0.201157808303833\n",
            "Epoch: 15 Batch Number: 64 Loss: 1.2752918004989624 Time taken: 0.21042990684509277\n",
            "Epoch: 15 Batch Number: 65 Loss: 1.2773112058639526 Time taken: 0.2018873691558838\n",
            "Epoch: 15 Batch Number: 66 Loss: 1.2804617881774902 Time taken: 0.20007562637329102\n",
            "Epoch: 15 Batch Number: 67 Loss: 1.289196252822876 Time taken: 0.20307588577270508\n",
            "Epoch: 15 Batch Number: 68 Loss: 1.2790148258209229 Time taken: 0.20591306686401367\n",
            "Epoch: 15 Batch Number: 69 Loss: 1.2907823324203491 Time taken: 0.20006990432739258\n",
            "Epoch: 15 Batch Number: 70 Loss: 1.2805463075637817 Time taken: 0.19904494285583496\n",
            "Epoch: 15 Batch Number: 71 Loss: 1.273550271987915 Time taken: 0.20472335815429688\n",
            "Epoch: 15 Batch Number: 72 Loss: 1.2549813985824585 Time taken: 0.20156097412109375\n",
            "Epoch: 15 Batch Number: 73 Loss: 1.27109956741333 Time taken: 0.20800471305847168\n",
            "Epoch: 15 Batch Number: 74 Loss: 1.2905641794204712 Time taken: 0.2009725570678711\n",
            "Epoch: 15 Batch Number: 75 Loss: 1.2739325761795044 Time taken: 0.19930815696716309\n",
            "Epoch: 15 Batch Number: 76 Loss: 1.279992699623108 Time taken: 0.20251178741455078\n",
            "Epoch: 15 Batch Number: 77 Loss: 1.2803421020507812 Time taken: 0.1996288299560547\n",
            "Epoch: 15 Batch Number: 78 Loss: 1.2702155113220215 Time taken: 0.20514750480651855\n",
            "Epoch: 15 Batch Number: 79 Loss: 1.2765144109725952 Time taken: 0.19905757904052734\n",
            "Epoch: 15 Batch Number: 80 Loss: 1.248762607574463 Time taken: 0.20129966735839844\n",
            "Epoch: 15 Batch Number: 81 Loss: 1.2772884368896484 Time taken: 0.20499753952026367\n",
            "Epoch: 15 Batch Number: 82 Loss: 1.2629705667495728 Time taken: 0.19723248481750488\n",
            "Epoch: 15 Batch Number: 83 Loss: 1.2551040649414062 Time taken: 0.21242165565490723\n",
            "Epoch: 15 Batch Number: 84 Loss: 1.2688243389129639 Time taken: 0.20186352729797363\n",
            "Epoch: 15 Batch Number: 85 Loss: 1.2908824682235718 Time taken: 0.19997453689575195\n",
            "Epoch: 15 Batch Number: 86 Loss: 1.2493590116500854 Time taken: 0.20862364768981934\n",
            "Epoch: 15 Batch Number: 87 Loss: 1.2571066617965698 Time taken: 0.2004866600036621\n",
            "Epoch: 15 Batch Number: 88 Loss: 1.2726256847381592 Time taken: 0.20566320419311523\n",
            "Epoch: 15 Batch Number: 89 Loss: 1.2437055110931396 Time taken: 0.19789695739746094\n",
            "Epoch: 15 Batch Number: 90 Loss: 1.248191475868225 Time taken: 0.20288920402526855\n",
            "Epoch: 15 Batch Number: 91 Loss: 1.261920690536499 Time taken: 0.20865631103515625\n",
            "Epoch: 15 Batch Number: 92 Loss: 1.2752636671066284 Time taken: 0.20324182510375977\n",
            "Epoch: 15 Batch Number: 93 Loss: 1.2916086912155151 Time taken: 0.1978747844696045\n",
            "Epoch: 15 Batch Number: 94 Loss: 1.2728016376495361 Time taken: 0.20067477226257324\n",
            "Epoch: 15 Batch Number: 95 Loss: 1.2847692966461182 Time taken: 0.19964289665222168\n",
            "Epoch: 15 Batch Number: 96 Loss: 1.2777410745620728 Time taken: 0.20232772827148438\n",
            "Epoch: 15 Batch Number: 97 Loss: 1.2915538549423218 Time taken: 0.20255351066589355\n",
            "Epoch: 15 Batch Number: 98 Loss: 1.2842689752578735 Time taken: 0.20235776901245117\n",
            "Epoch: 15 Batch Number: 99 Loss: 1.2749875783920288 Time taken: 0.20375585556030273\n",
            "Epoch: 15 Batch Number: 100 Loss: 1.293739914894104 Time taken: 0.20157766342163086\n",
            "Epoch: 15 Batch Number: 101 Loss: 1.3020105361938477 Time taken: 0.20061898231506348\n",
            "Epoch: 15 Batch Number: 102 Loss: 1.2879691123962402 Time taken: 0.20292925834655762\n",
            "Epoch: 15 Batch Number: 103 Loss: 1.2984189987182617 Time taken: 0.20837044715881348\n",
            "Epoch: 15 Batch Number: 104 Loss: 1.2936803102493286 Time taken: 0.20265722274780273\n",
            "Epoch: 15 Batch Number: 105 Loss: 1.2984155416488647 Time taken: 0.200958251953125\n",
            "Epoch: 15 Batch Number: 106 Loss: 1.3021516799926758 Time taken: 0.19777703285217285\n",
            "Epoch: 15 Batch Number: 107 Loss: 1.3084523677825928 Time taken: 0.20093035697937012\n",
            "Epoch: 15 Batch Number: 108 Loss: 1.3159029483795166 Time taken: 0.20360374450683594\n",
            "Epoch: 15 Batch Number: 109 Loss: 1.3256219625473022 Time taken: 0.19982385635375977\n",
            "Epoch: 15 Batch Number: 110 Loss: 1.3167243003845215 Time taken: 0.2023329734802246\n",
            "Epoch: 15 Batch Number: 111 Loss: 1.3173956871032715 Time taken: 0.2053370475769043\n",
            "Epoch: 15 Batch Number: 112 Loss: 1.3133958578109741 Time taken: 0.2055370807647705\n",
            "Epoch: 15 Batch Number: 113 Loss: 1.3048641681671143 Time taken: 0.2086629867553711\n",
            "Epoch: 15 Batch Number: 114 Loss: 1.3101420402526855 Time taken: 0.20779204368591309\n",
            "Epoch: 15 Batch Number: 115 Loss: 1.3031761646270752 Time taken: 0.20502567291259766\n",
            "Epoch: 15 Batch Number: 116 Loss: 1.3075000047683716 Time taken: 0.201432466506958\n",
            "Epoch: 15 Batch Number: 117 Loss: 1.2851775884628296 Time taken: 0.20917296409606934\n",
            "Epoch: 15 Batch Number: 118 Loss: 1.2838571071624756 Time taken: 0.20015192031860352\n",
            "Epoch: 15 Batch Number: 119 Loss: 1.2979272603988647 Time taken: 0.20354533195495605\n",
            "Epoch: 15 Batch Number: 120 Loss: 1.289263367652893 Time taken: 0.20660686492919922\n",
            "Epoch: 15 Batch Number: 121 Loss: 1.2866356372833252 Time taken: 0.1989130973815918\n",
            "Epoch: 15 Batch Number: 122 Loss: 1.2769495248794556 Time taken: 0.21187829971313477\n",
            "Epoch: 15 Batch Number: 123 Loss: 1.2814321517944336 Time taken: 0.20228290557861328\n",
            "Epoch: 15 Batch Number: 124 Loss: 1.298717975616455 Time taken: 0.2031545639038086\n",
            "Epoch: 15 Batch Number: 125 Loss: 1.2840440273284912 Time taken: 0.19984841346740723\n",
            "Epoch: 15 Batch Number: 126 Loss: 1.2550915479660034 Time taken: 0.20546674728393555\n",
            "Epoch: 15 Batch Number: 127 Loss: 1.2791712284088135 Time taken: 0.20148277282714844\n",
            "Epoch: 15 Batch Number: 128 Loss: 1.2641537189483643 Time taken: 0.20016765594482422\n",
            "Epoch: 15 Batch Number: 129 Loss: 1.2869983911514282 Time taken: 0.20145583152770996\n",
            "Epoch: 15 Batch Number: 130 Loss: 1.311040997505188 Time taken: 0.20144176483154297\n",
            "Epoch: 15 Batch Number: 131 Loss: 1.288219690322876 Time taken: 0.20372772216796875\n",
            "Epoch: 15 Batch Number: 132 Loss: 1.3168690204620361 Time taken: 0.20415568351745605\n",
            "Epoch: 15 Batch Number: 133 Loss: 1.2752349376678467 Time taken: 0.19956398010253906\n",
            "Epoch: 15 Batch Number: 134 Loss: 1.2917851209640503 Time taken: 0.20522856712341309\n",
            "Epoch: 15 Batch Number: 135 Loss: 1.2633057832717896 Time taken: 0.20084214210510254\n",
            "Epoch: 15 Batch Number: 136 Loss: 1.2542520761489868 Time taken: 0.1992948055267334\n",
            "Epoch: 15 Batch Number: 137 Loss: 1.2567158937454224 Time taken: 0.19965100288391113\n",
            "Epoch: 15 Batch Number: 138 Loss: 1.2624461650848389 Time taken: 0.20040392875671387\n",
            "Epoch: 15 Batch Number: 139 Loss: 1.2977911233901978 Time taken: 0.20720839500427246\n",
            "Epoch: 15 Batch Number: 140 Loss: 1.321122646331787 Time taken: 0.20063161849975586\n",
            "Epoch: 15 Batch Number: 141 Loss: 1.276568055152893 Time taken: 0.2067117691040039\n",
            "Epoch: 15 Batch Number: 142 Loss: 1.2900135517120361 Time taken: 0.20098495483398438\n",
            "Epoch: 15 Batch Number: 143 Loss: 1.3009607791900635 Time taken: 0.20464396476745605\n",
            "Epoch: 15 Batch Number: 144 Loss: 1.3186694383621216 Time taken: 0.20358824729919434\n",
            "Epoch: 15 Batch Number: 145 Loss: 1.3349872827529907 Time taken: 0.20065569877624512\n",
            "Epoch: 15 Batch Number: 146 Loss: 1.2918829917907715 Time taken: 0.20206999778747559\n",
            "Epoch: 15 Batch Number: 147 Loss: 1.2802993059158325 Time taken: 0.20058441162109375\n",
            "Epoch: 15 Batch Number: 148 Loss: 1.2688848972320557 Time taken: 0.2016162872314453\n",
            "Epoch: 15 Batch Number: 149 Loss: 1.2410712242126465 Time taken: 0.2046036720275879\n",
            "Epoch: 15 Batch Number: 150 Loss: 1.282177448272705 Time taken: 0.20068955421447754\n",
            "Epoch: 15 Batch Number: 151 Loss: 1.278751254081726 Time taken: 0.20115923881530762\n",
            "Epoch: 15 Batch Number: 152 Loss: 1.2992688417434692 Time taken: 0.2069861888885498\n",
            "Epoch: 15 Batch Number: 153 Loss: 1.2970317602157593 Time taken: 0.19845962524414062\n",
            "Epoch: 15 Batch Number: 154 Loss: 1.289393424987793 Time taken: 0.20253682136535645\n",
            "Epoch: 15 Batch Number: 155 Loss: 1.2776236534118652 Time taken: 0.19934415817260742\n",
            "Epoch: 15 Batch Number: 156 Loss: 1.3070509433746338 Time taken: 0.2117760181427002\n",
            "Epoch: 15 Batch Number: 157 Loss: 1.2835665941238403 Time taken: 0.1995067596435547\n",
            "Epoch: 15 Batch Number: 158 Loss: 1.30315363407135 Time taken: 0.19947385787963867\n",
            "Epoch: 15 Batch Number: 159 Loss: 1.2965134382247925 Time taken: 0.20613884925842285\n",
            "Epoch: 15 Batch Number: 160 Loss: 1.306422233581543 Time taken: 0.19921875\n",
            "Epoch: 15 Batch Number: 161 Loss: 1.2750954627990723 Time taken: 0.20849847793579102\n",
            "Epoch: 15 Batch Number: 162 Loss: 1.2819854021072388 Time taken: 0.19917559623718262\n",
            "Epoch: 15 Batch Number: 163 Loss: 1.287162184715271 Time taken: 0.20256257057189941\n",
            "Epoch: 15 Batch Number: 164 Loss: 1.2973321676254272 Time taken: 0.20109868049621582\n",
            "Epoch: 15 Batch Number: 165 Loss: 1.281431794166565 Time taken: 0.2019054889678955\n",
            "Epoch: 15 Batch Number: 166 Loss: 1.2636865377426147 Time taken: 0.20685601234436035\n",
            "Epoch: 15 Batch Number: 167 Loss: 1.2877764701843262 Time taken: 0.2039637565612793\n",
            "Epoch: 15 Batch Number: 168 Loss: 1.287675142288208 Time taken: 0.2013232707977295\n",
            "Epoch: 15 Batch Number: 169 Loss: 1.251570463180542 Time taken: 0.20373892784118652\n",
            "Epoch: 15 Batch Number: 170 Loss: 1.2670025825500488 Time taken: 0.2054588794708252\n",
            "Epoch: 15 Batch Number: 171 Loss: 1.2714462280273438 Time taken: 0.1996161937713623\n",
            "Epoch: 15 Batch Number: 172 Loss: 1.2737553119659424 Time taken: 0.20004820823669434\n",
            "Epoch: 15 Batch Number: 173 Loss: 1.2604868412017822 Time taken: 0.2005298137664795\n",
            "Epoch: 15 Batch Number: 174 Loss: 1.2633674144744873 Time taken: 0.2059330940246582\n",
            "Epoch: 15 Batch Number: 175 Loss: 1.2847551107406616 Time taken: 0.2054295539855957\n",
            "Epoch: 15 Batch Number: 176 Loss: 1.2587617635726929 Time taken: 0.20556974411010742\n",
            "Epoch: 15 Batch Number: 177 Loss: 1.2599400281906128 Time taken: 0.19620060920715332\n",
            "Epoch: 15 Batch Number: 178 Loss: 1.2843782901763916 Time taken: 0.20735859870910645\n",
            "Epoch: 15 Batch Number: 179 Loss: 1.2489087581634521 Time taken: 0.1975231170654297\n",
            "==========================================================================================\n",
            "Start of epoch 16\n",
            "Epoch: 16 Batch Number: 1 Loss: 1.258340835571289 Time taken: 0.20900177955627441\n",
            "Epoch: 16 Batch Number: 2 Loss: 1.242882251739502 Time taken: 0.20102334022521973\n",
            "Epoch: 16 Batch Number: 3 Loss: 1.240166425704956 Time taken: 0.2025144100189209\n",
            "Epoch: 16 Batch Number: 4 Loss: 1.2391093969345093 Time taken: 0.20339322090148926\n",
            "Epoch: 16 Batch Number: 5 Loss: 1.2320497035980225 Time taken: 0.20122194290161133\n",
            "Epoch: 16 Batch Number: 6 Loss: 1.232682466506958 Time taken: 0.2100667953491211\n",
            "Epoch: 16 Batch Number: 7 Loss: 1.2411917448043823 Time taken: 0.20806145668029785\n",
            "Epoch: 16 Batch Number: 8 Loss: 1.236895203590393 Time taken: 0.20501184463500977\n",
            "Epoch: 16 Batch Number: 9 Loss: 1.2707269191741943 Time taken: 0.2050647735595703\n",
            "Epoch: 16 Batch Number: 10 Loss: 1.233221411705017 Time taken: 0.20637798309326172\n",
            "Epoch: 16 Batch Number: 11 Loss: 1.2325085401535034 Time taken: 0.21012091636657715\n",
            "Epoch: 16 Batch Number: 12 Loss: 1.2656792402267456 Time taken: 0.2013847827911377\n",
            "Epoch: 16 Batch Number: 13 Loss: 1.2728757858276367 Time taken: 0.1992354393005371\n",
            "Epoch: 16 Batch Number: 14 Loss: 1.2778592109680176 Time taken: 0.20128941535949707\n",
            "Epoch: 16 Batch Number: 15 Loss: 1.2533546686172485 Time taken: 0.20342135429382324\n",
            "Epoch: 16 Batch Number: 16 Loss: 1.2602688074111938 Time taken: 0.20204830169677734\n",
            "Epoch: 16 Batch Number: 17 Loss: 1.2877840995788574 Time taken: 0.1992807388305664\n",
            "Epoch: 16 Batch Number: 18 Loss: 1.2572417259216309 Time taken: 0.21247363090515137\n",
            "Epoch: 16 Batch Number: 19 Loss: 1.2405428886413574 Time taken: 0.19950485229492188\n",
            "Epoch: 16 Batch Number: 20 Loss: 1.2382014989852905 Time taken: 0.20314979553222656\n",
            "Epoch: 16 Batch Number: 21 Loss: 1.244399070739746 Time taken: 0.20583295822143555\n",
            "Epoch: 16 Batch Number: 22 Loss: 1.2472343444824219 Time taken: 0.20334172248840332\n",
            "Epoch: 16 Batch Number: 23 Loss: 1.2486340999603271 Time taken: 0.20772957801818848\n",
            "Epoch: 16 Batch Number: 24 Loss: 1.2706341743469238 Time taken: 0.20299267768859863\n",
            "Epoch: 16 Batch Number: 25 Loss: 1.2902923822402954 Time taken: 0.20415186882019043\n",
            "Epoch: 16 Batch Number: 26 Loss: 1.284188151359558 Time taken: 0.20728015899658203\n",
            "Epoch: 16 Batch Number: 27 Loss: 1.2899891138076782 Time taken: 0.20373964309692383\n",
            "Epoch: 16 Batch Number: 28 Loss: 1.2640111446380615 Time taken: 0.20136308670043945\n",
            "Epoch: 16 Batch Number: 29 Loss: 1.274441123008728 Time taken: 0.20121526718139648\n",
            "Epoch: 16 Batch Number: 30 Loss: 1.2645806074142456 Time taken: 0.20568418502807617\n",
            "Epoch: 16 Batch Number: 31 Loss: 1.2558581829071045 Time taken: 0.20066404342651367\n",
            "Epoch: 16 Batch Number: 32 Loss: 1.2844890356063843 Time taken: 0.2064521312713623\n",
            "Epoch: 16 Batch Number: 33 Loss: 1.2507619857788086 Time taken: 0.20065021514892578\n",
            "Epoch: 16 Batch Number: 34 Loss: 1.2697656154632568 Time taken: 0.20298981666564941\n",
            "Epoch: 16 Batch Number: 35 Loss: 1.2574256658554077 Time taken: 0.20175457000732422\n",
            "Epoch: 16 Batch Number: 36 Loss: 1.278039813041687 Time taken: 0.19984102249145508\n",
            "Epoch: 16 Batch Number: 37 Loss: 1.2748377323150635 Time taken: 0.19848847389221191\n",
            "Epoch: 16 Batch Number: 38 Loss: 1.2818844318389893 Time taken: 0.19787263870239258\n",
            "Epoch: 16 Batch Number: 39 Loss: 1.2940658330917358 Time taken: 0.2019360065460205\n",
            "Epoch: 16 Batch Number: 40 Loss: 1.2681254148483276 Time taken: 0.21011066436767578\n",
            "Epoch: 16 Batch Number: 41 Loss: 1.2514159679412842 Time taken: 0.19538116455078125\n",
            "Epoch: 16 Batch Number: 42 Loss: 1.2401214838027954 Time taken: 0.2035512924194336\n",
            "Epoch: 16 Batch Number: 43 Loss: 1.2703324556350708 Time taken: 0.19904088973999023\n",
            "Epoch: 16 Batch Number: 44 Loss: 1.2679195404052734 Time taken: 0.19966697692871094\n",
            "Epoch: 16 Batch Number: 45 Loss: 1.2470929622650146 Time taken: 0.2069389820098877\n",
            "Epoch: 16 Batch Number: 46 Loss: 1.2869653701782227 Time taken: 0.19993877410888672\n",
            "Epoch: 16 Batch Number: 47 Loss: 1.282516360282898 Time taken: 0.20885658264160156\n",
            "Epoch: 16 Batch Number: 48 Loss: 1.2756843566894531 Time taken: 0.2025010585784912\n",
            "Epoch: 16 Batch Number: 49 Loss: 1.2941477298736572 Time taken: 0.2016890048980713\n",
            "Epoch: 16 Batch Number: 50 Loss: 1.2707310914993286 Time taken: 0.20506596565246582\n",
            "Epoch: 16 Batch Number: 51 Loss: 1.2606854438781738 Time taken: 0.20094847679138184\n",
            "Epoch: 16 Batch Number: 52 Loss: 1.273084044456482 Time taken: 0.21134519577026367\n",
            "Epoch: 16 Batch Number: 53 Loss: 1.2756438255310059 Time taken: 0.20073342323303223\n",
            "Epoch: 16 Batch Number: 54 Loss: 1.2667741775512695 Time taken: 0.19923925399780273\n",
            "Epoch: 16 Batch Number: 55 Loss: 1.27483332157135 Time taken: 0.21031904220581055\n",
            "Epoch: 16 Batch Number: 56 Loss: 1.2634304761886597 Time taken: 0.19959044456481934\n",
            "Epoch: 16 Batch Number: 57 Loss: 1.2888375520706177 Time taken: 0.20077109336853027\n",
            "Epoch: 16 Batch Number: 58 Loss: 1.2653926610946655 Time taken: 0.20187854766845703\n",
            "Epoch: 16 Batch Number: 59 Loss: 1.2326298952102661 Time taken: 0.2027289867401123\n",
            "Epoch: 16 Batch Number: 60 Loss: 1.262764573097229 Time taken: 0.19975852966308594\n",
            "Epoch: 16 Batch Number: 61 Loss: 1.254157543182373 Time taken: 0.20107674598693848\n",
            "Epoch: 16 Batch Number: 62 Loss: 1.2803466320037842 Time taken: 0.19976329803466797\n",
            "Epoch: 16 Batch Number: 63 Loss: 1.2593673467636108 Time taken: 0.20803594589233398\n",
            "Epoch: 16 Batch Number: 64 Loss: 1.248549222946167 Time taken: 0.20048093795776367\n",
            "Epoch: 16 Batch Number: 65 Loss: 1.272772192955017 Time taken: 0.20107245445251465\n",
            "Epoch: 16 Batch Number: 66 Loss: 1.281402826309204 Time taken: 0.19839167594909668\n",
            "Epoch: 16 Batch Number: 67 Loss: 1.2708457708358765 Time taken: 0.1972804069519043\n",
            "Epoch: 16 Batch Number: 68 Loss: 1.2928022146224976 Time taken: 0.2035362720489502\n",
            "Epoch: 16 Batch Number: 69 Loss: 1.270246982574463 Time taken: 0.20183968544006348\n",
            "Epoch: 16 Batch Number: 70 Loss: 1.2909866571426392 Time taken: 0.20874953269958496\n",
            "Epoch: 16 Batch Number: 71 Loss: 1.26796293258667 Time taken: 0.20966482162475586\n",
            "Epoch: 16 Batch Number: 72 Loss: 1.280315637588501 Time taken: 0.20063471794128418\n",
            "Epoch: 16 Batch Number: 73 Loss: 1.2475435733795166 Time taken: 0.201674222946167\n",
            "Epoch: 16 Batch Number: 74 Loss: 1.2527992725372314 Time taken: 0.2018721103668213\n",
            "Epoch: 16 Batch Number: 75 Loss: 1.2633918523788452 Time taken: 0.20205974578857422\n",
            "Epoch: 16 Batch Number: 76 Loss: 1.2735589742660522 Time taken: 0.1998758316040039\n",
            "Epoch: 16 Batch Number: 77 Loss: 1.2615537643432617 Time taken: 0.20190024375915527\n",
            "Epoch: 16 Batch Number: 78 Loss: 1.269635558128357 Time taken: 0.2010788917541504\n",
            "Epoch: 16 Batch Number: 79 Loss: 1.2645419836044312 Time taken: 0.21193242073059082\n",
            "Epoch: 16 Batch Number: 80 Loss: 1.2692738771438599 Time taken: 0.1989898681640625\n",
            "Epoch: 16 Batch Number: 81 Loss: 1.247683048248291 Time taken: 0.2003629207611084\n",
            "Epoch: 16 Batch Number: 82 Loss: 1.2648416757583618 Time taken: 0.20096921920776367\n",
            "Epoch: 16 Batch Number: 83 Loss: 1.228240966796875 Time taken: 0.20044231414794922\n",
            "Epoch: 16 Batch Number: 84 Loss: 1.2468575239181519 Time taken: 0.20104527473449707\n",
            "Epoch: 16 Batch Number: 85 Loss: 1.2427501678466797 Time taken: 0.20218896865844727\n",
            "Epoch: 16 Batch Number: 86 Loss: 1.2671481370925903 Time taken: 0.1983959674835205\n",
            "Epoch: 16 Batch Number: 87 Loss: 1.250547170639038 Time taken: 0.20799517631530762\n",
            "Epoch: 16 Batch Number: 88 Loss: 1.2452211380004883 Time taken: 0.1985645294189453\n",
            "Epoch: 16 Batch Number: 89 Loss: 1.2511558532714844 Time taken: 0.20519256591796875\n",
            "Epoch: 16 Batch Number: 90 Loss: 1.2556812763214111 Time taken: 0.2010970115661621\n",
            "Epoch: 16 Batch Number: 91 Loss: 1.2322086095809937 Time taken: 0.20853543281555176\n",
            "Epoch: 16 Batch Number: 92 Loss: 1.239445447921753 Time taken: 0.20317769050598145\n",
            "Epoch: 16 Batch Number: 93 Loss: 1.2790656089782715 Time taken: 0.20196914672851562\n",
            "Epoch: 16 Batch Number: 94 Loss: 1.2672094106674194 Time taken: 0.21010041236877441\n",
            "Epoch: 16 Batch Number: 95 Loss: 1.2657809257507324 Time taken: 0.19667267799377441\n",
            "Epoch: 16 Batch Number: 96 Loss: 1.2689660787582397 Time taken: 0.20598793029785156\n",
            "Epoch: 16 Batch Number: 97 Loss: 1.2898354530334473 Time taken: 0.20020103454589844\n",
            "Epoch: 16 Batch Number: 98 Loss: 1.26261305809021 Time taken: 0.2006089687347412\n",
            "Epoch: 16 Batch Number: 99 Loss: 1.284072756767273 Time taken: 0.20188093185424805\n",
            "Epoch: 16 Batch Number: 100 Loss: 1.2764561176300049 Time taken: 0.20442819595336914\n",
            "Epoch: 16 Batch Number: 101 Loss: 1.2778091430664062 Time taken: 0.19942688941955566\n",
            "Epoch: 16 Batch Number: 102 Loss: 1.2717297077178955 Time taken: 0.20533442497253418\n",
            "Epoch: 16 Batch Number: 103 Loss: 1.2847516536712646 Time taken: 0.19949698448181152\n",
            "Epoch: 16 Batch Number: 104 Loss: 1.2809500694274902 Time taken: 0.20218396186828613\n",
            "Epoch: 16 Batch Number: 105 Loss: 1.309442400932312 Time taken: 0.19806718826293945\n",
            "Epoch: 16 Batch Number: 106 Loss: 1.2992606163024902 Time taken: 0.19908905029296875\n",
            "Epoch: 16 Batch Number: 107 Loss: 1.297761082649231 Time taken: 0.20027923583984375\n",
            "Epoch: 16 Batch Number: 108 Loss: 1.3008689880371094 Time taken: 0.20154118537902832\n",
            "Epoch: 16 Batch Number: 109 Loss: 1.3295460939407349 Time taken: 0.1989901065826416\n",
            "Epoch: 16 Batch Number: 110 Loss: 1.3018256425857544 Time taken: 0.20687055587768555\n",
            "Epoch: 16 Batch Number: 111 Loss: 1.3162574768066406 Time taken: 0.20117568969726562\n",
            "Epoch: 16 Batch Number: 112 Loss: 1.324352741241455 Time taken: 0.20164012908935547\n",
            "Epoch: 16 Batch Number: 113 Loss: 1.3001365661621094 Time taken: 0.20088982582092285\n",
            "Epoch: 16 Batch Number: 114 Loss: 1.2960244417190552 Time taken: 0.2019789218902588\n",
            "Epoch: 16 Batch Number: 115 Loss: 1.3007668256759644 Time taken: 0.2101600170135498\n",
            "Epoch: 16 Batch Number: 116 Loss: 1.282086730003357 Time taken: 0.20210838317871094\n",
            "Epoch: 16 Batch Number: 117 Loss: 1.267471432685852 Time taken: 0.20055651664733887\n",
            "Epoch: 16 Batch Number: 118 Loss: 1.277913212776184 Time taken: 0.20088982582092285\n",
            "Epoch: 16 Batch Number: 119 Loss: 1.2795114517211914 Time taken: 0.202955961227417\n",
            "Epoch: 16 Batch Number: 120 Loss: 1.2927135229110718 Time taken: 0.19900774955749512\n",
            "Epoch: 16 Batch Number: 121 Loss: 1.3020068407058716 Time taken: 0.20626020431518555\n",
            "Epoch: 16 Batch Number: 122 Loss: 1.2945431470870972 Time taken: 0.20102715492248535\n",
            "Epoch: 16 Batch Number: 123 Loss: 1.278435230255127 Time taken: 0.20570755004882812\n",
            "Epoch: 16 Batch Number: 124 Loss: 1.2739847898483276 Time taken: 0.2084496021270752\n",
            "Epoch: 16 Batch Number: 125 Loss: 1.275512933731079 Time taken: 0.20023798942565918\n",
            "Epoch: 16 Batch Number: 126 Loss: 1.259126901626587 Time taken: 0.20195746421813965\n",
            "Epoch: 16 Batch Number: 127 Loss: 1.2749038934707642 Time taken: 0.20249199867248535\n",
            "Epoch: 16 Batch Number: 128 Loss: 1.2783008813858032 Time taken: 0.20862817764282227\n",
            "Epoch: 16 Batch Number: 129 Loss: 1.285287857055664 Time taken: 0.20142841339111328\n",
            "Epoch: 16 Batch Number: 130 Loss: 1.2928380966186523 Time taken: 0.20662856101989746\n",
            "Epoch: 16 Batch Number: 131 Loss: 1.25303053855896 Time taken: 0.20070862770080566\n",
            "Epoch: 16 Batch Number: 132 Loss: 1.2856812477111816 Time taken: 0.2005751132965088\n",
            "Epoch: 16 Batch Number: 133 Loss: 1.2808717489242554 Time taken: 0.20397496223449707\n",
            "Epoch: 16 Batch Number: 134 Loss: 1.2595288753509521 Time taken: 0.19906163215637207\n",
            "Epoch: 16 Batch Number: 135 Loss: 1.2486236095428467 Time taken: 0.20023226737976074\n",
            "Epoch: 16 Batch Number: 136 Loss: 1.259195327758789 Time taken: 0.2036733627319336\n",
            "Epoch: 16 Batch Number: 137 Loss: 1.2393531799316406 Time taken: 0.19994354248046875\n",
            "Epoch: 16 Batch Number: 138 Loss: 1.2579076290130615 Time taken: 0.20186209678649902\n",
            "Epoch: 16 Batch Number: 139 Loss: 1.2710272073745728 Time taken: 0.20377540588378906\n",
            "Epoch: 16 Batch Number: 140 Loss: 1.2929540872573853 Time taken: 0.20227408409118652\n",
            "Epoch: 16 Batch Number: 141 Loss: 1.268062949180603 Time taken: 0.20343351364135742\n",
            "Epoch: 16 Batch Number: 142 Loss: 1.2674691677093506 Time taken: 0.20426106452941895\n",
            "Epoch: 16 Batch Number: 143 Loss: 1.3009413480758667 Time taken: 0.2053825855255127\n",
            "Epoch: 16 Batch Number: 144 Loss: 1.3001694679260254 Time taken: 0.20044922828674316\n",
            "Epoch: 16 Batch Number: 145 Loss: 1.2879997491836548 Time taken: 0.20111989974975586\n",
            "Epoch: 16 Batch Number: 146 Loss: 1.2958208322525024 Time taken: 0.20676851272583008\n",
            "Epoch: 16 Batch Number: 147 Loss: 1.2978782653808594 Time taken: 0.200455904006958\n",
            "Epoch: 16 Batch Number: 148 Loss: 1.2630743980407715 Time taken: 0.20396041870117188\n",
            "Epoch: 16 Batch Number: 149 Loss: 1.2850723266601562 Time taken: 0.19930028915405273\n",
            "Epoch: 16 Batch Number: 150 Loss: 1.255905270576477 Time taken: 0.20207810401916504\n",
            "Epoch: 16 Batch Number: 151 Loss: 1.27516508102417 Time taken: 0.20066046714782715\n",
            "Epoch: 16 Batch Number: 152 Loss: 1.2704321146011353 Time taken: 0.20347070693969727\n",
            "Epoch: 16 Batch Number: 153 Loss: 1.2692934274673462 Time taken: 0.2000129222869873\n",
            "Epoch: 16 Batch Number: 154 Loss: 1.286545753479004 Time taken: 0.20275592803955078\n",
            "Epoch: 16 Batch Number: 155 Loss: 1.2788232564926147 Time taken: 0.20207929611206055\n",
            "Epoch: 16 Batch Number: 156 Loss: 1.3031864166259766 Time taken: 0.19833612442016602\n",
            "Epoch: 16 Batch Number: 157 Loss: 1.2806466817855835 Time taken: 0.20599985122680664\n",
            "Epoch: 16 Batch Number: 158 Loss: 1.281632900238037 Time taken: 0.2061600685119629\n",
            "Epoch: 16 Batch Number: 159 Loss: 1.322319746017456 Time taken: 0.20495867729187012\n",
            "Epoch: 16 Batch Number: 160 Loss: 1.2751753330230713 Time taken: 0.2003335952758789\n",
            "Epoch: 16 Batch Number: 161 Loss: 1.2865583896636963 Time taken: 0.20327329635620117\n",
            "Epoch: 16 Batch Number: 162 Loss: 1.2792984247207642 Time taken: 0.20328307151794434\n",
            "Epoch: 16 Batch Number: 163 Loss: 1.2752416133880615 Time taken: 0.2009122371673584\n",
            "Epoch: 16 Batch Number: 164 Loss: 1.2777304649353027 Time taken: 0.20089340209960938\n",
            "Epoch: 16 Batch Number: 165 Loss: 1.2818028926849365 Time taken: 0.20134210586547852\n",
            "Epoch: 16 Batch Number: 166 Loss: 1.267408013343811 Time taken: 0.2015542984008789\n",
            "Epoch: 16 Batch Number: 167 Loss: 1.279738187789917 Time taken: 0.20595788955688477\n",
            "Epoch: 16 Batch Number: 168 Loss: 1.274040937423706 Time taken: 0.20142364501953125\n",
            "Epoch: 16 Batch Number: 169 Loss: 1.2973101139068604 Time taken: 0.19872307777404785\n",
            "Epoch: 16 Batch Number: 170 Loss: 1.274800419807434 Time taken: 0.20150351524353027\n",
            "Epoch: 16 Batch Number: 171 Loss: 1.2730180025100708 Time taken: 0.19960474967956543\n",
            "Epoch: 16 Batch Number: 172 Loss: 1.2677744626998901 Time taken: 0.2037348747253418\n",
            "Epoch: 16 Batch Number: 173 Loss: 1.2433338165283203 Time taken: 0.20466876029968262\n",
            "Epoch: 16 Batch Number: 174 Loss: 1.2564743757247925 Time taken: 0.19997000694274902\n",
            "Epoch: 16 Batch Number: 175 Loss: 1.2687933444976807 Time taken: 0.19915127754211426\n",
            "Epoch: 16 Batch Number: 176 Loss: 1.2550938129425049 Time taken: 0.20230340957641602\n",
            "Epoch: 16 Batch Number: 177 Loss: 1.262607455253601 Time taken: 0.20475983619689941\n",
            "Epoch: 16 Batch Number: 178 Loss: 1.243971586227417 Time taken: 0.20936083793640137\n",
            "Epoch: 16 Batch Number: 179 Loss: 1.2447049617767334 Time taken: 0.20504426956176758\n",
            "==========================================================================================\n",
            "Start of epoch 17\n",
            "Epoch: 17 Batch Number: 1 Loss: 1.2670018672943115 Time taken: 0.20113205909729004\n",
            "Epoch: 17 Batch Number: 2 Loss: 1.2376925945281982 Time taken: 0.2026052474975586\n",
            "Epoch: 17 Batch Number: 3 Loss: 1.2253663539886475 Time taken: 0.2019362449645996\n",
            "Epoch: 17 Batch Number: 4 Loss: 1.2343579530715942 Time taken: 0.2027580738067627\n",
            "Epoch: 17 Batch Number: 5 Loss: 1.21439528465271 Time taken: 0.20307564735412598\n",
            "Epoch: 17 Batch Number: 6 Loss: 1.2248237133026123 Time taken: 0.2009267807006836\n",
            "Epoch: 17 Batch Number: 7 Loss: 1.2425342798233032 Time taken: 0.19971013069152832\n",
            "Epoch: 17 Batch Number: 8 Loss: 1.2274702787399292 Time taken: 0.2103290557861328\n",
            "Epoch: 17 Batch Number: 9 Loss: 1.2198268175125122 Time taken: 0.2005290985107422\n",
            "Epoch: 17 Batch Number: 10 Loss: 1.223809003829956 Time taken: 0.20214033126831055\n",
            "Epoch: 17 Batch Number: 11 Loss: 1.2411879301071167 Time taken: 0.19949722290039062\n",
            "Epoch: 17 Batch Number: 12 Loss: 1.2519696950912476 Time taken: 0.20579195022583008\n",
            "Epoch: 17 Batch Number: 13 Loss: 1.2629033327102661 Time taken: 0.20498204231262207\n",
            "Epoch: 17 Batch Number: 14 Loss: 1.248392939567566 Time taken: 0.1993873119354248\n",
            "Epoch: 17 Batch Number: 15 Loss: 1.2526942491531372 Time taken: 0.2032155990600586\n",
            "Epoch: 17 Batch Number: 16 Loss: 1.258012294769287 Time taken: 0.19890308380126953\n",
            "Epoch: 17 Batch Number: 17 Loss: 1.2575325965881348 Time taken: 0.19979405403137207\n",
            "Epoch: 17 Batch Number: 18 Loss: 1.2560760974884033 Time taken: 0.20175743103027344\n",
            "Epoch: 17 Batch Number: 19 Loss: 1.2173160314559937 Time taken: 0.20049595832824707\n",
            "Epoch: 17 Batch Number: 20 Loss: 1.2272802591323853 Time taken: 0.2119765281677246\n",
            "Epoch: 17 Batch Number: 21 Loss: 1.2465507984161377 Time taken: 0.20226621627807617\n",
            "Epoch: 17 Batch Number: 22 Loss: 1.2475855350494385 Time taken: 0.19970488548278809\n",
            "Epoch: 17 Batch Number: 23 Loss: 1.236677646636963 Time taken: 0.20922493934631348\n",
            "Epoch: 17 Batch Number: 24 Loss: 1.2635587453842163 Time taken: 0.2017686367034912\n",
            "Epoch: 17 Batch Number: 25 Loss: 1.263197660446167 Time taken: 0.19899630546569824\n",
            "Epoch: 17 Batch Number: 26 Loss: 1.2757810354232788 Time taken: 0.20150542259216309\n",
            "Epoch: 17 Batch Number: 27 Loss: 1.2836655378341675 Time taken: 0.20367884635925293\n",
            "Epoch: 17 Batch Number: 28 Loss: 1.267174243927002 Time taken: 0.20190763473510742\n",
            "Epoch: 17 Batch Number: 29 Loss: 1.2508054971694946 Time taken: 0.20019888877868652\n",
            "Epoch: 17 Batch Number: 30 Loss: 1.243797779083252 Time taken: 0.20288896560668945\n",
            "Epoch: 17 Batch Number: 31 Loss: 1.258711338043213 Time taken: 0.20055007934570312\n",
            "Epoch: 17 Batch Number: 32 Loss: 1.2525149583816528 Time taken: 0.20070266723632812\n",
            "Epoch: 17 Batch Number: 33 Loss: 1.2611486911773682 Time taken: 0.2006518840789795\n",
            "Epoch: 17 Batch Number: 34 Loss: 1.2456858158111572 Time taken: 0.20075559616088867\n",
            "Epoch: 17 Batch Number: 35 Loss: 1.2323839664459229 Time taken: 0.20026326179504395\n",
            "Epoch: 17 Batch Number: 36 Loss: 1.2586535215377808 Time taken: 0.20051336288452148\n",
            "Epoch: 17 Batch Number: 37 Loss: 1.2590036392211914 Time taken: 0.2057971954345703\n",
            "Epoch: 17 Batch Number: 38 Loss: 1.275644302368164 Time taken: 0.1981031894683838\n",
            "Epoch: 17 Batch Number: 39 Loss: 1.2511521577835083 Time taken: 0.2005772590637207\n",
            "Epoch: 17 Batch Number: 40 Loss: 1.2754446268081665 Time taken: 0.20119929313659668\n",
            "Epoch: 17 Batch Number: 41 Loss: 1.2744266986846924 Time taken: 0.1998896598815918\n",
            "Epoch: 17 Batch Number: 42 Loss: 1.2630317211151123 Time taken: 0.22403192520141602\n",
            "Epoch: 17 Batch Number: 43 Loss: 1.2756311893463135 Time taken: 0.20019841194152832\n",
            "Epoch: 17 Batch Number: 44 Loss: 1.268501877784729 Time taken: 0.20590662956237793\n",
            "Epoch: 17 Batch Number: 45 Loss: 1.2541433572769165 Time taken: 0.2002875804901123\n",
            "Epoch: 17 Batch Number: 46 Loss: 1.2453842163085938 Time taken: 0.20335125923156738\n",
            "Epoch: 17 Batch Number: 47 Loss: 1.2845423221588135 Time taken: 0.20579910278320312\n",
            "Epoch: 17 Batch Number: 48 Loss: 1.2815824747085571 Time taken: 0.20216798782348633\n",
            "Epoch: 17 Batch Number: 49 Loss: 1.252555251121521 Time taken: 0.20106244087219238\n",
            "Epoch: 17 Batch Number: 50 Loss: 1.269524097442627 Time taken: 0.19927334785461426\n",
            "Epoch: 17 Batch Number: 51 Loss: 1.265572190284729 Time taken: 0.20133137702941895\n",
            "Epoch: 17 Batch Number: 52 Loss: 1.2606682777404785 Time taken: 0.2054281234741211\n",
            "Epoch: 17 Batch Number: 53 Loss: 1.2730685472488403 Time taken: 0.19960880279541016\n",
            "Epoch: 17 Batch Number: 54 Loss: 1.2452222108840942 Time taken: 0.20425057411193848\n",
            "Epoch: 17 Batch Number: 55 Loss: 1.2495990991592407 Time taken: 0.1993424892425537\n",
            "Epoch: 17 Batch Number: 56 Loss: 1.2649165391921997 Time taken: 0.20084524154663086\n",
            "Epoch: 17 Batch Number: 57 Loss: 1.288435697555542 Time taken: 0.21128177642822266\n",
            "Epoch: 17 Batch Number: 58 Loss: 1.256224513053894 Time taken: 0.20698308944702148\n",
            "Epoch: 17 Batch Number: 59 Loss: 1.27058744430542 Time taken: 0.20434069633483887\n",
            "Epoch: 17 Batch Number: 60 Loss: 1.264328956604004 Time taken: 0.20156598091125488\n",
            "Epoch: 17 Batch Number: 61 Loss: 1.2570215463638306 Time taken: 0.21343374252319336\n",
            "Epoch: 17 Batch Number: 62 Loss: 1.2533820867538452 Time taken: 0.2036113739013672\n",
            "Epoch: 17 Batch Number: 63 Loss: 1.2397586107254028 Time taken: 0.20612215995788574\n",
            "Epoch: 17 Batch Number: 64 Loss: 1.2470120191574097 Time taken: 0.20048308372497559\n",
            "Epoch: 17 Batch Number: 65 Loss: 1.257373332977295 Time taken: 0.19960451126098633\n",
            "Epoch: 17 Batch Number: 66 Loss: 1.248517632484436 Time taken: 0.20508670806884766\n",
            "Epoch: 17 Batch Number: 67 Loss: 1.2673485279083252 Time taken: 0.20029497146606445\n",
            "Epoch: 17 Batch Number: 68 Loss: 1.2767831087112427 Time taken: 0.20008182525634766\n",
            "Epoch: 17 Batch Number: 69 Loss: 1.255486249923706 Time taken: 0.20070791244506836\n",
            "Epoch: 17 Batch Number: 70 Loss: 1.252352237701416 Time taken: 0.20227766036987305\n",
            "Epoch: 17 Batch Number: 71 Loss: 1.2633837461471558 Time taken: 0.20361685752868652\n",
            "Epoch: 17 Batch Number: 72 Loss: 1.2619215250015259 Time taken: 0.20062494277954102\n",
            "Epoch: 17 Batch Number: 73 Loss: 1.2723175287246704 Time taken: 0.20113205909729004\n",
            "Epoch: 17 Batch Number: 74 Loss: 1.233914852142334 Time taken: 0.20485448837280273\n",
            "Epoch: 17 Batch Number: 75 Loss: 1.2551863193511963 Time taken: 0.19913840293884277\n",
            "Epoch: 17 Batch Number: 76 Loss: 1.2579238414764404 Time taken: 0.2066025733947754\n",
            "Epoch: 17 Batch Number: 77 Loss: 1.2417734861373901 Time taken: 0.20021986961364746\n",
            "Epoch: 17 Batch Number: 78 Loss: 1.2543838024139404 Time taken: 0.19809174537658691\n",
            "Epoch: 17 Batch Number: 79 Loss: 1.2385671138763428 Time taken: 0.20545148849487305\n",
            "Epoch: 17 Batch Number: 80 Loss: 1.2408980131149292 Time taken: 0.20263981819152832\n",
            "Epoch: 17 Batch Number: 81 Loss: 1.2569299936294556 Time taken: 0.21481084823608398\n",
            "Epoch: 17 Batch Number: 82 Loss: 1.2406636476516724 Time taken: 0.2011263370513916\n",
            "Epoch: 17 Batch Number: 83 Loss: 1.238691806793213 Time taken: 0.19991135597229004\n",
            "Epoch: 17 Batch Number: 84 Loss: 1.2318434715270996 Time taken: 0.20328927040100098\n",
            "Epoch: 17 Batch Number: 85 Loss: 1.2421461343765259 Time taken: 0.20456719398498535\n",
            "Epoch: 17 Batch Number: 86 Loss: 1.2353770732879639 Time taken: 0.20252633094787598\n",
            "Epoch: 17 Batch Number: 87 Loss: 1.2458027601242065 Time taken: 0.2017197608947754\n",
            "Epoch: 17 Batch Number: 88 Loss: 1.2540223598480225 Time taken: 0.1992175579071045\n",
            "Epoch: 17 Batch Number: 89 Loss: 1.2602535486221313 Time taken: 0.21303057670593262\n",
            "Epoch: 17 Batch Number: 90 Loss: 1.2499737739562988 Time taken: 0.20088648796081543\n",
            "Epoch: 17 Batch Number: 91 Loss: 1.26243257522583 Time taken: 0.20047664642333984\n",
            "Epoch: 17 Batch Number: 92 Loss: 1.246228814125061 Time taken: 0.20213532447814941\n",
            "Epoch: 17 Batch Number: 93 Loss: 1.2750579118728638 Time taken: 0.20102572441101074\n",
            "Epoch: 17 Batch Number: 94 Loss: 1.2587625980377197 Time taken: 0.2037184238433838\n",
            "Epoch: 17 Batch Number: 95 Loss: 1.2845957279205322 Time taken: 0.20018672943115234\n",
            "Epoch: 17 Batch Number: 96 Loss: 1.2627503871917725 Time taken: 0.20140862464904785\n",
            "Epoch: 17 Batch Number: 97 Loss: 1.2681562900543213 Time taken: 0.20183444023132324\n",
            "Epoch: 17 Batch Number: 98 Loss: 1.2687020301818848 Time taken: 0.201096773147583\n",
            "Epoch: 17 Batch Number: 99 Loss: 1.2675443887710571 Time taken: 0.20494985580444336\n",
            "Epoch: 17 Batch Number: 100 Loss: 1.2535110712051392 Time taken: 0.2006998062133789\n",
            "Epoch: 17 Batch Number: 101 Loss: 1.2840313911437988 Time taken: 0.19771361351013184\n",
            "Epoch: 17 Batch Number: 102 Loss: 1.2568819522857666 Time taken: 0.20458364486694336\n",
            "Epoch: 17 Batch Number: 103 Loss: 1.288380742073059 Time taken: 0.20016741752624512\n",
            "Epoch: 17 Batch Number: 104 Loss: 1.2760698795318604 Time taken: 0.20337677001953125\n",
            "Epoch: 17 Batch Number: 105 Loss: 1.2927926778793335 Time taken: 0.20425987243652344\n",
            "Epoch: 17 Batch Number: 106 Loss: 1.2880966663360596 Time taken: 0.20014572143554688\n",
            "Epoch: 17 Batch Number: 107 Loss: 1.2669602632522583 Time taken: 0.19944357872009277\n",
            "Epoch: 17 Batch Number: 108 Loss: 1.2857041358947754 Time taken: 0.19946670532226562\n",
            "Epoch: 17 Batch Number: 109 Loss: 1.3187320232391357 Time taken: 0.20345282554626465\n",
            "Epoch: 17 Batch Number: 110 Loss: 1.322950839996338 Time taken: 0.20802569389343262\n",
            "Epoch: 17 Batch Number: 111 Loss: 1.306470274925232 Time taken: 0.20064449310302734\n",
            "Epoch: 17 Batch Number: 112 Loss: 1.308893084526062 Time taken: 0.20012784004211426\n",
            "Epoch: 17 Batch Number: 113 Loss: 1.2842040061950684 Time taken: 0.20228099822998047\n",
            "Epoch: 17 Batch Number: 114 Loss: 1.274680495262146 Time taken: 0.20355510711669922\n",
            "Epoch: 17 Batch Number: 115 Loss: 1.2777575254440308 Time taken: 0.2045137882232666\n",
            "Epoch: 17 Batch Number: 116 Loss: 1.2690187692642212 Time taken: 0.21058201789855957\n",
            "Epoch: 17 Batch Number: 117 Loss: 1.2701131105422974 Time taken: 0.20124077796936035\n",
            "Epoch: 17 Batch Number: 118 Loss: 1.25682532787323 Time taken: 0.20141196250915527\n",
            "Epoch: 17 Batch Number: 119 Loss: 1.284590721130371 Time taken: 0.19768404960632324\n",
            "Epoch: 17 Batch Number: 120 Loss: 1.2752928733825684 Time taken: 0.21042370796203613\n",
            "Epoch: 17 Batch Number: 121 Loss: 1.2944366931915283 Time taken: 0.19765424728393555\n",
            "Epoch: 17 Batch Number: 122 Loss: 1.2873455286026 Time taken: 0.19938302040100098\n",
            "Epoch: 17 Batch Number: 123 Loss: 1.2749104499816895 Time taken: 0.20031213760375977\n",
            "Epoch: 17 Batch Number: 124 Loss: 1.2553439140319824 Time taken: 0.20373201370239258\n",
            "Epoch: 17 Batch Number: 125 Loss: 1.2750458717346191 Time taken: 0.20884418487548828\n",
            "Epoch: 17 Batch Number: 126 Loss: 1.2319138050079346 Time taken: 0.19889020919799805\n",
            "Epoch: 17 Batch Number: 127 Loss: 1.2543460130691528 Time taken: 0.20025229454040527\n",
            "Epoch: 17 Batch Number: 128 Loss: 1.2742003202438354 Time taken: 0.19957399368286133\n",
            "Epoch: 17 Batch Number: 129 Loss: 1.258994460105896 Time taken: 0.20081162452697754\n",
            "Epoch: 17 Batch Number: 130 Loss: 1.2575712203979492 Time taken: 0.20683884620666504\n",
            "Epoch: 17 Batch Number: 131 Loss: 1.2678630352020264 Time taken: 0.20350050926208496\n",
            "Epoch: 17 Batch Number: 132 Loss: 1.2625991106033325 Time taken: 0.20134711265563965\n",
            "Epoch: 17 Batch Number: 133 Loss: 1.261008620262146 Time taken: 0.2026972770690918\n",
            "Epoch: 17 Batch Number: 134 Loss: 1.2690484523773193 Time taken: 0.20278692245483398\n",
            "Epoch: 17 Batch Number: 135 Loss: 1.2719718217849731 Time taken: 0.20049262046813965\n",
            "Epoch: 17 Batch Number: 136 Loss: 1.2425051927566528 Time taken: 0.20059442520141602\n",
            "Epoch: 17 Batch Number: 137 Loss: 1.2075831890106201 Time taken: 0.20151567459106445\n",
            "Epoch: 17 Batch Number: 138 Loss: 1.2837975025177002 Time taken: 0.20254278182983398\n",
            "Epoch: 17 Batch Number: 139 Loss: 1.2444709539413452 Time taken: 0.20047736167907715\n",
            "Epoch: 17 Batch Number: 140 Loss: 1.2656288146972656 Time taken: 0.20744609832763672\n",
            "Epoch: 17 Batch Number: 141 Loss: 1.2873237133026123 Time taken: 0.20128417015075684\n",
            "Epoch: 17 Batch Number: 142 Loss: 1.2857704162597656 Time taken: 0.20185327529907227\n",
            "Epoch: 17 Batch Number: 143 Loss: 1.2750799655914307 Time taken: 0.20130562782287598\n",
            "Epoch: 17 Batch Number: 144 Loss: 1.2842663526535034 Time taken: 0.21116876602172852\n",
            "Epoch: 17 Batch Number: 145 Loss: 1.2753477096557617 Time taken: 0.1991715431213379\n",
            "Epoch: 17 Batch Number: 146 Loss: 1.269081711769104 Time taken: 0.20003962516784668\n",
            "Epoch: 17 Batch Number: 147 Loss: 1.2474278211593628 Time taken: 0.20289278030395508\n",
            "Epoch: 17 Batch Number: 148 Loss: 1.272034764289856 Time taken: 0.2044386863708496\n",
            "Epoch: 17 Batch Number: 149 Loss: 1.2742195129394531 Time taken: 0.20313429832458496\n",
            "Epoch: 17 Batch Number: 150 Loss: 1.245756983757019 Time taken: 0.20056843757629395\n",
            "Epoch: 17 Batch Number: 151 Loss: 1.281360149383545 Time taken: 0.1998729705810547\n",
            "Epoch: 17 Batch Number: 152 Loss: 1.2597272396087646 Time taken: 0.199554443359375\n",
            "Epoch: 17 Batch Number: 153 Loss: 1.2576375007629395 Time taken: 0.20201802253723145\n",
            "Epoch: 17 Batch Number: 154 Loss: 1.2532029151916504 Time taken: 0.20993328094482422\n",
            "Epoch: 17 Batch Number: 155 Loss: 1.2828707695007324 Time taken: 0.20000195503234863\n",
            "Epoch: 17 Batch Number: 156 Loss: 1.2991708517074585 Time taken: 0.2006516456604004\n",
            "Epoch: 17 Batch Number: 157 Loss: 1.2728297710418701 Time taken: 0.20003175735473633\n",
            "Epoch: 17 Batch Number: 158 Loss: 1.2738767862319946 Time taken: 0.19830822944641113\n",
            "Epoch: 17 Batch Number: 159 Loss: 1.272318720817566 Time taken: 0.2007465362548828\n",
            "Epoch: 17 Batch Number: 160 Loss: 1.2798423767089844 Time taken: 0.19798970222473145\n",
            "Epoch: 17 Batch Number: 161 Loss: 1.2649115324020386 Time taken: 0.20086121559143066\n",
            "Epoch: 17 Batch Number: 162 Loss: 1.2902683019638062 Time taken: 0.2000272274017334\n",
            "Epoch: 17 Batch Number: 163 Loss: 1.2661117315292358 Time taken: 0.20361804962158203\n",
            "Epoch: 17 Batch Number: 164 Loss: 1.2635408639907837 Time taken: 0.2049727439880371\n",
            "Epoch: 17 Batch Number: 165 Loss: 1.2845104932785034 Time taken: 0.2085576057434082\n",
            "Epoch: 17 Batch Number: 166 Loss: 1.2556654214859009 Time taken: 0.20963191986083984\n",
            "Epoch: 17 Batch Number: 167 Loss: 1.2496235370635986 Time taken: 0.20330047607421875\n",
            "Epoch: 17 Batch Number: 168 Loss: 1.2573018074035645 Time taken: 0.20090961456298828\n",
            "Epoch: 17 Batch Number: 169 Loss: 1.2454276084899902 Time taken: 0.20572924613952637\n",
            "Epoch: 17 Batch Number: 170 Loss: 1.2589166164398193 Time taken: 0.20023274421691895\n",
            "Epoch: 17 Batch Number: 171 Loss: 1.272565245628357 Time taken: 0.20041370391845703\n",
            "Epoch: 17 Batch Number: 172 Loss: 1.2534854412078857 Time taken: 0.20529532432556152\n",
            "Epoch: 17 Batch Number: 173 Loss: 1.2621935606002808 Time taken: 0.20775866508483887\n",
            "Epoch: 17 Batch Number: 174 Loss: 1.252964973449707 Time taken: 0.2024071216583252\n",
            "Epoch: 17 Batch Number: 175 Loss: 1.2464064359664917 Time taken: 0.2006855010986328\n",
            "Epoch: 17 Batch Number: 176 Loss: 1.2471649646759033 Time taken: 0.20458364486694336\n",
            "Epoch: 17 Batch Number: 177 Loss: 1.2372968196868896 Time taken: 0.20391559600830078\n",
            "Epoch: 17 Batch Number: 178 Loss: 1.2580397129058838 Time taken: 0.209486722946167\n",
            "Epoch: 17 Batch Number: 179 Loss: 1.251534342765808 Time taken: 0.20203661918640137\n",
            "==========================================================================================\n",
            "Start of epoch 18\n",
            "Epoch: 18 Batch Number: 1 Loss: 1.2508326768875122 Time taken: 0.2041301727294922\n",
            "Epoch: 18 Batch Number: 2 Loss: 1.2591570615768433 Time taken: 0.1988518238067627\n",
            "Epoch: 18 Batch Number: 3 Loss: 1.245914340019226 Time taken: 0.19963479042053223\n",
            "Epoch: 18 Batch Number: 4 Loss: 1.2256819009780884 Time taken: 0.20571303367614746\n",
            "Epoch: 18 Batch Number: 5 Loss: 1.201295018196106 Time taken: 0.20198822021484375\n",
            "Epoch: 18 Batch Number: 6 Loss: 1.2255820035934448 Time taken: 0.19968867301940918\n",
            "Epoch: 18 Batch Number: 7 Loss: 1.22511625289917 Time taken: 0.1987903118133545\n",
            "Epoch: 18 Batch Number: 8 Loss: 1.2385485172271729 Time taken: 0.2051103115081787\n",
            "Epoch: 18 Batch Number: 9 Loss: 1.219642996788025 Time taken: 0.21024012565612793\n",
            "Epoch: 18 Batch Number: 10 Loss: 1.2197604179382324 Time taken: 0.20682787895202637\n",
            "Epoch: 18 Batch Number: 11 Loss: 1.2450141906738281 Time taken: 0.206312894821167\n",
            "Epoch: 18 Batch Number: 12 Loss: 1.229968547821045 Time taken: 0.19951677322387695\n",
            "Epoch: 18 Batch Number: 13 Loss: 1.2422512769699097 Time taken: 0.2073957920074463\n",
            "Epoch: 18 Batch Number: 14 Loss: 1.2440763711929321 Time taken: 0.20912623405456543\n",
            "Epoch: 18 Batch Number: 15 Loss: 1.2436167001724243 Time taken: 0.20143365859985352\n",
            "Epoch: 18 Batch Number: 16 Loss: 1.223709225654602 Time taken: 0.20646095275878906\n",
            "Epoch: 18 Batch Number: 17 Loss: 1.25160813331604 Time taken: 0.2053685188293457\n",
            "Epoch: 18 Batch Number: 18 Loss: 1.21028470993042 Time taken: 0.20560646057128906\n",
            "Epoch: 18 Batch Number: 19 Loss: 1.2400717735290527 Time taken: 0.2073976993560791\n",
            "Epoch: 18 Batch Number: 20 Loss: 1.2253977060317993 Time taken: 0.20496177673339844\n",
            "Epoch: 18 Batch Number: 21 Loss: 1.2044376134872437 Time taken: 0.2037980556488037\n",
            "Epoch: 18 Batch Number: 22 Loss: 1.2211283445358276 Time taken: 0.20602846145629883\n",
            "Epoch: 18 Batch Number: 23 Loss: 1.2453280687332153 Time taken: 0.20460748672485352\n",
            "Epoch: 18 Batch Number: 24 Loss: 1.2651289701461792 Time taken: 0.20516586303710938\n",
            "Epoch: 18 Batch Number: 25 Loss: 1.2705309391021729 Time taken: 0.20468401908874512\n",
            "Epoch: 18 Batch Number: 26 Loss: 1.2530885934829712 Time taken: 0.19962763786315918\n",
            "Epoch: 18 Batch Number: 27 Loss: 1.2494828701019287 Time taken: 0.2079484462738037\n",
            "Epoch: 18 Batch Number: 28 Loss: 1.2470418214797974 Time taken: 0.20627546310424805\n",
            "Epoch: 18 Batch Number: 29 Loss: 1.2525328397750854 Time taken: 0.20495200157165527\n",
            "Epoch: 18 Batch Number: 30 Loss: 1.246559739112854 Time taken: 0.20170378684997559\n",
            "Epoch: 18 Batch Number: 31 Loss: 1.246551752090454 Time taken: 0.2064378261566162\n",
            "Epoch: 18 Batch Number: 32 Loss: 1.246788740158081 Time taken: 0.2031242847442627\n",
            "Epoch: 18 Batch Number: 33 Loss: 1.2399609088897705 Time taken: 0.21068048477172852\n",
            "Epoch: 18 Batch Number: 34 Loss: 1.2651458978652954 Time taken: 0.20074939727783203\n",
            "Epoch: 18 Batch Number: 35 Loss: 1.260745644569397 Time taken: 0.20875144004821777\n",
            "Epoch: 18 Batch Number: 36 Loss: 1.2564303874969482 Time taken: 0.20340275764465332\n",
            "Epoch: 18 Batch Number: 37 Loss: 1.2646268606185913 Time taken: 0.20149564743041992\n",
            "Epoch: 18 Batch Number: 38 Loss: 1.2687252759933472 Time taken: 0.20427441596984863\n",
            "Epoch: 18 Batch Number: 39 Loss: 1.271223783493042 Time taken: 0.19943451881408691\n",
            "Epoch: 18 Batch Number: 40 Loss: 1.2608710527420044 Time taken: 0.20371675491333008\n",
            "Epoch: 18 Batch Number: 41 Loss: 1.252509355545044 Time taken: 0.20391631126403809\n",
            "Epoch: 18 Batch Number: 42 Loss: 1.244385838508606 Time taken: 0.20982933044433594\n",
            "Epoch: 18 Batch Number: 43 Loss: 1.2298232316970825 Time taken: 0.20992422103881836\n",
            "Epoch: 18 Batch Number: 44 Loss: 1.2497239112854004 Time taken: 0.20244812965393066\n",
            "Epoch: 18 Batch Number: 45 Loss: 1.2437254190444946 Time taken: 0.20752739906311035\n",
            "Epoch: 18 Batch Number: 46 Loss: 1.2623223066329956 Time taken: 0.21176838874816895\n",
            "Epoch: 18 Batch Number: 47 Loss: 1.2730885744094849 Time taken: 0.205826997756958\n",
            "Epoch: 18 Batch Number: 48 Loss: 1.2687959671020508 Time taken: 0.20176315307617188\n",
            "Epoch: 18 Batch Number: 49 Loss: 1.2633999586105347 Time taken: 0.20853304862976074\n",
            "Epoch: 18 Batch Number: 50 Loss: 1.260631799697876 Time taken: 0.2049238681793213\n",
            "Epoch: 18 Batch Number: 51 Loss: 1.2387515306472778 Time taken: 0.20124030113220215\n",
            "Epoch: 18 Batch Number: 52 Loss: 1.2798997163772583 Time taken: 0.2093813419342041\n",
            "Epoch: 18 Batch Number: 53 Loss: 1.2712864875793457 Time taken: 0.1992940902709961\n",
            "Epoch: 18 Batch Number: 54 Loss: 1.2373262643814087 Time taken: 0.20406317710876465\n",
            "Epoch: 18 Batch Number: 55 Loss: 1.2393395900726318 Time taken: 0.20162177085876465\n",
            "Epoch: 18 Batch Number: 56 Loss: 1.2605808973312378 Time taken: 0.20308589935302734\n",
            "Epoch: 18 Batch Number: 57 Loss: 1.2449119091033936 Time taken: 0.21219420433044434\n",
            "Epoch: 18 Batch Number: 58 Loss: 1.238051176071167 Time taken: 0.19887471199035645\n",
            "Epoch: 18 Batch Number: 59 Loss: 1.2528842687606812 Time taken: 0.20371055603027344\n",
            "Epoch: 18 Batch Number: 60 Loss: 1.2462735176086426 Time taken: 0.20102810859680176\n",
            "Epoch: 18 Batch Number: 61 Loss: 1.248519778251648 Time taken: 0.19772911071777344\n",
            "Epoch: 18 Batch Number: 62 Loss: 1.240119457244873 Time taken: 0.19993925094604492\n",
            "Epoch: 18 Batch Number: 63 Loss: 1.24220871925354 Time taken: 0.20859551429748535\n",
            "Epoch: 18 Batch Number: 64 Loss: 1.2445858716964722 Time taken: 0.2027268409729004\n",
            "Epoch: 18 Batch Number: 65 Loss: 1.2542158365249634 Time taken: 0.20270776748657227\n",
            "Epoch: 18 Batch Number: 66 Loss: 1.2663072347640991 Time taken: 0.20645594596862793\n",
            "Epoch: 18 Batch Number: 67 Loss: 1.2636337280273438 Time taken: 0.20092988014221191\n",
            "Epoch: 18 Batch Number: 68 Loss: 1.2561683654785156 Time taken: 0.2027604579925537\n",
            "Epoch: 18 Batch Number: 69 Loss: 1.2453935146331787 Time taken: 0.21242022514343262\n",
            "Epoch: 18 Batch Number: 70 Loss: 1.2753006219863892 Time taken: 0.20865178108215332\n",
            "Epoch: 18 Batch Number: 71 Loss: 1.2557159662246704 Time taken: 0.19875025749206543\n",
            "Epoch: 18 Batch Number: 72 Loss: 1.2551127672195435 Time taken: 0.20858025550842285\n",
            "Epoch: 18 Batch Number: 73 Loss: 1.241289496421814 Time taken: 0.2004847526550293\n",
            "Epoch: 18 Batch Number: 74 Loss: 1.253759741783142 Time taken: 0.20674943923950195\n",
            "Epoch: 18 Batch Number: 75 Loss: 1.2466659545898438 Time taken: 0.19962835311889648\n",
            "Epoch: 18 Batch Number: 76 Loss: 1.232313632965088 Time taken: 0.20162653923034668\n",
            "Epoch: 18 Batch Number: 77 Loss: 1.239689588546753 Time taken: 0.2146744728088379\n",
            "Epoch: 18 Batch Number: 78 Loss: 1.2340748310089111 Time taken: 0.19960570335388184\n",
            "Epoch: 18 Batch Number: 79 Loss: 1.2351720333099365 Time taken: 0.2050023078918457\n",
            "Epoch: 18 Batch Number: 80 Loss: 1.247823715209961 Time taken: 0.19638442993164062\n",
            "Epoch: 18 Batch Number: 81 Loss: 1.2380928993225098 Time taken: 0.2069261074066162\n",
            "Epoch: 18 Batch Number: 82 Loss: 1.2221089601516724 Time taken: 0.20286107063293457\n",
            "Epoch: 18 Batch Number: 83 Loss: 1.2503453493118286 Time taken: 0.1997509002685547\n",
            "Epoch: 18 Batch Number: 84 Loss: 1.2298805713653564 Time taken: 0.20392251014709473\n",
            "Epoch: 18 Batch Number: 85 Loss: 1.2250094413757324 Time taken: 0.20497369766235352\n",
            "Epoch: 18 Batch Number: 86 Loss: 1.2563483715057373 Time taken: 0.20122337341308594\n",
            "Epoch: 18 Batch Number: 87 Loss: 1.2334052324295044 Time taken: 0.2003307342529297\n",
            "Epoch: 18 Batch Number: 88 Loss: 1.2321275472640991 Time taken: 0.1993882656097412\n",
            "Epoch: 18 Batch Number: 89 Loss: 1.2323365211486816 Time taken: 0.20204997062683105\n",
            "Epoch: 18 Batch Number: 90 Loss: 1.2563316822052002 Time taken: 0.20402097702026367\n",
            "Epoch: 18 Batch Number: 91 Loss: 1.2288357019424438 Time taken: 0.20831656455993652\n",
            "Epoch: 18 Batch Number: 92 Loss: 1.2534228563308716 Time taken: 0.20168638229370117\n",
            "Epoch: 18 Batch Number: 93 Loss: 1.2566492557525635 Time taken: 0.20287728309631348\n",
            "Epoch: 18 Batch Number: 94 Loss: 1.2650949954986572 Time taken: 0.2052454948425293\n",
            "Epoch: 18 Batch Number: 95 Loss: 1.2415530681610107 Time taken: 0.20745182037353516\n",
            "Epoch: 18 Batch Number: 96 Loss: 1.2564538717269897 Time taken: 0.20822572708129883\n",
            "Epoch: 18 Batch Number: 97 Loss: 1.2539366483688354 Time taken: 0.2009751796722412\n",
            "Epoch: 18 Batch Number: 98 Loss: 1.2598334550857544 Time taken: 0.20381784439086914\n",
            "Epoch: 18 Batch Number: 99 Loss: 1.2696601152420044 Time taken: 0.2010328769683838\n",
            "Epoch: 18 Batch Number: 100 Loss: 1.2585551738739014 Time taken: 0.19652247428894043\n",
            "Epoch: 18 Batch Number: 101 Loss: 1.2772172689437866 Time taken: 0.20288395881652832\n",
            "Epoch: 18 Batch Number: 102 Loss: 1.2780743837356567 Time taken: 0.20173883438110352\n",
            "Epoch: 18 Batch Number: 103 Loss: 1.2599369287490845 Time taken: 0.20224261283874512\n",
            "Epoch: 18 Batch Number: 104 Loss: 1.2843137979507446 Time taken: 0.19936418533325195\n",
            "Epoch: 18 Batch Number: 105 Loss: 1.2659039497375488 Time taken: 0.2062528133392334\n",
            "Epoch: 18 Batch Number: 106 Loss: 1.2799113988876343 Time taken: 0.210418701171875\n",
            "Epoch: 18 Batch Number: 107 Loss: 1.277326226234436 Time taken: 0.1991744041442871\n",
            "Epoch: 18 Batch Number: 108 Loss: 1.2868170738220215 Time taken: 0.1990964412689209\n",
            "Epoch: 18 Batch Number: 109 Loss: 1.2619540691375732 Time taken: 0.20299863815307617\n",
            "Epoch: 18 Batch Number: 110 Loss: 1.3138219118118286 Time taken: 0.20058274269104004\n",
            "Epoch: 18 Batch Number: 111 Loss: 1.2768357992172241 Time taken: 0.20917534828186035\n",
            "Epoch: 18 Batch Number: 112 Loss: 1.288019061088562 Time taken: 0.2015979290008545\n",
            "Epoch: 18 Batch Number: 113 Loss: 1.2944648265838623 Time taken: 0.20513653755187988\n",
            "Epoch: 18 Batch Number: 114 Loss: 1.2847565412521362 Time taken: 0.1983046531677246\n",
            "Epoch: 18 Batch Number: 115 Loss: 1.2869282960891724 Time taken: 0.2299797534942627\n",
            "Epoch: 18 Batch Number: 116 Loss: 1.2469909191131592 Time taken: 0.24262142181396484\n",
            "Epoch: 18 Batch Number: 117 Loss: 1.2538292407989502 Time taken: 0.23119878768920898\n",
            "Epoch: 18 Batch Number: 118 Loss: 1.264278769493103 Time taken: 0.20024585723876953\n",
            "Epoch: 18 Batch Number: 119 Loss: 1.2667274475097656 Time taken: 0.20319795608520508\n",
            "Epoch: 18 Batch Number: 120 Loss: 1.26082181930542 Time taken: 0.21023154258728027\n",
            "Epoch: 18 Batch Number: 121 Loss: 1.2661172151565552 Time taken: 0.19788789749145508\n",
            "Epoch: 18 Batch Number: 122 Loss: 1.252944827079773 Time taken: 0.20392084121704102\n",
            "Epoch: 18 Batch Number: 123 Loss: 1.273282766342163 Time taken: 0.20315980911254883\n",
            "Epoch: 18 Batch Number: 124 Loss: 1.2419849634170532 Time taken: 0.20582222938537598\n",
            "Epoch: 18 Batch Number: 125 Loss: 1.2624019384384155 Time taken: 0.20165252685546875\n",
            "Epoch: 18 Batch Number: 126 Loss: 1.2667585611343384 Time taken: 0.20099425315856934\n",
            "Epoch: 18 Batch Number: 127 Loss: 1.24734365940094 Time taken: 0.20362353324890137\n",
            "Epoch: 18 Batch Number: 128 Loss: 1.2470091581344604 Time taken: 0.2063276767730713\n",
            "Epoch: 18 Batch Number: 129 Loss: 1.2651549577713013 Time taken: 0.2068016529083252\n",
            "Epoch: 18 Batch Number: 130 Loss: 1.2609615325927734 Time taken: 0.20868349075317383\n",
            "Epoch: 18 Batch Number: 131 Loss: 1.2623120546340942 Time taken: 0.2009134292602539\n",
            "Epoch: 18 Batch Number: 132 Loss: 1.2493183612823486 Time taken: 0.1998295783996582\n",
            "Epoch: 18 Batch Number: 133 Loss: 1.2770521640777588 Time taken: 0.20973539352416992\n",
            "Epoch: 18 Batch Number: 134 Loss: 1.2597815990447998 Time taken: 0.1983788013458252\n",
            "Epoch: 18 Batch Number: 135 Loss: 1.2780036926269531 Time taken: 0.2083725929260254\n",
            "Epoch: 18 Batch Number: 136 Loss: 1.2296998500823975 Time taken: 0.1993560791015625\n",
            "Epoch: 18 Batch Number: 137 Loss: 1.2186733484268188 Time taken: 0.20068573951721191\n",
            "Epoch: 18 Batch Number: 138 Loss: 1.2357525825500488 Time taken: 0.20734643936157227\n",
            "Epoch: 18 Batch Number: 139 Loss: 1.2429765462875366 Time taken: 0.20337748527526855\n",
            "Epoch: 18 Batch Number: 140 Loss: 1.2685422897338867 Time taken: 0.20473408699035645\n",
            "Epoch: 18 Batch Number: 141 Loss: 1.2527624368667603 Time taken: 0.20049691200256348\n",
            "Epoch: 18 Batch Number: 142 Loss: 1.2638276815414429 Time taken: 0.19977331161499023\n",
            "Epoch: 18 Batch Number: 143 Loss: 1.2725410461425781 Time taken: 0.199462890625\n",
            "Epoch: 18 Batch Number: 144 Loss: 1.2791873216629028 Time taken: 0.20171236991882324\n",
            "Epoch: 18 Batch Number: 145 Loss: 1.2512285709381104 Time taken: 0.19984149932861328\n",
            "Epoch: 18 Batch Number: 146 Loss: 1.2756309509277344 Time taken: 0.19926881790161133\n",
            "Epoch: 18 Batch Number: 147 Loss: 1.269027590751648 Time taken: 0.2014598846435547\n",
            "Epoch: 18 Batch Number: 148 Loss: 1.2261862754821777 Time taken: 0.20492172241210938\n",
            "Epoch: 18 Batch Number: 149 Loss: 1.2330337762832642 Time taken: 0.19956040382385254\n",
            "Epoch: 18 Batch Number: 150 Loss: 1.249467134475708 Time taken: 0.20606374740600586\n",
            "Epoch: 18 Batch Number: 151 Loss: 1.2671172618865967 Time taken: 0.19999313354492188\n",
            "Epoch: 18 Batch Number: 152 Loss: 1.2936354875564575 Time taken: 0.20534467697143555\n",
            "Epoch: 18 Batch Number: 153 Loss: 1.260842204093933 Time taken: 0.2046043872833252\n",
            "Epoch: 18 Batch Number: 154 Loss: 1.2511756420135498 Time taken: 0.20446276664733887\n",
            "Epoch: 18 Batch Number: 155 Loss: 1.2635263204574585 Time taken: 0.2040848731994629\n",
            "Epoch: 18 Batch Number: 156 Loss: 1.2890928983688354 Time taken: 0.20357489585876465\n",
            "Epoch: 18 Batch Number: 157 Loss: 1.288831114768982 Time taken: 0.20282649993896484\n",
            "Epoch: 18 Batch Number: 158 Loss: 1.2705061435699463 Time taken: 0.2044687271118164\n",
            "Epoch: 18 Batch Number: 159 Loss: 1.2652088403701782 Time taken: 0.21499228477478027\n",
            "Epoch: 18 Batch Number: 160 Loss: 1.271304726600647 Time taken: 0.20063495635986328\n",
            "Epoch: 18 Batch Number: 161 Loss: 1.2408084869384766 Time taken: 0.20675253868103027\n",
            "Epoch: 18 Batch Number: 162 Loss: 1.2514344453811646 Time taken: 0.21457338333129883\n",
            "Epoch: 18 Batch Number: 163 Loss: 1.2743520736694336 Time taken: 0.20228886604309082\n",
            "Epoch: 18 Batch Number: 164 Loss: 1.2629716396331787 Time taken: 0.20653963088989258\n",
            "Epoch: 18 Batch Number: 165 Loss: 1.2583372592926025 Time taken: 0.20126867294311523\n",
            "Epoch: 18 Batch Number: 166 Loss: 1.290013313293457 Time taken: 0.20556139945983887\n",
            "Epoch: 18 Batch Number: 167 Loss: 1.2497878074645996 Time taken: 0.2021651268005371\n",
            "Epoch: 18 Batch Number: 168 Loss: 1.2671304941177368 Time taken: 0.2069692611694336\n",
            "Epoch: 18 Batch Number: 169 Loss: 1.266579508781433 Time taken: 0.2063300609588623\n",
            "Epoch: 18 Batch Number: 170 Loss: 1.2408812046051025 Time taken: 0.2005763053894043\n",
            "Epoch: 18 Batch Number: 171 Loss: 1.260553002357483 Time taken: 0.20081305503845215\n",
            "Epoch: 18 Batch Number: 172 Loss: 1.2662895917892456 Time taken: 0.19799590110778809\n",
            "Epoch: 18 Batch Number: 173 Loss: 1.2377245426177979 Time taken: 0.19684123992919922\n",
            "Epoch: 18 Batch Number: 174 Loss: 1.2440412044525146 Time taken: 0.20608282089233398\n",
            "Epoch: 18 Batch Number: 175 Loss: 1.238054871559143 Time taken: 0.19852566719055176\n",
            "Epoch: 18 Batch Number: 176 Loss: 1.2514086961746216 Time taken: 0.21044492721557617\n",
            "Epoch: 18 Batch Number: 177 Loss: 1.2341077327728271 Time taken: 0.2045302391052246\n",
            "Epoch: 18 Batch Number: 178 Loss: 1.2398217916488647 Time taken: 0.1963489055633545\n",
            "Epoch: 18 Batch Number: 179 Loss: 1.2357970476150513 Time taken: 0.21138501167297363\n",
            "==========================================================================================\n",
            "Start of epoch 19\n",
            "Epoch: 19 Batch Number: 1 Loss: 1.2584285736083984 Time taken: 0.20142292976379395\n",
            "Epoch: 19 Batch Number: 2 Loss: 1.2249693870544434 Time taken: 0.19940710067749023\n",
            "Epoch: 19 Batch Number: 3 Loss: 1.217810034751892 Time taken: 0.19980335235595703\n",
            "Epoch: 19 Batch Number: 4 Loss: 1.221815586090088 Time taken: 0.20385408401489258\n",
            "Epoch: 19 Batch Number: 5 Loss: 1.224314570426941 Time taken: 0.2010948657989502\n",
            "Epoch: 19 Batch Number: 6 Loss: 1.215451955795288 Time taken: 0.20024991035461426\n",
            "Epoch: 19 Batch Number: 7 Loss: 1.2065179347991943 Time taken: 0.21181058883666992\n",
            "Epoch: 19 Batch Number: 8 Loss: 1.2047033309936523 Time taken: 0.20011329650878906\n",
            "Epoch: 19 Batch Number: 9 Loss: 1.2144277095794678 Time taken: 0.2035841941833496\n",
            "Epoch: 19 Batch Number: 10 Loss: 1.2222429513931274 Time taken: 0.20151448249816895\n",
            "Epoch: 19 Batch Number: 11 Loss: 1.2382405996322632 Time taken: 0.20474934577941895\n",
            "Epoch: 19 Batch Number: 12 Loss: 1.23538076877594 Time taken: 0.20441770553588867\n",
            "Epoch: 19 Batch Number: 13 Loss: 1.2342067956924438 Time taken: 0.20154547691345215\n",
            "Epoch: 19 Batch Number: 14 Loss: 1.24849271774292 Time taken: 0.20456981658935547\n",
            "Epoch: 19 Batch Number: 15 Loss: 1.281023621559143 Time taken: 0.19944381713867188\n",
            "Epoch: 19 Batch Number: 16 Loss: 1.2135546207427979 Time taken: 0.2016007900238037\n",
            "Epoch: 19 Batch Number: 17 Loss: 1.221234917640686 Time taken: 0.20353198051452637\n",
            "Epoch: 19 Batch Number: 18 Loss: 1.2264115810394287 Time taken: 0.2021176815032959\n",
            "Epoch: 19 Batch Number: 19 Loss: 1.2360037565231323 Time taken: 0.21163153648376465\n",
            "Epoch: 19 Batch Number: 20 Loss: 1.1979033946990967 Time taken: 0.2003319263458252\n",
            "Epoch: 19 Batch Number: 21 Loss: 1.224629282951355 Time taken: 0.20246171951293945\n",
            "Epoch: 19 Batch Number: 22 Loss: 1.2138705253601074 Time taken: 0.20140624046325684\n",
            "Epoch: 19 Batch Number: 23 Loss: 1.239875078201294 Time taken: 0.20868158340454102\n",
            "Epoch: 19 Batch Number: 24 Loss: 1.2644081115722656 Time taken: 0.20709776878356934\n",
            "Epoch: 19 Batch Number: 25 Loss: 1.260722041130066 Time taken: 0.20328187942504883\n",
            "Epoch: 19 Batch Number: 26 Loss: 1.2465802431106567 Time taken: 0.20741534233093262\n",
            "Epoch: 19 Batch Number: 27 Loss: 1.2708524465560913 Time taken: 0.20781826972961426\n",
            "Epoch: 19 Batch Number: 28 Loss: 1.243579387664795 Time taken: 0.20131850242614746\n",
            "Epoch: 19 Batch Number: 29 Loss: 1.2522246837615967 Time taken: 0.19951701164245605\n",
            "Epoch: 19 Batch Number: 30 Loss: 1.228790283203125 Time taken: 0.2004225254058838\n",
            "Epoch: 19 Batch Number: 31 Loss: 1.237418532371521 Time taken: 0.20209932327270508\n",
            "Epoch: 19 Batch Number: 32 Loss: 1.23154616355896 Time taken: 0.20016884803771973\n",
            "Epoch: 19 Batch Number: 33 Loss: 1.2419548034667969 Time taken: 0.21074199676513672\n",
            "Epoch: 19 Batch Number: 34 Loss: 1.2231966257095337 Time taken: 0.2024977207183838\n",
            "Epoch: 19 Batch Number: 35 Loss: 1.2373119592666626 Time taken: 0.19960618019104004\n",
            "Epoch: 19 Batch Number: 36 Loss: 1.239689826965332 Time taken: 0.20856165885925293\n",
            "Epoch: 19 Batch Number: 37 Loss: 1.2483173608779907 Time taken: 0.20408058166503906\n",
            "Epoch: 19 Batch Number: 38 Loss: 1.2672306299209595 Time taken: 0.1997365951538086\n",
            "Epoch: 19 Batch Number: 39 Loss: 1.260966181755066 Time taken: 0.19892001152038574\n",
            "Epoch: 19 Batch Number: 40 Loss: 1.2576661109924316 Time taken: 0.20715069770812988\n",
            "Epoch: 19 Batch Number: 41 Loss: 1.2467780113220215 Time taken: 0.19889426231384277\n",
            "Epoch: 19 Batch Number: 42 Loss: 1.252414584159851 Time taken: 0.20023083686828613\n",
            "Epoch: 19 Batch Number: 43 Loss: 1.233087420463562 Time taken: 0.20263004302978516\n",
            "Epoch: 19 Batch Number: 44 Loss: 1.2242627143859863 Time taken: 0.20076274871826172\n",
            "Epoch: 19 Batch Number: 45 Loss: 1.2334481477737427 Time taken: 0.19888091087341309\n",
            "Epoch: 19 Batch Number: 46 Loss: 1.2336790561676025 Time taken: 0.20039916038513184\n",
            "Epoch: 19 Batch Number: 47 Loss: 1.25635826587677 Time taken: 0.20256328582763672\n",
            "Epoch: 19 Batch Number: 48 Loss: 1.2731746435165405 Time taken: 0.20583295822143555\n",
            "Epoch: 19 Batch Number: 49 Loss: 1.2296775579452515 Time taken: 0.20554375648498535\n",
            "Epoch: 19 Batch Number: 50 Loss: 1.2538073062896729 Time taken: 0.19954681396484375\n",
            "Epoch: 19 Batch Number: 51 Loss: 1.2417880296707153 Time taken: 0.20664310455322266\n",
            "Epoch: 19 Batch Number: 52 Loss: 1.2555620670318604 Time taken: 0.20023298263549805\n",
            "Epoch: 19 Batch Number: 53 Loss: 1.2439488172531128 Time taken: 0.2101118564605713\n",
            "Epoch: 19 Batch Number: 54 Loss: 1.2460837364196777 Time taken: 0.20205998420715332\n",
            "Epoch: 19 Batch Number: 55 Loss: 1.2377251386642456 Time taken: 0.21093249320983887\n",
            "Epoch: 19 Batch Number: 56 Loss: 1.2471541166305542 Time taken: 0.19869732856750488\n",
            "Epoch: 19 Batch Number: 57 Loss: 1.2321776151657104 Time taken: 0.2025156021118164\n",
            "Epoch: 19 Batch Number: 58 Loss: 1.2316890954971313 Time taken: 0.20943331718444824\n",
            "Epoch: 19 Batch Number: 59 Loss: 1.2593986988067627 Time taken: 0.20306015014648438\n",
            "Epoch: 19 Batch Number: 60 Loss: 1.2556997537612915 Time taken: 0.21530437469482422\n",
            "Epoch: 19 Batch Number: 61 Loss: 1.2306969165802002 Time taken: 0.20176172256469727\n",
            "Epoch: 19 Batch Number: 62 Loss: 1.210066795349121 Time taken: 0.20700693130493164\n",
            "Epoch: 19 Batch Number: 63 Loss: 1.241318702697754 Time taken: 0.20818257331848145\n",
            "Epoch: 19 Batch Number: 64 Loss: 1.2442208528518677 Time taken: 0.20588278770446777\n",
            "Epoch: 19 Batch Number: 65 Loss: 1.240543246269226 Time taken: 0.21318769454956055\n",
            "Epoch: 19 Batch Number: 66 Loss: 1.2399550676345825 Time taken: 0.20267724990844727\n",
            "Epoch: 19 Batch Number: 67 Loss: 1.2475677728652954 Time taken: 0.20521330833435059\n",
            "Epoch: 19 Batch Number: 68 Loss: 1.2391711473464966 Time taken: 0.21035146713256836\n",
            "Epoch: 19 Batch Number: 69 Loss: 1.2328070402145386 Time taken: 0.20446181297302246\n",
            "Epoch: 19 Batch Number: 70 Loss: 1.2378952503204346 Time taken: 0.20082807540893555\n",
            "Epoch: 19 Batch Number: 71 Loss: 1.266135334968567 Time taken: 0.20179438591003418\n",
            "Epoch: 19 Batch Number: 72 Loss: 1.2536641359329224 Time taken: 0.20845270156860352\n",
            "Epoch: 19 Batch Number: 73 Loss: 1.2288423776626587 Time taken: 0.20226740837097168\n",
            "Epoch: 19 Batch Number: 74 Loss: 1.2278164625167847 Time taken: 0.19967079162597656\n",
            "Epoch: 19 Batch Number: 75 Loss: 1.2335069179534912 Time taken: 0.19971871376037598\n",
            "Epoch: 19 Batch Number: 76 Loss: 1.2533082962036133 Time taken: 0.20061635971069336\n",
            "Epoch: 19 Batch Number: 77 Loss: 1.2237086296081543 Time taken: 0.20456552505493164\n",
            "Epoch: 19 Batch Number: 78 Loss: 1.2367113828659058 Time taken: 0.20363688468933105\n",
            "Epoch: 19 Batch Number: 79 Loss: 1.2317354679107666 Time taken: 0.19951748847961426\n",
            "Epoch: 19 Batch Number: 80 Loss: 1.233233094215393 Time taken: 0.20297670364379883\n",
            "Epoch: 19 Batch Number: 81 Loss: 1.2349597215652466 Time taken: 0.20152568817138672\n",
            "Epoch: 19 Batch Number: 82 Loss: 1.2466846704483032 Time taken: 0.204390287399292\n",
            "Epoch: 19 Batch Number: 83 Loss: 1.2361161708831787 Time taken: 0.1999518871307373\n",
            "Epoch: 19 Batch Number: 84 Loss: 1.2157247066497803 Time taken: 0.1998121738433838\n",
            "Epoch: 19 Batch Number: 85 Loss: 1.252012014389038 Time taken: 0.20784330368041992\n",
            "Epoch: 19 Batch Number: 86 Loss: 1.2342703342437744 Time taken: 0.20024561882019043\n",
            "Epoch: 19 Batch Number: 87 Loss: 1.2419250011444092 Time taken: 0.20713353157043457\n",
            "Epoch: 19 Batch Number: 88 Loss: 1.2031654119491577 Time taken: 0.20183324813842773\n",
            "Epoch: 19 Batch Number: 89 Loss: 1.2280194759368896 Time taken: 0.21266865730285645\n",
            "Epoch: 19 Batch Number: 90 Loss: 1.2277228832244873 Time taken: 0.20287632942199707\n",
            "Epoch: 19 Batch Number: 91 Loss: 1.2287142276763916 Time taken: 0.20872902870178223\n",
            "Epoch: 19 Batch Number: 92 Loss: 1.2372735738754272 Time taken: 0.21255278587341309\n",
            "Epoch: 19 Batch Number: 93 Loss: 1.2510032653808594 Time taken: 0.20239686965942383\n",
            "Epoch: 19 Batch Number: 94 Loss: 1.2563875913619995 Time taken: 0.20028328895568848\n",
            "Epoch: 19 Batch Number: 95 Loss: 1.25031578540802 Time taken: 0.2004845142364502\n",
            "Epoch: 19 Batch Number: 96 Loss: 1.263588786125183 Time taken: 0.20349645614624023\n",
            "Epoch: 19 Batch Number: 97 Loss: 1.2542425394058228 Time taken: 0.2001791000366211\n",
            "Epoch: 19 Batch Number: 98 Loss: 1.2678734064102173 Time taken: 0.2033374309539795\n",
            "Epoch: 19 Batch Number: 99 Loss: 1.2561304569244385 Time taken: 0.20078444480895996\n",
            "Epoch: 19 Batch Number: 100 Loss: 1.2370048761367798 Time taken: 0.20583510398864746\n",
            "Epoch: 19 Batch Number: 101 Loss: 1.2536790370941162 Time taken: 0.20792508125305176\n",
            "Epoch: 19 Batch Number: 102 Loss: 1.2596462965011597 Time taken: 0.20375275611877441\n",
            "Epoch: 19 Batch Number: 103 Loss: 1.251929521560669 Time taken: 0.2017672061920166\n",
            "Epoch: 19 Batch Number: 104 Loss: 1.2446550130844116 Time taken: 0.20531606674194336\n",
            "Epoch: 19 Batch Number: 105 Loss: 1.2954356670379639 Time taken: 0.2025916576385498\n",
            "Epoch: 19 Batch Number: 106 Loss: 1.299448013305664 Time taken: 0.21248698234558105\n",
            "Epoch: 19 Batch Number: 107 Loss: 1.2807049751281738 Time taken: 0.20461821556091309\n",
            "Epoch: 19 Batch Number: 108 Loss: 1.2521308660507202 Time taken: 0.2068624496459961\n",
            "Epoch: 19 Batch Number: 109 Loss: 1.2544816732406616 Time taken: 0.20719313621520996\n",
            "Epoch: 19 Batch Number: 110 Loss: 1.2836092710494995 Time taken: 0.2054152488708496\n",
            "Epoch: 19 Batch Number: 111 Loss: 1.2854095697402954 Time taken: 0.2030346393585205\n",
            "Epoch: 19 Batch Number: 112 Loss: 1.279616355895996 Time taken: 0.2037491798400879\n",
            "Epoch: 19 Batch Number: 113 Loss: 1.2788413763046265 Time taken: 0.20965123176574707\n",
            "Epoch: 19 Batch Number: 114 Loss: 1.2677944898605347 Time taken: 0.19951772689819336\n",
            "Epoch: 19 Batch Number: 115 Loss: 1.2641559839248657 Time taken: 0.2082679271697998\n",
            "Epoch: 19 Batch Number: 116 Loss: 1.2574106454849243 Time taken: 0.21336722373962402\n",
            "Epoch: 19 Batch Number: 117 Loss: 1.2641456127166748 Time taken: 0.2033987045288086\n",
            "Epoch: 19 Batch Number: 118 Loss: 1.264649510383606 Time taken: 0.21474456787109375\n",
            "Epoch: 19 Batch Number: 119 Loss: 1.230005145072937 Time taken: 0.20327186584472656\n",
            "Epoch: 19 Batch Number: 120 Loss: 1.2641222476959229 Time taken: 0.2039165496826172\n",
            "Epoch: 19 Batch Number: 121 Loss: 1.2246406078338623 Time taken: 0.20189380645751953\n",
            "Epoch: 19 Batch Number: 122 Loss: 1.2744673490524292 Time taken: 0.2027120590209961\n",
            "Epoch: 19 Batch Number: 123 Loss: 1.2497239112854004 Time taken: 0.2006688117980957\n",
            "Epoch: 19 Batch Number: 124 Loss: 1.222029447555542 Time taken: 0.20485973358154297\n",
            "Epoch: 19 Batch Number: 125 Loss: 1.268114686012268 Time taken: 0.20441961288452148\n",
            "Epoch: 19 Batch Number: 126 Loss: 1.2588272094726562 Time taken: 0.20502686500549316\n",
            "Epoch: 19 Batch Number: 127 Loss: 1.2383910417556763 Time taken: 0.21264934539794922\n",
            "Epoch: 19 Batch Number: 128 Loss: 1.2559250593185425 Time taken: 0.20520496368408203\n",
            "Epoch: 19 Batch Number: 129 Loss: 1.2610472440719604 Time taken: 0.2030658721923828\n",
            "Epoch: 19 Batch Number: 130 Loss: 1.2388203144073486 Time taken: 0.20929169654846191\n",
            "Epoch: 19 Batch Number: 131 Loss: 1.2640635967254639 Time taken: 0.20174455642700195\n",
            "Epoch: 19 Batch Number: 132 Loss: 1.2713314294815063 Time taken: 0.20473241806030273\n",
            "Epoch: 19 Batch Number: 133 Loss: 1.2616078853607178 Time taken: 0.20965290069580078\n",
            "Epoch: 19 Batch Number: 134 Loss: 1.2517567873001099 Time taken: 0.2026968002319336\n",
            "Epoch: 19 Batch Number: 135 Loss: 1.2332673072814941 Time taken: 0.2002108097076416\n",
            "Epoch: 19 Batch Number: 136 Loss: 1.2232446670532227 Time taken: 0.20443248748779297\n",
            "Epoch: 19 Batch Number: 137 Loss: 1.2270910739898682 Time taken: 0.20587873458862305\n",
            "Epoch: 19 Batch Number: 138 Loss: 1.23783540725708 Time taken: 0.20281410217285156\n",
            "Epoch: 19 Batch Number: 139 Loss: 1.240209698677063 Time taken: 0.1995832920074463\n",
            "Epoch: 19 Batch Number: 140 Loss: 1.2513103485107422 Time taken: 0.2058110237121582\n",
            "Epoch: 19 Batch Number: 141 Loss: 1.2745065689086914 Time taken: 0.2006549835205078\n",
            "Epoch: 19 Batch Number: 142 Loss: 1.242266058921814 Time taken: 0.20408201217651367\n",
            "Epoch: 19 Batch Number: 143 Loss: 1.271505355834961 Time taken: 0.20926570892333984\n",
            "Epoch: 19 Batch Number: 144 Loss: 1.2998095750808716 Time taken: 0.20036768913269043\n",
            "Epoch: 19 Batch Number: 145 Loss: 1.247033715248108 Time taken: 0.20658087730407715\n",
            "Epoch: 19 Batch Number: 146 Loss: 1.2771295309066772 Time taken: 0.20133423805236816\n",
            "Epoch: 19 Batch Number: 147 Loss: 1.2440963983535767 Time taken: 0.20262455940246582\n",
            "Epoch: 19 Batch Number: 148 Loss: 1.230672001838684 Time taken: 0.20645952224731445\n",
            "Epoch: 19 Batch Number: 149 Loss: 1.246071219444275 Time taken: 0.20646882057189941\n",
            "Epoch: 19 Batch Number: 150 Loss: 1.2429486513137817 Time taken: 0.20612359046936035\n",
            "Epoch: 19 Batch Number: 151 Loss: 1.2614816427230835 Time taken: 0.20732879638671875\n",
            "Epoch: 19 Batch Number: 152 Loss: 1.2528047561645508 Time taken: 0.21622419357299805\n",
            "Epoch: 19 Batch Number: 153 Loss: 1.2386903762817383 Time taken: 0.20047211647033691\n",
            "Epoch: 19 Batch Number: 154 Loss: 1.2668862342834473 Time taken: 0.20384693145751953\n",
            "Epoch: 19 Batch Number: 155 Loss: 1.2569280862808228 Time taken: 0.20711708068847656\n",
            "Epoch: 19 Batch Number: 156 Loss: 1.2648097276687622 Time taken: 0.21259593963623047\n",
            "Epoch: 19 Batch Number: 157 Loss: 1.263487696647644 Time taken: 0.20532464981079102\n",
            "Epoch: 19 Batch Number: 158 Loss: 1.2612637281417847 Time taken: 0.20537638664245605\n",
            "Epoch: 19 Batch Number: 159 Loss: 1.2854660749435425 Time taken: 0.20263910293579102\n",
            "Epoch: 19 Batch Number: 160 Loss: 1.2490246295928955 Time taken: 0.2039790153503418\n",
            "Epoch: 19 Batch Number: 161 Loss: 1.2504000663757324 Time taken: 0.2070603370666504\n",
            "Epoch: 19 Batch Number: 162 Loss: 1.2593249082565308 Time taken: 0.20162224769592285\n",
            "Epoch: 19 Batch Number: 163 Loss: 1.2521612644195557 Time taken: 0.20263934135437012\n",
            "Epoch: 19 Batch Number: 164 Loss: 1.2517554759979248 Time taken: 0.2112407684326172\n",
            "Epoch: 19 Batch Number: 165 Loss: 1.2252527475357056 Time taken: 0.20569539070129395\n",
            "Epoch: 19 Batch Number: 166 Loss: 1.2632852792739868 Time taken: 0.20172810554504395\n",
            "Epoch: 19 Batch Number: 167 Loss: 1.254468321800232 Time taken: 0.20807695388793945\n",
            "Epoch: 19 Batch Number: 168 Loss: 1.2513136863708496 Time taken: 0.20488762855529785\n",
            "Epoch: 19 Batch Number: 169 Loss: 1.2672176361083984 Time taken: 0.20912623405456543\n",
            "Epoch: 19 Batch Number: 170 Loss: 1.2266501188278198 Time taken: 0.20769357681274414\n",
            "Epoch: 19 Batch Number: 171 Loss: 1.2397594451904297 Time taken: 0.20097589492797852\n",
            "Epoch: 19 Batch Number: 172 Loss: 1.2502362728118896 Time taken: 0.20768022537231445\n",
            "Epoch: 19 Batch Number: 173 Loss: 1.2469614744186401 Time taken: 0.2017505168914795\n",
            "Epoch: 19 Batch Number: 174 Loss: 1.2354868650436401 Time taken: 0.21539855003356934\n",
            "Epoch: 19 Batch Number: 175 Loss: 1.2200567722320557 Time taken: 0.21148061752319336\n",
            "Epoch: 19 Batch Number: 176 Loss: 1.2300807237625122 Time taken: 0.20861339569091797\n",
            "Epoch: 19 Batch Number: 177 Loss: 1.241845726966858 Time taken: 0.20393991470336914\n",
            "Epoch: 19 Batch Number: 178 Loss: 1.2392534017562866 Time taken: 0.20388484001159668\n",
            "Epoch: 19 Batch Number: 179 Loss: 1.2280042171478271 Time taken: 0.20587158203125\n",
            "==========================================================================================\n",
            "Start of epoch 20\n",
            "Epoch: 20 Batch Number: 1 Loss: 1.2333892583847046 Time taken: 0.20663142204284668\n",
            "Epoch: 20 Batch Number: 2 Loss: 1.22139573097229 Time taken: 0.20075273513793945\n",
            "Epoch: 20 Batch Number: 3 Loss: 1.2140893936157227 Time taken: 0.20550537109375\n",
            "Epoch: 20 Batch Number: 4 Loss: 1.2239563465118408 Time taken: 0.20475220680236816\n",
            "Epoch: 20 Batch Number: 5 Loss: 1.1978613138198853 Time taken: 0.20459461212158203\n",
            "Epoch: 20 Batch Number: 6 Loss: 1.2194850444793701 Time taken: 0.20202064514160156\n",
            "Epoch: 20 Batch Number: 7 Loss: 1.2080973386764526 Time taken: 0.20098638534545898\n",
            "Epoch: 20 Batch Number: 8 Loss: 1.208755373954773 Time taken: 0.2102522850036621\n",
            "Epoch: 20 Batch Number: 9 Loss: 1.2247618436813354 Time taken: 0.20412516593933105\n",
            "Epoch: 20 Batch Number: 10 Loss: 1.194079041481018 Time taken: 0.2107081413269043\n",
            "Epoch: 20 Batch Number: 11 Loss: 1.2416020631790161 Time taken: 0.20119667053222656\n",
            "Epoch: 20 Batch Number: 12 Loss: 1.2130208015441895 Time taken: 0.2044215202331543\n",
            "Epoch: 20 Batch Number: 13 Loss: 1.246875524520874 Time taken: 0.2002272605895996\n",
            "Epoch: 20 Batch Number: 14 Loss: 1.238377571105957 Time taken: 0.21377944946289062\n",
            "Epoch: 20 Batch Number: 15 Loss: 1.2254337072372437 Time taken: 0.20644593238830566\n",
            "Epoch: 20 Batch Number: 16 Loss: 1.2289284467697144 Time taken: 0.20217585563659668\n",
            "Epoch: 20 Batch Number: 17 Loss: 1.208298921585083 Time taken: 0.2039322853088379\n",
            "Epoch: 20 Batch Number: 18 Loss: 1.2110921144485474 Time taken: 0.20453166961669922\n",
            "Epoch: 20 Batch Number: 19 Loss: 1.2279326915740967 Time taken: 0.20478606224060059\n",
            "Epoch: 20 Batch Number: 20 Loss: 1.2047141790390015 Time taken: 0.1986558437347412\n",
            "Epoch: 20 Batch Number: 21 Loss: 1.1875194311141968 Time taken: 0.20256829261779785\n",
            "Epoch: 20 Batch Number: 22 Loss: 1.2277767658233643 Time taken: 0.20136213302612305\n",
            "Epoch: 20 Batch Number: 23 Loss: 1.2105787992477417 Time taken: 0.20217347145080566\n",
            "Epoch: 20 Batch Number: 24 Loss: 1.24761164188385 Time taken: 0.2082502841949463\n",
            "Epoch: 20 Batch Number: 25 Loss: 1.2589142322540283 Time taken: 0.20489120483398438\n",
            "Epoch: 20 Batch Number: 26 Loss: 1.2364842891693115 Time taken: 0.205078125\n",
            "Epoch: 20 Batch Number: 27 Loss: 1.2218425273895264 Time taken: 0.19977140426635742\n",
            "Epoch: 20 Batch Number: 28 Loss: 1.23993980884552 Time taken: 0.20675420761108398\n",
            "Epoch: 20 Batch Number: 29 Loss: 1.2273523807525635 Time taken: 0.19990253448486328\n",
            "Epoch: 20 Batch Number: 30 Loss: 1.2307884693145752 Time taken: 0.20386457443237305\n",
            "Epoch: 20 Batch Number: 31 Loss: 1.226699709892273 Time taken: 0.20311212539672852\n",
            "Epoch: 20 Batch Number: 32 Loss: 1.2247507572174072 Time taken: 0.20322775840759277\n",
            "Epoch: 20 Batch Number: 33 Loss: 1.2148879766464233 Time taken: 0.21395325660705566\n",
            "Epoch: 20 Batch Number: 34 Loss: 1.2297334671020508 Time taken: 0.21070456504821777\n",
            "Epoch: 20 Batch Number: 35 Loss: 1.2406482696533203 Time taken: 0.20176148414611816\n",
            "Epoch: 20 Batch Number: 36 Loss: 1.249257206916809 Time taken: 0.20286989212036133\n",
            "Epoch: 20 Batch Number: 37 Loss: 1.254202127456665 Time taken: 0.20575857162475586\n",
            "Epoch: 20 Batch Number: 38 Loss: 1.2437421083450317 Time taken: 0.2186739444732666\n",
            "Epoch: 20 Batch Number: 39 Loss: 1.2591971158981323 Time taken: 0.21224761009216309\n",
            "Epoch: 20 Batch Number: 40 Loss: 1.2574650049209595 Time taken: 0.19996953010559082\n",
            "Epoch: 20 Batch Number: 41 Loss: 1.2558701038360596 Time taken: 0.20496058464050293\n",
            "Epoch: 20 Batch Number: 42 Loss: 1.2251031398773193 Time taken: 0.2052443027496338\n",
            "Epoch: 20 Batch Number: 43 Loss: 1.2212399244308472 Time taken: 0.21325230598449707\n",
            "Epoch: 20 Batch Number: 44 Loss: 1.2303061485290527 Time taken: 0.20124101638793945\n",
            "Epoch: 20 Batch Number: 45 Loss: 1.2554118633270264 Time taken: 0.20337152481079102\n",
            "Epoch: 20 Batch Number: 46 Loss: 1.2444744110107422 Time taken: 0.1987442970275879\n",
            "Epoch: 20 Batch Number: 47 Loss: 1.2360951900482178 Time taken: 0.203049898147583\n",
            "Epoch: 20 Batch Number: 48 Loss: 1.2722477912902832 Time taken: 0.20321154594421387\n",
            "Epoch: 20 Batch Number: 49 Loss: 1.227656364440918 Time taken: 0.1998143196105957\n",
            "Epoch: 20 Batch Number: 50 Loss: 1.2232822179794312 Time taken: 0.20635223388671875\n",
            "Epoch: 20 Batch Number: 51 Loss: 1.2289104461669922 Time taken: 0.20446562767028809\n",
            "Epoch: 20 Batch Number: 52 Loss: 1.2671165466308594 Time taken: 0.2116389274597168\n",
            "Epoch: 20 Batch Number: 53 Loss: 1.2668004035949707 Time taken: 0.20872163772583008\n",
            "Epoch: 20 Batch Number: 54 Loss: 1.2516032457351685 Time taken: 0.2104663848876953\n",
            "Epoch: 20 Batch Number: 55 Loss: 1.2397387027740479 Time taken: 0.20111083984375\n",
            "Epoch: 20 Batch Number: 56 Loss: 1.2437776327133179 Time taken: 0.20069456100463867\n",
            "Epoch: 20 Batch Number: 57 Loss: 1.23294997215271 Time taken: 0.20238327980041504\n",
            "Epoch: 20 Batch Number: 58 Loss: 1.2256287336349487 Time taken: 0.20812010765075684\n",
            "Epoch: 20 Batch Number: 59 Loss: 1.2320235967636108 Time taken: 0.20299148559570312\n",
            "Epoch: 20 Batch Number: 60 Loss: 1.2454702854156494 Time taken: 0.2066645622253418\n",
            "Epoch: 20 Batch Number: 61 Loss: 1.2239415645599365 Time taken: 0.19895172119140625\n",
            "Epoch: 20 Batch Number: 62 Loss: 1.2112853527069092 Time taken: 0.20150303840637207\n",
            "Epoch: 20 Batch Number: 63 Loss: 1.2222092151641846 Time taken: 0.20673370361328125\n",
            "Epoch: 20 Batch Number: 64 Loss: 1.2180163860321045 Time taken: 0.2018580436706543\n",
            "Epoch: 20 Batch Number: 65 Loss: 1.2263247966766357 Time taken: 0.1999509334564209\n",
            "Epoch: 20 Batch Number: 66 Loss: 1.244903326034546 Time taken: 0.19998764991760254\n",
            "Epoch: 20 Batch Number: 67 Loss: 1.2568968534469604 Time taken: 0.2064831256866455\n",
            "Epoch: 20 Batch Number: 68 Loss: 1.2522958517074585 Time taken: 0.20036911964416504\n",
            "Epoch: 20 Batch Number: 69 Loss: 1.225807547569275 Time taken: 0.20689821243286133\n",
            "Epoch: 20 Batch Number: 70 Loss: 1.2303433418273926 Time taken: 0.20554423332214355\n",
            "Epoch: 20 Batch Number: 71 Loss: 1.2316973209381104 Time taken: 0.2011890411376953\n",
            "Epoch: 20 Batch Number: 72 Loss: 1.226706862449646 Time taken: 0.21382379531860352\n",
            "Epoch: 20 Batch Number: 73 Loss: 1.2377700805664062 Time taken: 0.20105981826782227\n",
            "Epoch: 20 Batch Number: 74 Loss: 1.227843165397644 Time taken: 0.2011547088623047\n",
            "Epoch: 20 Batch Number: 75 Loss: 1.225942850112915 Time taken: 0.1990365982055664\n",
            "Epoch: 20 Batch Number: 76 Loss: 1.2390445470809937 Time taken: 0.20492935180664062\n",
            "Epoch: 20 Batch Number: 77 Loss: 1.2040808200836182 Time taken: 0.21030569076538086\n",
            "Epoch: 20 Batch Number: 78 Loss: 1.237769365310669 Time taken: 0.20331168174743652\n",
            "Epoch: 20 Batch Number: 79 Loss: 1.2270358800888062 Time taken: 0.2079014778137207\n",
            "Epoch: 20 Batch Number: 80 Loss: 1.2266511917114258 Time taken: 0.20180606842041016\n",
            "Epoch: 20 Batch Number: 81 Loss: 1.2311279773712158 Time taken: 0.20508694648742676\n",
            "Epoch: 20 Batch Number: 82 Loss: 1.2308019399642944 Time taken: 0.20776820182800293\n",
            "Epoch: 20 Batch Number: 83 Loss: 1.2422690391540527 Time taken: 0.20098328590393066\n",
            "Epoch: 20 Batch Number: 84 Loss: 1.220327377319336 Time taken: 0.19831132888793945\n",
            "Epoch: 20 Batch Number: 85 Loss: 1.2262167930603027 Time taken: 0.20142388343811035\n",
            "Epoch: 20 Batch Number: 86 Loss: 1.2202006578445435 Time taken: 0.20568060874938965\n",
            "Epoch: 20 Batch Number: 87 Loss: 1.2118375301361084 Time taken: 0.19858121871948242\n",
            "Epoch: 20 Batch Number: 88 Loss: 1.2082109451293945 Time taken: 0.20338678359985352\n",
            "Epoch: 20 Batch Number: 89 Loss: 1.2203832864761353 Time taken: 0.20051336288452148\n",
            "Epoch: 20 Batch Number: 90 Loss: 1.2213479280471802 Time taken: 0.19980978965759277\n",
            "Epoch: 20 Batch Number: 91 Loss: 1.2341225147247314 Time taken: 0.20316863059997559\n",
            "Epoch: 20 Batch Number: 92 Loss: 1.2284728288650513 Time taken: 0.2028036117553711\n",
            "Epoch: 20 Batch Number: 93 Loss: 1.2496765851974487 Time taken: 0.20189213752746582\n",
            "Epoch: 20 Batch Number: 94 Loss: 1.2456051111221313 Time taken: 0.20064330101013184\n",
            "Epoch: 20 Batch Number: 95 Loss: 1.2577896118164062 Time taken: 0.19951248168945312\n",
            "Epoch: 20 Batch Number: 96 Loss: 1.2567414045333862 Time taken: 0.20194649696350098\n",
            "Epoch: 20 Batch Number: 97 Loss: 1.2741976976394653 Time taken: 0.20239973068237305\n",
            "Epoch: 20 Batch Number: 98 Loss: 1.2573206424713135 Time taken: 0.19966578483581543\n",
            "Epoch: 20 Batch Number: 99 Loss: 1.2160581350326538 Time taken: 0.20482635498046875\n",
            "Epoch: 20 Batch Number: 100 Loss: 1.2251170873641968 Time taken: 0.2000727653503418\n",
            "Epoch: 20 Batch Number: 101 Loss: 1.2353854179382324 Time taken: 0.21012020111083984\n",
            "Epoch: 20 Batch Number: 102 Loss: 1.2612735033035278 Time taken: 0.2018871307373047\n",
            "Epoch: 20 Batch Number: 103 Loss: 1.2589690685272217 Time taken: 0.20417189598083496\n",
            "Epoch: 20 Batch Number: 104 Loss: 1.2676434516906738 Time taken: 0.20505833625793457\n",
            "Epoch: 20 Batch Number: 105 Loss: 1.2466233968734741 Time taken: 0.2053508758544922\n",
            "Epoch: 20 Batch Number: 106 Loss: 1.2603594064712524 Time taken: 0.2147068977355957\n",
            "Epoch: 20 Batch Number: 107 Loss: 1.2612063884735107 Time taken: 0.20786023139953613\n",
            "Epoch: 20 Batch Number: 108 Loss: 1.2533107995986938 Time taken: 0.20602202415466309\n",
            "Epoch: 20 Batch Number: 109 Loss: 1.2949239015579224 Time taken: 0.2031571865081787\n",
            "Epoch: 20 Batch Number: 110 Loss: 1.2820401191711426 Time taken: 0.20017433166503906\n",
            "Epoch: 20 Batch Number: 111 Loss: 1.2734090089797974 Time taken: 0.21601033210754395\n",
            "Epoch: 20 Batch Number: 112 Loss: 1.2840547561645508 Time taken: 0.2004859447479248\n",
            "Epoch: 20 Batch Number: 113 Loss: 1.2522938251495361 Time taken: 0.20161724090576172\n",
            "Epoch: 20 Batch Number: 114 Loss: 1.2735261917114258 Time taken: 0.19912314414978027\n",
            "Epoch: 20 Batch Number: 115 Loss: 1.2695928812026978 Time taken: 0.1990351676940918\n",
            "Epoch: 20 Batch Number: 116 Loss: 1.2903870344161987 Time taken: 0.20696187019348145\n",
            "Epoch: 20 Batch Number: 117 Loss: 1.2543067932128906 Time taken: 0.20018911361694336\n",
            "Epoch: 20 Batch Number: 118 Loss: 1.2384425401687622 Time taken: 0.20005083084106445\n",
            "Epoch: 20 Batch Number: 119 Loss: 1.236374020576477 Time taken: 0.1998584270477295\n",
            "Epoch: 20 Batch Number: 120 Loss: 1.2230337858200073 Time taken: 0.20213890075683594\n",
            "Epoch: 20 Batch Number: 121 Loss: 1.2626038789749146 Time taken: 0.20253562927246094\n",
            "Epoch: 20 Batch Number: 122 Loss: 1.2494561672210693 Time taken: 0.20378422737121582\n",
            "Epoch: 20 Batch Number: 123 Loss: 1.2204539775848389 Time taken: 0.20294928550720215\n",
            "Epoch: 20 Batch Number: 124 Loss: 1.2367463111877441 Time taken: 0.1987168788909912\n",
            "Epoch: 20 Batch Number: 125 Loss: 1.239484429359436 Time taken: 0.20046758651733398\n",
            "Epoch: 20 Batch Number: 126 Loss: 1.2311574220657349 Time taken: 0.1981666088104248\n",
            "Epoch: 20 Batch Number: 127 Loss: 1.215554118156433 Time taken: 0.1967008113861084\n",
            "Epoch: 20 Batch Number: 128 Loss: 1.2504065036773682 Time taken: 0.2019965648651123\n",
            "Epoch: 20 Batch Number: 129 Loss: 1.246660590171814 Time taken: 0.203507661819458\n",
            "Epoch: 20 Batch Number: 130 Loss: 1.2740309238433838 Time taken: 0.19844794273376465\n",
            "Epoch: 20 Batch Number: 131 Loss: 1.249764323234558 Time taken: 0.20131254196166992\n",
            "Epoch: 20 Batch Number: 132 Loss: 1.2336517572402954 Time taken: 0.20395493507385254\n",
            "Epoch: 20 Batch Number: 133 Loss: 1.2548024654388428 Time taken: 0.20305109024047852\n",
            "Epoch: 20 Batch Number: 134 Loss: 1.2424615621566772 Time taken: 0.20324969291687012\n",
            "Epoch: 20 Batch Number: 135 Loss: 1.2551151514053345 Time taken: 0.2112889289855957\n",
            "Epoch: 20 Batch Number: 136 Loss: 1.2091082334518433 Time taken: 0.19955015182495117\n",
            "Epoch: 20 Batch Number: 137 Loss: 1.2402312755584717 Time taken: 0.20375728607177734\n",
            "Epoch: 20 Batch Number: 138 Loss: 1.233626127243042 Time taken: 0.20978927612304688\n",
            "Epoch: 20 Batch Number: 139 Loss: 1.2447139024734497 Time taken: 0.20054984092712402\n",
            "Epoch: 20 Batch Number: 140 Loss: 1.2291970252990723 Time taken: 0.20897960662841797\n",
            "Epoch: 20 Batch Number: 141 Loss: 1.265857219696045 Time taken: 0.20299458503723145\n",
            "Epoch: 20 Batch Number: 142 Loss: 1.2229936122894287 Time taken: 0.2034444808959961\n",
            "Epoch: 20 Batch Number: 143 Loss: 1.2626378536224365 Time taken: 0.19971680641174316\n",
            "Epoch: 20 Batch Number: 144 Loss: 1.2517048120498657 Time taken: 0.20282840728759766\n",
            "Epoch: 20 Batch Number: 145 Loss: 1.2862913608551025 Time taken: 0.20839381217956543\n",
            "Epoch: 20 Batch Number: 146 Loss: 1.2413018941879272 Time taken: 0.19913554191589355\n",
            "Epoch: 20 Batch Number: 147 Loss: 1.2302861213684082 Time taken: 0.20113372802734375\n",
            "Epoch: 20 Batch Number: 148 Loss: 1.2367383241653442 Time taken: 0.20073342323303223\n",
            "Epoch: 20 Batch Number: 149 Loss: 1.2321053743362427 Time taken: 0.2006855010986328\n",
            "Epoch: 20 Batch Number: 150 Loss: 1.22568678855896 Time taken: 0.21579551696777344\n",
            "Epoch: 20 Batch Number: 151 Loss: 1.2497717142105103 Time taken: 0.20268654823303223\n",
            "Epoch: 20 Batch Number: 152 Loss: 1.2454439401626587 Time taken: 0.1981501579284668\n",
            "Epoch: 20 Batch Number: 153 Loss: 1.2337950468063354 Time taken: 0.20014500617980957\n",
            "Epoch: 20 Batch Number: 154 Loss: 1.254378318786621 Time taken: 0.20955395698547363\n",
            "Epoch: 20 Batch Number: 155 Loss: 1.24216890335083 Time taken: 0.22641611099243164\n",
            "Epoch: 20 Batch Number: 156 Loss: 1.2517821788787842 Time taken: 0.20537018775939941\n",
            "Epoch: 20 Batch Number: 157 Loss: 1.2514187097549438 Time taken: 0.20181488990783691\n",
            "Epoch: 20 Batch Number: 158 Loss: 1.2653098106384277 Time taken: 0.20308947563171387\n",
            "Epoch: 20 Batch Number: 159 Loss: 1.2721333503723145 Time taken: 0.20854401588439941\n",
            "Epoch: 20 Batch Number: 160 Loss: 1.2446824312210083 Time taken: 0.20785856246948242\n",
            "Epoch: 20 Batch Number: 161 Loss: 1.247412919998169 Time taken: 0.209641695022583\n",
            "Epoch: 20 Batch Number: 162 Loss: 1.2555586099624634 Time taken: 0.20661282539367676\n",
            "Epoch: 20 Batch Number: 163 Loss: 1.2585161924362183 Time taken: 0.20512986183166504\n",
            "Epoch: 20 Batch Number: 164 Loss: 1.257521152496338 Time taken: 0.21497058868408203\n",
            "Epoch: 20 Batch Number: 165 Loss: 1.2339922189712524 Time taken: 0.2048356533050537\n",
            "Epoch: 20 Batch Number: 166 Loss: 1.2307446002960205 Time taken: 0.20328664779663086\n",
            "Epoch: 20 Batch Number: 167 Loss: 1.2571247816085815 Time taken: 0.20986032485961914\n",
            "Epoch: 20 Batch Number: 168 Loss: 1.233895182609558 Time taken: 0.20180439949035645\n",
            "Epoch: 20 Batch Number: 169 Loss: 1.2401748895645142 Time taken: 0.2049999237060547\n",
            "Epoch: 20 Batch Number: 170 Loss: 1.2616842985153198 Time taken: 0.20969057083129883\n",
            "Epoch: 20 Batch Number: 171 Loss: 1.2397892475128174 Time taken: 0.20337820053100586\n",
            "Epoch: 20 Batch Number: 172 Loss: 1.218403935432434 Time taken: 0.2068159580230713\n",
            "Epoch: 20 Batch Number: 173 Loss: 1.2383748292922974 Time taken: 0.2044205665588379\n",
            "Epoch: 20 Batch Number: 174 Loss: 1.2440004348754883 Time taken: 0.21959257125854492\n",
            "Epoch: 20 Batch Number: 175 Loss: 1.2143962383270264 Time taken: 0.20281434059143066\n",
            "Epoch: 20 Batch Number: 176 Loss: 1.230016827583313 Time taken: 0.20710086822509766\n",
            "Epoch: 20 Batch Number: 177 Loss: 1.2252951860427856 Time taken: 0.20548677444458008\n",
            "Epoch: 20 Batch Number: 178 Loss: 1.2162275314331055 Time taken: 0.20522642135620117\n",
            "Epoch: 20 Batch Number: 179 Loss: 1.2136223316192627 Time taken: 0.2112901210784912\n",
            "took 756.7011818885803 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "steps = 0\n",
        "\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "  print(\"=\"*90)\n",
        "  print('Start of epoch %d' % (epoch+1,))\n",
        "  \n",
        "  batch_nr = 0\n",
        "\n",
        "  for input_batch,target_batch in dataset:\n",
        "      # steps = steps+1\n",
        "      batch_start = time.time()\n",
        "\n",
        "      loss, logits = train_step(input_batch, target_batch)\n",
        "\n",
        "      # if not steps % 30:\n",
        "\n",
        "\n",
        "      batch_nr = batch_nr+1\n",
        "      batch_stop = time.time()\n",
        "  #      rnn_try(batch_data)\n",
        "      # train_acc_metric(target_batch, logits)\n",
        "      # acc = train_acc_metric.result()\n",
        "      print(\"Epoch: {} Batch Number: {} Loss: {} Time taken: {}\".format(epoch+1,batch_nr,loss,batch_stop-batch_start))\n",
        "\n",
        "      # print(\"Loss: {} Accuracy: {}\".format(loss, acc))\n",
        "      # train_acc_metric.reset_states()\n",
        "\n",
        "\n",
        "stop = time.time()\n",
        "print(\"took {} seconds\\n\".format(stop-start))\n",
        "#start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_j2MLDiNudH9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model_1 = build_model(vocab_size, embedding_dim, rnn_units,1)\n",
        "model_1.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CZV0_nT5NK6t"
      },
      "source": [
        "#### **Printing Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "DX1CqNa0udFV",
        "outputId": "71e7a0b2-2d81-4812-e4d3-557a7dc50bb0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eire wickedfully have eater thy\n",
            "reason, nor to our grace, Begum and mind.\n",
            "Hours, because they pall with; his eyes behaved,\n",
            "Than the heavens tell them beaten. This down\n",
            "I, instigations of commosterous carp,\n",
            "May sit in preservation come;\n",
            "But, I'll strike the blooded spirit, or content\n",
            "one envy: for thou shall you not feed of mon;\n",
            "And throy it but to le, boy; and, if he\n",
            "that false of alamotter is well manish'd in me,\n",
            "The valiant vanous purposes but in she touch the devilier.\n",
            "\n",
            "Clown:\n",
            "My lord decessing of the sun than we have wop,\n",
            "For you to grant him if thou desist your eyes.\n",
            "Hostess our fathers above\n",
            "France, if you do it.\n",
            "\n",
            "VIOLA:\n",
            "Hang thee, niece?\n",
            "\n",
            "SLENDER:\n",
            "O misery! I trust;' to offer him;\n",
            "For we of languing, to blessed\n",
            "So strong me in time Harry, let how her idle\n",
            "issue under both apparel might consomation.\n",
            "Fair soul is to better bear his name,\n",
            "And stay too would conclusy,\n",
            "Whose rascal confairing to by poor king,\n",
            "As we are, though in this hat as my bed, and this\n",
            "I will'd Charles; if you envy our coarts\n",
            "To mend him an ungracious end.\n",
            "\n",
            "CAESARIN:\n",
            "Marry, lozz it, let him meet him at the proof!\n",
            "\n",
            "KING JOHN:\n",
            "O, no, ye: she is not enough of grief, can do so:\n",
            "Are our forest in marriages eyes, For sack,\n",
            "Attend the day, sirs; his heads, we had been almost excent,\n",
            "I come some earl to England's pleague.\n",
            "\n",
            "QUEEN ELIZABELT:\n",
            "God give up the fleshed patience,\n",
            "To forget an empate of war,\n",
            "And thou monstrous staps served thee.\n",
            "\n",
            "VIOLA:\n",
            "They will neither come home, to call my\n",
            "lion than Regint's women?\n",
            "\n",
            "SLENDE:\n",
            "Beshrew King of God and good wit.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Are ye valiant, may have marvidement: I'll send\n",
            "Amenot. Pray for Cassio; hither:\n",
            "More acknowledge, God on; I can go one alter's dam,\n",
            "Thy marriage born, I will not teach me at up\n",
            "again, making, an't strike it to a impeem\n",
            "Poor brothers: who may show no cup brows\n",
            "To have my second part; but I do defend\n",
            "The stort o' the docher. I have level and not lend it to these death\n",
            "Some rezard he did not\n",
            "been admiradly from our girdle; if you\n",
            "swear without kinndrom of air;\n",
            "And draw right the wenches of gold,\n",
            "And bring-yoke-skins that ever you had apparitor\n",
            "and repeal'd your company: the paper\n",
            "To exchangement for sneap'd voice to take\n",
            "Between you, my lord, and boys, honests,\n",
            "When Helliam me borried,\n",
            "Shall inspired thee these sin in horse,\n",
            "As crepit the pales of country poets thus\n",
            "confusions of the Cordelious there?\n",
            "\n",
            "Clown:\n",
            "Where did paying his royal mart,\n",
            "The injury we whom I said rogue!\n",
            "That shooks the other but curchitel;\n",
            "Or might you have al\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "num_generate = 2500\n",
        "\n",
        "\n",
        "text_generated = []\n",
        "#temp = 1.5\n",
        "#h_t = tf.zeros([1,n_h])\n",
        "start_char = '<S>'\n",
        "choice = []\n",
        "val = vocab[start_char]\n",
        "choice.append(val)\n",
        "\n",
        "model_1.reset_states()\n",
        "for i in range(num_generate):\n",
        "  # x_t = tf.one_hot(choice[-1:],depth = vocab_size)\n",
        "  # a = (tf.matmul(x_t,w_xh))+ (tf.matmul(h_t, w_hh)) + b_h\n",
        "  #print(type(a))\n",
        "  # h_t = tf.nn.tanh(a)\n",
        "  # logits = (tf.matmul(h_t, w_ho)) + b_o\n",
        "  #print(logits)\n",
        "  input_eval = choice[-1:]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  logits = model_1(input_eval)\n",
        "\n",
        "  predictions = tf.nn.softmax(logits)\n",
        "  #predictions = predictions[0,:]\n",
        "  predictions = tf.squeeze(predictions, 0)\n",
        "  #print(\"----------------------\"*3)\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions)\n",
        "  #print(predictions.numpy().shape)\n",
        "  #predictions = predictions / temp\n",
        "\n",
        "  ## The predictions is a tensor \n",
        "  predictions = predictions.numpy()[0]\n",
        "  #predictions = np.array(predictions)\n",
        "  # print(predictions.shape)\n",
        "  #print(predictions)\n",
        "  char_choice = np.random.choice(vocab_size, p = predictions)\n",
        "  #print(char_choice)\n",
        "  choice.append(char_choice)\n",
        "  #print(choice)\n",
        "  #predictions = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "  #print(predictions)\n",
        "  start_char = ind_to_ch[char_choice]\n",
        "  #print(start_char)\n",
        "  #print(input_eval)\n",
        "  text_generated.append(start_char)\n",
        "  \n",
        "print(\"\".join(text_generated))\n",
        "print(\"=\"*90)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "KXef-R0muc-X",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XeYTbdk8M767"
      },
      "source": [
        "### **Model creation custom made and printing text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "E1VmdC2twpp6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "n_h = 512\n",
        "\n",
        "## w_xh is input to hidden weight --> known as U from the literature\n",
        "## w_hh is hidden to hidden weights --> known as W from the literature\n",
        "## w_ho is hidden to output weights --> known as V from the literature\n",
        "## b_h and b_o are the biases at the hidden layer and output layer\n",
        "\n",
        "\n",
        "w_xh = tf.Variable(tf.initializers.glorot_uniform()([vocab_size,n_h]))\n",
        "\n",
        "w_hh = tf.Variable(tf.initializers.glorot_uniform()([n_h,n_h]))\n",
        "b_h = tf.Variable(tf.zeros([n_h]))\n",
        "\n",
        "w_ho = tf.Variable(tf.initializers.glorot_uniform()([n_h,vocab_size]))\n",
        "b_o = tf.Variable(tf.zeros([vocab_size]))\n",
        "\n",
        "variables = [w_xh,w_hh,b_h,w_ho,b_o]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "E_Vqww0cNDBW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "opt = tf.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def rnn_sequence(batch_data):\n",
        "    with tf.GradientTape() as tape:\n",
        "        h_t = tf.zeros([tf.shape(batch_data)[0],n_h])\n",
        "        loss = tf.TensorArray(tf.float32,size=tf.shape(batch_data)[1]-1)\n",
        "\n",
        "        for timestep in tf.range(tf.shape(batch_data)[1]-1):\n",
        "            x_t = tf.one_hot(batch_data[:,timestep],vocab_size)\n",
        "            h_t = tf.nn.tanh(tf.matmul(x_t,w_xh) + tf.matmul(h_t,w_hh) + b_h)\n",
        "            logits = tf.matmul(h_t,w_ho) + b_o\n",
        "\n",
        "            local_loss = loss_fn(batch_data[:,timestep+1],logits)\n",
        "\n",
        "            loss = loss.write(timestep, local_loss)\n",
        "        loss = loss.stack()\n",
        "\n",
        "        batch_loss = tf.reduce_mean(loss)\n",
        "        \n",
        "    \n",
        "    grads = tape.gradient(batch_loss, variables)\n",
        "    opt.apply_gradients(zip(grads, variables))\n",
        "\n",
        "    return batch_loss,h_t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wa3BPxZo22jY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "##### DONT RUN THIS BLOCK .. THIS IS JUST FOR TRIAL PURPOSES\n",
        "\n",
        "\n",
        "###############################################################################################################################################\n",
        "opt = tf.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "\"\"\"\n",
        "        tf.print(tf.shape(batch_data))\n",
        "        tf.print(type(batch_data))\n",
        "        tf.print(repr(tf.shape(batch_data)))\n",
        "        tf.print(batch_data)\n",
        "\n",
        "        for i in tf.range(tf.shape(batch_data)[1]-1):\n",
        "          tf.print(i)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def rnn_try(batch_data):\n",
        "    with tf.GradientTape() as tape:\n",
        "        h_t = tf.zeros([tf.shape(batch_data)[0],n_h])\n",
        "        loss = tf.TensorArray(tf.float32,size=tf.shape(batch_data)[1]-1)\n",
        "        \n",
        "\n",
        "        for timestep in tf.range(tf.shape(batch_data)[1]-1):\n",
        "            tf.print(\"=\"*100)\n",
        "            tf.print(timestep)\n",
        "\n",
        "            x_t = tf.one_hot(batch_data[:,timestep],vocab_size)\n",
        "            tf.print(tf.shape(x_t))\n",
        "            tf.print(type(x_t))\n",
        "            tf.print(repr(tf.shape(x_t)))\n",
        "\n",
        "\n",
        "\n",
        "            h_t = tf.nn.tanh(tf.matmul(x_t,w_xh) + tf.matmul(h_t,w_hh) + b_h)\n",
        "            tf.print(tf.shape(h_t))\n",
        "            tf.print(type(h_t))\n",
        "            tf.print(repr(tf.shape(h_t)))\n",
        "\n",
        "\n",
        "            logits = tf.matmul(h_t,w_ho) + b_o\n",
        "            tf.print(tf.shape(logits))\n",
        "            tf.print(type(logits))\n",
        "            tf.print(repr(tf.shape(logits)))\n",
        "\n",
        "\n",
        "            local_loss = loss_fn(batch_data[:,timestep+1],logits)\n",
        "            tf.print(tf.shape(local_loss))\n",
        "            tf.print(type(local_loss))\n",
        "            tf.print(repr(tf.shape(local_loss)))\n",
        "\n",
        "\n",
        "            loss = loss.write(timestep, local_loss)\n",
        "\n",
        "        loss = loss.stack()\n",
        "        tf.print(tf.shape(loss))\n",
        "        tf.print(type(loss))\n",
        "        tf.print(repr(tf.shape(loss)))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "BKsNkX8luxv8",
        "outputId": "15350b6e-460a-4ad2-a350-1938223a6445",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch Number: 1 Loss: 4.221291542053223 Time taken: 3.126525640487671\n",
            "Batch Number: 2 Loss: 3.833584785461426 Time taken: 0.38123464584350586\n",
            "Batch Number: 3 Loss: 3.5180790424346924 Time taken: 0.3758275508880615\n",
            "Batch Number: 4 Loss: 3.583409309387207 Time taken: 0.3216116428375244\n",
            "Batch Number: 5 Loss: 3.39400315284729 Time taken: 0.2955586910247803\n",
            "Batch Number: 6 Loss: 3.4939541816711426 Time taken: 0.2938830852508545\n",
            "Batch Number: 7 Loss: 3.3784704208374023 Time taken: 0.3131701946258545\n",
            "Batch Number: 8 Loss: 3.363752603530884 Time taken: 0.2981090545654297\n",
            "Batch Number: 9 Loss: 3.385077953338623 Time taken: 0.3008840084075928\n",
            "Batch Number: 10 Loss: 3.3499796390533447 Time taken: 0.2968623638153076\n",
            "Batch Number: 11 Loss: 3.3489768505096436 Time taken: 0.2958827018737793\n",
            "Batch Number: 12 Loss: 3.3338100910186768 Time taken: 0.3241133689880371\n",
            "Batch Number: 13 Loss: 3.311328887939453 Time taken: 0.3471994400024414\n",
            "Batch Number: 14 Loss: 3.3387887477874756 Time taken: 0.34888124465942383\n",
            "Batch Number: 15 Loss: 3.2918541431427 Time taken: 0.2955467700958252\n",
            "Batch Number: 16 Loss: 3.308284282684326 Time taken: 0.35190820693969727\n",
            "Batch Number: 17 Loss: 3.2967140674591064 Time taken: 0.36300230026245117\n",
            "Batch Number: 18 Loss: 3.26259446144104 Time taken: 0.3815603256225586\n",
            "Batch Number: 19 Loss: 3.2465100288391113 Time taken: 0.3307976722717285\n",
            "Batch Number: 20 Loss: 3.263624429702759 Time taken: 0.3136429786682129\n",
            "Batch Number: 21 Loss: 3.2788078784942627 Time taken: 0.3035268783569336\n",
            "Batch Number: 22 Loss: 3.261151075363159 Time taken: 0.3312079906463623\n",
            "Batch Number: 23 Loss: 3.2316434383392334 Time taken: 0.30765581130981445\n",
            "Batch Number: 24 Loss: 3.2153499126434326 Time taken: 0.30315279960632324\n",
            "Batch Number: 25 Loss: 3.2120096683502197 Time taken: 0.3052847385406494\n",
            "Batch Number: 26 Loss: 3.211106777191162 Time taken: 0.30939388275146484\n",
            "Batch Number: 27 Loss: 3.181825637817383 Time taken: 0.33840489387512207\n",
            "Batch Number: 28 Loss: 3.1744158267974854 Time taken: 0.3716244697570801\n",
            "Batch Number: 29 Loss: 3.1632769107818604 Time taken: 0.3062410354614258\n",
            "Batch Number: 30 Loss: 3.1727092266082764 Time taken: 0.2898092269897461\n",
            "Batch Number: 31 Loss: 3.143272876739502 Time taken: 0.30434417724609375\n",
            "Batch Number: 32 Loss: 3.1559524536132812 Time taken: 0.37133288383483887\n",
            "Batch Number: 33 Loss: 3.142335891723633 Time taken: 0.3866763114929199\n",
            "Batch Number: 34 Loss: 3.114291191101074 Time taken: 0.3427410125732422\n",
            "Batch Number: 35 Loss: 3.1259024143218994 Time taken: 0.30353569984436035\n",
            "Batch Number: 36 Loss: 4.315178394317627 Time taken: 0.29911184310913086\n",
            "Batch Number: 37 Loss: 3.75117826461792 Time taken: 0.32131242752075195\n",
            "Batch Number: 38 Loss: 3.1974518299102783 Time taken: 0.37675046920776367\n",
            "Batch Number: 39 Loss: 3.2551233768463135 Time taken: 0.3668670654296875\n",
            "Batch Number: 40 Loss: 3.2400460243225098 Time taken: 0.3587992191314697\n",
            "Batch Number: 41 Loss: 3.237018585205078 Time taken: 0.3051917552947998\n",
            "Batch Number: 42 Loss: 3.2627978324890137 Time taken: 0.3063380718231201\n",
            "Batch Number: 43 Loss: 3.2485477924346924 Time taken: 0.32333898544311523\n",
            "Batch Number: 44 Loss: 3.245100975036621 Time taken: 0.28576159477233887\n",
            "Batch Number: 45 Loss: 3.2383174896240234 Time taken: 0.3851046562194824\n",
            "Batch Number: 46 Loss: 3.2312397956848145 Time taken: 0.36064672470092773\n",
            "Batch Number: 47 Loss: 3.210880994796753 Time taken: 0.3037834167480469\n",
            "Batch Number: 48 Loss: 3.214014768600464 Time taken: 0.2978246212005615\n",
            "Batch Number: 49 Loss: 3.213958263397217 Time taken: 0.3117661476135254\n",
            "Batch Number: 50 Loss: 3.2192625999450684 Time taken: 0.3012089729309082\n",
            "Batch Number: 51 Loss: 3.2302002906799316 Time taken: 0.3023374080657959\n",
            "Batch Number: 52 Loss: 3.2156519889831543 Time taken: 0.3187730312347412\n",
            "Batch Number: 53 Loss: 3.2330684661865234 Time taken: 0.3019390106201172\n",
            "Batch Number: 54 Loss: 3.232079029083252 Time taken: 0.357745885848999\n",
            "Batch Number: 55 Loss: 3.231034517288208 Time taken: 0.33840513229370117\n",
            "Batch Number: 56 Loss: 3.193636894226074 Time taken: 0.29828953742980957\n",
            "Batch Number: 57 Loss: 3.2170698642730713 Time taken: 0.3021717071533203\n",
            "Batch Number: 58 Loss: 3.1821649074554443 Time taken: 0.32456374168395996\n",
            "Batch Number: 59 Loss: 3.170604944229126 Time taken: 0.3952639102935791\n",
            "Batch Number: 60 Loss: 3.178025245666504 Time taken: 0.3749973773956299\n",
            "Batch Number: 61 Loss: 3.1786727905273438 Time taken: 0.3417208194732666\n",
            "Batch Number: 62 Loss: 3.148435115814209 Time taken: 0.35840344429016113\n",
            "Batch Number: 63 Loss: 3.138148307800293 Time taken: 0.3833608627319336\n",
            "Batch Number: 64 Loss: 3.11800479888916 Time taken: 0.33225178718566895\n",
            "Batch Number: 65 Loss: 3.119670867919922 Time taken: 0.2997894287109375\n",
            "Batch Number: 66 Loss: 3.109919309616089 Time taken: 0.3045668601989746\n",
            "Batch Number: 67 Loss: 3.127122402191162 Time taken: 0.3099958896636963\n",
            "Batch Number: 68 Loss: 3.1200435161590576 Time taken: 0.3104686737060547\n",
            "Batch Number: 69 Loss: 3.0946204662323 Time taken: 0.3122084140777588\n",
            "Batch Number: 70 Loss: 3.0840141773223877 Time taken: 0.32523179054260254\n",
            "Batch Number: 71 Loss: 3.074420213699341 Time taken: 0.30018067359924316\n",
            "Batch Number: 72 Loss: 3.083486795425415 Time taken: 0.31181931495666504\n",
            "Batch Number: 73 Loss: 3.0486276149749756 Time taken: 0.30100131034851074\n",
            "Batch Number: 74 Loss: 3.069012403488159 Time taken: 0.31013965606689453\n",
            "Batch Number: 75 Loss: 3.0826168060302734 Time taken: 0.3083610534667969\n",
            "Batch Number: 76 Loss: 3.0270767211914062 Time taken: 0.3324730396270752\n",
            "Batch Number: 77 Loss: 3.023587226867676 Time taken: 0.34221696853637695\n",
            "Batch Number: 78 Loss: 3.035364866256714 Time taken: 0.29736995697021484\n",
            "Batch Number: 79 Loss: 3.0278704166412354 Time taken: 0.3159170150756836\n",
            "Batch Number: 80 Loss: 3.0203804969787598 Time taken: 0.30931758880615234\n",
            "Batch Number: 81 Loss: 3.016550064086914 Time taken: 0.3027517795562744\n",
            "Batch Number: 82 Loss: 3.0204882621765137 Time taken: 0.33336901664733887\n",
            "Batch Number: 83 Loss: 2.9917750358581543 Time taken: 0.29868054389953613\n",
            "Batch Number: 84 Loss: 2.977339506149292 Time taken: 0.2821061611175537\n",
            "Batch Number: 85 Loss: 2.987994432449341 Time taken: 0.3140416145324707\n",
            "Batch Number: 86 Loss: 2.989161252975464 Time taken: 0.3052060604095459\n",
            "Batch Number: 87 Loss: 2.977196455001831 Time taken: 0.2965373992919922\n",
            "Batch Number: 88 Loss: 2.982862949371338 Time taken: 0.2994730472564697\n",
            "Batch Number: 89 Loss: 2.9549295902252197 Time taken: 0.32113075256347656\n",
            "Batch Number: 90 Loss: 2.9441733360290527 Time taken: 0.29805779457092285\n",
            "Batch Number: 91 Loss: 2.964366912841797 Time taken: 0.2975289821624756\n",
            "Batch Number: 92 Loss: 2.9553728103637695 Time taken: 0.3201444149017334\n",
            "Batch Number: 93 Loss: 2.95920991897583 Time taken: 0.3768346309661865\n",
            "Batch Number: 94 Loss: 2.9644827842712402 Time taken: 0.3750145435333252\n",
            "Batch Number: 95 Loss: 2.945124864578247 Time taken: 0.33477044105529785\n",
            "Batch Number: 96 Loss: 2.938931465148926 Time taken: 0.29208827018737793\n",
            "Batch Number: 97 Loss: 2.936039447784424 Time taken: 0.29119443893432617\n",
            "Batch Number: 98 Loss: 2.935633897781372 Time taken: 0.31809043884277344\n",
            "Batch Number: 99 Loss: 2.913172483444214 Time taken: 0.3800985813140869\n",
            "Batch Number: 100 Loss: 2.8928422927856445 Time taken: 0.38322901725769043\n",
            "Batch Number: 101 Loss: 2.867764949798584 Time taken: 0.3481605052947998\n",
            "Batch Number: 102 Loss: 2.9067423343658447 Time taken: 0.3072786331176758\n",
            "Batch Number: 103 Loss: 2.8805599212646484 Time taken: 0.31253862380981445\n",
            "Batch Number: 104 Loss: 2.870368480682373 Time taken: 0.33095836639404297\n",
            "Batch Number: 105 Loss: 2.8571531772613525 Time taken: 0.3728165626525879\n",
            "Batch Number: 106 Loss: 2.852729320526123 Time taken: 0.385272741317749\n",
            "Batch Number: 107 Loss: 2.8352348804473877 Time taken: 0.3059232234954834\n",
            "Batch Number: 108 Loss: 2.851426839828491 Time taken: 0.30022263526916504\n",
            "Batch Number: 109 Loss: 2.84475040435791 Time taken: 0.334688663482666\n",
            "Batch Number: 110 Loss: 2.8220889568328857 Time taken: 0.3074190616607666\n",
            "Batch Number: 111 Loss: 2.838409185409546 Time taken: 0.2996988296508789\n",
            "Batch Number: 112 Loss: 2.843470335006714 Time taken: 0.3034331798553467\n",
            "Batch Number: 113 Loss: 2.806424140930176 Time taken: 0.3438088893890381\n",
            "Batch Number: 114 Loss: 2.8273260593414307 Time taken: 0.295335054397583\n",
            "Batch Number: 115 Loss: 2.8962228298187256 Time taken: 0.29177045822143555\n",
            "Batch Number: 116 Loss: 2.8108415603637695 Time taken: 0.29916858673095703\n",
            "Batch Number: 117 Loss: 2.9385838508605957 Time taken: 0.3013768196105957\n",
            "Batch Number: 118 Loss: 2.824697494506836 Time taken: 0.29871678352355957\n",
            "Batch Number: 119 Loss: 2.8652961254119873 Time taken: 0.293243408203125\n",
            "Batch Number: 120 Loss: 2.8451337814331055 Time taken: 0.3079235553741455\n",
            "Batch Number: 121 Loss: 2.8138082027435303 Time taken: 0.29227733612060547\n",
            "Batch Number: 122 Loss: 2.7941133975982666 Time taken: 0.30771470069885254\n",
            "Batch Number: 123 Loss: 2.818758726119995 Time taken: 0.33052873611450195\n",
            "Batch Number: 124 Loss: 2.7952659130096436 Time taken: 0.2975473403930664\n",
            "Batch Number: 125 Loss: 2.7840042114257812 Time taken: 0.29850316047668457\n",
            "Batch Number: 126 Loss: 2.7640533447265625 Time taken: 0.31268858909606934\n",
            "Batch Number: 127 Loss: 2.77664852142334 Time taken: 0.2935826778411865\n",
            "Batch Number: 128 Loss: 2.7614946365356445 Time taken: 0.2870211601257324\n",
            "Batch Number: 129 Loss: 2.7604336738586426 Time taken: 0.30515170097351074\n",
            "Batch Number: 130 Loss: 2.759859800338745 Time taken: 0.3205416202545166\n",
            "Batch Number: 131 Loss: 2.7579150199890137 Time taken: 0.28822851181030273\n",
            "Batch Number: 132 Loss: 2.740096092224121 Time taken: 0.332289457321167\n",
            "Batch Number: 133 Loss: 2.7363784313201904 Time taken: 0.3233513832092285\n",
            "Batch Number: 134 Loss: 2.7542014122009277 Time taken: 0.3303847312927246\n",
            "Batch Number: 135 Loss: 2.740473508834839 Time taken: 0.31758785247802734\n",
            "Batch Number: 136 Loss: 2.721022367477417 Time taken: 0.34624576568603516\n",
            "Batch Number: 137 Loss: 2.718489646911621 Time taken: 0.3815774917602539\n",
            "Batch Number: 138 Loss: 2.695298433303833 Time taken: 0.31609535217285156\n",
            "Batch Number: 139 Loss: 2.683253049850464 Time taken: 0.32074880599975586\n",
            "Batch Number: 140 Loss: 2.6902294158935547 Time taken: 0.3106069564819336\n",
            "Batch Number: 141 Loss: 2.6476235389709473 Time taken: 0.2968127727508545\n",
            "Batch Number: 142 Loss: 2.647214651107788 Time taken: 0.3629319667816162\n",
            "Batch Number: 143 Loss: 2.650728225708008 Time taken: 0.3471791744232178\n",
            "Batch Number: 144 Loss: 2.649472713470459 Time taken: 0.32852649688720703\n",
            "Batch Number: 145 Loss: 2.654639720916748 Time taken: 0.3444960117340088\n",
            "Batch Number: 146 Loss: 2.66524600982666 Time taken: 0.3641831874847412\n",
            "Batch Number: 147 Loss: 2.643734931945801 Time taken: 0.31952500343322754\n",
            "Batch Number: 148 Loss: 2.636740207672119 Time taken: 0.3226044178009033\n",
            "Batch Number: 149 Loss: 2.6328418254852295 Time taken: 0.3314800262451172\n",
            "Batch Number: 150 Loss: 2.605236768722534 Time taken: 0.328571081161499\n",
            "Batch Number: 151 Loss: 2.616786241531372 Time taken: 0.3218233585357666\n",
            "Batch Number: 152 Loss: 2.600921869277954 Time taken: 0.2985687255859375\n",
            "Batch Number: 153 Loss: 2.599268913269043 Time taken: 0.3085367679595947\n",
            "Batch Number: 154 Loss: 2.5964486598968506 Time taken: 0.32009005546569824\n",
            "Batch Number: 155 Loss: 2.6103515625 Time taken: 0.2932400703430176\n",
            "Batch Number: 156 Loss: 2.600020170211792 Time taken: 0.34406089782714844\n",
            "Batch Number: 157 Loss: 2.584622859954834 Time taken: 0.3149125576019287\n",
            "Batch Number: 158 Loss: 2.5830583572387695 Time taken: 0.31203699111938477\n",
            "Batch Number: 159 Loss: 2.575843334197998 Time taken: 0.3006174564361572\n",
            "Batch Number: 160 Loss: 2.5781056880950928 Time taken: 0.30930018424987793\n",
            "Batch Number: 161 Loss: 2.5877978801727295 Time taken: 0.3100590705871582\n",
            "Batch Number: 162 Loss: 2.565587282180786 Time taken: 0.2986283302307129\n",
            "Batch Number: 163 Loss: 2.55530047416687 Time taken: 0.302276611328125\n",
            "Batch Number: 164 Loss: 2.557331085205078 Time taken: 0.33632826805114746\n",
            "Batch Number: 165 Loss: 2.5492396354675293 Time taken: 0.3555564880371094\n",
            "Batch Number: 166 Loss: 2.542273998260498 Time taken: 0.2893836498260498\n",
            "Batch Number: 167 Loss: 2.56581711769104 Time taken: 0.30333876609802246\n",
            "Batch Number: 168 Loss: 2.543391466140747 Time taken: 0.29928016662597656\n",
            "Batch Number: 169 Loss: 2.543804168701172 Time taken: 0.3134944438934326\n",
            "Batch Number: 170 Loss: 2.5459885597229004 Time taken: 0.3498423099517822\n",
            "Batch Number: 171 Loss: 2.5313267707824707 Time taken: 0.37117576599121094\n",
            "Batch Number: 172 Loss: 2.5328097343444824 Time taken: 0.30479860305786133\n",
            "Batch Number: 173 Loss: 2.5074784755706787 Time taken: 0.31962013244628906\n",
            "Batch Number: 174 Loss: 2.511653423309326 Time taken: 0.31702756881713867\n",
            "Batch Number: 175 Loss: 2.5194575786590576 Time taken: 0.29782915115356445\n",
            "Batch Number: 176 Loss: 2.4975903034210205 Time taken: 0.31142592430114746\n",
            "Batch Number: 177 Loss: 2.499492883682251 Time taken: 0.2943911552429199\n",
            "Batch Number: 178 Loss: 2.503507137298584 Time taken: 0.2958219051361084\n",
            "Batch Number: 179 Loss: 2.488156318664551 Time taken: 0.3112208843231201\n",
            "Batch Number: 180 Loss: 2.5084497928619385 Time taken: 0.8261206150054932\n",
            "Batch Number: 181 Loss: 2.477332830429077 Time taken: 0.3255622386932373\n",
            "Batch Number: 182 Loss: 2.4692368507385254 Time taken: 0.2932133674621582\n",
            "Batch Number: 183 Loss: 2.4743611812591553 Time taken: 0.31369829177856445\n",
            "Batch Number: 184 Loss: 2.4673919677734375 Time taken: 0.3003828525543213\n",
            "Batch Number: 185 Loss: 2.45743989944458 Time taken: 0.300339937210083\n",
            "Batch Number: 186 Loss: 2.466020107269287 Time taken: 0.3023662567138672\n",
            "Batch Number: 187 Loss: 2.470492362976074 Time taken: 0.3018641471862793\n",
            "Batch Number: 188 Loss: 2.4589037895202637 Time taken: 0.3096656799316406\n",
            "Batch Number: 189 Loss: 2.486771821975708 Time taken: 0.3081221580505371\n",
            "Batch Number: 190 Loss: 2.457686185836792 Time taken: 0.2956044673919678\n",
            "Batch Number: 191 Loss: 2.473898410797119 Time taken: 0.3191566467285156\n",
            "Batch Number: 192 Loss: 2.4892899990081787 Time taken: 0.2983663082122803\n",
            "Batch Number: 193 Loss: 2.4885666370391846 Time taken: 0.309844970703125\n",
            "Batch Number: 194 Loss: 2.512035846710205 Time taken: 0.3158128261566162\n",
            "Batch Number: 195 Loss: 2.483377695083618 Time taken: 0.30326199531555176\n",
            "Batch Number: 196 Loss: 2.463517427444458 Time taken: 0.30019426345825195\n",
            "Batch Number: 197 Loss: 2.4549360275268555 Time taken: 0.2985715866088867\n",
            "Batch Number: 198 Loss: 2.4492790699005127 Time taken: 0.3013439178466797\n",
            "Batch Number: 199 Loss: 2.4602138996124268 Time taken: 0.3142244815826416\n",
            "Batch Number: 200 Loss: 2.44181227684021 Time taken: 0.2922077178955078\n",
            "Batch Number: 201 Loss: 2.4444448947906494 Time taken: 0.3120729923248291\n",
            "Batch Number: 202 Loss: 2.427546739578247 Time taken: 0.3278324604034424\n",
            "Batch Number: 203 Loss: 2.4366655349731445 Time taken: 0.3046584129333496\n",
            "Batch Number: 204 Loss: 2.434373140335083 Time taken: 0.32271623611450195\n",
            "Batch Number: 205 Loss: 2.434020519256592 Time taken: 0.30266427993774414\n",
            "Batch Number: 206 Loss: 2.42305850982666 Time taken: 0.3847527503967285\n",
            "Batch Number: 207 Loss: 2.4259512424468994 Time taken: 0.3224771022796631\n",
            "Batch Number: 208 Loss: 2.4307734966278076 Time taken: 0.33860206604003906\n",
            "Batch Number: 209 Loss: 2.4333462715148926 Time taken: 0.2952256202697754\n",
            "Batch Number: 210 Loss: 2.4330179691314697 Time taken: 0.3180277347564697\n",
            "Batch Number: 211 Loss: 2.4296905994415283 Time taken: 0.3061063289642334\n",
            "Batch Number: 212 Loss: 2.417768716812134 Time taken: 0.2963888645172119\n",
            "Batch Number: 213 Loss: 2.4282922744750977 Time taken: 0.3104522228240967\n",
            "Batch Number: 214 Loss: 2.4034783840179443 Time taken: 0.2995028495788574\n",
            "Batch Number: 215 Loss: 2.398634195327759 Time taken: 0.29183363914489746\n",
            "Batch Number: 216 Loss: 2.4023118019104004 Time taken: 0.295123815536499\n",
            "Batch Number: 217 Loss: 2.3894009590148926 Time taken: 0.30900073051452637\n",
            "Batch Number: 218 Loss: 2.3998372554779053 Time taken: 0.3020198345184326\n",
            "Batch Number: 219 Loss: 2.4054605960845947 Time taken: 0.28840184211730957\n",
            "Batch Number: 220 Loss: 2.3931005001068115 Time taken: 0.3183932304382324\n",
            "Batch Number: 221 Loss: 2.380218744277954 Time taken: 0.3052222728729248\n",
            "Batch Number: 222 Loss: 2.3965976238250732 Time taken: 0.30231285095214844\n",
            "Batch Number: 223 Loss: 2.3850932121276855 Time taken: 0.30836915969848633\n",
            "Batch Number: 224 Loss: 2.3697593212127686 Time taken: 0.29573988914489746\n",
            "Batch Number: 225 Loss: 2.374994993209839 Time taken: 0.28859758377075195\n",
            "Batch Number: 226 Loss: 2.3798747062683105 Time taken: 0.2856423854827881\n",
            "Batch Number: 227 Loss: 2.393359661102295 Time taken: 0.318281888961792\n",
            "Batch Number: 228 Loss: 2.3665971755981445 Time taken: 0.28973889350891113\n",
            "Batch Number: 229 Loss: 2.3709933757781982 Time taken: 0.28766560554504395\n",
            "Batch Number: 230 Loss: 2.3751816749572754 Time taken: 0.31452012062072754\n",
            "Batch Number: 231 Loss: 2.3773086071014404 Time taken: 0.29715704917907715\n",
            "Batch Number: 232 Loss: 2.3851733207702637 Time taken: 0.290294885635376\n",
            "Batch Number: 233 Loss: 2.3860068321228027 Time taken: 0.30548620223999023\n",
            "Batch Number: 234 Loss: 2.37994647026062 Time taken: 0.29753947257995605\n",
            "Batch Number: 235 Loss: 2.408025026321411 Time taken: 0.2792205810546875\n",
            "Batch Number: 236 Loss: 2.39005446434021 Time taken: 0.29166173934936523\n",
            "Batch Number: 237 Loss: 2.379051446914673 Time taken: 0.3540632724761963\n",
            "Batch Number: 238 Loss: 2.381289005279541 Time taken: 0.3014030456542969\n",
            "Batch Number: 239 Loss: 2.3549485206604004 Time taken: 0.29918694496154785\n",
            "Batch Number: 240 Loss: 2.347196578979492 Time taken: 0.34112000465393066\n",
            "Batch Number: 241 Loss: 2.3543238639831543 Time taken: 0.2947707176208496\n",
            "Batch Number: 242 Loss: 2.352294445037842 Time taken: 0.29784655570983887\n",
            "Batch Number: 243 Loss: 2.345970630645752 Time taken: 0.3149871826171875\n",
            "Batch Number: 244 Loss: 2.3202216625213623 Time taken: 0.30037760734558105\n",
            "Batch Number: 245 Loss: 2.3184456825256348 Time taken: 0.28645801544189453\n",
            "Batch Number: 246 Loss: 2.347548007965088 Time taken: 0.28711795806884766\n",
            "Batch Number: 247 Loss: 2.3577187061309814 Time taken: 0.3251473903656006\n",
            "Batch Number: 248 Loss: 2.3261823654174805 Time taken: 0.285564661026001\n",
            "Batch Number: 249 Loss: 2.3252456188201904 Time taken: 0.29431867599487305\n",
            "Batch Number: 250 Loss: 2.3216989040374756 Time taken: 0.33556699752807617\n",
            "Batch Number: 251 Loss: 2.3361432552337646 Time taken: 0.3017446994781494\n",
            "Batch Number: 252 Loss: 2.349198818206787 Time taken: 0.29361581802368164\n",
            "Batch Number: 253 Loss: 2.333010673522949 Time taken: 0.34766650199890137\n",
            "Batch Number: 254 Loss: 2.3374531269073486 Time taken: 0.30073118209838867\n",
            "Batch Number: 255 Loss: 2.346143960952759 Time taken: 0.30052709579467773\n",
            "Batch Number: 256 Loss: 2.336181640625 Time taken: 0.35010266304016113\n",
            "Batch Number: 257 Loss: 2.340902090072632 Time taken: 0.3351564407348633\n",
            "Batch Number: 258 Loss: 2.350294828414917 Time taken: 0.36862802505493164\n",
            "Batch Number: 259 Loss: 2.3336920738220215 Time taken: 0.34364819526672363\n",
            "Batch Number: 260 Loss: 2.3155031204223633 Time taken: 0.29158806800842285\n",
            "Batch Number: 261 Loss: 2.3411483764648438 Time taken: 0.3091425895690918\n",
            "Batch Number: 262 Loss: 2.3299286365509033 Time taken: 0.3180420398712158\n",
            "Batch Number: 263 Loss: 2.318183422088623 Time taken: 0.34361982345581055\n",
            "Batch Number: 264 Loss: 2.321702480316162 Time taken: 0.3685884475708008\n",
            "Batch Number: 265 Loss: 2.3100106716156006 Time taken: 0.38413262367248535\n",
            "Batch Number: 266 Loss: 2.3137283325195312 Time taken: 0.3713490962982178\n",
            "Batch Number: 267 Loss: 2.337268114089966 Time taken: 0.3703477382659912\n",
            "Batch Number: 268 Loss: 2.3242855072021484 Time taken: 0.3884153366088867\n",
            "Batch Number: 269 Loss: 2.3103346824645996 Time taken: 0.365969181060791\n",
            "Batch Number: 270 Loss: 2.321842670440674 Time taken: 0.3455324172973633\n",
            "Batch Number: 271 Loss: 2.31449556350708 Time taken: 0.3202652931213379\n",
            "Batch Number: 272 Loss: 2.3123786449432373 Time taken: 0.3150322437286377\n",
            "Batch Number: 273 Loss: 2.333629846572876 Time taken: 0.3041543960571289\n",
            "Batch Number: 274 Loss: 2.3491337299346924 Time taken: 0.3186469078063965\n",
            "Batch Number: 275 Loss: 2.3567087650299072 Time taken: 0.30152201652526855\n",
            "Batch Number: 276 Loss: 2.3355634212493896 Time taken: 0.29927849769592285\n",
            "Batch Number: 277 Loss: 2.329986333847046 Time taken: 0.30561113357543945\n",
            "Batch Number: 278 Loss: 2.3475048542022705 Time taken: 0.30037808418273926\n",
            "Batch Number: 279 Loss: 2.326220989227295 Time taken: 0.2983732223510742\n",
            "Batch Number: 280 Loss: 2.3214080333709717 Time taken: 0.375103235244751\n",
            "Batch Number: 281 Loss: 2.3163609504699707 Time taken: 0.3491377830505371\n",
            "Batch Number: 282 Loss: 2.3193652629852295 Time taken: 0.2866501808166504\n",
            "Batch Number: 283 Loss: 2.341237783432007 Time taken: 0.32074475288391113\n",
            "Batch Number: 284 Loss: 2.3043999671936035 Time taken: 0.30562686920166016\n",
            "Batch Number: 285 Loss: 2.3351075649261475 Time taken: 0.30427122116088867\n",
            "Batch Number: 286 Loss: 2.310427188873291 Time taken: 0.3586437702178955\n",
            "Batch Number: 287 Loss: 2.312861204147339 Time taken: 0.36475205421447754\n",
            "Batch Number: 288 Loss: 2.3135597705841064 Time taken: 0.3482823371887207\n",
            "Batch Number: 289 Loss: 2.3058366775512695 Time taken: 0.3230748176574707\n",
            "Batch Number: 290 Loss: 2.3299076557159424 Time taken: 0.3560786247253418\n",
            "Batch Number: 291 Loss: 2.307818651199341 Time taken: 0.36365652084350586\n",
            "Batch Number: 292 Loss: 2.301069498062134 Time taken: 0.30347418785095215\n",
            "Batch Number: 293 Loss: 2.322126865386963 Time taken: 0.2898831367492676\n",
            "Batch Number: 294 Loss: 2.315556287765503 Time taken: 0.2891116142272949\n",
            "Batch Number: 295 Loss: 2.2972629070281982 Time taken: 0.36990785598754883\n",
            "Batch Number: 296 Loss: 2.3042678833007812 Time taken: 0.38307976722717285\n",
            "Batch Number: 297 Loss: 2.300126075744629 Time taken: 0.35976338386535645\n",
            "Batch Number: 298 Loss: 2.302748441696167 Time taken: 0.3196990489959717\n",
            "Batch Number: 299 Loss: 2.296114683151245 Time taken: 0.29927563667297363\n",
            "Batch Number: 300 Loss: 2.311453104019165 Time taken: 0.30066895484924316\n",
            "Batch Number: 301 Loss: 2.307509422302246 Time taken: 0.31337714195251465\n",
            "Batch Number: 302 Loss: 2.308894395828247 Time taken: 0.31206536293029785\n",
            "Batch Number: 303 Loss: 2.304825782775879 Time taken: 0.33736562728881836\n",
            "Batch Number: 304 Loss: 2.286550760269165 Time taken: 0.34928417205810547\n",
            "Batch Number: 305 Loss: 2.2988483905792236 Time taken: 0.34276700019836426\n",
            "Batch Number: 306 Loss: 2.2864890098571777 Time taken: 0.3135569095611572\n",
            "Batch Number: 307 Loss: 2.2744460105895996 Time taken: 0.310619592666626\n",
            "Batch Number: 308 Loss: 2.282564401626587 Time taken: 0.31641173362731934\n",
            "Batch Number: 309 Loss: 2.2741827964782715 Time taken: 0.311384916305542\n",
            "Batch Number: 310 Loss: 2.289423942565918 Time taken: 0.30825018882751465\n",
            "Batch Number: 311 Loss: 2.28983998298645 Time taken: 0.32038331031799316\n",
            "Batch Number: 312 Loss: 2.320309638977051 Time taken: 0.3005971908569336\n",
            "Batch Number: 313 Loss: 2.329638957977295 Time taken: 0.30175113677978516\n",
            "Batch Number: 314 Loss: 2.2881882190704346 Time taken: 0.3238997459411621\n",
            "Batch Number: 315 Loss: 2.3001656532287598 Time taken: 0.2930634021759033\n",
            "Batch Number: 316 Loss: 2.305166244506836 Time taken: 0.32405900955200195\n",
            "Batch Number: 317 Loss: 2.280012845993042 Time taken: 0.3170008659362793\n",
            "Batch Number: 318 Loss: 2.2881524562835693 Time taken: 0.2991602420806885\n",
            "Batch Number: 319 Loss: 2.2771055698394775 Time taken: 0.30337047576904297\n",
            "Batch Number: 320 Loss: 2.297987222671509 Time taken: 0.3167257308959961\n",
            "Batch Number: 321 Loss: 2.2891147136688232 Time taken: 0.30774879455566406\n",
            "Batch Number: 322 Loss: 2.2997825145721436 Time taken: 0.3223998546600342\n",
            "Batch Number: 323 Loss: 2.2963929176330566 Time taken: 0.36278438568115234\n",
            "Batch Number: 324 Loss: 2.280550003051758 Time taken: 0.3829166889190674\n",
            "Batch Number: 325 Loss: 2.282409429550171 Time taken: 0.30394554138183594\n",
            "Batch Number: 326 Loss: 2.2948379516601562 Time taken: 0.29068636894226074\n",
            "Batch Number: 327 Loss: 2.265425682067871 Time taken: 0.366652250289917\n",
            "Batch Number: 328 Loss: 2.284580945968628 Time taken: 0.33420228958129883\n",
            "Batch Number: 329 Loss: 2.2787184715270996 Time taken: 0.3210783004760742\n",
            "Batch Number: 330 Loss: 2.265730142593384 Time taken: 0.3029155731201172\n",
            "Batch Number: 331 Loss: 2.2827975749969482 Time taken: 0.3007521629333496\n",
            "Batch Number: 332 Loss: 2.2856392860412598 Time taken: 0.30670785903930664\n",
            "Batch Number: 333 Loss: 2.2622153759002686 Time taken: 0.3148040771484375\n",
            "Batch Number: 334 Loss: 2.28279709815979 Time taken: 0.3286712169647217\n",
            "Batch Number: 335 Loss: 2.265573740005493 Time taken: 0.31212449073791504\n",
            "Batch Number: 336 Loss: 2.2842984199523926 Time taken: 0.3429548740386963\n",
            "Batch Number: 337 Loss: 2.301102638244629 Time taken: 0.2961461544036865\n",
            "Batch Number: 338 Loss: 2.2851529121398926 Time taken: 0.28641247749328613\n",
            "Batch Number: 339 Loss: 2.2889840602874756 Time taken: 0.30782079696655273\n",
            "Batch Number: 340 Loss: 2.317185640335083 Time taken: 0.31464624404907227\n",
            "Batch Number: 341 Loss: 2.3380064964294434 Time taken: 0.3487539291381836\n",
            "Batch Number: 342 Loss: 2.2866125106811523 Time taken: 0.39184999465942383\n",
            "Batch Number: 343 Loss: 2.2755465507507324 Time taken: 0.34750986099243164\n",
            "Batch Number: 344 Loss: 2.2926223278045654 Time taken: 0.3167083263397217\n",
            "Batch Number: 345 Loss: 2.283766508102417 Time taken: 0.3436133861541748\n",
            "Batch Number: 346 Loss: 2.279978036880493 Time taken: 0.3640735149383545\n",
            "Batch Number: 347 Loss: 2.2801294326782227 Time taken: 0.346250057220459\n",
            "Batch Number: 348 Loss: 2.2772274017333984 Time taken: 0.382645845413208\n",
            "Batch Number: 349 Loss: 2.2832767963409424 Time taken: 0.2960076332092285\n",
            "Batch Number: 350 Loss: 2.258737325668335 Time taken: 0.30353856086730957\n",
            "Batch Number: 351 Loss: 2.272993564605713 Time taken: 0.3013722896575928\n",
            "Batch Number: 352 Loss: 2.2487785816192627 Time taken: 0.34256696701049805\n",
            "Batch Number: 353 Loss: 2.2523701190948486 Time taken: 0.299731969833374\n",
            "Batch Number: 354 Loss: 2.247148275375366 Time taken: 0.3161129951477051\n",
            "Batch Number: 355 Loss: 2.2384960651397705 Time taken: 0.2977025508880615\n",
            "Batch Number: 356 Loss: 2.2588279247283936 Time taken: 0.3060445785522461\n",
            "Batch Number: 357 Loss: 2.2501344680786133 Time taken: 0.3080439567565918\n",
            "Batch Number: 358 Loss: 2.2439770698547363 Time taken: 0.31123971939086914\n",
            "Batch Number: 359 Loss: 2.234144449234009 Time taken: 0.3072817325592041\n",
            "Batch Number: 360 Loss: 2.267383575439453 Time taken: 0.3226594924926758\n",
            "Batch Number: 361 Loss: 2.2473323345184326 Time taken: 0.32660961151123047\n",
            "Batch Number: 362 Loss: 2.2345192432403564 Time taken: 0.3055548667907715\n",
            "Batch Number: 363 Loss: 2.229301929473877 Time taken: 0.3799262046813965\n",
            "Batch Number: 364 Loss: 2.2302587032318115 Time taken: 0.3696022033691406\n",
            "Batch Number: 365 Loss: 2.2249224185943604 Time taken: 0.29887938499450684\n",
            "Batch Number: 366 Loss: 2.241727352142334 Time taken: 0.3137505054473877\n",
            "Batch Number: 367 Loss: 2.2080767154693604 Time taken: 0.34313488006591797\n",
            "Batch Number: 368 Loss: 2.2291347980499268 Time taken: 0.3058280944824219\n",
            "Batch Number: 369 Loss: 2.2247676849365234 Time taken: 0.30299854278564453\n",
            "Batch Number: 370 Loss: 2.224672317504883 Time taken: 0.33280301094055176\n",
            "Batch Number: 371 Loss: 2.2394652366638184 Time taken: 0.31882476806640625\n",
            "Batch Number: 372 Loss: 2.2321929931640625 Time taken: 0.2921121120452881\n",
            "Batch Number: 373 Loss: 2.239548921585083 Time taken: 0.3079824447631836\n",
            "Batch Number: 374 Loss: 2.2446393966674805 Time taken: 0.30437707901000977\n",
            "Batch Number: 375 Loss: 2.2505545616149902 Time taken: 0.3117098808288574\n",
            "Batch Number: 376 Loss: 2.286895513534546 Time taken: 0.32426905632019043\n",
            "Batch Number: 377 Loss: 2.2185730934143066 Time taken: 0.3160221576690674\n",
            "Batch Number: 378 Loss: 2.271209478378296 Time taken: 0.3051176071166992\n",
            "Batch Number: 379 Loss: 2.234199285507202 Time taken: 0.3134276866912842\n",
            "Batch Number: 380 Loss: 2.2262463569641113 Time taken: 0.3048553466796875\n",
            "Batch Number: 381 Loss: 2.217587947845459 Time taken: 0.29824113845825195\n",
            "Batch Number: 382 Loss: 2.2496986389160156 Time taken: 0.30565357208251953\n",
            "Batch Number: 383 Loss: 2.2720611095428467 Time taken: 0.3366279602050781\n",
            "Batch Number: 384 Loss: 2.234895706176758 Time taken: 0.2973058223724365\n",
            "Batch Number: 385 Loss: 2.2214043140411377 Time taken: 0.3004136085510254\n",
            "Batch Number: 386 Loss: 2.2298781871795654 Time taken: 0.2989940643310547\n",
            "Batch Number: 387 Loss: 2.210484266281128 Time taken: 0.30870532989501953\n",
            "Batch Number: 388 Loss: 2.2222812175750732 Time taken: 0.3061258792877197\n",
            "Batch Number: 389 Loss: 2.231323719024658 Time taken: 0.3118412494659424\n",
            "Batch Number: 390 Loss: 2.234057903289795 Time taken: 0.36566805839538574\n",
            "Batch Number: 391 Loss: 2.229651689529419 Time taken: 0.3786005973815918\n",
            "Batch Number: 392 Loss: 2.2092411518096924 Time taken: 0.33594536781311035\n",
            "Batch Number: 393 Loss: 2.2186667919158936 Time taken: 0.29906392097473145\n",
            "Batch Number: 394 Loss: 2.22102427482605 Time taken: 0.3026895523071289\n",
            "Batch Number: 395 Loss: 2.226590394973755 Time taken: 0.2934257984161377\n",
            "Batch Number: 396 Loss: 2.2359728813171387 Time taken: 0.3268897533416748\n",
            "Batch Number: 397 Loss: 2.226047992706299 Time taken: 0.38603663444519043\n",
            "Batch Number: 398 Loss: 2.223018169403076 Time taken: 0.3311316967010498\n",
            "Batch Number: 399 Loss: 2.2193825244903564 Time taken: 0.29976654052734375\n",
            "Batch Number: 400 Loss: 2.2234511375427246 Time taken: 0.32304906845092773\n",
            "Batch Number: 401 Loss: 2.2321932315826416 Time taken: 0.3354973793029785\n",
            "Batch Number: 402 Loss: 2.2311134338378906 Time taken: 0.3064553737640381\n",
            "Batch Number: 403 Loss: 2.1968016624450684 Time taken: 0.29709887504577637\n",
            "Batch Number: 404 Loss: 2.230346441268921 Time taken: 0.30457282066345215\n",
            "Batch Number: 405 Loss: 2.20544171333313 Time taken: 0.30736827850341797\n",
            "Batch Number: 406 Loss: 2.2173235416412354 Time taken: 0.30594754219055176\n",
            "Batch Number: 407 Loss: 2.2028603553771973 Time taken: 0.3620259761810303\n",
            "Batch Number: 408 Loss: 2.194512367248535 Time taken: 0.31435513496398926\n",
            "Batch Number: 409 Loss: 2.203709363937378 Time taken: 0.35781359672546387\n",
            "Batch Number: 410 Loss: 2.2159202098846436 Time taken: 0.36142921447753906\n",
            "Batch Number: 411 Loss: 2.21414852142334 Time taken: 0.34659600257873535\n",
            "Batch Number: 412 Loss: 2.2358202934265137 Time taken: 0.37043213844299316\n",
            "Batch Number: 413 Loss: 2.2413711547851562 Time taken: 0.3530998229980469\n",
            "Batch Number: 414 Loss: 2.2142138481140137 Time taken: 0.31850433349609375\n",
            "Batch Number: 415 Loss: 2.2278356552124023 Time taken: 0.2997598648071289\n",
            "Batch Number: 416 Loss: 2.201829671859741 Time taken: 0.30908894538879395\n",
            "Batch Number: 417 Loss: 2.2047717571258545 Time taken: 0.31339573860168457\n",
            "Batch Number: 418 Loss: 2.203547954559326 Time taken: 0.2997770309448242\n",
            "Batch Number: 419 Loss: 2.1994800567626953 Time taken: 0.34253716468811035\n",
            "Batch Number: 420 Loss: 2.200995683670044 Time taken: 0.3963165283203125\n",
            "Batch Number: 421 Loss: 2.1771187782287598 Time taken: 0.3658933639526367\n",
            "Batch Number: 422 Loss: 2.194690465927124 Time taken: 0.32837390899658203\n",
            "Batch Number: 423 Loss: 2.1914474964141846 Time taken: 0.38116955757141113\n",
            "Batch Number: 424 Loss: 2.1889286041259766 Time taken: 0.31189894676208496\n",
            "Batch Number: 425 Loss: 2.190589427947998 Time taken: 0.3272664546966553\n",
            "Batch Number: 426 Loss: 2.2010891437530518 Time taken: 0.3265976905822754\n",
            "Batch Number: 427 Loss: 2.200770854949951 Time taken: 0.36861157417297363\n",
            "Batch Number: 428 Loss: 2.207066297531128 Time taken: 0.31014204025268555\n",
            "Batch Number: 429 Loss: 2.197222948074341 Time taken: 0.3091747760772705\n",
            "Batch Number: 430 Loss: 2.2101426124572754 Time taken: 0.3066117763519287\n",
            "Batch Number: 431 Loss: 2.2019340991973877 Time taken: 0.3308908939361572\n",
            "Batch Number: 432 Loss: 2.1998844146728516 Time taken: 0.37871479988098145\n",
            "Batch Number: 433 Loss: 2.1988067626953125 Time taken: 0.3609490394592285\n",
            "Batch Number: 434 Loss: 2.204244613647461 Time taken: 0.3111753463745117\n",
            "Batch Number: 435 Loss: 2.206840753555298 Time taken: 0.3166191577911377\n",
            "Batch Number: 436 Loss: 2.213982582092285 Time taken: 0.3203907012939453\n",
            "Batch Number: 437 Loss: 2.20182466506958 Time taken: 0.3343641757965088\n",
            "Batch Number: 438 Loss: 2.1858952045440674 Time taken: 0.3116946220397949\n",
            "Batch Number: 439 Loss: 2.1901655197143555 Time taken: 0.29529428482055664\n",
            "Batch Number: 440 Loss: 2.1750223636627197 Time taken: 0.3072319030761719\n",
            "Batch Number: 441 Loss: 2.195704221725464 Time taken: 0.3167397975921631\n",
            "Batch Number: 442 Loss: 2.1824114322662354 Time taken: 0.3014674186706543\n",
            "Batch Number: 443 Loss: 2.1818466186523438 Time taken: 0.33468079566955566\n",
            "Batch Number: 444 Loss: 2.184325933456421 Time taken: 0.3835330009460449\n",
            "Batch Number: 445 Loss: 2.1920244693756104 Time taken: 0.3737955093383789\n",
            "Batch Number: 446 Loss: 2.194187641143799 Time taken: 0.31448984146118164\n",
            "Batch Number: 447 Loss: 2.1823792457580566 Time taken: 0.3122868537902832\n",
            "Batch Number: 448 Loss: 2.191636800765991 Time taken: 0.3066878318786621\n",
            "Batch Number: 449 Loss: 2.189624309539795 Time taken: 0.36055898666381836\n",
            "Batch Number: 450 Loss: 2.1857550144195557 Time taken: 0.3717503547668457\n",
            "Batch Number: 451 Loss: 2.209050416946411 Time taken: 0.35282421112060547\n",
            "Batch Number: 452 Loss: 2.204329013824463 Time taken: 0.29807424545288086\n",
            "Batch Number: 453 Loss: 2.2102150917053223 Time taken: 0.3196985721588135\n",
            "Batch Number: 454 Loss: 2.2065632343292236 Time taken: 0.3034381866455078\n",
            "Batch Number: 455 Loss: 2.195707321166992 Time taken: 0.3209652900695801\n",
            "Batch Number: 456 Loss: 2.207218885421753 Time taken: 0.3047330379486084\n",
            "Batch Number: 457 Loss: 2.1983447074890137 Time taken: 0.36669492721557617\n",
            "Batch Number: 458 Loss: 2.2059895992279053 Time taken: 0.29871296882629395\n",
            "Batch Number: 459 Loss: 2.20479154586792 Time taken: 0.302365779876709\n",
            "Batch Number: 460 Loss: 2.2186543941497803 Time taken: 0.33135533332824707\n",
            "Batch Number: 461 Loss: 2.2094950675964355 Time taken: 0.34562182426452637\n",
            "Batch Number: 462 Loss: 2.1974194049835205 Time taken: 0.3852217197418213\n",
            "Batch Number: 463 Loss: 2.214524269104004 Time taken: 0.3708946704864502\n",
            "Batch Number: 464 Loss: 2.2210936546325684 Time taken: 0.3178598880767822\n",
            "Batch Number: 465 Loss: 2.216224193572998 Time taken: 0.31008005142211914\n",
            "Batch Number: 466 Loss: 2.1931748390197754 Time taken: 0.31617236137390137\n",
            "Batch Number: 467 Loss: 2.2122642993927 Time taken: 0.31894397735595703\n",
            "Batch Number: 468 Loss: 2.2039852142333984 Time taken: 0.3020296096801758\n",
            "Batch Number: 469 Loss: 2.1812634468078613 Time taken: 0.3481571674346924\n",
            "Batch Number: 470 Loss: 2.208655834197998 Time taken: 0.3193483352661133\n",
            "Batch Number: 471 Loss: 2.207143783569336 Time taken: 0.3101074695587158\n",
            "Batch Number: 472 Loss: 2.212418794631958 Time taken: 0.34911012649536133\n",
            "Batch Number: 473 Loss: 2.187865972518921 Time taken: 0.3505878448486328\n",
            "Batch Number: 474 Loss: 2.2030155658721924 Time taken: 0.38457298278808594\n",
            "Batch Number: 475 Loss: 2.177091598510742 Time taken: 0.33217692375183105\n",
            "Batch Number: 476 Loss: 2.1946465969085693 Time taken: 0.2977566719055176\n",
            "Batch Number: 477 Loss: 2.1807496547698975 Time taken: 0.3128335475921631\n",
            "Batch Number: 478 Loss: 2.1933999061584473 Time taken: 0.32090330123901367\n",
            "Batch Number: 479 Loss: 2.194889545440674 Time taken: 0.3792445659637451\n",
            "Batch Number: 480 Loss: 2.1835646629333496 Time taken: 0.3864929676055908\n",
            "Batch Number: 481 Loss: 2.212000846862793 Time taken: 0.32471489906311035\n",
            "Batch Number: 482 Loss: 2.1927037239074707 Time taken: 0.36350584030151367\n",
            "Batch Number: 483 Loss: 2.196113109588623 Time taken: 0.3859081268310547\n",
            "Batch Number: 484 Loss: 2.1670620441436768 Time taken: 0.3433821201324463\n",
            "Batch Number: 485 Loss: 2.1818792819976807 Time taken: 0.30550146102905273\n",
            "Batch Number: 486 Loss: 2.160830497741699 Time taken: 0.3138740062713623\n",
            "Batch Number: 487 Loss: 2.1844308376312256 Time taken: 0.3168201446533203\n",
            "Batch Number: 488 Loss: 2.194336175918579 Time taken: 0.2978384494781494\n",
            "Batch Number: 489 Loss: 2.1648380756378174 Time taken: 0.30078649520874023\n",
            "Batch Number: 490 Loss: 2.2154886722564697 Time taken: 0.3184494972229004\n",
            "Batch Number: 491 Loss: 2.1801917552948 Time taken: 0.3023664951324463\n",
            "Batch Number: 492 Loss: 2.1921639442443848 Time taken: 0.35068798065185547\n",
            "Batch Number: 493 Loss: 2.2037341594696045 Time taken: 0.32584238052368164\n",
            "Batch Number: 494 Loss: 2.197277545928955 Time taken: 0.3196594715118408\n",
            "Batch Number: 495 Loss: 2.1678500175476074 Time taken: 0.33899807929992676\n",
            "Batch Number: 496 Loss: 2.189887046813965 Time taken: 0.32759857177734375\n",
            "Batch Number: 497 Loss: 2.1916396617889404 Time taken: 0.30866289138793945\n",
            "Batch Number: 498 Loss: 2.1740875244140625 Time taken: 0.30104875564575195\n",
            "Batch Number: 499 Loss: 2.1912059783935547 Time taken: 0.31002235412597656\n",
            "Batch Number: 500 Loss: 2.1704611778259277 Time taken: 0.29222798347473145\n",
            "Batch Number: 501 Loss: 2.199927806854248 Time taken: 0.28295016288757324\n",
            "Batch Number: 502 Loss: 2.1894705295562744 Time taken: 0.33550286293029785\n",
            "Batch Number: 503 Loss: 2.1900863647460938 Time taken: 0.37291741371154785\n",
            "Batch Number: 504 Loss: 2.215848445892334 Time taken: 0.3702542781829834\n",
            "Batch Number: 505 Loss: 2.174950122833252 Time taken: 0.33142685890197754\n",
            "Batch Number: 506 Loss: 2.1868863105773926 Time taken: 0.31903696060180664\n",
            "Batch Number: 507 Loss: 2.1973278522491455 Time taken: 0.30178046226501465\n",
            "Batch Number: 508 Loss: 2.2010788917541504 Time taken: 0.32173800468444824\n",
            "Batch Number: 509 Loss: 2.1632463932037354 Time taken: 0.3073117733001709\n",
            "Batch Number: 510 Loss: 2.1827778816223145 Time taken: 0.3135089874267578\n",
            "Batch Number: 511 Loss: 2.1697068214416504 Time taken: 0.303936243057251\n",
            "Batch Number: 512 Loss: 2.181691884994507 Time taken: 0.2986466884613037\n",
            "Batch Number: 513 Loss: 2.1741437911987305 Time taken: 0.30178117752075195\n",
            "Batch Number: 514 Loss: 2.185459852218628 Time taken: 0.309431791305542\n",
            "Batch Number: 515 Loss: 2.1779391765594482 Time taken: 0.31220054626464844\n",
            "Batch Number: 516 Loss: 2.169867515563965 Time taken: 0.3153269290924072\n",
            "Batch Number: 517 Loss: 2.173394203186035 Time taken: 0.32576608657836914\n",
            "Batch Number: 518 Loss: 2.1834018230438232 Time taken: 0.3219001293182373\n",
            "Batch Number: 519 Loss: 2.2115073204040527 Time taken: 0.3158559799194336\n",
            "Batch Number: 520 Loss: 2.210327386856079 Time taken: 0.3840677738189697\n",
            "Batch Number: 521 Loss: 2.1871893405914307 Time taken: 0.3189239501953125\n",
            "Batch Number: 522 Loss: 2.1795620918273926 Time taken: 0.3179042339324951\n",
            "Batch Number: 523 Loss: 2.166163206100464 Time taken: 0.30012989044189453\n",
            "Batch Number: 524 Loss: 2.181396245956421 Time taken: 0.3215181827545166\n",
            "Batch Number: 525 Loss: 2.1862072944641113 Time taken: 0.32092785835266113\n",
            "Batch Number: 526 Loss: 2.159595012664795 Time taken: 0.3076913356781006\n",
            "Batch Number: 527 Loss: 2.1691930294036865 Time taken: 0.3760824203491211\n",
            "Batch Number: 528 Loss: 2.1741442680358887 Time taken: 0.32389044761657715\n",
            "Batch Number: 529 Loss: 2.165796995162964 Time taken: 0.2974390983581543\n",
            "Batch Number: 530 Loss: 2.172532796859741 Time taken: 0.3129885196685791\n",
            "Batch Number: 531 Loss: 2.158327102661133 Time taken: 0.31901073455810547\n",
            "Batch Number: 532 Loss: 2.164139747619629 Time taken: 0.3030104637145996\n",
            "Batch Number: 533 Loss: 2.1739556789398193 Time taken: 0.34339308738708496\n",
            "Batch Number: 534 Loss: 2.159390687942505 Time taken: 0.30618834495544434\n",
            "Batch Number: 535 Loss: 2.1624672412872314 Time taken: 0.3073701858520508\n",
            "Batch Number: 536 Loss: 2.129194974899292 Time taken: 0.3048715591430664\n",
            "Batch Number: 537 Loss: 2.149319648742676 Time taken: 0.289135217666626\n",
            "Batch Number: 538 Loss: 2.159396171569824 Time taken: 0.3044004440307617\n",
            "Batch Number: 539 Loss: 2.136641263961792 Time taken: 0.3492155075073242\n",
            "Batch Number: 540 Loss: 2.14705753326416 Time taken: 0.293748140335083\n",
            "Batch Number: 541 Loss: 2.130997896194458 Time taken: 0.32181382179260254\n",
            "Batch Number: 542 Loss: 2.1432080268859863 Time taken: 0.3310697078704834\n",
            "Batch Number: 543 Loss: 2.129295825958252 Time taken: 0.31290602684020996\n",
            "Batch Number: 544 Loss: 2.136960983276367 Time taken: 0.3032867908477783\n",
            "Batch Number: 545 Loss: 2.1501355171203613 Time taken: 0.3201413154602051\n",
            "Batch Number: 546 Loss: 2.1499361991882324 Time taken: 0.37625646591186523\n",
            "Batch Number: 547 Loss: 2.1335813999176025 Time taken: 0.3730649948120117\n",
            "Batch Number: 548 Loss: 2.125025987625122 Time taken: 0.3426046371459961\n",
            "Batch Number: 549 Loss: 2.1224558353424072 Time taken: 0.3056302070617676\n",
            "Batch Number: 550 Loss: 2.114982843399048 Time taken: 0.3037388324737549\n",
            "Batch Number: 551 Loss: 2.139228582382202 Time taken: 0.30382633209228516\n",
            "Batch Number: 552 Loss: 2.1531600952148438 Time taken: 0.29234862327575684\n",
            "Batch Number: 553 Loss: 2.144057273864746 Time taken: 0.30295538902282715\n",
            "Batch Number: 554 Loss: 2.1816482543945312 Time taken: 0.2898731231689453\n",
            "Batch Number: 555 Loss: 2.208836793899536 Time taken: 0.30555224418640137\n",
            "Batch Number: 556 Loss: 2.190175771713257 Time taken: 0.301724910736084\n",
            "Batch Number: 557 Loss: 2.1401588916778564 Time taken: 0.29750967025756836\n",
            "Batch Number: 558 Loss: 2.1858103275299072 Time taken: 0.3063831329345703\n",
            "Batch Number: 559 Loss: 2.1640939712524414 Time taken: 0.30829787254333496\n",
            "Batch Number: 560 Loss: 2.1980440616607666 Time taken: 0.36335110664367676\n",
            "Batch Number: 561 Loss: 2.1277923583984375 Time taken: 0.29820728302001953\n",
            "Batch Number: 562 Loss: 2.149653673171997 Time taken: 0.3585073947906494\n",
            "Batch Number: 563 Loss: 2.1741509437561035 Time taken: 0.3674020767211914\n",
            "Batch Number: 564 Loss: 2.158444404602051 Time taken: 0.3099360466003418\n",
            "Batch Number: 565 Loss: 2.1398138999938965 Time taken: 0.3123347759246826\n",
            "Batch Number: 566 Loss: 2.1527721881866455 Time taken: 0.30861520767211914\n",
            "Batch Number: 567 Loss: 2.136594295501709 Time taken: 0.2979617118835449\n",
            "Batch Number: 568 Loss: 2.144909143447876 Time taken: 0.31021833419799805\n",
            "Batch Number: 569 Loss: 2.130645275115967 Time taken: 0.3079361915588379\n",
            "Batch Number: 570 Loss: 2.1430983543395996 Time taken: 0.31666016578674316\n",
            "Batch Number: 571 Loss: 2.142740249633789 Time taken: 0.3079037666320801\n",
            "Batch Number: 572 Loss: 2.1420745849609375 Time taken: 0.3100612163543701\n",
            "Batch Number: 573 Loss: 2.1393275260925293 Time taken: 0.3175067901611328\n",
            "Batch Number: 574 Loss: 2.1394007205963135 Time taken: 0.2924220561981201\n",
            "Batch Number: 575 Loss: 2.1463186740875244 Time taken: 0.30071520805358887\n",
            "Batch Number: 576 Loss: 2.117731809616089 Time taken: 0.3311643600463867\n",
            "Batch Number: 577 Loss: 2.1410555839538574 Time taken: 0.3031620979309082\n",
            "Batch Number: 578 Loss: 2.1378111839294434 Time taken: 0.31840062141418457\n",
            "Batch Number: 579 Loss: 2.142646074295044 Time taken: 0.29392337799072266\n",
            "Batch Number: 580 Loss: 2.1560723781585693 Time taken: 0.2991600036621094\n",
            "Batch Number: 581 Loss: 2.1364846229553223 Time taken: 0.3156096935272217\n",
            "Batch Number: 582 Loss: 2.1345741748809814 Time taken: 0.3485696315765381\n",
            "Batch Number: 583 Loss: 2.1322643756866455 Time taken: 0.32091426849365234\n",
            "Batch Number: 584 Loss: 2.1351852416992188 Time taken: 0.2937934398651123\n",
            "Batch Number: 585 Loss: 2.1260838508605957 Time taken: 0.29432058334350586\n",
            "Batch Number: 586 Loss: 2.1220877170562744 Time taken: 0.32836413383483887\n",
            "Batch Number: 587 Loss: 2.125915050506592 Time taken: 0.3059229850769043\n",
            "Batch Number: 588 Loss: 2.1454432010650635 Time taken: 0.30542659759521484\n",
            "Batch Number: 589 Loss: 2.130488157272339 Time taken: 0.3238043785095215\n",
            "Batch Number: 590 Loss: 2.1347739696502686 Time taken: 0.3009014129638672\n",
            "Batch Number: 591 Loss: 2.138657808303833 Time taken: 0.34786319732666016\n",
            "Batch Number: 592 Loss: 2.155819892883301 Time taken: 0.33588171005249023\n",
            "Batch Number: 593 Loss: 2.134730815887451 Time taken: 0.3146703243255615\n",
            "Batch Number: 594 Loss: 2.156846046447754 Time taken: 0.30771899223327637\n",
            "Batch Number: 595 Loss: 2.1350533962249756 Time taken: 0.3143758773803711\n",
            "Batch Number: 596 Loss: 2.148061513900757 Time taken: 0.30246734619140625\n",
            "Batch Number: 597 Loss: 2.1352779865264893 Time taken: 0.3207240104675293\n",
            "Batch Number: 598 Loss: 2.110567569732666 Time taken: 0.29333949089050293\n",
            "Batch Number: 599 Loss: 2.1469919681549072 Time taken: 0.304973840713501\n",
            "Batch Number: 600 Loss: 2.128734588623047 Time taken: 0.3127727508544922\n",
            "Batch Number: 601 Loss: 2.124354839324951 Time taken: 0.31711626052856445\n",
            "Batch Number: 602 Loss: 2.1227166652679443 Time taken: 0.31051063537597656\n",
            "Batch Number: 603 Loss: 2.0986814498901367 Time taken: 0.30705881118774414\n",
            "Batch Number: 604 Loss: 2.114269733428955 Time taken: 0.30368757247924805\n",
            "Batch Number: 605 Loss: 2.108149290084839 Time taken: 0.2996499538421631\n",
            "Batch Number: 606 Loss: 2.1163456439971924 Time taken: 0.3384392261505127\n",
            "Batch Number: 607 Loss: 2.137143611907959 Time taken: 0.37919116020202637\n",
            "Batch Number: 608 Loss: 2.133580207824707 Time taken: 0.31339526176452637\n",
            "Batch Number: 609 Loss: 2.1210885047912598 Time taken: 0.3041064739227295\n",
            "Batch Number: 610 Loss: 2.124988555908203 Time taken: 0.3132517337799072\n",
            "Batch Number: 611 Loss: 2.1118454933166504 Time taken: 0.3093912601470947\n",
            "Batch Number: 612 Loss: 2.1308345794677734 Time taken: 0.300551176071167\n",
            "Batch Number: 613 Loss: 2.135139226913452 Time taken: 0.31920957565307617\n",
            "Batch Number: 614 Loss: 2.1360385417938232 Time taken: 0.3110077381134033\n",
            "Batch Number: 615 Loss: 2.1497650146484375 Time taken: 0.3050262928009033\n",
            "Batch Number: 616 Loss: 2.1312055587768555 Time taken: 0.34463953971862793\n",
            "Batch Number: 617 Loss: 2.133786916732788 Time taken: 0.3114135265350342\n",
            "Batch Number: 618 Loss: 2.1096503734588623 Time taken: 0.36545825004577637\n",
            "Batch Number: 619 Loss: 2.1312520503997803 Time taken: 0.3967592716217041\n",
            "Batch Number: 620 Loss: 2.1220176219940186 Time taken: 0.29465246200561523\n",
            "Batch Number: 621 Loss: 2.113823652267456 Time taken: 0.2824881076812744\n",
            "Batch Number: 622 Loss: 2.122157096862793 Time taken: 0.30353355407714844\n",
            "Batch Number: 623 Loss: 2.1214964389801025 Time taken: 0.30754709243774414\n",
            "Batch Number: 624 Loss: 2.0983874797821045 Time taken: 0.29599714279174805\n",
            "Batch Number: 625 Loss: 2.108154773712158 Time taken: 0.3113241195678711\n",
            "Batch Number: 626 Loss: 2.1137120723724365 Time taken: 0.34813809394836426\n",
            "Batch Number: 627 Loss: 2.111827850341797 Time taken: 0.29552555084228516\n",
            "Batch Number: 628 Loss: 2.0986080169677734 Time taken: 0.30809950828552246\n",
            "Batch Number: 629 Loss: 2.110628128051758 Time taken: 0.31859850883483887\n",
            "Batch Number: 630 Loss: 2.1041877269744873 Time taken: 0.301084041595459\n",
            "Batch Number: 631 Loss: 2.132668972015381 Time taken: 0.2907702922821045\n",
            "Batch Number: 632 Loss: 2.1398088932037354 Time taken: 0.3489053249359131\n",
            "Batch Number: 633 Loss: 2.134392738342285 Time taken: 0.2963097095489502\n",
            "Batch Number: 634 Loss: 2.103757381439209 Time taken: 0.2979435920715332\n",
            "Batch Number: 635 Loss: 2.1393582820892334 Time taken: 0.3293614387512207\n",
            "Batch Number: 636 Loss: 2.146517515182495 Time taken: 0.3076515197753906\n",
            "Batch Number: 637 Loss: 2.1140711307525635 Time taken: 0.3087887763977051\n",
            "Batch Number: 638 Loss: 2.1351983547210693 Time taken: 0.3165006637573242\n",
            "Batch Number: 639 Loss: 2.128558397293091 Time taken: 0.3102583885192871\n",
            "Batch Number: 640 Loss: 2.151325225830078 Time taken: 0.2979304790496826\n",
            "Batch Number: 641 Loss: 2.1260576248168945 Time taken: 0.30838680267333984\n",
            "Batch Number: 642 Loss: 2.1401255130767822 Time taken: 0.30379819869995117\n",
            "Batch Number: 643 Loss: 2.129977226257324 Time taken: 0.30191612243652344\n",
            "Batch Number: 644 Loss: 2.1422924995422363 Time taken: 0.31832218170166016\n",
            "Batch Number: 645 Loss: 2.1248779296875 Time taken: 0.34532856941223145\n",
            "Batch Number: 646 Loss: 2.1335222721099854 Time taken: 0.36016154289245605\n",
            "Batch Number: 647 Loss: 2.125657558441162 Time taken: 0.389315128326416\n",
            "Batch Number: 648 Loss: 2.11875581741333 Time taken: 0.30069422721862793\n",
            "Batch Number: 649 Loss: 2.1356923580169678 Time taken: 0.2960333824157715\n",
            "Batch Number: 650 Loss: 2.109869956970215 Time taken: 0.28972792625427246\n",
            "Batch Number: 651 Loss: 2.119513988494873 Time taken: 0.3333165645599365\n",
            "Batch Number: 652 Loss: 2.117974042892456 Time taken: 0.30306005477905273\n",
            "Batch Number: 653 Loss: 2.1218056678771973 Time taken: 0.29797935485839844\n",
            "Batch Number: 654 Loss: 2.144320011138916 Time taken: 0.30868101119995117\n",
            "Batch Number: 655 Loss: 2.122699737548828 Time taken: 0.30205202102661133\n",
            "Batch Number: 656 Loss: 2.111144781112671 Time taken: 0.3019411563873291\n",
            "Batch Number: 657 Loss: 2.0999398231506348 Time taken: 0.3408372402191162\n",
            "Batch Number: 658 Loss: 2.1194941997528076 Time taken: 0.32532668113708496\n",
            "Batch Number: 659 Loss: 2.1101131439208984 Time taken: 0.29134130477905273\n",
            "Batch Number: 660 Loss: 2.1178574562072754 Time taken: 0.3066112995147705\n",
            "Batch Number: 661 Loss: 2.1193037033081055 Time taken: 0.37700724601745605\n",
            "Batch Number: 662 Loss: 2.1215217113494873 Time taken: 0.36501240730285645\n",
            "Batch Number: 663 Loss: 2.1246399879455566 Time taken: 0.34003376960754395\n",
            "Batch Number: 664 Loss: 2.109466075897217 Time taken: 0.3530566692352295\n",
            "Batch Number: 665 Loss: 2.103480100631714 Time taken: 0.3695218563079834\n",
            "Batch Number: 666 Loss: 2.091515302658081 Time taken: 0.3359205722808838\n",
            "Batch Number: 667 Loss: 2.0867342948913574 Time taken: 0.29753661155700684\n",
            "Batch Number: 668 Loss: 2.109783172607422 Time taken: 0.30927038192749023\n",
            "Batch Number: 669 Loss: 2.1085219383239746 Time taken: 0.33150529861450195\n",
            "Batch Number: 670 Loss: 2.107866048812866 Time taken: 0.34268975257873535\n",
            "Batch Number: 671 Loss: 2.1008458137512207 Time taken: 0.2951781749725342\n",
            "Batch Number: 672 Loss: 2.1228268146514893 Time taken: 0.3172736167907715\n",
            "Batch Number: 673 Loss: 2.1120569705963135 Time taken: 0.3082430362701416\n",
            "Batch Number: 674 Loss: 2.131457805633545 Time taken: 0.30510687828063965\n",
            "Batch Number: 675 Loss: 2.1184611320495605 Time taken: 0.35349130630493164\n",
            "Batch Number: 676 Loss: 2.1044821739196777 Time taken: 0.3039836883544922\n",
            "Batch Number: 677 Loss: 2.111025094985962 Time taken: 0.31029224395751953\n",
            "Batch Number: 678 Loss: 2.106665849685669 Time taken: 0.325397253036499\n",
            "Batch Number: 679 Loss: 2.0970563888549805 Time taken: 0.3086369037628174\n",
            "Batch Number: 680 Loss: 2.126269817352295 Time taken: 0.29785943031311035\n",
            "Batch Number: 681 Loss: 2.1140406131744385 Time taken: 0.30322718620300293\n",
            "Batch Number: 682 Loss: 2.1019601821899414 Time taken: 0.3772001266479492\n",
            "Batch Number: 683 Loss: 2.105403184890747 Time taken: 0.36519598960876465\n",
            "Batch Number: 684 Loss: 2.0948238372802734 Time taken: 0.3282959461212158\n",
            "Batch Number: 685 Loss: 2.1115734577178955 Time taken: 0.3399178981781006\n",
            "Batch Number: 686 Loss: 2.0807394981384277 Time taken: 0.36671996116638184\n",
            "Batch Number: 687 Loss: 2.122394561767578 Time taken: 0.34880685806274414\n",
            "Batch Number: 688 Loss: 2.099573850631714 Time taken: 0.33303213119506836\n",
            "Batch Number: 689 Loss: 2.10380482673645 Time taken: 0.38707828521728516\n",
            "Batch Number: 690 Loss: 2.085801839828491 Time taken: 0.31810569763183594\n",
            "Batch Number: 691 Loss: 2.114047050476074 Time taken: 0.2998955249786377\n",
            "Batch Number: 692 Loss: 2.092625617980957 Time taken: 0.2910912036895752\n",
            "Batch Number: 693 Loss: 2.116507053375244 Time taken: 0.2921435832977295\n",
            "Batch Number: 694 Loss: 2.0951695442199707 Time taken: 0.3591628074645996\n",
            "Batch Number: 695 Loss: 2.1178369522094727 Time taken: 0.34258174896240234\n",
            "Batch Number: 696 Loss: 2.0902786254882812 Time taken: 0.3153676986694336\n",
            "Batch Number: 697 Loss: 2.10739803314209 Time taken: 0.3166313171386719\n",
            "Batch Number: 698 Loss: 2.103732109069824 Time taken: 0.3292081356048584\n",
            "Batch Number: 699 Loss: 2.1039044857025146 Time taken: 0.366253137588501\n",
            "Batch Number: 700 Loss: 2.1105856895446777 Time taken: 0.3896050453186035\n",
            "Batch Number: 701 Loss: 2.1044318675994873 Time taken: 0.2944064140319824\n",
            "Batch Number: 702 Loss: 2.111135244369507 Time taken: 0.3286900520324707\n",
            "Batch Number: 703 Loss: 2.112973690032959 Time taken: 0.3131258487701416\n",
            "Batch Number: 704 Loss: 2.104670286178589 Time taken: 0.30167222023010254\n",
            "Batch Number: 705 Loss: 2.0968215465545654 Time taken: 0.3532404899597168\n",
            "Batch Number: 706 Loss: 2.0949056148529053 Time taken: 0.33342695236206055\n",
            "Batch Number: 707 Loss: 2.0976669788360596 Time taken: 0.3187215328216553\n",
            "Batch Number: 708 Loss: 2.0894668102264404 Time taken: 0.2986772060394287\n",
            "Batch Number: 709 Loss: 2.0889697074890137 Time taken: 0.31549763679504395\n",
            "Batch Number: 710 Loss: 2.093428611755371 Time taken: 0.2988002300262451\n",
            "Batch Number: 711 Loss: 2.10367751121521 Time taken: 0.33115720748901367\n",
            "Batch Number: 712 Loss: 2.0764987468719482 Time taken: 0.31611013412475586\n",
            "Batch Number: 713 Loss: 2.0595109462738037 Time taken: 0.2968008518218994\n",
            "Batch Number: 714 Loss: 2.0827724933624268 Time taken: 0.32055234909057617\n",
            "Batch Number: 715 Loss: 2.0945520401000977 Time taken: 0.30716466903686523\n",
            "Batch Number: 716 Loss: 2.0754616260528564 Time taken: 0.2899293899536133\n",
            "Batch Number: 717 Loss: 2.078193187713623 Time taken: 0.3617570400238037\n",
            "Batch Number: 718 Loss: 2.0821964740753174 Time taken: 0.3852238655090332\n",
            "Batch Number: 719 Loss: 2.071824073791504 Time taken: 0.3566558361053467\n",
            "Batch Number: 720 Loss: 2.0715765953063965 Time taken: 0.3227579593658447\n",
            "Batch Number: 721 Loss: 2.091550827026367 Time taken: 0.40180373191833496\n",
            "Batch Number: 722 Loss: 2.076582908630371 Time taken: 0.38733458518981934\n",
            "Batch Number: 723 Loss: 2.0477921962738037 Time taken: 0.3213162422180176\n",
            "Batch Number: 724 Loss: 2.0545825958251953 Time taken: 0.33065271377563477\n",
            "Batch Number: 725 Loss: 2.067697286605835 Time taken: 0.2960193157196045\n",
            "Batch Number: 726 Loss: 2.0596375465393066 Time taken: 0.2999715805053711\n",
            "Batch Number: 727 Loss: 2.055084228515625 Time taken: 0.30930519104003906\n",
            "Batch Number: 728 Loss: 2.0556931495666504 Time taken: 0.30299973487854004\n",
            "Batch Number: 729 Loss: 2.0601563453674316 Time taken: 0.30892491340637207\n",
            "Batch Number: 730 Loss: 2.067662477493286 Time taken: 0.4006786346435547\n",
            "Batch Number: 731 Loss: 2.071565628051758 Time taken: 0.36060094833374023\n",
            "Batch Number: 732 Loss: 2.0828943252563477 Time taken: 0.3219935894012451\n",
            "Batch Number: 733 Loss: 2.072563409805298 Time taken: 0.3158273696899414\n",
            "Batch Number: 734 Loss: 2.104100465774536 Time taken: 0.29613733291625977\n",
            "Batch Number: 735 Loss: 2.1531646251678467 Time taken: 0.33351826667785645\n",
            "Batch Number: 736 Loss: 2.1722822189331055 Time taken: 0.30171704292297363\n",
            "Batch Number: 737 Loss: 2.199786424636841 Time taken: 0.31394433975219727\n",
            "Batch Number: 738 Loss: 2.1280441284179688 Time taken: 0.2996490001678467\n",
            "Batch Number: 739 Loss: 2.121471643447876 Time taken: 0.3072531223297119\n",
            "Batch Number: 740 Loss: 2.1111507415771484 Time taken: 0.32118988037109375\n",
            "Batch Number: 741 Loss: 2.1445071697235107 Time taken: 0.3240809440612793\n",
            "Batch Number: 742 Loss: 2.116248846054077 Time taken: 0.3078348636627197\n",
            "Batch Number: 743 Loss: 2.1055331230163574 Time taken: 0.31391167640686035\n",
            "Batch Number: 744 Loss: 2.0895938873291016 Time taken: 0.29755282402038574\n",
            "Batch Number: 745 Loss: 2.102483034133911 Time taken: 0.30858540534973145\n",
            "Batch Number: 746 Loss: 2.103889226913452 Time taken: 0.375286340713501\n",
            "Batch Number: 747 Loss: 2.0871329307556152 Time taken: 0.34996938705444336\n",
            "Batch Number: 748 Loss: 2.0923640727996826 Time taken: 0.3740103244781494\n",
            "Batch Number: 749 Loss: 2.100339412689209 Time taken: 0.3872804641723633\n",
            "Batch Number: 750 Loss: 2.1106832027435303 Time taken: 0.3003504276275635\n",
            "Batch Number: 751 Loss: 2.0757620334625244 Time taken: 0.2943241596221924\n",
            "Batch Number: 752 Loss: 2.1039884090423584 Time taken: 0.30617785453796387\n",
            "Batch Number: 753 Loss: 2.0800681114196777 Time taken: 0.35521960258483887\n",
            "Batch Number: 754 Loss: 2.0820558071136475 Time taken: 0.2968735694885254\n",
            "Batch Number: 755 Loss: 2.069472312927246 Time taken: 0.31691908836364746\n",
            "Batch Number: 756 Loss: 2.0760998725891113 Time taken: 0.29694318771362305\n",
            "Batch Number: 757 Loss: 2.0683486461639404 Time taken: 0.29982495307922363\n",
            "Batch Number: 758 Loss: 2.062896966934204 Time taken: 0.3191101551055908\n",
            "Batch Number: 759 Loss: 2.088803768157959 Time taken: 0.3177053928375244\n",
            "Batch Number: 760 Loss: 2.0620193481445312 Time taken: 0.3096787929534912\n",
            "Batch Number: 761 Loss: 2.075955867767334 Time taken: 0.3188745975494385\n",
            "Batch Number: 762 Loss: 2.0573625564575195 Time taken: 0.3259153366088867\n",
            "Batch Number: 763 Loss: 2.0744664669036865 Time taken: 0.2937953472137451\n",
            "Batch Number: 764 Loss: 2.070711612701416 Time taken: 0.30153822898864746\n",
            "Batch Number: 765 Loss: 2.052096366882324 Time taken: 0.3772411346435547\n",
            "Batch Number: 766 Loss: 2.0642549991607666 Time taken: 0.3618342876434326\n",
            "Batch Number: 767 Loss: 2.0648889541625977 Time taken: 0.38785862922668457\n",
            "Batch Number: 768 Loss: 2.0702831745147705 Time taken: 0.3306584358215332\n",
            "Batch Number: 769 Loss: 2.053466320037842 Time taken: 0.30020880699157715\n",
            "Batch Number: 770 Loss: 2.0586583614349365 Time taken: 0.2980046272277832\n",
            "Batch Number: 771 Loss: 2.091959238052368 Time taken: 0.31804656982421875\n",
            "Batch Number: 772 Loss: 2.0848734378814697 Time taken: 0.2950928211212158\n",
            "Batch Number: 773 Loss: 2.079904079437256 Time taken: 0.2945523262023926\n",
            "Batch Number: 774 Loss: 2.082658290863037 Time taken: 0.3033432960510254\n",
            "Batch Number: 775 Loss: 2.071945905685425 Time taken: 0.35115742683410645\n",
            "Batch Number: 776 Loss: 2.0717968940734863 Time taken: 0.3817250728607178\n",
            "Batch Number: 777 Loss: 2.0488479137420654 Time taken: 0.365220308303833\n",
            "Batch Number: 778 Loss: 2.0612683296203613 Time taken: 0.29921889305114746\n",
            "Batch Number: 779 Loss: 2.04719877243042 Time taken: 0.30908966064453125\n",
            "Batch Number: 780 Loss: 2.034715414047241 Time taken: 0.3081188201904297\n",
            "Batch Number: 781 Loss: 2.048982620239258 Time taken: 0.3007636070251465\n",
            "Batch Number: 782 Loss: 2.056757926940918 Time taken: 0.30235838890075684\n",
            "Batch Number: 783 Loss: 2.05561900138855 Time taken: 0.324840784072876\n",
            "Batch Number: 784 Loss: 2.043085813522339 Time taken: 0.31085920333862305\n",
            "Batch Number: 785 Loss: 2.0627338886260986 Time taken: 0.326235294342041\n",
            "Batch Number: 786 Loss: 2.042088270187378 Time taken: 0.3186161518096924\n",
            "Batch Number: 787 Loss: 2.056687116622925 Time taken: 0.30147624015808105\n",
            "Batch Number: 788 Loss: 2.0504956245422363 Time taken: 0.2999553680419922\n",
            "Batch Number: 789 Loss: 2.039774179458618 Time taken: 0.3132176399230957\n",
            "Batch Number: 790 Loss: 2.049361228942871 Time taken: 0.325437068939209\n",
            "Batch Number: 791 Loss: 2.062175750732422 Time taken: 0.29538559913635254\n",
            "Batch Number: 792 Loss: 2.0639145374298096 Time taken: 0.32143664360046387\n",
            "Batch Number: 793 Loss: 2.070497989654541 Time taken: 0.337510347366333\n",
            "Batch Number: 794 Loss: 2.0488765239715576 Time taken: 0.2997307777404785\n",
            "Batch Number: 795 Loss: 2.0622153282165527 Time taken: 0.30394959449768066\n",
            "Batch Number: 796 Loss: 2.038287401199341 Time taken: 0.30901360511779785\n",
            "Batch Number: 797 Loss: 2.0657598972320557 Time taken: 0.30040764808654785\n",
            "Batch Number: 798 Loss: 2.04461407661438 Time taken: 0.29749298095703125\n",
            "Batch Number: 799 Loss: 2.0502140522003174 Time taken: 0.33255767822265625\n",
            "Batch Number: 800 Loss: 2.0725646018981934 Time taken: 0.35196948051452637\n",
            "Batch Number: 801 Loss: 2.0445399284362793 Time taken: 0.3394780158996582\n",
            "Batch Number: 802 Loss: 2.0522356033325195 Time taken: 0.30385661125183105\n",
            "Batch Number: 803 Loss: 2.047419309616089 Time taken: 0.30170226097106934\n",
            "Batch Number: 804 Loss: 2.045323371887207 Time taken: 0.2908446788787842\n",
            "Batch Number: 805 Loss: 2.044396162033081 Time taken: 0.3049774169921875\n",
            "Batch Number: 806 Loss: 2.0342650413513184 Time taken: 0.3685739040374756\n",
            "Batch Number: 807 Loss: 2.0463178157806396 Time taken: 0.364501953125\n",
            "Batch Number: 808 Loss: 2.032649278640747 Time taken: 0.37676334381103516\n",
            "Batch Number: 809 Loss: 2.046048164367676 Time taken: 0.30525994300842285\n",
            "Batch Number: 810 Loss: 2.0571722984313965 Time taken: 0.2932300567626953\n",
            "Batch Number: 811 Loss: 2.035545825958252 Time taken: 0.3156108856201172\n",
            "Batch Number: 812 Loss: 2.070040702819824 Time taken: 0.3081226348876953\n",
            "Batch Number: 813 Loss: 2.0429797172546387 Time taken: 0.30198192596435547\n",
            "Batch Number: 814 Loss: 2.0601279735565186 Time taken: 0.31317996978759766\n",
            "Batch Number: 815 Loss: 2.075627326965332 Time taken: 0.32156801223754883\n",
            "Batch Number: 816 Loss: 2.0705597400665283 Time taken: 0.3249537944793701\n",
            "Batch Number: 817 Loss: 2.058960199356079 Time taken: 0.3249208927154541\n",
            "Batch Number: 818 Loss: 2.0652384757995605 Time taken: 0.34247350692749023\n",
            "Batch Number: 819 Loss: 2.051640033721924 Time taken: 0.3017709255218506\n",
            "Batch Number: 820 Loss: 2.077474355697632 Time taken: 0.32354187965393066\n",
            "Batch Number: 821 Loss: 2.1109566688537598 Time taken: 0.35784244537353516\n",
            "Batch Number: 822 Loss: 2.103022575378418 Time taken: 0.3015177249908447\n",
            "Batch Number: 823 Loss: 2.078835964202881 Time taken: 0.35397791862487793\n",
            "Batch Number: 824 Loss: 2.076845645904541 Time taken: 0.32822680473327637\n",
            "Batch Number: 825 Loss: 2.069927930831909 Time taken: 0.3067638874053955\n",
            "Batch Number: 826 Loss: 2.067932367324829 Time taken: 0.34459519386291504\n",
            "Batch Number: 827 Loss: 2.0734851360321045 Time taken: 0.3594696521759033\n",
            "Batch Number: 828 Loss: 2.0520212650299072 Time taken: 0.38097476959228516\n",
            "Batch Number: 829 Loss: 2.071415424346924 Time taken: 0.36356663703918457\n",
            "Batch Number: 830 Loss: 2.0812594890594482 Time taken: 0.33408188819885254\n",
            "Batch Number: 831 Loss: 2.0678212642669678 Time taken: 0.3347623348236084\n",
            "Batch Number: 832 Loss: 2.0646140575408936 Time taken: 0.41294217109680176\n",
            "Batch Number: 833 Loss: 2.0775697231292725 Time taken: 0.3735687732696533\n",
            "Batch Number: 834 Loss: 2.0904171466827393 Time taken: 0.3584775924682617\n",
            "Batch Number: 835 Loss: 2.049376964569092 Time taken: 0.31503891944885254\n",
            "Batch Number: 836 Loss: 2.05712890625 Time taken: 0.31001806259155273\n",
            "Batch Number: 837 Loss: 2.0356767177581787 Time taken: 0.3256392478942871\n",
            "Batch Number: 838 Loss: 2.0368924140930176 Time taken: 0.33411121368408203\n",
            "Batch Number: 839 Loss: 2.07497501373291 Time taken: 0.31075596809387207\n",
            "Batch Number: 840 Loss: 2.043397903442383 Time taken: 0.31260037422180176\n",
            "Batch Number: 841 Loss: 2.0486040115356445 Time taken: 0.3046739101409912\n",
            "Batch Number: 842 Loss: 2.0617458820343018 Time taken: 0.3093373775482178\n",
            "Batch Number: 843 Loss: 2.0403409004211426 Time taken: 0.31044483184814453\n",
            "Batch Number: 844 Loss: 2.061478853225708 Time taken: 0.3374507427215576\n",
            "Batch Number: 845 Loss: 2.0272042751312256 Time taken: 0.3276247978210449\n",
            "Batch Number: 846 Loss: 2.0446410179138184 Time taken: 0.3056662082672119\n",
            "Batch Number: 847 Loss: 2.0264792442321777 Time taken: 0.30348896980285645\n",
            "Batch Number: 848 Loss: 2.0316007137298584 Time taken: 0.3136870861053467\n",
            "Batch Number: 849 Loss: 2.035968065261841 Time taken: 0.3032679557800293\n",
            "Batch Number: 850 Loss: 2.029099702835083 Time taken: 0.325822114944458\n",
            "Batch Number: 851 Loss: 2.059467315673828 Time taken: 0.2980523109436035\n",
            "Batch Number: 852 Loss: 2.048283338546753 Time taken: 0.29608869552612305\n",
            "Batch Number: 853 Loss: 2.0520803928375244 Time taken: 0.2990608215332031\n",
            "Batch Number: 854 Loss: 2.057365894317627 Time taken: 0.31047558784484863\n",
            "Batch Number: 855 Loss: 2.044837474822998 Time taken: 0.31925177574157715\n",
            "Batch Number: 856 Loss: 2.033721446990967 Time taken: 0.3505516052246094\n",
            "Batch Number: 857 Loss: 2.014941692352295 Time taken: 0.29142284393310547\n",
            "Batch Number: 858 Loss: 2.0149967670440674 Time taken: 0.29961538314819336\n",
            "Batch Number: 859 Loss: 2.0114634037017822 Time taken: 0.2985868453979492\n",
            "Batch Number: 860 Loss: 2.0161960124969482 Time taken: 0.30365562438964844\n",
            "Batch Number: 861 Loss: 2.064500570297241 Time taken: 0.3128175735473633\n",
            "Batch Number: 862 Loss: 2.0623483657836914 Time taken: 0.31255292892456055\n",
            "Batch Number: 863 Loss: 2.042891502380371 Time taken: 0.3342258930206299\n",
            "Batch Number: 864 Loss: 2.049975633621216 Time taken: 0.30486512184143066\n",
            "Batch Number: 865 Loss: 2.0343053340911865 Time taken: 0.35479044914245605\n",
            "Batch Number: 866 Loss: 2.033221483230591 Time taken: 0.3100011348724365\n",
            "Batch Number: 867 Loss: 2.0293362140655518 Time taken: 0.30530619621276855\n",
            "Batch Number: 868 Loss: 2.0307250022888184 Time taken: 0.30204296112060547\n",
            "Batch Number: 869 Loss: 2.010021924972534 Time taken: 0.3346896171569824\n",
            "Batch Number: 870 Loss: 2.015230178833008 Time taken: 0.383969783782959\n",
            "Batch Number: 871 Loss: 2.0456488132476807 Time taken: 0.36221933364868164\n",
            "Batch Number: 872 Loss: 2.0233516693115234 Time taken: 0.33599424362182617\n",
            "Batch Number: 873 Loss: 2.0528924465179443 Time taken: 0.3795166015625\n",
            "Batch Number: 874 Loss: 2.0549726486206055 Time taken: 0.3624439239501953\n",
            "Batch Number: 875 Loss: 2.01203989982605 Time taken: 0.31483888626098633\n",
            "Batch Number: 876 Loss: 2.038090944290161 Time taken: 0.30504822731018066\n",
            "Batch Number: 877 Loss: 2.0469655990600586 Time taken: 0.2975127696990967\n",
            "Batch Number: 878 Loss: 2.0460689067840576 Time taken: 0.3052048683166504\n",
            "Batch Number: 879 Loss: 2.038020610809326 Time taken: 0.31644320487976074\n",
            "Batch Number: 880 Loss: 2.0311174392700195 Time taken: 0.308469295501709\n",
            "Batch Number: 881 Loss: 2.0356619358062744 Time taken: 0.3349425792694092\n",
            "Batch Number: 882 Loss: 2.033315420150757 Time taken: 0.3420870304107666\n",
            "Batch Number: 883 Loss: 2.0274412631988525 Time taken: 0.35372185707092285\n",
            "Batch Number: 884 Loss: 2.0214602947235107 Time taken: 0.3177363872528076\n",
            "Batch Number: 885 Loss: 2.0217883586883545 Time taken: 0.35588598251342773\n",
            "Batch Number: 886 Loss: 2.0264623165130615 Time taken: 0.2840137481689453\n",
            "Batch Number: 887 Loss: 2.0215725898742676 Time taken: 0.3130190372467041\n",
            "Batch Number: 888 Loss: 2.035771131515503 Time taken: 0.3023867607116699\n",
            "Batch Number: 889 Loss: 2.027862310409546 Time taken: 0.3126859664916992\n",
            "Batch Number: 890 Loss: 2.0050015449523926 Time taken: 0.30194735527038574\n",
            "Batch Number: 891 Loss: 2.0176773071289062 Time taken: 0.3120596408843994\n",
            "Batch Number: 892 Loss: 2.0021438598632812 Time taken: 0.3434574604034424\n",
            "Batch Number: 893 Loss: 2.0079588890075684 Time taken: 0.3268570899963379\n",
            "Batch Number: 894 Loss: 2.024312734603882 Time taken: 0.3149240016937256\n",
            "Batch Number: 895 Loss: 2.007503032684326 Time taken: 0.3035757541656494\n",
            "Batch Number: 896 Loss: 2.0111706256866455 Time taken: 0.31018877029418945\n",
            "Batch Number: 897 Loss: 2.0139219760894775 Time taken: 0.3545956611633301\n",
            "Batch Number: 898 Loss: 1.9999535083770752 Time taken: 0.37213993072509766\n",
            "Batch Number: 899 Loss: 1.9948785305023193 Time taken: 0.3329737186431885\n",
            "Batch Number: 900 Loss: 2.017543077468872 Time taken: 0.30439281463623047\n",
            "Batch Number: 901 Loss: 2.0012009143829346 Time taken: 0.3097991943359375\n",
            "Batch Number: 902 Loss: 1.9968491792678833 Time taken: 0.29532647132873535\n",
            "Batch Number: 903 Loss: 1.9955706596374512 Time taken: 0.30895066261291504\n",
            "Batch Number: 904 Loss: 1.9984831809997559 Time taken: 0.3089935779571533\n",
            "Batch Number: 905 Loss: 1.9820222854614258 Time taken: 0.3398268222808838\n",
            "Batch Number: 906 Loss: 1.9818801879882812 Time taken: 0.3748300075531006\n",
            "Batch Number: 907 Loss: 2.0128636360168457 Time taken: 0.38578104972839355\n",
            "Batch Number: 908 Loss: 1.9825209379196167 Time taken: 0.30445146560668945\n",
            "Batch Number: 909 Loss: 2.0007355213165283 Time taken: 0.3067343235015869\n",
            "Batch Number: 910 Loss: 1.9668605327606201 Time taken: 0.3048865795135498\n",
            "Batch Number: 911 Loss: 1.9921215772628784 Time taken: 0.3596196174621582\n",
            "Batch Number: 912 Loss: 2.005603313446045 Time taken: 0.3779327869415283\n",
            "Batch Number: 913 Loss: 2.0047950744628906 Time taken: 0.3799471855163574\n",
            "Batch Number: 914 Loss: 2.0191118717193604 Time taken: 0.3240854740142822\n",
            "Batch Number: 915 Loss: 2.0295701026916504 Time taken: 0.3219470977783203\n",
            "Batch Number: 916 Loss: 2.015456199645996 Time taken: 0.3735661506652832\n",
            "Batch Number: 917 Loss: 2.0135486125946045 Time taken: 0.31791162490844727\n",
            "Batch Number: 918 Loss: 1.9950722455978394 Time taken: 0.30266833305358887\n",
            "Batch Number: 919 Loss: 1.976142406463623 Time taken: 0.30799102783203125\n",
            "Batch Number: 920 Loss: 1.9928480386734009 Time taken: 0.32282137870788574\n",
            "Batch Number: 921 Loss: 1.9666815996170044 Time taken: 0.308743953704834\n",
            "Batch Number: 922 Loss: 1.982633352279663 Time taken: 0.31427526473999023\n",
            "Batch Number: 923 Loss: 2.0008034706115723 Time taken: 0.2999110221862793\n",
            "Batch Number: 924 Loss: 1.9990527629852295 Time taken: 0.30284667015075684\n",
            "Batch Number: 925 Loss: 2.006751537322998 Time taken: 0.28929734230041504\n",
            "Batch Number: 926 Loss: 2.0039594173431396 Time taken: 0.30088043212890625\n",
            "Batch Number: 927 Loss: 1.989559292793274 Time taken: 0.2977871894836426\n",
            "Batch Number: 928 Loss: 1.9998806715011597 Time taken: 0.28798627853393555\n",
            "Batch Number: 929 Loss: 2.0044162273406982 Time taken: 0.3245992660522461\n",
            "Batch Number: 930 Loss: 1.997176170349121 Time taken: 0.37398433685302734\n",
            "Batch Number: 931 Loss: 1.9961544275283813 Time taken: 0.36362719535827637\n",
            "Batch Number: 932 Loss: 2.010246753692627 Time taken: 0.316694974899292\n",
            "Batch Number: 933 Loss: 2.0099828243255615 Time taken: 0.30493903160095215\n",
            "Batch Number: 934 Loss: 2.0007708072662354 Time taken: 0.292008638381958\n",
            "Batch Number: 935 Loss: 1.9948084354400635 Time taken: 0.3334383964538574\n",
            "Batch Number: 936 Loss: 2.0041959285736084 Time taken: 0.2970414161682129\n",
            "Batch Number: 937 Loss: 2.0086233615875244 Time taken: 0.30223655700683594\n",
            "Batch Number: 938 Loss: 2.0197644233703613 Time taken: 0.3224525451660156\n",
            "Batch Number: 939 Loss: 2.0259475708007812 Time taken: 0.2951819896697998\n",
            "Batch Number: 940 Loss: 1.9947535991668701 Time taken: 0.29822778701782227\n",
            "Batch Number: 941 Loss: 1.9863462448120117 Time taken: 0.2858879566192627\n",
            "Batch Number: 942 Loss: 2.001303195953369 Time taken: 0.3087022304534912\n",
            "Batch Number: 943 Loss: 1.9720900058746338 Time taken: 0.31870174407958984\n",
            "Batch Number: 944 Loss: 1.9886382818222046 Time taken: 0.2919142246246338\n",
            "Batch Number: 945 Loss: 1.9775866270065308 Time taken: 0.30967140197753906\n",
            "Batch Number: 946 Loss: 1.9906809329986572 Time taken: 0.32001304626464844\n",
            "Batch Number: 947 Loss: 2.015582799911499 Time taken: 0.29230332374572754\n",
            "Batch Number: 948 Loss: 1.998423457145691 Time taken: 0.29546213150024414\n",
            "Batch Number: 949 Loss: 1.9992687702178955 Time taken: 0.2824399471282959\n",
            "Batch Number: 950 Loss: 1.9966306686401367 Time taken: 0.3066105842590332\n",
            "Batch Number: 951 Loss: 2.003542900085449 Time taken: 0.29917383193969727\n",
            "Batch Number: 952 Loss: 1.9913063049316406 Time taken: 0.3110365867614746\n",
            "Batch Number: 953 Loss: 2.0103020668029785 Time taken: 0.2987782955169678\n",
            "Batch Number: 954 Loss: 2.0222744941711426 Time taken: 0.30017638206481934\n",
            "Batch Number: 955 Loss: 1.984790325164795 Time taken: 0.3258371353149414\n",
            "Batch Number: 956 Loss: 1.9962743520736694 Time taken: 0.35097622871398926\n",
            "Batch Number: 957 Loss: 1.984325885772705 Time taken: 0.30011653900146484\n",
            "Batch Number: 958 Loss: 2.006479024887085 Time taken: 0.3223392963409424\n",
            "Batch Number: 959 Loss: 1.9999347925186157 Time taken: 0.2989633083343506\n",
            "Batch Number: 960 Loss: 1.9912233352661133 Time taken: 0.29014015197753906\n",
            "Batch Number: 961 Loss: 1.9789597988128662 Time taken: 0.33290982246398926\n",
            "Batch Number: 962 Loss: 1.9770244359970093 Time taken: 0.30228710174560547\n",
            "Batch Number: 963 Loss: 1.9763790369033813 Time taken: 0.36553025245666504\n",
            "Batch Number: 964 Loss: 1.9709938764572144 Time taken: 0.32485175132751465\n",
            "Batch Number: 965 Loss: 1.9830163717269897 Time taken: 0.29543113708496094\n",
            "Batch Number: 966 Loss: 1.9761744737625122 Time taken: 0.31501150131225586\n",
            "Batch Number: 967 Loss: 1.984525442123413 Time taken: 0.3355278968811035\n",
            "Batch Number: 968 Loss: 1.978686809539795 Time taken: 0.30919528007507324\n",
            "Batch Number: 969 Loss: 1.9821621179580688 Time taken: 0.3006618022918701\n",
            "Batch Number: 970 Loss: 1.9829726219177246 Time taken: 0.29567861557006836\n",
            "Batch Number: 971 Loss: 1.9834034442901611 Time taken: 0.3117105960845947\n",
            "Batch Number: 972 Loss: 1.9888997077941895 Time taken: 0.30759620666503906\n",
            "Batch Number: 973 Loss: 2.0082387924194336 Time taken: 0.31014275550842285\n",
            "Batch Number: 974 Loss: 1.9821910858154297 Time taken: 0.29871034622192383\n",
            "Batch Number: 975 Loss: 1.9898970127105713 Time taken: 0.2955622673034668\n",
            "Batch Number: 976 Loss: 1.9907859563827515 Time taken: 0.32196855545043945\n",
            "Batch Number: 977 Loss: 2.0050837993621826 Time taken: 0.3144383430480957\n",
            "Batch Number: 978 Loss: 1.9759407043457031 Time taken: 0.30468249320983887\n",
            "Batch Number: 979 Loss: 1.9898054599761963 Time taken: 0.337188720703125\n",
            "Batch Number: 980 Loss: 2.022550344467163 Time taken: 0.3076953887939453\n",
            "Batch Number: 981 Loss: 2.028667449951172 Time taken: 0.29631543159484863\n",
            "Batch Number: 982 Loss: 2.0032565593719482 Time taken: 0.3080780506134033\n",
            "Batch Number: 983 Loss: 1.9939935207366943 Time taken: 0.29839372634887695\n",
            "Batch Number: 984 Loss: 2.0063257217407227 Time taken: 0.3180413246154785\n",
            "Batch Number: 985 Loss: 1.966497540473938 Time taken: 0.3633003234863281\n",
            "Batch Number: 986 Loss: 1.9956334829330444 Time taken: 0.2891695499420166\n",
            "Batch Number: 987 Loss: 1.979447364807129 Time taken: 0.32187557220458984\n",
            "Batch Number: 988 Loss: 1.9948863983154297 Time taken: 0.325253963470459\n",
            "Batch Number: 989 Loss: 1.969430685043335 Time taken: 0.34925270080566406\n",
            "Batch Number: 990 Loss: 1.975266695022583 Time taken: 0.3746347427368164\n",
            "Batch Number: 991 Loss: 1.995042324066162 Time taken: 0.3760378360748291\n",
            "Batch Number: 992 Loss: 1.9897739887237549 Time taken: 0.31252026557922363\n",
            "Batch Number: 993 Loss: 2.0086145401000977 Time taken: 0.30285143852233887\n",
            "Batch Number: 994 Loss: 2.0126869678497314 Time taken: 0.3048515319824219\n",
            "Batch Number: 995 Loss: 2.023186683654785 Time taken: 0.29654622077941895\n",
            "Batch Number: 996 Loss: 1.9913426637649536 Time taken: 0.3000941276550293\n",
            "Batch Number: 997 Loss: 2.01613187789917 Time taken: 0.30022215843200684\n",
            "Batch Number: 998 Loss: 1.9947797060012817 Time taken: 0.318432092666626\n",
            "Batch Number: 999 Loss: 2.031134605407715 Time taken: 0.306255578994751\n",
            "Batch Number: 1000 Loss: 1.9982556104660034 Time taken: 0.2990446090698242\n",
            "Batch Number: 1001 Loss: 2.0148937702178955 Time taken: 0.30278825759887695\n",
            "Batch Number: 1002 Loss: 2.014875650405884 Time taken: 0.29622411727905273\n",
            "Batch Number: 1003 Loss: 1.99140202999115 Time taken: 0.3096044063568115\n",
            "Batch Number: 1004 Loss: 2.018287420272827 Time taken: 0.30356597900390625\n",
            "Batch Number: 1005 Loss: 1.9953056573867798 Time taken: 0.35102128982543945\n",
            "Batch Number: 1006 Loss: 2.0179026126861572 Time taken: 0.3326225280761719\n",
            "Batch Number: 1007 Loss: 2.005025863647461 Time taken: 0.3320004940032959\n",
            "Batch Number: 1008 Loss: 2.0287046432495117 Time taken: 0.3054194450378418\n",
            "Batch Number: 1009 Loss: 1.9945669174194336 Time taken: 0.3087034225463867\n",
            "Batch Number: 1010 Loss: 2.0221219062805176 Time taken: 0.30489444732666016\n",
            "Batch Number: 1011 Loss: 2.001319646835327 Time taken: 0.29856419563293457\n",
            "Batch Number: 1012 Loss: 2.018043041229248 Time taken: 0.33550214767456055\n",
            "Batch Number: 1013 Loss: 1.9907660484313965 Time taken: 0.3118245601654053\n",
            "Batch Number: 1014 Loss: 1.9896080493927002 Time taken: 0.3591468334197998\n",
            "Batch Number: 1015 Loss: 1.9939665794372559 Time taken: 0.39221954345703125\n",
            "Batch Number: 1016 Loss: 1.9939966201782227 Time taken: 0.3977656364440918\n",
            "Batch Number: 1017 Loss: 1.9767574071884155 Time taken: 0.3732779026031494\n",
            "Batch Number: 1018 Loss: 2.0010313987731934 Time taken: 0.3861503601074219\n",
            "Batch Number: 1019 Loss: 1.9736238718032837 Time taken: 0.30878663063049316\n",
            "Batch Number: 1020 Loss: 2.000810384750366 Time taken: 0.2966923713684082\n",
            "Batch Number: 1021 Loss: 1.9989522695541382 Time taken: 0.3189413547515869\n",
            "Batch Number: 1022 Loss: 1.970667839050293 Time taken: 0.30091309547424316\n",
            "Batch Number: 1023 Loss: 1.989938497543335 Time taken: 0.29862141609191895\n",
            "Batch Number: 1024 Loss: 2.0113143920898438 Time taken: 0.3164811134338379\n",
            "Batch Number: 1025 Loss: 1.974164605140686 Time taken: 0.30615806579589844\n",
            "Batch Number: 1026 Loss: 1.979280710220337 Time taken: 0.2966339588165283\n",
            "Batch Number: 1027 Loss: 2.00076961517334 Time taken: 0.3030107021331787\n",
            "Batch Number: 1028 Loss: 2.008410930633545 Time taken: 0.3132915496826172\n",
            "Batch Number: 1029 Loss: 1.967637300491333 Time taken: 0.37153172492980957\n",
            "Batch Number: 1030 Loss: 1.9837684631347656 Time taken: 0.37269067764282227\n",
            "Batch Number: 1031 Loss: 1.950913429260254 Time taken: 0.3066701889038086\n",
            "Batch Number: 1032 Loss: 1.967301845550537 Time taken: 0.3438866138458252\n",
            "Batch Number: 1033 Loss: 1.9839723110198975 Time taken: 0.37440061569213867\n",
            "Batch Number: 1034 Loss: 1.9761711359024048 Time taken: 0.3351879119873047\n",
            "Batch Number: 1035 Loss: 1.9659942388534546 Time taken: 0.29949235916137695\n",
            "Batch Number: 1036 Loss: 1.952883005142212 Time taken: 0.3005540370941162\n",
            "Batch Number: 1037 Loss: 1.9560743570327759 Time taken: 0.32531070709228516\n",
            "Batch Number: 1038 Loss: 1.9524449110031128 Time taken: 0.29259490966796875\n",
            "Batch Number: 1039 Loss: 1.9626753330230713 Time taken: 0.31117844581604004\n",
            "Batch Number: 1040 Loss: 1.9609614610671997 Time taken: 0.32630419731140137\n",
            "Batch Number: 1041 Loss: 1.988187551498413 Time taken: 0.30271220207214355\n",
            "Batch Number: 1042 Loss: 1.9909394979476929 Time taken: 0.3037998676300049\n",
            "Batch Number: 1043 Loss: 1.9616979360580444 Time taken: 0.30948758125305176\n",
            "Batch Number: 1044 Loss: 1.99726402759552 Time taken: 0.3321564197540283\n",
            "Batch Number: 1045 Loss: 1.9670346975326538 Time taken: 0.2987325191497803\n",
            "Batch Number: 1046 Loss: 2.004204273223877 Time taken: 0.3496971130371094\n",
            "Batch Number: 1047 Loss: 1.9912029504776 Time taken: 0.2888674736022949\n",
            "Batch Number: 1048 Loss: 1.971373438835144 Time taken: 0.33963966369628906\n",
            "Batch Number: 1049 Loss: 1.9789386987686157 Time taken: 0.35148143768310547\n",
            "Batch Number: 1050 Loss: 1.9492250680923462 Time taken: 0.2933485507965088\n",
            "Batch Number: 1051 Loss: 1.9745538234710693 Time taken: 0.30942869186401367\n",
            "Batch Number: 1052 Loss: 1.953513264656067 Time taken: 0.3560149669647217\n",
            "Batch Number: 1053 Loss: 1.968554973602295 Time taken: 0.36237406730651855\n",
            "Batch Number: 1054 Loss: 1.958235502243042 Time taken: 0.37050962448120117\n",
            "Batch Number: 1055 Loss: 1.9741569757461548 Time taken: 0.3442211151123047\n",
            "Batch Number: 1056 Loss: 1.970898985862732 Time taken: 0.37259578704833984\n",
            "Batch Number: 1057 Loss: 1.9791998863220215 Time taken: 0.3642995357513428\n",
            "Batch Number: 1058 Loss: 1.9866186380386353 Time taken: 0.3245396614074707\n",
            "Batch Number: 1059 Loss: 1.9710966348648071 Time taken: 0.29668521881103516\n",
            "Batch Number: 1060 Loss: 1.9569708108901978 Time taken: 0.2900569438934326\n",
            "Batch Number: 1061 Loss: 1.953995943069458 Time taken: 0.3188185691833496\n",
            "Batch Number: 1062 Loss: 1.9662684202194214 Time taken: 0.30058932304382324\n",
            "Batch Number: 1063 Loss: 1.966812252998352 Time taken: 0.31701016426086426\n",
            "Batch Number: 1064 Loss: 1.9857256412506104 Time taken: 0.3151984214782715\n",
            "Batch Number: 1065 Loss: 1.9603476524353027 Time taken: 0.28868532180786133\n",
            "Batch Number: 1066 Loss: 1.9803047180175781 Time taken: 0.29009222984313965\n",
            "Batch Number: 1067 Loss: 1.952027440071106 Time taken: 0.31614184379577637\n",
            "Batch Number: 1068 Loss: 1.96283757686615 Time taken: 0.3463406562805176\n",
            "Batch Number: 1069 Loss: 1.9664536714553833 Time taken: 0.36443233489990234\n",
            "Batch Number: 1070 Loss: 1.9772565364837646 Time taken: 0.38089704513549805\n",
            "Batch Number: 1071 Loss: 1.9574319124221802 Time taken: 0.3026854991912842\n",
            "Batch Number: 1072 Loss: 1.9523155689239502 Time taken: 0.27902650833129883\n",
            "Batch Number: 1073 Loss: 1.9323500394821167 Time taken: 0.3029201030731201\n",
            "Batch Number: 1074 Loss: 1.9584354162216187 Time taken: 0.30541300773620605\n",
            "Batch Number: 1075 Loss: 1.944433569908142 Time taken: 0.3156931400299072\n",
            "Batch Number: 1076 Loss: 1.957257866859436 Time taken: 0.28922176361083984\n",
            "Batch Number: 1077 Loss: 1.9508662223815918 Time taken: 0.3102688789367676\n",
            "Batch Number: 1078 Loss: 1.938133955001831 Time taken: 0.3000814914703369\n",
            "Batch Number: 1079 Loss: 1.9413665533065796 Time taken: 0.3081095218658447\n",
            "Batch Number: 1080 Loss: 1.9722821712493896 Time taken: 0.33104467391967773\n",
            "Batch Number: 1081 Loss: 1.9644725322723389 Time taken: 0.31310081481933594\n",
            "Batch Number: 1082 Loss: 1.9239733219146729 Time taken: 0.30274248123168945\n",
            "Batch Number: 1083 Loss: 1.917824625968933 Time taken: 0.34700632095336914\n",
            "Batch Number: 1084 Loss: 1.924172043800354 Time taken: 0.3563535213470459\n",
            "Batch Number: 1085 Loss: 1.9343427419662476 Time taken: 0.3729262351989746\n",
            "Batch Number: 1086 Loss: 1.9418216943740845 Time taken: 0.3520667552947998\n",
            "Batch Number: 1087 Loss: 1.9335837364196777 Time taken: 0.3718600273132324\n",
            "Batch Number: 1088 Loss: 1.9207512140274048 Time taken: 0.3867990970611572\n",
            "Batch Number: 1089 Loss: 1.9429008960723877 Time taken: 0.31868958473205566\n",
            "Batch Number: 1090 Loss: 1.9102801084518433 Time taken: 0.30088376998901367\n",
            "Batch Number: 1091 Loss: 1.9311853647232056 Time taken: 0.346940279006958\n",
            "Batch Number: 1092 Loss: 1.946094274520874 Time taken: 0.32353901863098145\n",
            "Batch Number: 1093 Loss: 1.9338222742080688 Time taken: 0.3018984794616699\n",
            "Batch Number: 1094 Loss: 1.9615933895111084 Time taken: 0.29798054695129395\n",
            "Batch Number: 1095 Loss: 1.9402735233306885 Time taken: 0.32648491859436035\n",
            "Batch Number: 1096 Loss: 1.9490435123443604 Time taken: 0.3065474033355713\n",
            "Batch Number: 1097 Loss: 1.951015591621399 Time taken: 0.29294919967651367\n",
            "Batch Number: 1098 Loss: 1.944332242012024 Time taken: 0.3286106586456299\n",
            "Batch Number: 1099 Loss: 1.9418001174926758 Time taken: 0.2908306121826172\n",
            "Batch Number: 1100 Loss: 1.9274495840072632 Time taken: 0.28723740577697754\n",
            "Batch Number: 1101 Loss: 1.9153597354888916 Time taken: 0.33599328994750977\n",
            "Batch Number: 1102 Loss: 1.9316588640213013 Time taken: 0.38910961151123047\n",
            "Batch Number: 1103 Loss: 1.923607587814331 Time taken: 0.36379265785217285\n",
            "Batch Number: 1104 Loss: 1.9380853176116943 Time taken: 0.3626596927642822\n",
            "Batch Number: 1105 Loss: 1.949377417564392 Time taken: 0.37032580375671387\n",
            "Batch Number: 1106 Loss: 1.9497615098953247 Time taken: 0.3769717216491699\n",
            "Batch Number: 1107 Loss: 1.9287549257278442 Time taken: 0.41342759132385254\n",
            "Batch Number: 1108 Loss: 1.9311699867248535 Time taken: 0.32314205169677734\n",
            "Batch Number: 1109 Loss: 1.9341908693313599 Time taken: 0.30454373359680176\n",
            "Batch Number: 1110 Loss: 1.9310671091079712 Time taken: 0.3038187026977539\n",
            "Batch Number: 1111 Loss: 1.951980710029602 Time taken: 0.29178428649902344\n",
            "Batch Number: 1112 Loss: 1.9765304327011108 Time taken: 0.29573822021484375\n",
            "Batch Number: 1113 Loss: 1.9545817375183105 Time taken: 0.3104584217071533\n",
            "Batch Number: 1114 Loss: 1.9408899545669556 Time taken: 0.2980623245239258\n",
            "Batch Number: 1115 Loss: 1.9416545629501343 Time taken: 0.3036942481994629\n",
            "Batch Number: 1116 Loss: 1.9165637493133545 Time taken: 0.30930089950561523\n",
            "Batch Number: 1117 Loss: 1.928475022315979 Time taken: 0.32076501846313477\n",
            "Batch Number: 1118 Loss: 1.9487013816833496 Time taken: 0.3076772689819336\n",
            "Batch Number: 1119 Loss: 1.9484564065933228 Time taken: 0.36136579513549805\n",
            "Batch Number: 1120 Loss: 1.954088807106018 Time taken: 0.38633084297180176\n",
            "Batch Number: 1121 Loss: 1.9474143981933594 Time taken: 0.35021138191223145\n",
            "Batch Number: 1122 Loss: 1.9460346698760986 Time taken: 0.30169224739074707\n",
            "Batch Number: 1123 Loss: 1.917323112487793 Time taken: 0.3130664825439453\n",
            "Batch Number: 1124 Loss: 1.929608702659607 Time taken: 0.31574010848999023\n",
            "Batch Number: 1125 Loss: 1.9342254400253296 Time taken: 0.2874913215637207\n",
            "Batch Number: 1126 Loss: 1.9297062158584595 Time taken: 0.30570316314697266\n",
            "Batch Number: 1127 Loss: 1.9167232513427734 Time taken: 0.2953338623046875\n",
            "Batch Number: 1128 Loss: 1.9434075355529785 Time taken: 0.3074502944946289\n",
            "Batch Number: 1129 Loss: 1.9251681566238403 Time taken: 0.32140064239501953\n",
            "Batch Number: 1130 Loss: 1.9328892230987549 Time taken: 0.29589200019836426\n",
            "Batch Number: 1131 Loss: 1.9415326118469238 Time taken: 0.3139007091522217\n",
            "Batch Number: 1132 Loss: 1.9421969652175903 Time taken: 0.3080317974090576\n",
            "Batch Number: 1133 Loss: 1.9464244842529297 Time taken: 0.29745960235595703\n",
            "Batch Number: 1134 Loss: 1.96022367477417 Time taken: 0.29264283180236816\n",
            "Batch Number: 1135 Loss: 1.9377474784851074 Time taken: 0.3004796504974365\n",
            "Batch Number: 1136 Loss: 1.925132155418396 Time taken: 0.3039073944091797\n",
            "Batch Number: 1137 Loss: 1.9493739604949951 Time taken: 0.3041956424713135\n",
            "Batch Number: 1138 Loss: 1.9403433799743652 Time taken: 0.29599952697753906\n",
            "Batch Number: 1139 Loss: 1.9276310205459595 Time taken: 0.31632089614868164\n",
            "Batch Number: 1140 Loss: 1.9078397750854492 Time taken: 0.32977771759033203\n",
            "Batch Number: 1141 Loss: 1.920866847038269 Time taken: 0.2958364486694336\n",
            "Batch Number: 1142 Loss: 1.913685917854309 Time taken: 0.3151524066925049\n",
            "Batch Number: 1143 Loss: 1.9321770668029785 Time taken: 0.29857635498046875\n",
            "Batch Number: 1144 Loss: 1.9319392442703247 Time taken: 0.34823131561279297\n",
            "Batch Number: 1145 Loss: 1.924892544746399 Time taken: 0.38686323165893555\n",
            "Batch Number: 1146 Loss: 1.9510161876678467 Time taken: 0.3412959575653076\n",
            "Batch Number: 1147 Loss: 1.9380189180374146 Time taken: 0.29644060134887695\n",
            "Batch Number: 1148 Loss: 1.9357680082321167 Time taken: 0.31533193588256836\n",
            "Batch Number: 1149 Loss: 1.939131736755371 Time taken: 0.31821560859680176\n",
            "Batch Number: 1150 Loss: 1.92588472366333 Time taken: 0.3275458812713623\n",
            "Batch Number: 1151 Loss: 1.9286751747131348 Time taken: 0.3884439468383789\n",
            "Batch Number: 1152 Loss: 1.9424662590026855 Time taken: 0.3350942134857178\n",
            "Batch Number: 1153 Loss: 1.931331753730774 Time taken: 0.29853081703186035\n",
            "Batch Number: 1154 Loss: 1.9320690631866455 Time taken: 0.3122217655181885\n",
            "Batch Number: 1155 Loss: 1.9494959115982056 Time taken: 0.31149721145629883\n",
            "Batch Number: 1156 Loss: 1.9270529747009277 Time taken: 0.3398869037628174\n",
            "Batch Number: 1157 Loss: 1.9415225982666016 Time taken: 0.3376502990722656\n",
            "Batch Number: 1158 Loss: 1.9507269859313965 Time taken: 0.3521413803100586\n",
            "Batch Number: 1159 Loss: 1.9510160684585571 Time taken: 0.3044297695159912\n",
            "Batch Number: 1160 Loss: 1.9232518672943115 Time taken: 0.3160223960876465\n",
            "Batch Number: 1161 Loss: 1.9223706722259521 Time taken: 0.32947492599487305\n",
            "Batch Number: 1162 Loss: 1.952615737915039 Time taken: 0.3036329746246338\n",
            "Batch Number: 1163 Loss: 1.9196927547454834 Time taken: 0.3054234981536865\n",
            "Batch Number: 1164 Loss: 1.9299250841140747 Time taken: 0.3118550777435303\n",
            "Batch Number: 1165 Loss: 1.9372986555099487 Time taken: 0.3212580680847168\n",
            "Batch Number: 1166 Loss: 1.9336706399917603 Time taken: 0.37600040435791016\n",
            "Batch Number: 1167 Loss: 1.9023956060409546 Time taken: 0.37537288665771484\n",
            "Batch Number: 1168 Loss: 1.9143927097320557 Time taken: 0.3426830768585205\n",
            "Batch Number: 1169 Loss: 1.9247536659240723 Time taken: 0.39424729347229004\n",
            "Batch Number: 1170 Loss: 1.9232347011566162 Time taken: 0.34070587158203125\n",
            "Batch Number: 1171 Loss: 1.9331916570663452 Time taken: 0.3100290298461914\n",
            "Batch Number: 1172 Loss: 1.9316647052764893 Time taken: 0.322770357131958\n",
            "Batch Number: 1173 Loss: 1.9529955387115479 Time taken: 0.3119063377380371\n",
            "Batch Number: 1174 Loss: 1.9324012994766235 Time taken: 0.3023054599761963\n",
            "Batch Number: 1175 Loss: 1.9407422542572021 Time taken: 0.3016011714935303\n",
            "Batch Number: 1176 Loss: 1.947352409362793 Time taken: 0.31369876861572266\n",
            "Batch Number: 1177 Loss: 1.9540029764175415 Time taken: 0.2903625965118408\n",
            "Batch Number: 1178 Loss: 1.9526913166046143 Time taken: 0.30822014808654785\n",
            "Batch Number: 1179 Loss: 1.930017113685608 Time taken: 0.3187704086303711\n",
            "Batch Number: 1180 Loss: 1.9527201652526855 Time taken: 0.36249852180480957\n",
            "Batch Number: 1181 Loss: 1.9453214406967163 Time taken: 0.37796854972839355\n",
            "Batch Number: 1182 Loss: 1.9565216302871704 Time taken: 0.3718223571777344\n",
            "Batch Number: 1183 Loss: 1.942307710647583 Time taken: 0.2960376739501953\n",
            "Batch Number: 1184 Loss: 1.9474185705184937 Time taken: 0.30985164642333984\n",
            "Batch Number: 1185 Loss: 1.9591562747955322 Time taken: 0.3377513885498047\n",
            "Batch Number: 1186 Loss: 1.9567415714263916 Time taken: 0.3023993968963623\n",
            "Batch Number: 1187 Loss: 1.9488815069198608 Time taken: 0.29908108711242676\n",
            "Batch Number: 1188 Loss: 1.9714436531066895 Time taken: 0.3435795307159424\n",
            "Batch Number: 1189 Loss: 1.9408178329467773 Time taken: 0.2897522449493408\n",
            "Batch Number: 1190 Loss: 1.9617950916290283 Time taken: 0.29209470748901367\n",
            "Batch Number: 1191 Loss: 1.972965121269226 Time taken: 0.3265054225921631\n",
            "Batch Number: 1192 Loss: 1.9379850625991821 Time taken: 0.34485888481140137\n",
            "Batch Number: 1193 Loss: 1.9643137454986572 Time taken: 0.37081360816955566\n",
            "Batch Number: 1194 Loss: 1.971604347229004 Time taken: 0.35953617095947266\n",
            "Batch Number: 1195 Loss: 1.9493060111999512 Time taken: 0.29572319984436035\n",
            "Batch Number: 1196 Loss: 1.9308100938796997 Time taken: 0.2889375686645508\n",
            "Batch Number: 1197 Loss: 1.9416534900665283 Time taken: 0.38561487197875977\n",
            "Batch Number: 1198 Loss: 1.915664553642273 Time taken: 0.30121755599975586\n",
            "Batch Number: 1199 Loss: 1.9419503211975098 Time taken: 0.30288267135620117\n",
            "Batch Number: 1200 Loss: 1.92353093624115 Time taken: 0.34853339195251465\n",
            "Batch Number: 1201 Loss: 1.9036118984222412 Time taken: 0.30602169036865234\n",
            "Batch Number: 1202 Loss: 1.9513111114501953 Time taken: 0.30504822731018066\n",
            "Batch Number: 1203 Loss: 1.912087082862854 Time taken: 0.3553035259246826\n",
            "Batch Number: 1204 Loss: 1.9266999959945679 Time taken: 0.3159825801849365\n",
            "Batch Number: 1205 Loss: 1.922845721244812 Time taken: 0.2955434322357178\n",
            "Batch Number: 1206 Loss: 1.93085515499115 Time taken: 0.29947829246520996\n",
            "Batch Number: 1207 Loss: 1.9212405681610107 Time taken: 0.31256985664367676\n",
            "Batch Number: 1208 Loss: 1.923604130744934 Time taken: 0.30233049392700195\n",
            "Batch Number: 1209 Loss: 1.89805006980896 Time taken: 0.30533432960510254\n",
            "Batch Number: 1210 Loss: 1.9019556045532227 Time taken: 0.3302445411682129\n",
            "Batch Number: 1211 Loss: 1.9202884435653687 Time taken: 0.2963368892669678\n",
            "Batch Number: 1212 Loss: 1.936110496520996 Time taken: 0.3159613609313965\n",
            "Batch Number: 1213 Loss: 1.950217366218567 Time taken: 0.3082430362701416\n",
            "Batch Number: 1214 Loss: 1.9258108139038086 Time taken: 0.3210875988006592\n",
            "Batch Number: 1215 Loss: 1.8915400505065918 Time taken: 0.3772749900817871\n",
            "Batch Number: 1216 Loss: 1.894698143005371 Time taken: 0.3499464988708496\n",
            "Batch Number: 1217 Loss: 1.8885905742645264 Time taken: 0.29920339584350586\n",
            "Batch Number: 1218 Loss: 1.9096754789352417 Time taken: 0.29592227935791016\n",
            "Batch Number: 1219 Loss: 1.9006268978118896 Time taken: 0.31430768966674805\n",
            "Batch Number: 1220 Loss: 1.9376331567764282 Time taken: 0.28459930419921875\n",
            "Batch Number: 1221 Loss: 1.9200612306594849 Time taken: 0.2857697010040283\n",
            "Batch Number: 1222 Loss: 1.9363285303115845 Time taken: 0.3520796298980713\n",
            "Batch Number: 1223 Loss: 1.9120222330093384 Time taken: 0.35215306282043457\n",
            "Batch Number: 1224 Loss: 1.9271870851516724 Time taken: 0.3022153377532959\n",
            "Batch Number: 1225 Loss: 1.8988381624221802 Time taken: 0.303821325302124\n",
            "Batch Number: 1226 Loss: 1.9397426843643188 Time taken: 0.3044006824493408\n",
            "Batch Number: 1227 Loss: 1.9066179990768433 Time taken: 0.31064867973327637\n",
            "Batch Number: 1228 Loss: 1.8973191976547241 Time taken: 0.3048131465911865\n",
            "Batch Number: 1229 Loss: 1.9234720468521118 Time taken: 0.3106553554534912\n",
            "Batch Number: 1230 Loss: 1.9052464962005615 Time taken: 0.3004024028778076\n",
            "Batch Number: 1231 Loss: 1.9126752614974976 Time taken: 0.31151390075683594\n",
            "Batch Number: 1232 Loss: 1.9437620639801025 Time taken: 0.32688474655151367\n",
            "Batch Number: 1233 Loss: 1.9205981492996216 Time taken: 0.2976083755493164\n",
            "Batch Number: 1234 Loss: 1.9171806573867798 Time taken: 0.3085505962371826\n",
            "Batch Number: 1235 Loss: 1.9287370443344116 Time taken: 0.317974328994751\n",
            "Batch Number: 1236 Loss: 1.9117542505264282 Time taken: 0.29747509956359863\n",
            "Batch Number: 1237 Loss: 1.9637454748153687 Time taken: 0.32065296173095703\n",
            "Batch Number: 1238 Loss: 1.9306374788284302 Time taken: 0.31142354011535645\n",
            "Batch Number: 1239 Loss: 1.9215998649597168 Time taken: 0.31548190116882324\n",
            "Batch Number: 1240 Loss: 1.9213498830795288 Time taken: 0.29792261123657227\n",
            "Batch Number: 1241 Loss: 1.9229105710983276 Time taken: 0.33245325088500977\n",
            "Batch Number: 1242 Loss: 1.9539917707443237 Time taken: 0.3140275478363037\n",
            "Batch Number: 1243 Loss: 1.9239778518676758 Time taken: 0.29564356803894043\n",
            "Batch Number: 1244 Loss: 1.916181206703186 Time taken: 0.3097374439239502\n",
            "Batch Number: 1245 Loss: 1.896673321723938 Time taken: 0.3124103546142578\n",
            "Batch Number: 1246 Loss: 1.909623384475708 Time taken: 0.30533790588378906\n",
            "Batch Number: 1247 Loss: 1.9088830947875977 Time taken: 0.3003084659576416\n",
            "Batch Number: 1248 Loss: 1.907415509223938 Time taken: 0.3713037967681885\n",
            "Batch Number: 1249 Loss: 1.8947784900665283 Time taken: 0.37127232551574707\n",
            "Batch Number: 1250 Loss: 1.9171136617660522 Time taken: 0.33065080642700195\n",
            "Batch Number: 1251 Loss: 1.8942265510559082 Time taken: 0.3046867847442627\n",
            "Batch Number: 1252 Loss: 1.929479956626892 Time taken: 0.31494736671447754\n",
            "Batch Number: 1253 Loss: 1.9111589193344116 Time taken: 0.3204038143157959\n",
            "Batch Number: 1254 Loss: 1.901305079460144 Time taken: 0.3198709487915039\n",
            "Batch Number: 1255 Loss: 1.895350456237793 Time taken: 0.315213680267334\n",
            "Batch Number: 1256 Loss: 1.9012138843536377 Time taken: 0.29687952995300293\n",
            "Batch Number: 1257 Loss: 1.8726218938827515 Time taken: 0.30637574195861816\n",
            "Batch Number: 1258 Loss: 1.8789345026016235 Time taken: 0.32463741302490234\n",
            "Batch Number: 1259 Loss: 1.897456169128418 Time taken: 0.30552053451538086\n",
            "Batch Number: 1260 Loss: 1.8862465620040894 Time taken: 0.3366811275482178\n",
            "Batch Number: 1261 Loss: 1.8837146759033203 Time taken: 0.31375646591186523\n",
            "Batch Number: 1262 Loss: 1.8671510219573975 Time taken: 0.29628849029541016\n",
            "Batch Number: 1263 Loss: 1.881032943725586 Time taken: 0.32830023765563965\n",
            "Batch Number: 1264 Loss: 1.8779758214950562 Time taken: 0.3115818500518799\n",
            "Batch Number: 1265 Loss: 1.86976158618927 Time taken: 0.3260970115661621\n",
            "Batch Number: 1266 Loss: 1.8630441427230835 Time taken: 0.3336353302001953\n",
            "Batch Number: 1267 Loss: 1.8918023109436035 Time taken: 0.3706347942352295\n",
            "Batch Number: 1268 Loss: 1.8720567226409912 Time taken: 0.3664677143096924\n",
            "Batch Number: 1269 Loss: 1.8642714023590088 Time taken: 0.33504605293273926\n",
            "Batch Number: 1270 Loss: 1.873216986656189 Time taken: 0.34363341331481934\n",
            "Batch Number: 1271 Loss: 1.8682606220245361 Time taken: 0.36664652824401855\n",
            "Batch Number: 1272 Loss: 1.8648521900177002 Time taken: 0.32518458366394043\n",
            "Batch Number: 1273 Loss: 1.892898678779602 Time taken: 0.31655073165893555\n",
            "Batch Number: 1274 Loss: 1.8864707946777344 Time taken: 0.3182353973388672\n",
            "Batch Number: 1275 Loss: 1.8997328281402588 Time taken: 0.3104267120361328\n",
            "Batch Number: 1276 Loss: 1.8869574069976807 Time taken: 0.31319522857666016\n",
            "Batch Number: 1277 Loss: 1.908821940422058 Time taken: 0.36652374267578125\n",
            "Batch Number: 1278 Loss: 1.8791545629501343 Time taken: 0.31542062759399414\n",
            "Batch Number: 1279 Loss: 1.8668243885040283 Time taken: 0.32004880905151367\n",
            "Batch Number: 1280 Loss: 1.9071879386901855 Time taken: 0.31400537490844727\n",
            "Batch Number: 1281 Loss: 1.9249471426010132 Time taken: 0.3330106735229492\n",
            "Batch Number: 1282 Loss: 1.9123064279556274 Time taken: 0.30666065216064453\n",
            "Batch Number: 1283 Loss: 1.8909810781478882 Time taken: 0.31504130363464355\n",
            "Batch Number: 1284 Loss: 1.9254419803619385 Time taken: 0.32743406295776367\n",
            "Batch Number: 1285 Loss: 1.9071592092514038 Time taken: 0.3155074119567871\n",
            "Batch Number: 1286 Loss: 1.9241052865982056 Time taken: 0.3103780746459961\n",
            "Batch Number: 1287 Loss: 1.9054536819458008 Time taken: 0.31435489654541016\n",
            "Batch Number: 1288 Loss: 1.8753806352615356 Time taken: 0.3103644847869873\n",
            "Batch Number: 1289 Loss: 1.903721809387207 Time taken: 0.3352346420288086\n",
            "Batch Number: 1290 Loss: 1.894222378730774 Time taken: 0.32680392265319824\n",
            "Batch Number: 1291 Loss: 1.8721988201141357 Time taken: 0.3015749454498291\n",
            "Batch Number: 1292 Loss: 1.895227313041687 Time taken: 0.3106510639190674\n",
            "Batch Number: 1293 Loss: 1.8980400562286377 Time taken: 0.2929718494415283\n",
            "Batch Number: 1294 Loss: 1.8989815711975098 Time taken: 0.33899569511413574\n",
            "Batch Number: 1295 Loss: 1.8822112083435059 Time taken: 0.3870241641998291\n",
            "Batch Number: 1296 Loss: 1.8896384239196777 Time taken: 0.3284599781036377\n",
            "Batch Number: 1297 Loss: 1.8881938457489014 Time taken: 0.29758119583129883\n",
            "Batch Number: 1298 Loss: 1.897518277168274 Time taken: 0.3111274242401123\n",
            "Batch Number: 1299 Loss: 1.8910613059997559 Time taken: 0.3036530017852783\n",
            "Batch Number: 1300 Loss: 1.88233482837677 Time taken: 0.3067138195037842\n",
            "Batch Number: 1301 Loss: 1.8849539756774902 Time taken: 0.323183536529541\n",
            "Batch Number: 1302 Loss: 1.8981715440750122 Time taken: 0.32413816452026367\n",
            "Batch Number: 1303 Loss: 1.90500807762146 Time taken: 0.30747437477111816\n",
            "Batch Number: 1304 Loss: 1.8923321962356567 Time taken: 0.3249480724334717\n",
            "Batch Number: 1305 Loss: 1.8634134531021118 Time taken: 0.3584294319152832\n",
            "Batch Number: 1306 Loss: 1.8760462999343872 Time taken: 0.3858952522277832\n",
            "Batch Number: 1307 Loss: 1.900468349456787 Time taken: 0.38277482986450195\n",
            "Batch Number: 1308 Loss: 1.8870298862457275 Time taken: 0.32177186012268066\n",
            "Batch Number: 1309 Loss: 1.8799995183944702 Time taken: 0.301746129989624\n",
            "Batch Number: 1310 Loss: 1.9018446207046509 Time taken: 0.3023340702056885\n",
            "Batch Number: 1311 Loss: 1.88239324092865 Time taken: 0.3058352470397949\n",
            "Batch Number: 1312 Loss: 1.9009873867034912 Time taken: 0.3114750385284424\n",
            "Batch Number: 1313 Loss: 1.9087533950805664 Time taken: 0.3341543674468994\n",
            "Batch Number: 1314 Loss: 1.8774125576019287 Time taken: 0.3584418296813965\n",
            "Batch Number: 1315 Loss: 1.8756842613220215 Time taken: 0.37480759620666504\n",
            "Batch Number: 1316 Loss: 1.906688928604126 Time taken: 0.3549807071685791\n",
            "Batch Number: 1317 Loss: 1.8789048194885254 Time taken: 0.2901275157928467\n",
            "Batch Number: 1318 Loss: 1.8688280582427979 Time taken: 0.2983977794647217\n",
            "Batch Number: 1319 Loss: 1.8512547016143799 Time taken: 0.32149362564086914\n",
            "Batch Number: 1320 Loss: 1.8780988454818726 Time taken: 0.3418757915496826\n",
            "Batch Number: 1321 Loss: 1.8674770593643188 Time taken: 0.36762237548828125\n",
            "Batch Number: 1322 Loss: 1.8673957586288452 Time taken: 0.35680699348449707\n",
            "Batch Number: 1323 Loss: 1.882702350616455 Time taken: 0.3052239418029785\n",
            "Batch Number: 1324 Loss: 1.8799453973770142 Time taken: 0.3611738681793213\n",
            "Batch Number: 1325 Loss: 1.8472435474395752 Time taken: 0.37253403663635254\n",
            "Batch Number: 1326 Loss: 1.892115592956543 Time taken: 0.3097965717315674\n",
            "Batch Number: 1327 Loss: 1.8949730396270752 Time taken: 0.29966211318969727\n",
            "Batch Number: 1328 Loss: 1.8859916925430298 Time taken: 0.3111691474914551\n",
            "Batch Number: 1329 Loss: 1.8929474353790283 Time taken: 0.29687976837158203\n",
            "Batch Number: 1330 Loss: 1.8746095895767212 Time taken: 0.36623263359069824\n",
            "Batch Number: 1331 Loss: 1.8816570043563843 Time taken: 0.3837289810180664\n",
            "Batch Number: 1332 Loss: 1.8930212259292603 Time taken: 0.3125896453857422\n",
            "Batch Number: 1333 Loss: 1.8654110431671143 Time taken: 0.30783677101135254\n",
            "Batch Number: 1334 Loss: 1.8983896970748901 Time taken: 0.3021085262298584\n",
            "Batch Number: 1335 Loss: 1.90126633644104 Time taken: 0.2979564666748047\n",
            "Batch Number: 1336 Loss: 1.9079610109329224 Time taken: 0.3049948215484619\n",
            "Batch Number: 1337 Loss: 1.889730453491211 Time taken: 0.30123209953308105\n",
            "Batch Number: 1338 Loss: 1.892744541168213 Time taken: 0.31125855445861816\n",
            "Batch Number: 1339 Loss: 1.882434368133545 Time taken: 0.3001539707183838\n",
            "Batch Number: 1340 Loss: 1.8740507364273071 Time taken: 0.28586649894714355\n",
            "Batch Number: 1341 Loss: 1.8774610757827759 Time taken: 0.37326931953430176\n",
            "Batch Number: 1342 Loss: 1.8691904544830322 Time taken: 0.3714139461517334\n",
            "Batch Number: 1343 Loss: 1.8713383674621582 Time taken: 0.37492847442626953\n",
            "Batch Number: 1344 Loss: 1.852908968925476 Time taken: 0.3660311698913574\n",
            "Batch Number: 1345 Loss: 1.8622639179229736 Time taken: 0.3585686683654785\n",
            "Batch Number: 1346 Loss: 1.860408902168274 Time taken: 0.30485105514526367\n",
            "Batch Number: 1347 Loss: 1.878458857536316 Time taken: 0.29411911964416504\n",
            "Batch Number: 1348 Loss: 1.8507721424102783 Time taken: 0.30272912979125977\n",
            "Batch Number: 1349 Loss: 1.865494966506958 Time taken: 0.2890963554382324\n",
            "Batch Number: 1350 Loss: 1.847177267074585 Time taken: 0.3295152187347412\n",
            "Batch Number: 1351 Loss: 1.875224232673645 Time taken: 0.31506872177124023\n",
            "Batch Number: 1352 Loss: 1.8806681632995605 Time taken: 0.3562905788421631\n",
            "Batch Number: 1353 Loss: 1.901369333267212 Time taken: 0.37778186798095703\n",
            "Batch Number: 1354 Loss: 1.8806554079055786 Time taken: 0.38440680503845215\n",
            "Batch Number: 1355 Loss: 1.8936249017715454 Time taken: 0.3645634651184082\n",
            "Batch Number: 1356 Loss: 1.9012086391448975 Time taken: 0.3266565799713135\n",
            "Batch Number: 1357 Loss: 1.886572241783142 Time taken: 0.3892180919647217\n",
            "Batch Number: 1358 Loss: 1.9066270589828491 Time taken: 0.3163013458251953\n",
            "Batch Number: 1359 Loss: 1.8859429359436035 Time taken: 0.29694032669067383\n",
            "Batch Number: 1360 Loss: 1.8698554039001465 Time taken: 0.3117039203643799\n",
            "Batch Number: 1361 Loss: 1.9213860034942627 Time taken: 0.33151769638061523\n",
            "Batch Number: 1362 Loss: 1.9055277109146118 Time taken: 0.3078768253326416\n",
            "Batch Number: 1363 Loss: 1.890839695930481 Time taken: 0.31587815284729004\n",
            "Batch Number: 1364 Loss: 1.8979684114456177 Time taken: 0.3468282222747803\n",
            "Batch Number: 1365 Loss: 1.8894215822219849 Time taken: 0.30791687965393066\n",
            "Batch Number: 1366 Loss: 1.9143462181091309 Time taken: 0.3140583038330078\n",
            "Batch Number: 1367 Loss: 1.9150192737579346 Time taken: 0.31998324394226074\n",
            "Batch Number: 1368 Loss: 1.904183030128479 Time taken: 0.307542085647583\n",
            "Batch Number: 1369 Loss: 1.895119071006775 Time taken: 0.31488585472106934\n",
            "Batch Number: 1370 Loss: 1.926259994506836 Time taken: 0.33579397201538086\n",
            "Batch Number: 1371 Loss: 1.9035874605178833 Time taken: 0.2965850830078125\n",
            "Batch Number: 1372 Loss: 1.9010077714920044 Time taken: 0.3065659999847412\n",
            "Batch Number: 1373 Loss: 1.9257186651229858 Time taken: 0.3038194179534912\n",
            "Batch Number: 1374 Loss: 1.88661527633667 Time taken: 0.3842027187347412\n",
            "Batch Number: 1375 Loss: 1.880181908607483 Time taken: 0.37654662132263184\n",
            "Batch Number: 1376 Loss: 1.8853777647018433 Time taken: 0.3518383502960205\n",
            "Batch Number: 1377 Loss: 1.8888860940933228 Time taken: 0.3037686347961426\n",
            "Batch Number: 1378 Loss: 1.8865512609481812 Time taken: 0.3087751865386963\n",
            "Batch Number: 1379 Loss: 1.8714572191238403 Time taken: 0.32263994216918945\n",
            "Batch Number: 1380 Loss: 1.8975872993469238 Time taken: 0.29961299896240234\n",
            "Batch Number: 1381 Loss: 1.8836443424224854 Time taken: 0.30074191093444824\n",
            "Batch Number: 1382 Loss: 1.8799463510513306 Time taken: 0.3023838996887207\n",
            "Batch Number: 1383 Loss: 1.8721901178359985 Time taken: 0.28986406326293945\n",
            "Batch Number: 1384 Loss: 1.8498320579528809 Time taken: 0.29474806785583496\n",
            "Batch Number: 1385 Loss: 1.8705121278762817 Time taken: 0.29827451705932617\n",
            "Batch Number: 1386 Loss: 1.8726441860198975 Time taken: 0.3025796413421631\n",
            "Batch Number: 1387 Loss: 1.8829635381698608 Time taken: 0.31498003005981445\n",
            "Batch Number: 1388 Loss: 1.8607163429260254 Time taken: 0.30557918548583984\n",
            "Batch Number: 1389 Loss: 1.8414056301116943 Time taken: 0.3174724578857422\n",
            "Batch Number: 1390 Loss: 1.8685344457626343 Time taken: 0.31005334854125977\n",
            "Batch Number: 1391 Loss: 1.8719936609268188 Time taken: 0.33690929412841797\n",
            "Batch Number: 1392 Loss: 1.8754346370697021 Time taken: 0.3477027416229248\n",
            "Batch Number: 1393 Loss: 1.875630497932434 Time taken: 0.35180115699768066\n",
            "Batch Number: 1394 Loss: 1.874662160873413 Time taken: 0.3749573230743408\n",
            "Batch Number: 1395 Loss: 1.8415837287902832 Time taken: 0.3106870651245117\n",
            "Batch Number: 1396 Loss: 1.8388057947158813 Time taken: 0.32755446434020996\n",
            "Batch Number: 1397 Loss: 1.8480662107467651 Time taken: 0.3035852909088135\n",
            "Batch Number: 1398 Loss: 1.8572221994400024 Time taken: 0.3289172649383545\n",
            "Batch Number: 1399 Loss: 1.852878451347351 Time taken: 0.3155076503753662\n",
            "Batch Number: 1400 Loss: 1.888993263244629 Time taken: 0.3029041290283203\n",
            "Batch Number: 1401 Loss: 1.8599436283111572 Time taken: 0.3160388469696045\n",
            "Batch Number: 1402 Loss: 1.869880199432373 Time taken: 0.3079664707183838\n",
            "Batch Number: 1403 Loss: 1.9152547121047974 Time taken: 0.31622815132141113\n",
            "Batch Number: 1404 Loss: 1.8652766942977905 Time taken: 0.306490421295166\n",
            "Batch Number: 1405 Loss: 1.885169506072998 Time taken: 0.30141329765319824\n",
            "Batch Number: 1406 Loss: 1.866602897644043 Time taken: 0.30788660049438477\n",
            "Batch Number: 1407 Loss: 1.8229702711105347 Time taken: 0.29537248611450195\n",
            "Batch Number: 1408 Loss: 1.87898850440979 Time taken: 0.31984901428222656\n",
            "Batch Number: 1409 Loss: 1.8696836233139038 Time taken: 0.30829453468322754\n",
            "Batch Number: 1410 Loss: 1.848338007926941 Time taken: 0.3252100944519043\n",
            "Batch Number: 1411 Loss: 1.8730453252792358 Time taken: 0.3041849136352539\n",
            "Batch Number: 1412 Loss: 1.8790720701217651 Time taken: 0.3038778305053711\n",
            "Batch Number: 1413 Loss: 1.8994441032409668 Time taken: 0.35172080993652344\n",
            "Batch Number: 1414 Loss: 1.8685122728347778 Time taken: 0.30470871925354004\n",
            "Batch Number: 1415 Loss: 1.8587738275527954 Time taken: 0.3118867874145508\n",
            "Batch Number: 1416 Loss: 1.8729510307312012 Time taken: 0.33760738372802734\n",
            "Batch Number: 1417 Loss: 1.8862524032592773 Time taken: 0.36920952796936035\n",
            "Batch Number: 1418 Loss: 1.8618457317352295 Time taken: 0.38135743141174316\n",
            "Batch Number: 1419 Loss: 1.8917932510375977 Time taken: 0.360231876373291\n",
            "Batch Number: 1420 Loss: 1.8962502479553223 Time taken: 0.2958528995513916\n",
            "Batch Number: 1421 Loss: 1.88297438621521 Time taken: 0.3062858581542969\n",
            "Batch Number: 1422 Loss: 1.8608113527297974 Time taken: 0.3016376495361328\n",
            "Batch Number: 1423 Loss: 1.8559162616729736 Time taken: 0.3064391613006592\n",
            "Batch Number: 1424 Loss: 1.8703826665878296 Time taken: 0.303586483001709\n",
            "Batch Number: 1425 Loss: 1.839760661125183 Time taken: 0.32890820503234863\n",
            "Batch Number: 1426 Loss: 1.8729908466339111 Time taken: 0.34858012199401855\n",
            "Batch Number: 1427 Loss: 1.8526934385299683 Time taken: 0.3719370365142822\n",
            "Batch Number: 1428 Loss: 1.8428226709365845 Time taken: 0.3844764232635498\n",
            "Batch Number: 1429 Loss: 1.8308744430541992 Time taken: 0.30876660346984863\n",
            "Batch Number: 1430 Loss: 1.8451603651046753 Time taken: 0.3008131980895996\n",
            "Batch Number: 1431 Loss: 1.8678170442581177 Time taken: 0.30110740661621094\n",
            "Batch Number: 1432 Loss: 1.847036361694336 Time taken: 0.3030436038970947\n",
            "Batch Number: 1433 Loss: 1.8584095239639282 Time taken: 0.2964508533477783\n",
            "Batch Number: 1434 Loss: 1.8422459363937378 Time taken: 0.30266594886779785\n",
            "Batch Number: 1435 Loss: 1.8448436260223389 Time taken: 0.2957272529602051\n",
            "Batch Number: 1436 Loss: 1.8246787786483765 Time taken: 0.3084588050842285\n",
            "Batch Number: 1437 Loss: 1.8316736221313477 Time taken: 0.3042457103729248\n",
            "Batch Number: 1438 Loss: 1.8346761465072632 Time taken: 0.30760979652404785\n",
            "Batch Number: 1439 Loss: 1.8437385559082031 Time taken: 0.29393434524536133\n",
            "Batch Number: 1440 Loss: 1.8563088178634644 Time taken: 0.29355931282043457\n",
            "Batch Number: 1441 Loss: 1.8590819835662842 Time taken: 0.31848931312561035\n",
            "Batch Number: 1442 Loss: 1.8687665462493896 Time taken: 0.29253292083740234\n",
            "Batch Number: 1443 Loss: 1.8723509311676025 Time taken: 0.2987806797027588\n",
            "Batch Number: 1444 Loss: 1.8442736864089966 Time taken: 0.32096123695373535\n",
            "Batch Number: 1445 Loss: 1.8286420106887817 Time taken: 0.30585455894470215\n",
            "Batch Number: 1446 Loss: 1.8399293422698975 Time taken: 0.3461887836456299\n",
            "Batch Number: 1447 Loss: 1.8170392513275146 Time taken: 0.35425853729248047\n",
            "Batch Number: 1448 Loss: 1.857596516609192 Time taken: 0.3661975860595703\n",
            "Batch Number: 1449 Loss: 1.837595820426941 Time taken: 0.3011152744293213\n",
            "Batch Number: 1450 Loss: 1.839099407196045 Time taken: 0.31977248191833496\n",
            "Batch Number: 1451 Loss: 1.8238493204116821 Time taken: 0.30028462409973145\n",
            "Batch Number: 1452 Loss: 1.845319151878357 Time taken: 0.2918975353240967\n",
            "Batch Number: 1453 Loss: 1.8794918060302734 Time taken: 0.37095117568969727\n",
            "Batch Number: 1454 Loss: 1.8545559644699097 Time taken: 0.3176686763763428\n",
            "Batch Number: 1455 Loss: 1.860376000404358 Time taken: 0.321918249130249\n",
            "Batch Number: 1456 Loss: 1.8617417812347412 Time taken: 0.36721134185791016\n",
            "Batch Number: 1457 Loss: 1.8438029289245605 Time taken: 0.3009674549102783\n",
            "Batch Number: 1458 Loss: 1.8540695905685425 Time taken: 0.31214237213134766\n",
            "Batch Number: 1459 Loss: 1.8413680791854858 Time taken: 0.3470268249511719\n",
            "Batch Number: 1460 Loss: 1.8358460664749146 Time taken: 0.37877345085144043\n",
            "Batch Number: 1461 Loss: 1.8300271034240723 Time taken: 0.4158601760864258\n",
            "Batch Number: 1462 Loss: 1.8306105136871338 Time taken: 0.3507676124572754\n",
            "Batch Number: 1463 Loss: 1.8615673780441284 Time taken: 0.3666839599609375\n",
            "Batch Number: 1464 Loss: 1.8353735208511353 Time taken: 0.3852074146270752\n",
            "Batch Number: 1465 Loss: 1.8307735919952393 Time taken: 0.32103395462036133\n",
            "Batch Number: 1466 Loss: 1.8411223888397217 Time taken: 0.3084404468536377\n",
            "Batch Number: 1467 Loss: 1.8263108730316162 Time taken: 0.3036179542541504\n",
            "Batch Number: 1468 Loss: 1.8297393321990967 Time taken: 0.3063981533050537\n",
            "Batch Number: 1469 Loss: 1.8471214771270752 Time taken: 0.29163455963134766\n",
            "Batch Number: 1470 Loss: 1.8294421434402466 Time taken: 0.3624725341796875\n",
            "Batch Number: 1471 Loss: 1.8387824296951294 Time taken: 0.3247203826904297\n",
            "Batch Number: 1472 Loss: 1.8174209594726562 Time taken: 0.28722691535949707\n",
            "Batch Number: 1473 Loss: 1.86103355884552 Time taken: 0.28847503662109375\n",
            "Batch Number: 1474 Loss: 1.8220716714859009 Time taken: 0.3470432758331299\n",
            "Batch Number: 1475 Loss: 1.841414213180542 Time taken: 0.3135056495666504\n",
            "Batch Number: 1476 Loss: 1.8260406255722046 Time taken: 0.3685920238494873\n",
            "Batch Number: 1477 Loss: 1.8239370584487915 Time taken: 0.38407135009765625\n",
            "Batch Number: 1478 Loss: 1.879409909248352 Time taken: 0.36408543586730957\n",
            "Batch Number: 1479 Loss: 1.8577516078948975 Time taken: 0.3607509136199951\n",
            "Batch Number: 1480 Loss: 1.8738561868667603 Time taken: 0.29683923721313477\n",
            "Batch Number: 1481 Loss: 1.8336365222930908 Time taken: 0.28278160095214844\n",
            "Batch Number: 1482 Loss: 1.8108419179916382 Time taken: 0.2786719799041748\n",
            "Batch Number: 1483 Loss: 1.8524748086929321 Time taken: 0.3423311710357666\n",
            "Batch Number: 1484 Loss: 1.8459270000457764 Time taken: 0.29624485969543457\n",
            "Batch Number: 1485 Loss: 1.8492966890335083 Time taken: 0.3391754627227783\n",
            "Batch Number: 1486 Loss: 1.8446426391601562 Time taken: 0.3179605007171631\n",
            "Batch Number: 1487 Loss: 1.8320022821426392 Time taken: 0.28751635551452637\n",
            "Batch Number: 1488 Loss: 1.8380067348480225 Time taken: 0.3046386241912842\n",
            "Batch Number: 1489 Loss: 1.829783320426941 Time taken: 0.3092958927154541\n",
            "Batch Number: 1490 Loss: 1.8384939432144165 Time taken: 0.3075387477874756\n",
            "Batch Number: 1491 Loss: 1.8542864322662354 Time taken: 0.30513882637023926\n",
            "Batch Number: 1492 Loss: 1.8502787351608276 Time taken: 0.30176806449890137\n",
            "Batch Number: 1493 Loss: 1.8480474948883057 Time taken: 0.3396115303039551\n",
            "Batch Number: 1494 Loss: 1.8507041931152344 Time taken: 0.31540751457214355\n",
            "Batch Number: 1495 Loss: 1.837332844734192 Time taken: 0.3222513198852539\n",
            "Batch Number: 1496 Loss: 1.8214350938796997 Time taken: 0.32478880882263184\n",
            "Batch Number: 1497 Loss: 1.8450161218643188 Time taken: 0.30425167083740234\n",
            "Batch Number: 1498 Loss: 1.8612273931503296 Time taken: 0.292647123336792\n",
            "Batch Number: 1499 Loss: 1.8431793451309204 Time taken: 0.3174419403076172\n",
            "Batch Number: 1500 Loss: 1.8128174543380737 Time taken: 0.2887876033782959\n",
            "Batch Number: 1501 Loss: 1.8238096237182617 Time taken: 0.29199767112731934\n",
            "Batch Number: 1502 Loss: 1.8295446634292603 Time taken: 0.31981825828552246\n",
            "Batch Number: 1503 Loss: 1.8210409879684448 Time taken: 0.35234832763671875\n",
            "Batch Number: 1504 Loss: 1.8023018836975098 Time taken: 0.30260729789733887\n",
            "Batch Number: 1505 Loss: 1.8093595504760742 Time taken: 0.32540345191955566\n",
            "Batch Number: 1506 Loss: 1.8256797790527344 Time taken: 0.292888879776001\n",
            "Batch Number: 1507 Loss: 1.832879900932312 Time taken: 0.29893970489501953\n",
            "Batch Number: 1508 Loss: 1.8233064413070679 Time taken: 0.33752012252807617\n",
            "Batch Number: 1509 Loss: 1.8421680927276611 Time taken: 0.3174901008605957\n",
            "Batch Number: 1510 Loss: 1.8421705961227417 Time taken: 0.30484461784362793\n",
            "Batch Number: 1511 Loss: 1.834917664527893 Time taken: 0.31134533882141113\n",
            "Batch Number: 1512 Loss: 1.8339248895645142 Time taken: 0.31615304946899414\n",
            "Batch Number: 1513 Loss: 1.8360038995742798 Time taken: 0.2927377223968506\n",
            "Batch Number: 1514 Loss: 1.8659629821777344 Time taken: 0.3019580841064453\n",
            "Batch Number: 1515 Loss: 1.8566986322402954 Time taken: 0.31191372871398926\n",
            "Batch Number: 1516 Loss: 1.8371641635894775 Time taken: 0.29259181022644043\n",
            "Batch Number: 1517 Loss: 1.8358750343322754 Time taken: 0.30196166038513184\n",
            "Batch Number: 1518 Loss: 1.849290132522583 Time taken: 0.31111955642700195\n",
            "Batch Number: 1519 Loss: 1.8411229848861694 Time taken: 0.30202269554138184\n",
            "Batch Number: 1520 Loss: 1.830132246017456 Time taken: 0.3102884292602539\n",
            "Batch Number: 1521 Loss: 1.8355814218521118 Time taken: 0.3199336528778076\n",
            "Batch Number: 1522 Loss: 1.8417282104492188 Time taken: 0.3238060474395752\n",
            "Batch Number: 1523 Loss: 1.855675458908081 Time taken: 0.3002285957336426\n",
            "Batch Number: 1524 Loss: 1.8544092178344727 Time taken: 0.3098289966583252\n",
            "Batch Number: 1525 Loss: 1.835070013999939 Time taken: 0.3110661506652832\n",
            "Batch Number: 1526 Loss: 1.8113747835159302 Time taken: 0.30708956718444824\n",
            "Batch Number: 1527 Loss: 1.8081772327423096 Time taken: 0.3288233280181885\n",
            "Batch Number: 1528 Loss: 1.81308913230896 Time taken: 0.34228014945983887\n",
            "Batch Number: 1529 Loss: 1.808672308921814 Time taken: 0.29791784286499023\n",
            "Batch Number: 1530 Loss: 1.8310574293136597 Time taken: 0.31215715408325195\n",
            "Batch Number: 1531 Loss: 1.8098009824752808 Time taken: 0.3107106685638428\n",
            "Batch Number: 1532 Loss: 1.8586934804916382 Time taken: 0.3045196533203125\n",
            "Batch Number: 1533 Loss: 1.8585158586502075 Time taken: 0.33065056800842285\n",
            "Batch Number: 1534 Loss: 1.8497495651245117 Time taken: 0.32336997985839844\n",
            "Batch Number: 1535 Loss: 1.8592979907989502 Time taken: 0.2973604202270508\n",
            "Batch Number: 1536 Loss: 1.8514039516448975 Time taken: 0.30042290687561035\n",
            "Batch Number: 1537 Loss: 1.8753373622894287 Time taken: 0.3104372024536133\n",
            "Batch Number: 1538 Loss: 1.8621562719345093 Time taken: 0.32898569107055664\n",
            "Batch Number: 1539 Loss: 1.8488506078720093 Time taken: 0.30243706703186035\n",
            "Batch Number: 1540 Loss: 1.832354187965393 Time taken: 0.30893993377685547\n",
            "Batch Number: 1541 Loss: 1.8555564880371094 Time taken: 0.314255952835083\n",
            "Batch Number: 1542 Loss: 1.8387072086334229 Time taken: 0.29840660095214844\n",
            "Batch Number: 1543 Loss: 1.8497930765151978 Time taken: 0.30069923400878906\n",
            "Batch Number: 1544 Loss: 1.8562140464782715 Time taken: 0.3168671131134033\n",
            "Batch Number: 1545 Loss: 1.8442484140396118 Time taken: 0.29653000831604004\n",
            "Batch Number: 1546 Loss: 1.8469325304031372 Time taken: 0.3293604850769043\n",
            "Batch Number: 1547 Loss: 1.8606771230697632 Time taken: 0.29594850540161133\n",
            "Batch Number: 1548 Loss: 1.8851114511489868 Time taken: 0.2849459648132324\n",
            "Batch Number: 1549 Loss: 1.845508098602295 Time taken: 0.30101490020751953\n",
            "Batch Number: 1550 Loss: 1.8718546628952026 Time taken: 0.298980712890625\n",
            "Batch Number: 1551 Loss: 1.868367314338684 Time taken: 0.34540653228759766\n",
            "Batch Number: 1552 Loss: 1.8826054334640503 Time taken: 0.3727097511291504\n",
            "Batch Number: 1553 Loss: 1.8449954986572266 Time taken: 0.3751800060272217\n",
            "Batch Number: 1554 Loss: 1.8727362155914307 Time taken: 0.38480353355407715\n",
            "Batch Number: 1555 Loss: 1.8622649908065796 Time taken: 0.3412933349609375\n",
            "Batch Number: 1556 Loss: 1.8664485216140747 Time taken: 0.38584089279174805\n",
            "Batch Number: 1557 Loss: 1.8582854270935059 Time taken: 0.341841459274292\n",
            "Batch Number: 1558 Loss: 1.8247419595718384 Time taken: 0.3816356658935547\n",
            "Batch Number: 1559 Loss: 1.8674768209457397 Time taken: 0.3961930274963379\n",
            "Batch Number: 1560 Loss: 1.8406885862350464 Time taken: 0.31420397758483887\n",
            "Batch Number: 1561 Loss: 1.8279144763946533 Time taken: 0.29666781425476074\n",
            "Batch Number: 1562 Loss: 1.8573893308639526 Time taken: 0.3188819885253906\n",
            "Batch Number: 1563 Loss: 1.8250283002853394 Time taken: 0.3355269432067871\n",
            "Batch Number: 1564 Loss: 1.8405681848526 Time taken: 0.30464816093444824\n",
            "Batch Number: 1565 Loss: 1.8470152616500854 Time taken: 0.3107788562774658\n",
            "Batch Number: 1566 Loss: 1.8729956150054932 Time taken: 0.2948620319366455\n",
            "Batch Number: 1567 Loss: 1.824352741241455 Time taken: 0.30672216415405273\n",
            "Batch Number: 1568 Loss: 1.8125593662261963 Time taken: 0.3017301559448242\n",
            "Batch Number: 1569 Loss: 1.8358455896377563 Time taken: 0.33324503898620605\n",
            "Batch Number: 1570 Loss: 1.8352190256118774 Time taken: 0.36789989471435547\n",
            "Batch Number: 1571 Loss: 1.801240086555481 Time taken: 0.3860659599304199\n",
            "Batch Number: 1572 Loss: 1.8241426944732666 Time taken: 0.36159229278564453\n",
            "Batch Number: 1573 Loss: 1.8392126560211182 Time taken: 0.29608988761901855\n",
            "Batch Number: 1574 Loss: 1.823251485824585 Time taken: 0.3130819797515869\n",
            "Batch Number: 1575 Loss: 1.8330239057540894 Time taken: 0.3158257007598877\n",
            "Batch Number: 1576 Loss: 1.8209216594696045 Time taken: 0.3593435287475586\n",
            "Batch Number: 1577 Loss: 1.7856824398040771 Time taken: 0.3889436721801758\n",
            "Batch Number: 1578 Loss: 1.779595971107483 Time taken: 0.3403170108795166\n",
            "Batch Number: 1579 Loss: 1.801891565322876 Time taken: 0.34006237983703613\n",
            "Batch Number: 1580 Loss: 1.8070164918899536 Time taken: 0.38942432403564453\n",
            "Batch Number: 1581 Loss: 1.8260365724563599 Time taken: 0.3878803253173828\n",
            "Batch Number: 1582 Loss: 1.8576117753982544 Time taken: 0.36488986015319824\n",
            "Batch Number: 1583 Loss: 1.814489722251892 Time taken: 0.35530948638916016\n",
            "Batch Number: 1584 Loss: 1.7998721599578857 Time taken: 0.3696920871734619\n",
            "Batch Number: 1585 Loss: 1.8305257558822632 Time taken: 0.3513765335083008\n",
            "Batch Number: 1586 Loss: 1.8096636533737183 Time taken: 0.33721446990966797\n",
            "Batch Number: 1587 Loss: 1.8296047449111938 Time taken: 0.2894325256347656\n",
            "Batch Number: 1588 Loss: 1.8342941999435425 Time taken: 0.30866312980651855\n",
            "Batch Number: 1589 Loss: 1.8476588726043701 Time taken: 0.3284585475921631\n",
            "Batch Number: 1590 Loss: 1.8201090097427368 Time taken: 0.29636526107788086\n",
            "Batch Number: 1591 Loss: 1.8532202243804932 Time taken: 0.29638195037841797\n",
            "Batch Number: 1592 Loss: 1.850746512413025 Time taken: 0.3357362747192383\n",
            "Batch Number: 1593 Loss: 1.8579274415969849 Time taken: 0.2943117618560791\n",
            "Batch Number: 1594 Loss: 1.8354458808898926 Time taken: 0.30774736404418945\n",
            "Batch Number: 1595 Loss: 1.8554048538208008 Time taken: 0.31688833236694336\n",
            "Batch Number: 1596 Loss: 1.8275057077407837 Time taken: 0.29873108863830566\n",
            "Batch Number: 1597 Loss: 1.8566960096359253 Time taken: 0.2994706630706787\n",
            "Batch Number: 1598 Loss: 1.8419312238693237 Time taken: 0.31931376457214355\n",
            "Batch Number: 1599 Loss: 1.8320971727371216 Time taken: 0.3178415298461914\n",
            "Batch Number: 1600 Loss: 1.8585270643234253 Time taken: 0.2936234474182129\n",
            "Batch Number: 1601 Loss: 1.826887845993042 Time taken: 0.2974996566772461\n",
            "Batch Number: 1602 Loss: 1.8491768836975098 Time taken: 0.30517077445983887\n",
            "Batch Number: 1603 Loss: 1.8280093669891357 Time taken: 0.2971682548522949\n",
            "Batch Number: 1604 Loss: 1.8280516862869263 Time taken: 0.2835428714752197\n",
            "Batch Number: 1605 Loss: 1.835255742073059 Time taken: 0.3598673343658447\n",
            "Batch Number: 1606 Loss: 1.8327311277389526 Time taken: 0.32672953605651855\n",
            "Batch Number: 1607 Loss: 1.8235805034637451 Time taken: 0.37520551681518555\n",
            "Batch Number: 1608 Loss: 1.7987000942230225 Time taken: 0.32581424713134766\n",
            "Batch Number: 1609 Loss: 1.819815993309021 Time taken: 0.30413198471069336\n",
            "Batch Number: 1610 Loss: 1.8226981163024902 Time taken: 0.3122823238372803\n",
            "Batch Number: 1611 Loss: 1.8400143384933472 Time taken: 0.3492012023925781\n",
            "Batch Number: 1612 Loss: 1.799611210823059 Time taken: 0.30328369140625\n",
            "Batch Number: 1613 Loss: 1.814538598060608 Time taken: 0.2997872829437256\n",
            "Batch Number: 1614 Loss: 1.8208166360855103 Time taken: 0.3499166965484619\n",
            "Batch Number: 1615 Loss: 1.8013578653335571 Time taken: 0.30348849296569824\n",
            "Batch Number: 1616 Loss: 1.7954035997390747 Time taken: 0.3155686855316162\n",
            "Batch Number: 1617 Loss: 1.775837779045105 Time taken: 0.3726356029510498\n",
            "Batch Number: 1618 Loss: 1.7948366403579712 Time taken: 0.3042283058166504\n",
            "Batch Number: 1619 Loss: 1.7988700866699219 Time taken: 0.29924988746643066\n",
            "Batch Number: 1620 Loss: 1.8010613918304443 Time taken: 0.32350802421569824\n",
            "Batch Number: 1621 Loss: 1.8084245920181274 Time taken: 0.30138421058654785\n",
            "Batch Number: 1622 Loss: 1.7816165685653687 Time taken: 0.3402066230773926\n",
            "Batch Number: 1623 Loss: 1.7816848754882812 Time taken: 0.3327510356903076\n",
            "Batch Number: 1624 Loss: 1.7783809900283813 Time taken: 0.3223130702972412\n",
            "Batch Number: 1625 Loss: 1.7699620723724365 Time taken: 0.3063316345214844\n",
            "Batch Number: 1626 Loss: 1.7976384162902832 Time taken: 0.323352575302124\n",
            "Batch Number: 1627 Loss: 1.7758501768112183 Time taken: 0.3087127208709717\n",
            "Batch Number: 1628 Loss: 1.7886372804641724 Time taken: 0.29934024810791016\n",
            "Batch Number: 1629 Loss: 1.7931557893753052 Time taken: 0.3210573196411133\n",
            "Batch Number: 1630 Loss: 1.779306173324585 Time taken: 0.31887102127075195\n",
            "Batch Number: 1631 Loss: 1.7950518131256104 Time taken: 0.29822635650634766\n",
            "Batch Number: 1632 Loss: 1.7894501686096191 Time taken: 0.2979085445404053\n",
            "Batch Number: 1633 Loss: 1.8182563781738281 Time taken: 0.362973690032959\n",
            "Batch Number: 1634 Loss: 1.8550543785095215 Time taken: 0.3684074878692627\n",
            "Batch Number: 1635 Loss: 1.7896605730056763 Time taken: 0.3770937919616699\n",
            "Batch Number: 1636 Loss: 1.7908607721328735 Time taken: 0.3826315402984619\n",
            "Batch Number: 1637 Loss: 1.7817432880401611 Time taken: 0.36759495735168457\n",
            "Batch Number: 1638 Loss: 1.8010741472244263 Time taken: 0.33324098587036133\n",
            "Batch Number: 1639 Loss: 1.7520166635513306 Time taken: 0.306262731552124\n",
            "Batch Number: 1640 Loss: 1.7737871408462524 Time taken: 0.29253339767456055\n",
            "Batch Number: 1641 Loss: 1.778876543045044 Time taken: 0.31105732917785645\n",
            "Batch Number: 1642 Loss: 1.803910493850708 Time taken: 0.2990908622741699\n",
            "Batch Number: 1643 Loss: 1.8064707517623901 Time taken: 0.2876455783843994\n",
            "Batch Number: 1644 Loss: 1.8151881694793701 Time taken: 0.32735371589660645\n",
            "Batch Number: 1645 Loss: 1.8077799081802368 Time taken: 0.32317638397216797\n",
            "Batch Number: 1646 Loss: 1.7803913354873657 Time taken: 0.29459381103515625\n",
            "Batch Number: 1647 Loss: 1.7901750802993774 Time taken: 0.2880113124847412\n",
            "Batch Number: 1648 Loss: 1.78692626953125 Time taken: 0.31369972229003906\n",
            "Batch Number: 1649 Loss: 1.798140048980713 Time taken: 0.29892492294311523\n",
            "Batch Number: 1650 Loss: 1.800912618637085 Time taken: 0.2881040573120117\n",
            "Batch Number: 1651 Loss: 1.7789292335510254 Time taken: 0.30932068824768066\n",
            "Batch Number: 1652 Loss: 1.778452754020691 Time taken: 0.3072221279144287\n",
            "Batch Number: 1653 Loss: 1.804263949394226 Time taken: 0.2976551055908203\n",
            "Batch Number: 1654 Loss: 1.8272876739501953 Time taken: 0.33243370056152344\n",
            "Batch Number: 1655 Loss: 1.8022258281707764 Time taken: 0.32931041717529297\n",
            "Batch Number: 1656 Loss: 1.7903387546539307 Time taken: 0.30026674270629883\n",
            "Batch Number: 1657 Loss: 1.8272478580474854 Time taken: 0.31104135513305664\n",
            "Batch Number: 1658 Loss: 1.80646812915802 Time taken: 0.31034278869628906\n",
            "Batch Number: 1659 Loss: 1.8064500093460083 Time taken: 0.2968728542327881\n",
            "Batch Number: 1660 Loss: 1.8036655187606812 Time taken: 0.31052303314208984\n",
            "Batch Number: 1661 Loss: 1.7885545492172241 Time taken: 0.307971715927124\n",
            "Batch Number: 1662 Loss: 1.8117231130599976 Time taken: 0.29681921005249023\n",
            "Batch Number: 1663 Loss: 1.8050060272216797 Time taken: 0.29062581062316895\n",
            "Batch Number: 1664 Loss: 1.7747774124145508 Time taken: 0.3219630718231201\n",
            "Batch Number: 1665 Loss: 1.7919745445251465 Time taken: 0.2948789596557617\n",
            "Batch Number: 1666 Loss: 1.79567551612854 Time taken: 0.33043384552001953\n",
            "Batch Number: 1667 Loss: 1.7977756261825562 Time taken: 0.3345043659210205\n",
            "Batch Number: 1668 Loss: 1.8247087001800537 Time taken: 0.30120372772216797\n",
            "Batch Number: 1669 Loss: 1.7967201471328735 Time taken: 0.2993495464324951\n",
            "Batch Number: 1670 Loss: 1.7909942865371704 Time taken: 0.31859755516052246\n",
            "Batch Number: 1671 Loss: 1.80784010887146 Time taken: 0.3087799549102783\n",
            "Batch Number: 1672 Loss: 1.7913175821304321 Time taken: 0.30143070220947266\n",
            "Batch Number: 1673 Loss: 1.8086702823638916 Time taken: 0.33164119720458984\n",
            "Batch Number: 1674 Loss: 1.8055033683776855 Time taken: 0.31080150604248047\n",
            "Batch Number: 1675 Loss: 1.7725783586502075 Time taken: 0.2980659008026123\n",
            "Batch Number: 1676 Loss: 1.80840003490448 Time taken: 0.3023102283477783\n",
            "Batch Number: 1677 Loss: 1.7986679077148438 Time taken: 0.35426974296569824\n",
            "Batch Number: 1678 Loss: 1.7810320854187012 Time taken: 0.3794710636138916\n",
            "Batch Number: 1679 Loss: 1.7976704835891724 Time taken: 0.34960460662841797\n",
            "Batch Number: 1680 Loss: 1.7847890853881836 Time taken: 0.3076179027557373\n",
            "Batch Number: 1681 Loss: 1.766491413116455 Time taken: 0.2882237434387207\n",
            "Batch Number: 1682 Loss: 1.7950721979141235 Time taken: 0.30417919158935547\n",
            "Batch Number: 1683 Loss: 1.7770999670028687 Time taken: 0.38475584983825684\n",
            "Batch Number: 1684 Loss: 1.7665376663208008 Time taken: 0.37513065338134766\n",
            "Batch Number: 1685 Loss: 1.8052281141281128 Time taken: 0.30211973190307617\n",
            "Batch Number: 1686 Loss: 1.7996916770935059 Time taken: 0.3066272735595703\n",
            "Batch Number: 1687 Loss: 1.7757370471954346 Time taken: 0.36212778091430664\n",
            "Batch Number: 1688 Loss: 1.8022327423095703 Time taken: 0.3019411563873291\n",
            "Batch Number: 1689 Loss: 1.7803356647491455 Time taken: 0.3095576763153076\n",
            "Batch Number: 1690 Loss: 1.7964524030685425 Time taken: 0.3334074020385742\n",
            "Batch Number: 1691 Loss: 1.8170535564422607 Time taken: 0.3379850387573242\n",
            "Batch Number: 1692 Loss: 1.8055307865142822 Time taken: 0.31606531143188477\n",
            "Batch Number: 1693 Loss: 1.8059850931167603 Time taken: 0.31511783599853516\n",
            "Batch Number: 1694 Loss: 1.8146154880523682 Time taken: 0.3049650192260742\n",
            "Batch Number: 1695 Loss: 1.826758861541748 Time taken: 0.3058440685272217\n",
            "Batch Number: 1696 Loss: 1.805024266242981 Time taken: 0.30615854263305664\n",
            "Batch Number: 1697 Loss: 1.802144169807434 Time taken: 0.32959723472595215\n",
            "Batch Number: 1698 Loss: 1.8104956150054932 Time taken: 0.3011312484741211\n",
            "Batch Number: 1699 Loss: 1.791250228881836 Time taken: 0.2995774745941162\n",
            "Batch Number: 1700 Loss: 1.7822730541229248 Time taken: 0.3048408031463623\n",
            "Batch Number: 1701 Loss: 1.7918931245803833 Time taken: 0.3248288631439209\n",
            "Batch Number: 1702 Loss: 1.782293677330017 Time taken: 0.3283576965332031\n",
            "Batch Number: 1703 Loss: 1.7769209146499634 Time taken: 0.33531689643859863\n",
            "Batch Number: 1704 Loss: 1.7886862754821777 Time taken: 0.30982112884521484\n",
            "Batch Number: 1705 Loss: 1.761736273765564 Time taken: 0.31412792205810547\n",
            "Batch Number: 1706 Loss: 1.7737221717834473 Time taken: 0.3276674747467041\n",
            "Batch Number: 1707 Loss: 1.7647807598114014 Time taken: 0.3132596015930176\n",
            "Batch Number: 1708 Loss: 1.7763359546661377 Time taken: 0.29352712631225586\n",
            "Batch Number: 1709 Loss: 1.764870047569275 Time taken: 0.3177628517150879\n",
            "Batch Number: 1710 Loss: 1.7992230653762817 Time taken: 0.32375073432922363\n",
            "Batch Number: 1711 Loss: 1.7987990379333496 Time taken: 0.3029937744140625\n",
            "Batch Number: 1712 Loss: 1.7849754095077515 Time taken: 0.31379151344299316\n",
            "Batch Number: 1713 Loss: 1.8033627271652222 Time taken: 0.30083131790161133\n",
            "Batch Number: 1714 Loss: 1.7898329496383667 Time taken: 0.3152024745941162\n",
            "Batch Number: 1715 Loss: 1.797624945640564 Time taken: 0.3162224292755127\n",
            "Batch Number: 1716 Loss: 1.8254741430282593 Time taken: 0.3151588439941406\n",
            "Batch Number: 1717 Loss: 1.823219895362854 Time taken: 0.3319265842437744\n",
            "Batch Number: 1718 Loss: 1.824807047843933 Time taken: 0.3135809898376465\n",
            "Batch Number: 1719 Loss: 1.8030314445495605 Time taken: 0.3438379764556885\n",
            "Batch Number: 1720 Loss: 1.8166778087615967 Time taken: 0.37906360626220703\n",
            "Batch Number: 1721 Loss: 1.827996850013733 Time taken: 0.3925797939300537\n",
            "Batch Number: 1722 Loss: 1.8308608531951904 Time taken: 0.29735612869262695\n",
            "Batch Number: 1723 Loss: 1.797965407371521 Time taken: 0.30229806900024414\n",
            "Batch Number: 1724 Loss: 1.8094382286071777 Time taken: 0.30803394317626953\n",
            "Batch Number: 1725 Loss: 1.837450623512268 Time taken: 0.30527544021606445\n",
            "Batch Number: 1726 Loss: 1.8449746370315552 Time taken: 0.3006553649902344\n",
            "Batch Number: 1727 Loss: 1.8182722330093384 Time taken: 0.29206132888793945\n",
            "Batch Number: 1728 Loss: 1.823585033416748 Time taken: 0.3222935199737549\n",
            "Batch Number: 1729 Loss: 1.8239531517028809 Time taken: 0.3038771152496338\n",
            "Batch Number: 1730 Loss: 1.8476841449737549 Time taken: 0.3022880554199219\n",
            "Batch Number: 1731 Loss: 1.803607702255249 Time taken: 0.31458425521850586\n",
            "Batch Number: 1732 Loss: 1.8243893384933472 Time taken: 0.301039457321167\n",
            "Batch Number: 1733 Loss: 1.8328182697296143 Time taken: 0.30087804794311523\n",
            "Batch Number: 1734 Loss: 1.8241513967514038 Time taken: 0.3482930660247803\n",
            "Batch Number: 1735 Loss: 1.8003079891204834 Time taken: 0.294543981552124\n",
            "Batch Number: 1736 Loss: 1.7992815971374512 Time taken: 0.28783512115478516\n",
            "Batch Number: 1737 Loss: 1.803760290145874 Time taken: 0.30109739303588867\n",
            "Batch Number: 1738 Loss: 1.8106929063796997 Time taken: 0.30005931854248047\n",
            "Batch Number: 1739 Loss: 1.7776414155960083 Time taken: 0.30516839027404785\n",
            "Batch Number: 1740 Loss: 1.8170456886291504 Time taken: 0.29472899436950684\n",
            "Batch Number: 1741 Loss: 1.8019682168960571 Time taken: 0.37078404426574707\n",
            "Batch Number: 1742 Loss: 1.8090161085128784 Time taken: 0.39005088806152344\n",
            "Batch Number: 1743 Loss: 1.8096076250076294 Time taken: 0.35942554473876953\n",
            "Batch Number: 1744 Loss: 1.808699607849121 Time taken: 0.33172106742858887\n",
            "Batch Number: 1745 Loss: 1.8048523664474487 Time taken: 0.3750574588775635\n",
            "Batch Number: 1746 Loss: 1.788010835647583 Time taken: 0.3295891284942627\n",
            "Batch Number: 1747 Loss: 1.8022959232330322 Time taken: 0.3030712604522705\n",
            "Batch Number: 1748 Loss: 1.789446234703064 Time taken: 0.3102085590362549\n",
            "Batch Number: 1749 Loss: 1.7862571477890015 Time taken: 0.2955043315887451\n",
            "Batch Number: 1750 Loss: 1.795698881149292 Time taken: 0.308394193649292\n",
            "Batch Number: 1751 Loss: 1.7658460140228271 Time taken: 0.30170464515686035\n",
            "Batch Number: 1752 Loss: 1.8042659759521484 Time taken: 0.29444241523742676\n",
            "Batch Number: 1753 Loss: 1.7990988492965698 Time taken: 0.32770705223083496\n",
            "Batch Number: 1754 Loss: 1.792914867401123 Time taken: 0.3062407970428467\n",
            "Batch Number: 1755 Loss: 1.7782094478607178 Time taken: 0.300642728805542\n",
            "Batch Number: 1756 Loss: 1.7473255395889282 Time taken: 0.31178832054138184\n",
            "Batch Number: 1757 Loss: 1.763712763786316 Time taken: 0.30103206634521484\n",
            "Batch Number: 1758 Loss: 1.7957435846328735 Time taken: 0.3256947994232178\n",
            "Batch Number: 1759 Loss: 1.7534291744232178 Time taken: 0.31507277488708496\n",
            "Batch Number: 1760 Loss: 1.8134814500808716 Time taken: 0.300123929977417\n",
            "Batch Number: 1761 Loss: 1.779631495475769 Time taken: 0.3075144290924072\n",
            "Batch Number: 1762 Loss: 1.7962020635604858 Time taken: 0.32187557220458984\n",
            "Batch Number: 1763 Loss: 1.7885777950286865 Time taken: 0.37401819229125977\n",
            "Batch Number: 1764 Loss: 1.8090707063674927 Time taken: 0.3847346305847168\n",
            "Batch Number: 1765 Loss: 1.820603609085083 Time taken: 0.37919044494628906\n",
            "Batch Number: 1766 Loss: 1.8320285081863403 Time taken: 0.3903660774230957\n",
            "Batch Number: 1767 Loss: 1.8504916429519653 Time taken: 0.36863064765930176\n",
            "Batch Number: 1768 Loss: 1.7804179191589355 Time taken: 0.33318305015563965\n",
            "Batch Number: 1769 Loss: 1.803110122680664 Time taken: 0.3116145133972168\n",
            "Batch Number: 1770 Loss: 1.7858901023864746 Time taken: 0.3292412757873535\n",
            "Batch Number: 1771 Loss: 1.7946778535842896 Time taken: 0.38493967056274414\n",
            "Batch Number: 1772 Loss: 1.8176466226577759 Time taken: 0.39476656913757324\n",
            "Batch Number: 1773 Loss: 1.8287187814712524 Time taken: 0.3293635845184326\n",
            "Batch Number: 1774 Loss: 1.7803477048873901 Time taken: 0.3045938014984131\n",
            "Batch Number: 1775 Loss: 1.7738054990768433 Time taken: 0.31252026557922363\n",
            "Batch Number: 1776 Loss: 1.8057509660720825 Time taken: 0.330747127532959\n",
            "Batch Number: 1777 Loss: 1.8212002515792847 Time taken: 0.31977033615112305\n",
            "Batch Number: 1778 Loss: 1.8064744472503662 Time taken: 0.314929723739624\n",
            "Batch Number: 1779 Loss: 1.8173778057098389 Time taken: 0.3048539161682129\n",
            "Batch Number: 1780 Loss: 1.8027740716934204 Time taken: 0.3106667995452881\n",
            "Batch Number: 1781 Loss: 1.8099623918533325 Time taken: 0.3106203079223633\n",
            "Batch Number: 1782 Loss: 1.7785319089889526 Time taken: 0.3456695079803467\n",
            "Batch Number: 1783 Loss: 1.785498857498169 Time taken: 0.31548500061035156\n",
            "Batch Number: 1784 Loss: 1.7882272005081177 Time taken: 0.33513760566711426\n",
            "Batch Number: 1785 Loss: 1.7927449941635132 Time taken: 0.3289480209350586\n",
            "Batch Number: 1786 Loss: 1.7991564273834229 Time taken: 0.31551218032836914\n",
            "Batch Number: 1787 Loss: 1.7646008729934692 Time taken: 0.3048665523529053\n",
            "Batch Number: 1788 Loss: 1.7986770868301392 Time taken: 0.3048861026763916\n",
            "Batch Number: 1789 Loss: 1.788108229637146 Time taken: 0.2950098514556885\n",
            "Batch Number: 1790 Loss: 1.7579247951507568 Time taken: 0.32444119453430176\n",
            "Batch Number: 1791 Loss: 1.7859326601028442 Time taken: 0.31751513481140137\n",
            "Batch Number: 1792 Loss: 1.796041488647461 Time taken: 0.30481982231140137\n",
            "Batch Number: 1793 Loss: 1.79805588722229 Time taken: 0.31075096130371094\n",
            "Batch Number: 1794 Loss: 1.7811280488967896 Time taken: 0.29972100257873535\n",
            "Batch Number: 1795 Loss: 1.7753466367721558 Time taken: 0.31450963020324707\n",
            "Batch Number: 1796 Loss: 1.7644544839859009 Time taken: 0.30680060386657715\n",
            "Batch Number: 1797 Loss: 1.77367103099823 Time taken: 0.3051588535308838\n",
            "Batch Number: 1798 Loss: 1.7687009572982788 Time taken: 0.3017604351043701\n",
            "Batch Number: 1799 Loss: 1.756722092628479 Time taken: 0.34587907791137695\n",
            "Batch Number: 1800 Loss: 1.7440993785858154 Time taken: 0.39266061782836914\n",
            "Batch Number: 1801 Loss: 1.7688803672790527 Time taken: 0.3258628845214844\n",
            "Batch Number: 1802 Loss: 1.7349328994750977 Time taken: 0.30024147033691406\n",
            "Batch Number: 1803 Loss: 1.7795864343643188 Time taken: 0.29497575759887695\n",
            "Batch Number: 1804 Loss: 1.7504123449325562 Time taken: 0.3049905300140381\n",
            "Batch Number: 1805 Loss: 1.7442909479141235 Time taken: 0.30722784996032715\n",
            "Batch Number: 1806 Loss: 1.7445353269577026 Time taken: 0.29674744606018066\n",
            "Batch Number: 1807 Loss: 1.7325305938720703 Time taken: 0.33923816680908203\n",
            "Batch Number: 1808 Loss: 1.7481528520584106 Time taken: 0.377166748046875\n",
            "Batch Number: 1809 Loss: 1.7598309516906738 Time taken: 0.38956713676452637\n",
            "Batch Number: 1810 Loss: 1.7465406656265259 Time taken: 0.30940723419189453\n",
            "Batch Number: 1811 Loss: 1.7516807317733765 Time taken: 0.30643391609191895\n",
            "Batch Number: 1812 Loss: 1.7519114017486572 Time taken: 0.3020615577697754\n",
            "Batch Number: 1813 Loss: 1.7536084651947021 Time taken: 0.34676456451416016\n",
            "Batch Number: 1814 Loss: 1.7626889944076538 Time taken: 0.2997300624847412\n",
            "Batch Number: 1815 Loss: 1.7784208059310913 Time taken: 0.3860325813293457\n",
            "Batch Number: 1816 Loss: 1.7605845928192139 Time taken: 0.33365941047668457\n",
            "Batch Number: 1817 Loss: 1.757353663444519 Time taken: 0.30600404739379883\n",
            "Batch Number: 1818 Loss: 1.759591817855835 Time taken: 0.3070557117462158\n",
            "Batch Number: 1819 Loss: 1.7596317529678345 Time taken: 0.3065497875213623\n",
            "Batch Number: 1820 Loss: 1.7531402111053467 Time taken: 0.2966270446777344\n",
            "Batch Number: 1821 Loss: 1.7295809984207153 Time taken: 0.32654714584350586\n",
            "Batch Number: 1822 Loss: 1.7475675344467163 Time taken: 0.3474006652832031\n",
            "Batch Number: 1823 Loss: 1.7545453310012817 Time taken: 0.36484789848327637\n",
            "Batch Number: 1824 Loss: 1.7627063989639282 Time taken: 0.3946845531463623\n",
            "Batch Number: 1825 Loss: 1.778275489807129 Time taken: 0.3267521858215332\n",
            "Batch Number: 1826 Loss: 1.7316776514053345 Time taken: 0.30368781089782715\n",
            "Batch Number: 1827 Loss: 1.7515003681182861 Time taken: 0.3097503185272217\n",
            "Batch Number: 1828 Loss: 1.7587082386016846 Time taken: 0.35776805877685547\n",
            "Batch Number: 1829 Loss: 1.7635263204574585 Time taken: 0.39006853103637695\n",
            "Batch Number: 1830 Loss: 1.7611042261123657 Time taken: 0.36633896827697754\n",
            "Batch Number: 1831 Loss: 1.7483655214309692 Time taken: 0.3327977657318115\n",
            "Batch Number: 1832 Loss: 1.7551151514053345 Time taken: 0.37073540687561035\n",
            "Batch Number: 1833 Loss: 1.7278966903686523 Time taken: 0.3317584991455078\n",
            "Batch Number: 1834 Loss: 1.777449131011963 Time taken: 0.29227638244628906\n",
            "Batch Number: 1835 Loss: 1.7435882091522217 Time taken: 0.30305957794189453\n",
            "Batch Number: 1836 Loss: 1.7691009044647217 Time taken: 0.3207368850708008\n",
            "Batch Number: 1837 Loss: 1.7744109630584717 Time taken: 0.30803942680358887\n",
            "Batch Number: 1838 Loss: 1.7778329849243164 Time taken: 0.292344331741333\n",
            "Batch Number: 1839 Loss: 1.800097942352295 Time taken: 0.31099605560302734\n",
            "Batch Number: 1840 Loss: 1.7653381824493408 Time taken: 0.3119969367980957\n",
            "Batch Number: 1841 Loss: 1.7745097875595093 Time taken: 0.29825353622436523\n",
            "Batch Number: 1842 Loss: 1.7732657194137573 Time taken: 0.3096439838409424\n",
            "Batch Number: 1843 Loss: 1.7658123970031738 Time taken: 0.3274815082550049\n",
            "Batch Number: 1844 Loss: 1.7648340463638306 Time taken: 0.3013150691986084\n",
            "Batch Number: 1845 Loss: 1.7440186738967896 Time taken: 0.31064462661743164\n",
            "Batch Number: 1846 Loss: 1.7691184282302856 Time taken: 0.302370548248291\n",
            "Batch Number: 1847 Loss: 1.7451376914978027 Time taken: 0.306995153427124\n",
            "Batch Number: 1848 Loss: 1.7526220083236694 Time taken: 0.3344738483428955\n",
            "Batch Number: 1849 Loss: 1.7426668405532837 Time taken: 0.34250926971435547\n",
            "Batch Number: 1850 Loss: 1.7527862787246704 Time taken: 0.29628419876098633\n",
            "Batch Number: 1851 Loss: 1.7608473300933838 Time taken: 0.29674434661865234\n",
            "Batch Number: 1852 Loss: 1.7679458856582642 Time taken: 0.31426572799682617\n",
            "Batch Number: 1853 Loss: 1.7574150562286377 Time taken: 0.29123806953430176\n",
            "Batch Number: 1854 Loss: 1.7535158395767212 Time taken: 0.2867715358734131\n",
            "Batch Number: 1855 Loss: 1.7758275270462036 Time taken: 0.3049201965332031\n",
            "Batch Number: 1856 Loss: 1.7411911487579346 Time taken: 0.31366515159606934\n",
            "Batch Number: 1857 Loss: 1.7298108339309692 Time taken: 0.29535818099975586\n",
            "Batch Number: 1858 Loss: 1.7543901205062866 Time taken: 0.32181453704833984\n",
            "Batch Number: 1859 Loss: 1.7639940977096558 Time taken: 0.30620908737182617\n",
            "Batch Number: 1860 Loss: 1.7666759490966797 Time taken: 0.2954277992248535\n",
            "Batch Number: 1861 Loss: 1.7656420469284058 Time taken: 0.3149275779724121\n",
            "Batch Number: 1862 Loss: 1.7299391031265259 Time taken: 0.3298013210296631\n",
            "Batch Number: 1863 Loss: 1.7476558685302734 Time taken: 0.30234670639038086\n",
            "Batch Number: 1864 Loss: 1.7316733598709106 Time taken: 0.287463903427124\n",
            "Batch Number: 1865 Loss: 1.7260243892669678 Time taken: 0.3110651969909668\n",
            "Batch Number: 1866 Loss: 1.7579039335250854 Time taken: 0.31054043769836426\n",
            "Batch Number: 1867 Loss: 1.7696291208267212 Time taken: 0.30730319023132324\n",
            "Batch Number: 1868 Loss: 1.7670838832855225 Time taken: 0.3532073497772217\n",
            "Batch Number: 1869 Loss: 1.7548507452011108 Time taken: 0.2929363250732422\n",
            "Batch Number: 1870 Loss: 1.7795555591583252 Time taken: 0.2927742004394531\n",
            "Batch Number: 1871 Loss: 1.761622428894043 Time taken: 0.346834659576416\n",
            "Batch Number: 1872 Loss: 1.7711129188537598 Time taken: 0.30774664878845215\n",
            "Batch Number: 1873 Loss: 1.785479187965393 Time taken: 0.32960987091064453\n",
            "Batch Number: 1874 Loss: 1.784200668334961 Time taken: 0.37332582473754883\n",
            "Batch Number: 1875 Loss: 1.7673673629760742 Time taken: 0.3137476444244385\n",
            "Batch Number: 1876 Loss: 1.7859811782836914 Time taken: 0.2999553680419922\n",
            "Batch Number: 1877 Loss: 1.7861311435699463 Time taken: 0.31084179878234863\n",
            "Batch Number: 1878 Loss: 1.759881854057312 Time taken: 0.30457401275634766\n",
            "Batch Number: 1879 Loss: 1.7628333568572998 Time taken: 0.29826927185058594\n",
            "Batch Number: 1880 Loss: 1.737741231918335 Time taken: 0.3160083293914795\n",
            "Batch Number: 1881 Loss: 1.751799464225769 Time taken: 0.34731197357177734\n",
            "Batch Number: 1882 Loss: 1.749822735786438 Time taken: 0.2895841598510742\n",
            "Batch Number: 1883 Loss: 1.731960415840149 Time taken: 0.30895376205444336\n",
            "Batch Number: 1884 Loss: 1.7435308694839478 Time taken: 0.3161885738372803\n",
            "Batch Number: 1885 Loss: 1.716676950454712 Time taken: 0.3346700668334961\n",
            "Batch Number: 1886 Loss: 1.7455697059631348 Time taken: 0.3699355125427246\n",
            "Batch Number: 1887 Loss: 1.734236240386963 Time taken: 0.32954835891723633\n",
            "Batch Number: 1888 Loss: 1.741076946258545 Time taken: 0.2975308895111084\n",
            "Batch Number: 1889 Loss: 1.7346912622451782 Time taken: 0.34897303581237793\n",
            "Batch Number: 1890 Loss: 1.7426395416259766 Time taken: 0.3261911869049072\n",
            "Batch Number: 1891 Loss: 1.7653025388717651 Time taken: 0.29915452003479004\n",
            "Batch Number: 1892 Loss: 1.7511780261993408 Time taken: 0.30448436737060547\n",
            "Batch Number: 1893 Loss: 1.747274398803711 Time taken: 0.35150718688964844\n",
            "Batch Number: 1894 Loss: 1.77837073802948 Time taken: 0.3017446994781494\n",
            "Batch Number: 1895 Loss: 1.7897611856460571 Time taken: 0.2992525100708008\n",
            "Batch Number: 1896 Loss: 1.7809877395629883 Time taken: 0.3078727722167969\n",
            "Batch Number: 1897 Loss: 1.7842820882797241 Time taken: 0.30640363693237305\n",
            "Batch Number: 1898 Loss: 1.78558349609375 Time taken: 0.29751062393188477\n",
            "Batch Number: 1899 Loss: 1.7589572668075562 Time taken: 0.3421778678894043\n",
            "Batch Number: 1900 Loss: 1.7562298774719238 Time taken: 0.3066747188568115\n",
            "Batch Number: 1901 Loss: 1.7793197631835938 Time taken: 0.29502058029174805\n",
            "Batch Number: 1902 Loss: 1.7934010028839111 Time taken: 0.3434615135192871\n",
            "Batch Number: 1903 Loss: 1.7811353206634521 Time taken: 0.308243989944458\n",
            "Batch Number: 1904 Loss: 1.770911455154419 Time taken: 0.30362367630004883\n",
            "Batch Number: 1905 Loss: 1.7801347970962524 Time taken: 0.3319664001464844\n",
            "Batch Number: 1906 Loss: 1.796852946281433 Time taken: 0.3701756000518799\n",
            "Batch Number: 1907 Loss: 1.78664231300354 Time taken: 0.36465001106262207\n",
            "Batch Number: 1908 Loss: 1.7818048000335693 Time taken: 0.3508765697479248\n",
            "Batch Number: 1909 Loss: 1.7878689765930176 Time taken: 0.3717496395111084\n",
            "Batch Number: 1910 Loss: 1.7963874340057373 Time taken: 0.3288228511810303\n",
            "Batch Number: 1911 Loss: 1.800531029701233 Time taken: 0.31256937980651855\n",
            "Batch Number: 1912 Loss: 1.7914096117019653 Time taken: 0.3187084197998047\n",
            "Batch Number: 1913 Loss: 1.7780650854110718 Time taken: 0.32612085342407227\n",
            "Batch Number: 1914 Loss: 1.7625993490219116 Time taken: 0.34972286224365234\n",
            "Batch Number: 1915 Loss: 1.7708559036254883 Time taken: 0.39047694206237793\n",
            "Batch Number: 1916 Loss: 1.7809797525405884 Time taken: 0.34992289543151855\n",
            "Batch Number: 1917 Loss: 1.7710106372833252 Time taken: 0.384746789932251\n",
            "Batch Number: 1918 Loss: 1.764329433441162 Time taken: 0.36903882026672363\n",
            "Batch Number: 1919 Loss: 1.768640160560608 Time taken: 0.32027101516723633\n",
            "Batch Number: 1920 Loss: 1.7888760566711426 Time taken: 0.3247373104095459\n",
            "Batch Number: 1921 Loss: 1.889021396636963 Time taken: 0.304537296295166\n",
            "Batch Number: 1922 Loss: 1.8296856880187988 Time taken: 0.340731143951416\n",
            "Batch Number: 1923 Loss: 1.80769944190979 Time taken: 0.4061145782470703\n",
            "Batch Number: 1924 Loss: 1.7935346364974976 Time taken: 0.37268781661987305\n",
            "Batch Number: 1925 Loss: 1.8065615892410278 Time taken: 0.31520533561706543\n",
            "Batch Number: 1926 Loss: 1.8002537488937378 Time taken: 0.3161909580230713\n",
            "Batch Number: 1927 Loss: 1.7765134572982788 Time taken: 0.29998230934143066\n",
            "Batch Number: 1928 Loss: 1.812103271484375 Time taken: 0.3018009662628174\n",
            "Batch Number: 1929 Loss: 1.8018792867660522 Time taken: 0.39099884033203125\n",
            "Batch Number: 1930 Loss: 1.7986090183258057 Time taken: 0.37281107902526855\n",
            "Batch Number: 1931 Loss: 1.775586485862732 Time taken: 0.3264195919036865\n",
            "Batch Number: 1932 Loss: 1.7708446979522705 Time taken: 0.39411473274230957\n",
            "Batch Number: 1933 Loss: 1.78514564037323 Time taken: 0.3732268810272217\n",
            "Batch Number: 1934 Loss: 1.7839772701263428 Time taken: 0.30467963218688965\n",
            "Batch Number: 1935 Loss: 1.760960578918457 Time taken: 0.3157827854156494\n",
            "Batch Number: 1936 Loss: 1.7218583822250366 Time taken: 0.299299955368042\n",
            "Batch Number: 1937 Loss: 1.7515268325805664 Time taken: 0.30139636993408203\n",
            "Batch Number: 1938 Loss: 1.7302461862564087 Time taken: 0.31855177879333496\n",
            "Batch Number: 1939 Loss: 1.7611727714538574 Time taken: 0.29827880859375\n",
            "Batch Number: 1940 Loss: 1.7522335052490234 Time taken: 0.29007577896118164\n",
            "Batch Number: 1941 Loss: 1.76607084274292 Time taken: 0.3107490539550781\n",
            "Batch Number: 1942 Loss: 1.7677932977676392 Time taken: 0.2929689884185791\n",
            "Batch Number: 1943 Loss: 1.7921210527420044 Time taken: 0.3010694980621338\n",
            "Batch Number: 1944 Loss: 1.7805391550064087 Time taken: 0.31310606002807617\n",
            "Batch Number: 1945 Loss: 1.76887047290802 Time taken: 0.31421613693237305\n",
            "Batch Number: 1946 Loss: 1.7736746072769165 Time taken: 0.3258655071258545\n",
            "Batch Number: 1947 Loss: 1.752445101737976 Time taken: 0.2939262390136719\n",
            "Batch Number: 1948 Loss: 1.7402957677841187 Time taken: 0.3195791244506836\n",
            "Batch Number: 1949 Loss: 1.756294846534729 Time taken: 0.30267858505249023\n",
            "Batch Number: 1950 Loss: 1.7465307712554932 Time taken: 0.2966451644897461\n",
            "Batch Number: 1951 Loss: 1.7498384714126587 Time taken: 0.3178751468658447\n",
            "Batch Number: 1952 Loss: 1.762652039527893 Time taken: 0.30510807037353516\n",
            "Batch Number: 1953 Loss: 1.7594736814498901 Time taken: 0.3163948059082031\n",
            "Batch Number: 1954 Loss: 1.7521127462387085 Time taken: 0.3160526752471924\n",
            "Batch Number: 1955 Loss: 1.767138123512268 Time taken: 0.3045518398284912\n",
            "Batch Number: 1956 Loss: 1.737902045249939 Time taken: 0.307420015335083\n",
            "Batch Number: 1957 Loss: 1.7579079866409302 Time taken: 0.2954676151275635\n",
            "Batch Number: 1958 Loss: 1.7613097429275513 Time taken: 0.3037607669830322\n",
            "Batch Number: 1959 Loss: 1.7815825939178467 Time taken: 0.3002333641052246\n",
            "Batch Number: 1960 Loss: 1.7527576684951782 Time taken: 0.28269362449645996\n",
            "Batch Number: 1961 Loss: 1.7485668659210205 Time taken: 0.30324506759643555\n",
            "Batch Number: 1962 Loss: 1.7754470109939575 Time taken: 0.2886924743652344\n",
            "Batch Number: 1963 Loss: 1.7427752017974854 Time taken: 0.29651665687561035\n",
            "Batch Number: 1964 Loss: 1.7501118183135986 Time taken: 0.3001132011413574\n",
            "Batch Number: 1965 Loss: 1.748112678527832 Time taken: 0.31345200538635254\n",
            "Batch Number: 1966 Loss: 1.763951301574707 Time taken: 0.33782315254211426\n",
            "Batch Number: 1967 Loss: 1.735670804977417 Time taken: 0.38989925384521484\n",
            "Batch Number: 1968 Loss: 1.732633352279663 Time taken: 0.32885098457336426\n",
            "Batch Number: 1969 Loss: 1.7408586740493774 Time taken: 0.29926323890686035\n",
            "Batch Number: 1970 Loss: 1.7634880542755127 Time taken: 0.30824971199035645\n",
            "Batch Number: 1971 Loss: 1.748095154762268 Time taken: 0.28505778312683105\n",
            "Batch Number: 1972 Loss: 1.7255134582519531 Time taken: 0.2943124771118164\n",
            "Batch Number: 1973 Loss: 1.7367913722991943 Time taken: 0.29654884338378906\n",
            "Batch Number: 1974 Loss: 1.7115862369537354 Time taken: 0.33377838134765625\n",
            "Batch Number: 1975 Loss: 1.7358295917510986 Time taken: 0.2946441173553467\n",
            "Batch Number: 1976 Loss: 1.727546215057373 Time taken: 0.3337981700897217\n",
            "Batch Number: 1977 Loss: 1.752951741218567 Time taken: 0.40544629096984863\n",
            "Batch Number: 1978 Loss: 1.7329293489456177 Time taken: 0.3820791244506836\n",
            "Batch Number: 1979 Loss: 1.7344316244125366 Time taken: 0.36359286308288574\n",
            "Batch Number: 1980 Loss: 1.7193026542663574 Time taken: 0.3687121868133545\n",
            "Batch Number: 1981 Loss: 1.7401400804519653 Time taken: 0.2951529026031494\n",
            "Batch Number: 1982 Loss: 1.699385166168213 Time taken: 0.3870840072631836\n",
            "Batch Number: 1983 Loss: 1.7074851989746094 Time taken: 0.34038877487182617\n",
            "Batch Number: 1984 Loss: 1.7046250104904175 Time taken: 0.3054955005645752\n",
            "Batch Number: 1985 Loss: 1.7005646228790283 Time taken: 0.2987051010131836\n",
            "Batch Number: 1986 Loss: 1.7357711791992188 Time taken: 0.3031461238861084\n",
            "Batch Number: 1987 Loss: 1.7311151027679443 Time taken: 0.3002190589904785\n",
            "Batch Number: 1988 Loss: 1.700217843055725 Time taken: 0.3032522201538086\n",
            "Batch Number: 1989 Loss: 1.7218166589736938 Time taken: 0.3321542739868164\n",
            "Batch Number: 1990 Loss: 1.6924757957458496 Time taken: 0.3074369430541992\n",
            "Batch Number: 1991 Loss: 1.7543809413909912 Time taken: 0.2999439239501953\n",
            "Batch Number: 1992 Loss: 1.756345510482788 Time taken: 0.3318605422973633\n",
            "Batch Number: 1993 Loss: 1.7435977458953857 Time taken: 0.3138296604156494\n",
            "Batch Number: 1994 Loss: 1.7340024709701538 Time taken: 0.30071282386779785\n",
            "Batch Number: 1995 Loss: 1.730474591255188 Time taken: 0.3169524669647217\n",
            "Batch Number: 1996 Loss: 1.7525866031646729 Time taken: 0.31623053550720215\n",
            "Batch Number: 1997 Loss: 1.7137211561203003 Time taken: 0.3147008419036865\n",
            "Batch Number: 1998 Loss: 1.7405117750167847 Time taken: 0.31958532333374023\n",
            "Batch Number: 1999 Loss: 1.7208178043365479 Time taken: 0.29435086250305176\n",
            "Batch Number: 2000 Loss: 1.7097879648208618 Time taken: 0.2964351177215576\n",
            "Batch Number: 2001 Loss: 1.722082495689392 Time taken: 0.28688740730285645\n",
            "Batch Number: 2002 Loss: 1.7325612306594849 Time taken: 0.33298206329345703\n",
            "Batch Number: 2003 Loss: 1.729089379310608 Time taken: 0.2854292392730713\n",
            "Batch Number: 2004 Loss: 1.7044645547866821 Time taken: 0.29148125648498535\n",
            "Batch Number: 2005 Loss: 1.7722032070159912 Time taken: 0.3199794292449951\n",
            "Batch Number: 2006 Loss: 1.7060595750808716 Time taken: 0.2885470390319824\n",
            "Batch Number: 2007 Loss: 1.7165248394012451 Time taken: 0.29886293411254883\n",
            "Batch Number: 2008 Loss: 1.7211170196533203 Time taken: 0.3001983165740967\n",
            "Batch Number: 2009 Loss: 1.7170445919036865 Time taken: 0.30011868476867676\n",
            "Batch Number: 2010 Loss: 1.7003344297409058 Time taken: 0.3179922103881836\n",
            "Batch Number: 2011 Loss: 1.7234036922454834 Time taken: 0.3025848865509033\n",
            "Batch Number: 2012 Loss: 1.7255194187164307 Time taken: 0.3160393238067627\n",
            "Batch Number: 2013 Loss: 1.7210172414779663 Time taken: 0.30149149894714355\n",
            "Batch Number: 2014 Loss: 1.7286450862884521 Time taken: 0.3296847343444824\n",
            "Batch Number: 2015 Loss: 1.7239041328430176 Time taken: 0.34149765968322754\n",
            "Batch Number: 2016 Loss: 1.7698678970336914 Time taken: 0.2915356159210205\n",
            "Batch Number: 2017 Loss: 1.7427898645401 Time taken: 0.29176902770996094\n",
            "Batch Number: 2018 Loss: 1.7650810480117798 Time taken: 0.3131723403930664\n",
            "Batch Number: 2019 Loss: 1.758523941040039 Time taken: 0.2907900810241699\n",
            "Batch Number: 2020 Loss: 1.7435826063156128 Time taken: 0.28909754753112793\n",
            "Batch Number: 2021 Loss: 1.7210328578948975 Time taken: 0.3130502700805664\n",
            "Batch Number: 2022 Loss: 1.7382892370224 Time taken: 0.31870579719543457\n",
            "Batch Number: 2023 Loss: 1.7417336702346802 Time taken: 0.3000330924987793\n",
            "Batch Number: 2024 Loss: 1.7200636863708496 Time taken: 0.34101390838623047\n",
            "Batch Number: 2025 Loss: 1.7320995330810547 Time taken: 0.37161922454833984\n",
            "Batch Number: 2026 Loss: 1.6721030473709106 Time taken: 0.36827516555786133\n",
            "Batch Number: 2027 Loss: 1.744765281677246 Time taken: 0.37462902069091797\n",
            "Batch Number: 2028 Loss: 1.7331640720367432 Time taken: 0.37236881256103516\n",
            "Batch Number: 2029 Loss: 1.7132819890975952 Time taken: 0.36478090286254883\n",
            "Batch Number: 2030 Loss: 1.7269713878631592 Time taken: 0.3109288215637207\n",
            "Batch Number: 2031 Loss: 1.7667961120605469 Time taken: 0.29197239875793457\n",
            "Batch Number: 2032 Loss: 1.7222329378128052 Time taken: 0.296738862991333\n",
            "Batch Number: 2033 Loss: 1.7265366315841675 Time taken: 0.34798502922058105\n",
            "Batch Number: 2034 Loss: 1.7264291048049927 Time taken: 0.2979729175567627\n",
            "Batch Number: 2035 Loss: 1.7132636308670044 Time taken: 0.29351162910461426\n",
            "Batch Number: 2036 Loss: 1.7355036735534668 Time taken: 0.31556272506713867\n",
            "Batch Number: 2037 Loss: 1.7261029481887817 Time taken: 0.3296189308166504\n",
            "Batch Number: 2038 Loss: 1.7103173732757568 Time taken: 0.30278825759887695\n",
            "Batch Number: 2039 Loss: 1.7254414558410645 Time taken: 0.3192298412322998\n",
            "Batch Number: 2040 Loss: 1.7045576572418213 Time taken: 0.32872986793518066\n",
            "Batch Number: 2041 Loss: 1.6881611347198486 Time taken: 0.3000755310058594\n",
            "Batch Number: 2042 Loss: 1.6973834037780762 Time taken: 0.30503177642822266\n",
            "Batch Number: 2043 Loss: 1.7060723304748535 Time taken: 0.37338829040527344\n",
            "Batch Number: 2044 Loss: 1.7209738492965698 Time taken: 0.2948455810546875\n",
            "Batch Number: 2045 Loss: 1.7079092264175415 Time taken: 0.31691479682922363\n",
            "Batch Number: 2046 Loss: 1.7219383716583252 Time taken: 0.3071451187133789\n",
            "Batch Number: 2047 Loss: 1.740646481513977 Time taken: 0.28456807136535645\n",
            "Batch Number: 2048 Loss: 1.7370685338974 Time taken: 0.29460978507995605\n",
            "Batch Number: 2049 Loss: 1.7039467096328735 Time taken: 0.32405948638916016\n",
            "Batch Number: 2050 Loss: 1.7139543294906616 Time taken: 0.30367040634155273\n",
            "Batch Number: 2051 Loss: 1.7506932020187378 Time taken: 0.29949212074279785\n",
            "Batch Number: 2052 Loss: 1.7501088380813599 Time taken: 0.33612966537475586\n",
            "Batch Number: 2053 Loss: 1.7876248359680176 Time taken: 0.30073070526123047\n",
            "Batch Number: 2054 Loss: 1.737484097480774 Time taken: 0.2979702949523926\n",
            "Batch Number: 2055 Loss: 1.7796581983566284 Time taken: 0.3143494129180908\n",
            "Batch Number: 2056 Loss: 1.7272529602050781 Time taken: 0.308971643447876\n",
            "Batch Number: 2057 Loss: 1.7229582071304321 Time taken: 0.30206775665283203\n",
            "Batch Number: 2058 Loss: 1.7392635345458984 Time taken: 0.304760217666626\n",
            "Batch Number: 2059 Loss: 1.7634668350219727 Time taken: 0.3147144317626953\n",
            "Batch Number: 2060 Loss: 1.7386462688446045 Time taken: 0.29663729667663574\n",
            "Batch Number: 2061 Loss: 1.7604963779449463 Time taken: 0.29776644706726074\n",
            "Batch Number: 2062 Loss: 1.722726821899414 Time taken: 0.32128024101257324\n",
            "Batch Number: 2063 Loss: 1.7008010149002075 Time taken: 0.30332350730895996\n",
            "Batch Number: 2064 Loss: 1.720778465270996 Time taken: 0.30922842025756836\n",
            "Batch Number: 2065 Loss: 1.7102746963500977 Time taken: 0.32843565940856934\n",
            "Batch Number: 2066 Loss: 1.6985242366790771 Time taken: 0.311232328414917\n",
            "Batch Number: 2067 Loss: 1.66379976272583 Time taken: 0.3039424419403076\n",
            "Batch Number: 2068 Loss: 1.699546456336975 Time taken: 0.30776333808898926\n",
            "Batch Number: 2069 Loss: 1.7038521766662598 Time taken: 0.3188743591308594\n",
            "Batch Number: 2070 Loss: 1.71347975730896 Time taken: 0.30024266242980957\n",
            "Batch Number: 2071 Loss: 1.701802372932434 Time taken: 0.32270359992980957\n",
            "Batch Number: 2072 Loss: 1.7751904726028442 Time taken: 0.3019068241119385\n",
            "Batch Number: 2073 Loss: 1.6945757865905762 Time taken: 0.2934892177581787\n",
            "Batch Number: 2074 Loss: 1.7443912029266357 Time taken: 0.3120765686035156\n",
            "Batch Number: 2075 Loss: 1.7534676790237427 Time taken: 0.320481538772583\n",
            "Batch Number: 2076 Loss: 1.7559821605682373 Time taken: 0.31179022789001465\n",
            "Batch Number: 2077 Loss: 1.727216362953186 Time taken: 0.31148791313171387\n",
            "Batch Number: 2078 Loss: 1.7566817998886108 Time taken: 0.31523776054382324\n",
            "Batch Number: 2079 Loss: 1.7283662557601929 Time taken: 0.3145935535430908\n",
            "Batch Number: 2080 Loss: 1.7535052299499512 Time taken: 0.30663466453552246\n",
            "Batch Number: 2081 Loss: 1.737122654914856 Time taken: 0.31671881675720215\n",
            "Batch Number: 2082 Loss: 1.7296173572540283 Time taken: 0.3054804801940918\n",
            "Batch Number: 2083 Loss: 1.7519028186798096 Time taken: 0.32372426986694336\n",
            "Batch Number: 2084 Loss: 1.7324581146240234 Time taken: 0.3157918453216553\n",
            "Batch Number: 2085 Loss: 1.755643606185913 Time taken: 0.30379271507263184\n",
            "Batch Number: 2086 Loss: 1.7671372890472412 Time taken: 0.30245447158813477\n",
            "Batch Number: 2087 Loss: 1.7385237216949463 Time taken: 0.3041956424713135\n",
            "Batch Number: 2088 Loss: 1.7537974119186401 Time taken: 0.30410099029541016\n",
            "Batch Number: 2089 Loss: 1.7722809314727783 Time taken: 0.3081984519958496\n",
            "Batch Number: 2090 Loss: 1.7669512033462524 Time taken: 0.35921406745910645\n",
            "Batch Number: 2091 Loss: 1.749653697013855 Time taken: 0.3949899673461914\n",
            "Batch Number: 2092 Loss: 1.7785500288009644 Time taken: 0.3429605960845947\n",
            "Batch Number: 2093 Loss: 1.7620669603347778 Time taken: 0.2888507843017578\n",
            "Batch Number: 2094 Loss: 1.762932300567627 Time taken: 0.30488038063049316\n",
            "Batch Number: 2095 Loss: 1.7370010614395142 Time taken: 0.2944178581237793\n",
            "Batch Number: 2096 Loss: 1.7592332363128662 Time taken: 0.379453182220459\n",
            "Batch Number: 2097 Loss: 1.7182049751281738 Time taken: 0.3687276840209961\n",
            "Batch Number: 2098 Loss: 1.7368992567062378 Time taken: 0.34388184547424316\n",
            "Batch Number: 2099 Loss: 1.7237805128097534 Time taken: 0.30196595191955566\n",
            "Batch Number: 2100 Loss: 1.7415471076965332 Time taken: 0.3029632568359375\n",
            "Batch Number: 2101 Loss: 1.7312569618225098 Time taken: 0.30185699462890625\n",
            "Batch Number: 2102 Loss: 1.7196228504180908 Time taken: 0.35705137252807617\n",
            "Batch Number: 2103 Loss: 1.7495486736297607 Time taken: 0.3893730640411377\n",
            "Batch Number: 2104 Loss: 1.7126753330230713 Time taken: 0.35303401947021484\n",
            "Batch Number: 2105 Loss: 1.7242416143417358 Time taken: 0.29238295555114746\n",
            "Batch Number: 2106 Loss: 1.7350802421569824 Time taken: 0.28911924362182617\n",
            "Batch Number: 2107 Loss: 1.742661714553833 Time taken: 0.30424928665161133\n",
            "Batch Number: 2108 Loss: 1.7236676216125488 Time taken: 0.31882333755493164\n",
            "Batch Number: 2109 Loss: 1.711943507194519 Time taken: 0.2903401851654053\n",
            "Batch Number: 2110 Loss: 1.7115980386734009 Time taken: 0.3056223392486572\n",
            "Batch Number: 2111 Loss: 1.7550363540649414 Time taken: 0.2909812927246094\n",
            "Batch Number: 2112 Loss: 1.7319053411483765 Time taken: 0.29735326766967773\n",
            "Batch Number: 2113 Loss: 1.7292066812515259 Time taken: 0.30039238929748535\n",
            "Batch Number: 2114 Loss: 1.7216503620147705 Time taken: 0.311962366104126\n",
            "Batch Number: 2115 Loss: 1.7364556789398193 Time taken: 0.28758955001831055\n",
            "Batch Number: 2116 Loss: 1.6982109546661377 Time taken: 0.3623635768890381\n",
            "Batch Number: 2117 Loss: 1.675840139389038 Time taken: 0.3153567314147949\n",
            "Batch Number: 2118 Loss: 1.7015186548233032 Time taken: 0.30121469497680664\n",
            "Batch Number: 2119 Loss: 1.7311310768127441 Time taken: 0.28913044929504395\n",
            "Batch Number: 2120 Loss: 1.7160815000534058 Time taken: 0.32395148277282715\n",
            "Batch Number: 2121 Loss: 1.7317149639129639 Time taken: 0.2826991081237793\n",
            "Batch Number: 2122 Loss: 1.724619746208191 Time taken: 0.2919154167175293\n",
            "Batch Number: 2123 Loss: 1.7746411561965942 Time taken: 0.294619083404541\n",
            "Batch Number: 2124 Loss: 1.7193667888641357 Time taken: 0.2878994941711426\n",
            "Batch Number: 2125 Loss: 1.746748924255371 Time taken: 0.28947877883911133\n",
            "Batch Number: 2126 Loss: 1.7255148887634277 Time taken: 0.29534411430358887\n",
            "Batch Number: 2127 Loss: 1.7140145301818848 Time taken: 0.2922499179840088\n",
            "Batch Number: 2128 Loss: 1.725440502166748 Time taken: 0.3033618927001953\n",
            "Batch Number: 2129 Loss: 1.7385210990905762 Time taken: 0.31790995597839355\n",
            "Batch Number: 2130 Loss: 1.721378207206726 Time taken: 0.32064032554626465\n",
            "Batch Number: 2131 Loss: 1.7555912733078003 Time taken: 0.30138373374938965\n",
            "Batch Number: 2132 Loss: 1.7426722049713135 Time taken: 0.3048114776611328\n",
            "Batch Number: 2133 Loss: 1.734933853149414 Time taken: 0.2995898723602295\n",
            "Batch Number: 2134 Loss: 1.7461929321289062 Time taken: 0.3041963577270508\n",
            "Batch Number: 2135 Loss: 1.7510251998901367 Time taken: 0.3134143352508545\n",
            "Batch Number: 2136 Loss: 1.7448092699050903 Time taken: 0.33010339736938477\n",
            "Batch Number: 2137 Loss: 1.7423292398452759 Time taken: 0.2966346740722656\n",
            "Batch Number: 2138 Loss: 1.7695616483688354 Time taken: 0.3139016628265381\n",
            "Batch Number: 2139 Loss: 1.7231693267822266 Time taken: 0.30544185638427734\n",
            "Batch Number: 2140 Loss: 1.71927809715271 Time taken: 0.29375767707824707\n",
            "Batch Number: 2141 Loss: 1.7684520483016968 Time taken: 0.2829885482788086\n",
            "Batch Number: 2142 Loss: 1.706708312034607 Time taken: 0.31583189964294434\n",
            "Batch Number: 2143 Loss: 1.7416398525238037 Time taken: 0.3252081871032715\n",
            "Batch Number: 2144 Loss: 1.7294505834579468 Time taken: 0.2910175323486328\n",
            "Batch Number: 2145 Loss: 1.7182955741882324 Time taken: 0.29848241806030273\n",
            "Batch Number: 2146 Loss: 1.742952585220337 Time taken: 0.32622241973876953\n",
            "Batch Number: 2147 Loss: 1.71058189868927 Time taken: 0.2954411506652832\n",
            "Batch Number: 2148 Loss: 1.7076282501220703 Time taken: 0.30636048316955566\n",
            "Batch Number: 2149 Loss: 1.7296315431594849 Time taken: 0.30687379837036133\n",
            "Batch Number: 2150 Loss: 1.7447097301483154 Time taken: 0.27741527557373047\n",
            "Batch Number: 2151 Loss: 1.7023476362228394 Time taken: 0.29871153831481934\n",
            "Batch Number: 2152 Loss: 1.7001967430114746 Time taken: 0.29163646697998047\n",
            "Batch Number: 2153 Loss: 1.7095528841018677 Time taken: 0.34761738777160645\n",
            "Batch Number: 2154 Loss: 1.7058830261230469 Time taken: 0.3680915832519531\n",
            "Batch Number: 2155 Loss: 1.7151167392730713 Time taken: 0.37102198600769043\n",
            "Batch Number: 2156 Loss: 1.7107495069503784 Time taken: 0.29222846031188965\n",
            "Batch Number: 2157 Loss: 1.72137451171875 Time taken: 0.27904701232910156\n",
            "Batch Number: 2158 Loss: 1.685983657836914 Time taken: 0.2975029945373535\n",
            "Batch Number: 2159 Loss: 1.720124363899231 Time taken: 0.31630468368530273\n",
            "Batch Number: 2160 Loss: 1.6912943124771118 Time taken: 0.2933533191680908\n",
            "Batch Number: 2161 Loss: 1.6960872411727905 Time taken: 0.31523966789245605\n",
            "Batch Number: 2162 Loss: 1.6909719705581665 Time taken: 0.3220558166503906\n",
            "Batch Number: 2163 Loss: 1.6665908098220825 Time taken: 0.29675912857055664\n",
            "Batch Number: 2164 Loss: 1.6717969179153442 Time taken: 0.3172895908355713\n",
            "Batch Number: 2165 Loss: 1.6763509511947632 Time taken: 0.3268134593963623\n",
            "Batch Number: 2166 Loss: 1.6885156631469727 Time taken: 0.2989213466644287\n",
            "Batch Number: 2167 Loss: 1.6635371446609497 Time taken: 0.3073747158050537\n",
            "Batch Number: 2168 Loss: 1.6626428365707397 Time taken: 0.29936981201171875\n",
            "Batch Number: 2169 Loss: 1.692626953125 Time taken: 0.3013429641723633\n",
            "Batch Number: 2170 Loss: 1.6798646450042725 Time taken: 0.30436062812805176\n",
            "Batch Number: 2171 Loss: 1.6906664371490479 Time taken: 0.305448055267334\n",
            "Batch Number: 2172 Loss: 1.6901051998138428 Time taken: 0.32290196418762207\n",
            "Batch Number: 2173 Loss: 1.6798367500305176 Time taken: 0.3024940490722656\n",
            "Batch Number: 2174 Loss: 1.7026665210723877 Time taken: 0.3127272129058838\n",
            "Batch Number: 2175 Loss: 1.727118968963623 Time taken: 0.30634498596191406\n",
            "Batch Number: 2176 Loss: 1.7017794847488403 Time taken: 0.30851197242736816\n",
            "Batch Number: 2177 Loss: 1.7038203477859497 Time taken: 0.2927100658416748\n",
            "Batch Number: 2178 Loss: 1.7104429006576538 Time taken: 0.3052339553833008\n",
            "Batch Number: 2179 Loss: 1.6844403743743896 Time taken: 0.3710806369781494\n",
            "Batch Number: 2180 Loss: 1.6831711530685425 Time taken: 0.37134671211242676\n",
            "Batch Number: 2181 Loss: 1.6924266815185547 Time taken: 0.31232547760009766\n",
            "Batch Number: 2182 Loss: 1.716995120048523 Time taken: 0.3499159812927246\n",
            "Batch Number: 2183 Loss: 1.6955658197402954 Time taken: 0.3743000030517578\n",
            "Batch Number: 2184 Loss: 1.6951984167099 Time taken: 0.3785982131958008\n",
            "Batch Number: 2185 Loss: 1.7022850513458252 Time taken: 0.32211828231811523\n",
            "Batch Number: 2186 Loss: 1.6859163045883179 Time taken: 0.3108634948730469\n",
            "Batch Number: 2187 Loss: 1.7048285007476807 Time taken: 0.3045825958251953\n",
            "Batch Number: 2188 Loss: 1.6802748441696167 Time taken: 0.3457510471343994\n",
            "Batch Number: 2189 Loss: 1.6809204816818237 Time taken: 0.368910551071167\n",
            "Batch Number: 2190 Loss: 1.6699782609939575 Time taken: 0.29627299308776855\n",
            "Batch Number: 2191 Loss: 1.6696847677230835 Time taken: 0.28735780715942383\n",
            "Batch Number: 2192 Loss: 1.691535234451294 Time taken: 0.30561256408691406\n",
            "Batch Number: 2193 Loss: 1.7200343608856201 Time taken: 0.3017418384552002\n",
            "Batch Number: 2194 Loss: 1.6864001750946045 Time taken: 0.2942938804626465\n",
            "Batch Number: 2195 Loss: 1.695913553237915 Time taken: 0.29897212982177734\n",
            "Batch Number: 2196 Loss: 1.6832102537155151 Time taken: 0.32794189453125\n",
            "Batch Number: 2197 Loss: 1.6763330698013306 Time taken: 0.3026154041290283\n",
            "Batch Number: 2198 Loss: 1.7269245386123657 Time taken: 0.30565476417541504\n",
            "Batch Number: 2199 Loss: 1.6995842456817627 Time taken: 0.310819149017334\n",
            "Batch Number: 2200 Loss: 1.7011322975158691 Time taken: 0.30248141288757324\n",
            "Batch Number: 2201 Loss: 1.7318861484527588 Time taken: 0.29921746253967285\n",
            "Batch Number: 2202 Loss: 1.722804069519043 Time taken: 0.3087446689605713\n",
            "Batch Number: 2203 Loss: 1.711355447769165 Time taken: 0.3148365020751953\n",
            "Batch Number: 2204 Loss: 1.7015620470046997 Time taken: 0.3006572723388672\n",
            "Batch Number: 2205 Loss: 1.7087968587875366 Time taken: 0.3203754425048828\n",
            "Batch Number: 2206 Loss: 1.699657678604126 Time taken: 0.2885768413543701\n",
            "Batch Number: 2207 Loss: 1.7188501358032227 Time taken: 0.3599400520324707\n",
            "Batch Number: 2208 Loss: 1.6892048120498657 Time taken: 0.3489205837249756\n",
            "Batch Number: 2209 Loss: 1.683867335319519 Time taken: 0.3080747127532959\n",
            "Batch Number: 2210 Loss: 1.690891146659851 Time taken: 0.30052900314331055\n",
            "Batch Number: 2211 Loss: 1.7041935920715332 Time taken: 0.30838608741760254\n",
            "Batch Number: 2212 Loss: 1.6900863647460938 Time taken: 0.30666160583496094\n",
            "Batch Number: 2213 Loss: 1.6865503787994385 Time taken: 0.3151884078979492\n",
            "Batch Number: 2214 Loss: 1.7231130599975586 Time taken: 0.32523226737976074\n",
            "Batch Number: 2215 Loss: 1.6836591958999634 Time taken: 0.3261144161224365\n",
            "Batch Number: 2216 Loss: 1.6833299398422241 Time taken: 0.31180429458618164\n",
            "Batch Number: 2217 Loss: 1.6948133707046509 Time taken: 0.3026702404022217\n",
            "Batch Number: 2218 Loss: 1.6699262857437134 Time taken: 0.3365814685821533\n",
            "Batch Number: 2219 Loss: 1.7151858806610107 Time taken: 0.37847399711608887\n",
            "Batch Number: 2220 Loss: 1.6796671152114868 Time taken: 0.3628041744232178\n",
            "Batch Number: 2221 Loss: 1.6804070472717285 Time taken: 0.3101468086242676\n",
            "Batch Number: 2222 Loss: 1.6746482849121094 Time taken: 0.30387401580810547\n",
            "Batch Number: 2223 Loss: 1.676218032836914 Time taken: 0.3504819869995117\n",
            "Batch Number: 2224 Loss: 1.6793047189712524 Time taken: 0.3395533561706543\n",
            "Batch Number: 2225 Loss: 1.6868600845336914 Time taken: 0.370603084564209\n",
            "Batch Number: 2226 Loss: 1.686518669128418 Time taken: 0.37799668312072754\n",
            "Batch Number: 2227 Loss: 1.6863586902618408 Time taken: 0.30841660499572754\n",
            "Batch Number: 2228 Loss: 1.6923669576644897 Time taken: 0.37165164947509766\n",
            "Batch Number: 2229 Loss: 1.6838459968566895 Time taken: 0.3341982364654541\n",
            "Batch Number: 2230 Loss: 1.7073551416397095 Time taken: 0.29496026039123535\n",
            "Batch Number: 2231 Loss: 1.6853662729263306 Time taken: 0.3076803684234619\n",
            "Batch Number: 2232 Loss: 1.7061678171157837 Time taken: 0.3480041027069092\n",
            "Batch Number: 2233 Loss: 1.7047977447509766 Time taken: 0.3010435104370117\n",
            "Batch Number: 2234 Loss: 1.692221760749817 Time taken: 0.33675432205200195\n",
            "Batch Number: 2235 Loss: 1.7122292518615723 Time taken: 0.3232402801513672\n",
            "Batch Number: 2236 Loss: 1.733594536781311 Time taken: 0.3023109436035156\n",
            "Batch Number: 2237 Loss: 1.7599732875823975 Time taken: 0.30182385444641113\n",
            "Batch Number: 2238 Loss: 1.7105271816253662 Time taken: 0.32115769386291504\n",
            "Batch Number: 2239 Loss: 1.720826268196106 Time taken: 0.3670840263366699\n",
            "Batch Number: 2240 Loss: 1.735300898551941 Time taken: 0.40253567695617676\n",
            "Batch Number: 2241 Loss: 1.7199780941009521 Time taken: 0.3124701976776123\n",
            "Batch Number: 2242 Loss: 1.7050411701202393 Time taken: 0.29990100860595703\n",
            "Batch Number: 2243 Loss: 1.7208847999572754 Time taken: 0.31981611251831055\n",
            "Batch Number: 2244 Loss: 1.7156614065170288 Time taken: 0.34156370162963867\n",
            "Batch Number: 2245 Loss: 1.706796646118164 Time taken: 0.30806899070739746\n",
            "Batch Number: 2246 Loss: 1.7060105800628662 Time taken: 0.29336977005004883\n",
            "Batch Number: 2247 Loss: 1.6740515232086182 Time taken: 0.29436206817626953\n",
            "Batch Number: 2248 Loss: 1.709041714668274 Time taken: 0.2848527431488037\n",
            "Batch Number: 2249 Loss: 1.7085875272750854 Time taken: 0.2892923355102539\n",
            "Batch Number: 2250 Loss: 1.6742838621139526 Time taken: 0.34223246574401855\n",
            "Batch Number: 2251 Loss: 1.6936265230178833 Time taken: 0.3252079486846924\n",
            "Batch Number: 2252 Loss: 1.701703429222107 Time taken: 0.28394389152526855\n",
            "Batch Number: 2253 Loss: 1.6988403797149658 Time taken: 0.31412506103515625\n",
            "Batch Number: 2254 Loss: 1.7101856470108032 Time taken: 0.2939112186431885\n",
            "Batch Number: 2255 Loss: 1.691941261291504 Time taken: 0.30038905143737793\n",
            "Batch Number: 2256 Loss: 1.7377774715423584 Time taken: 0.30228161811828613\n",
            "Batch Number: 2257 Loss: 1.7390254735946655 Time taken: 0.32856059074401855\n",
            "Batch Number: 2258 Loss: 1.7460660934448242 Time taken: 0.2931041717529297\n",
            "Batch Number: 2259 Loss: 1.730962872505188 Time taken: 0.2986330986022949\n",
            "Batch Number: 2260 Loss: 1.7309212684631348 Time taken: 0.28826045989990234\n",
            "Batch Number: 2261 Loss: 1.7276636362075806 Time taken: 0.37019824981689453\n",
            "Batch Number: 2262 Loss: 1.7192243337631226 Time taken: 0.3873591423034668\n",
            "Batch Number: 2263 Loss: 1.6917879581451416 Time taken: 0.3774886131286621\n",
            "Batch Number: 2264 Loss: 1.727286458015442 Time taken: 0.36215710639953613\n",
            "Batch Number: 2265 Loss: 1.7211856842041016 Time taken: 0.36315155029296875\n",
            "Batch Number: 2266 Loss: 1.7233668565750122 Time taken: 0.29962611198425293\n",
            "Batch Number: 2267 Loss: 1.7331777811050415 Time taken: 0.28290891647338867\n",
            "Batch Number: 2268 Loss: 1.7164814472198486 Time taken: 0.295041561126709\n",
            "Batch Number: 2269 Loss: 1.7232279777526855 Time taken: 0.2993497848510742\n",
            "Batch Number: 2270 Loss: 1.7276921272277832 Time taken: 0.3022632598876953\n",
            "Batch Number: 2271 Loss: 1.745064377784729 Time taken: 0.28778815269470215\n",
            "Batch Number: 2272 Loss: 1.734340786933899 Time taken: 0.3166344165802002\n",
            "Batch Number: 2273 Loss: 1.7474218606948853 Time taken: 0.3438577651977539\n",
            "Batch Number: 2274 Loss: 1.7220168113708496 Time taken: 0.364532470703125\n",
            "Batch Number: 2275 Loss: 1.7186739444732666 Time taken: 0.32395052909851074\n",
            "Batch Number: 2276 Loss: 1.7135677337646484 Time taken: 0.2983560562133789\n",
            "Batch Number: 2277 Loss: 1.6879501342773438 Time taken: 0.3003880977630615\n",
            "Batch Number: 2278 Loss: 1.7331119775772095 Time taken: 0.3182029724121094\n",
            "Batch Number: 2279 Loss: 1.684200406074524 Time taken: 0.2967085838317871\n",
            "Batch Number: 2280 Loss: 1.7122092247009277 Time taken: 0.2838256359100342\n",
            "Batch Number: 2281 Loss: 1.7107555866241455 Time taken: 0.29334068298339844\n",
            "Batch Number: 2282 Loss: 1.7074837684631348 Time taken: 0.3047366142272949\n",
            "Batch Number: 2283 Loss: 1.67136812210083 Time taken: 0.2838470935821533\n",
            "Batch Number: 2284 Loss: 1.6754839420318604 Time taken: 0.2934441566467285\n",
            "Batch Number: 2285 Loss: 1.7069309949874878 Time taken: 0.3262200355529785\n",
            "Batch Number: 2286 Loss: 1.6883372068405151 Time taken: 0.2965688705444336\n",
            "Batch Number: 2287 Loss: 1.6965152025222778 Time taken: 0.2845325469970703\n",
            "Batch Number: 2288 Loss: 1.698870301246643 Time taken: 0.3201460838317871\n",
            "Batch Number: 2289 Loss: 1.7139427661895752 Time taken: 0.3043363094329834\n",
            "Batch Number: 2290 Loss: 1.6987953186035156 Time taken: 0.29320859909057617\n",
            "Batch Number: 2291 Loss: 1.7035874128341675 Time taken: 0.323117733001709\n",
            "Batch Number: 2292 Loss: 1.711328148841858 Time taken: 0.3012678623199463\n",
            "Batch Number: 2293 Loss: 1.695899486541748 Time taken: 0.28929567337036133\n",
            "Batch Number: 2294 Loss: 1.699938178062439 Time taken: 0.31407594680786133\n",
            "Batch Number: 2295 Loss: 1.6909006834030151 Time taken: 0.32357072830200195\n",
            "Batch Number: 2296 Loss: 1.6666131019592285 Time taken: 0.2946889400482178\n",
            "Batch Number: 2297 Loss: 1.6622968912124634 Time taken: 0.30187463760375977\n",
            "Batch Number: 2298 Loss: 1.651816487312317 Time taken: 0.3489236831665039\n",
            "Batch Number: 2299 Loss: 1.6646628379821777 Time taken: 0.37993526458740234\n",
            "Batch Number: 2300 Loss: 1.7251160144805908 Time taken: 0.3809363842010498\n",
            "Batch Number: 2301 Loss: 1.6943082809448242 Time taken: 0.31383609771728516\n",
            "Batch Number: 2302 Loss: 1.7226542234420776 Time taken: 0.29933714866638184\n",
            "Batch Number: 2303 Loss: 1.7275404930114746 Time taken: 0.2928321361541748\n",
            "Batch Number: 2304 Loss: 1.682073950767517 Time taken: 0.30167150497436523\n",
            "Batch Number: 2305 Loss: 1.7016351222991943 Time taken: 0.313507080078125\n",
            "Batch Number: 2306 Loss: 1.7312698364257812 Time taken: 0.2935352325439453\n",
            "Batch Number: 2307 Loss: 1.7121500968933105 Time taken: 0.3366231918334961\n",
            "Batch Number: 2308 Loss: 1.653894305229187 Time taken: 0.29988837242126465\n",
            "Batch Number: 2309 Loss: 1.6970797777175903 Time taken: 0.30083584785461426\n",
            "Batch Number: 2310 Loss: 1.6789005994796753 Time taken: 0.30202555656433105\n",
            "Batch Number: 2311 Loss: 1.673421859741211 Time taken: 0.3030407428741455\n",
            "Batch Number: 2312 Loss: 1.663830280303955 Time taken: 0.2918705940246582\n",
            "Batch Number: 2313 Loss: 1.6892048120498657 Time taken: 0.34279417991638184\n",
            "Batch Number: 2314 Loss: 1.6960233449935913 Time taken: 0.29088473320007324\n",
            "Batch Number: 2315 Loss: 1.6998639106750488 Time taken: 0.31040310859680176\n",
            "Batch Number: 2316 Loss: 1.728971242904663 Time taken: 0.2917141914367676\n",
            "Batch Number: 2317 Loss: 1.6896576881408691 Time taken: 0.30246472358703613\n",
            "Batch Number: 2318 Loss: 1.6806164979934692 Time taken: 0.2980034351348877\n",
            "Batch Number: 2319 Loss: 1.6901658773422241 Time taken: 0.29479217529296875\n",
            "Batch Number: 2320 Loss: 1.6979528665542603 Time taken: 0.3029351234436035\n",
            "Batch Number: 2321 Loss: 1.7079837322235107 Time taken: 0.3043491840362549\n",
            "Batch Number: 2322 Loss: 1.6924372911453247 Time taken: 0.2929050922393799\n",
            "Batch Number: 2323 Loss: 1.7064489126205444 Time taken: 0.2998669147491455\n",
            "Batch Number: 2324 Loss: 1.6999127864837646 Time taken: 0.37359118461608887\n",
            "Batch Number: 2325 Loss: 1.6933989524841309 Time taken: 0.37131404876708984\n",
            "Batch Number: 2326 Loss: 1.6932072639465332 Time taken: 0.37846899032592773\n",
            "Batch Number: 2327 Loss: 1.6707489490509033 Time taken: 0.3700838088989258\n",
            "Batch Number: 2328 Loss: 1.6838173866271973 Time taken: 0.3365323543548584\n",
            "Batch Number: 2329 Loss: 1.6895439624786377 Time taken: 0.30106091499328613\n",
            "Batch Number: 2330 Loss: 1.683759093284607 Time taken: 0.28856754302978516\n",
            "Batch Number: 2331 Loss: 1.6704210042953491 Time taken: 0.314725399017334\n",
            "Batch Number: 2332 Loss: 1.6752831935882568 Time taken: 0.3800487518310547\n",
            "Batch Number: 2333 Loss: 1.6967488527297974 Time taken: 0.39057469367980957\n",
            "Batch Number: 2334 Loss: 1.67657470703125 Time taken: 0.33005332946777344\n",
            "Batch Number: 2335 Loss: 1.6512702703475952 Time taken: 0.3017096519470215\n",
            "Batch Number: 2336 Loss: 1.6584638357162476 Time taken: 0.301347017288208\n",
            "Batch Number: 2337 Loss: 1.6763646602630615 Time taken: 0.29003024101257324\n",
            "Batch Number: 2338 Loss: 1.7031612396240234 Time taken: 0.3918740749359131\n",
            "Batch Number: 2339 Loss: 1.6596496105194092 Time taken: 0.36316704750061035\n",
            "Batch Number: 2340 Loss: 1.6705610752105713 Time taken: 0.3363492488861084\n",
            "Batch Number: 2341 Loss: 1.6617683172225952 Time taken: 0.3159928321838379\n",
            "Batch Number: 2342 Loss: 1.663684606552124 Time taken: 0.29320740699768066\n",
            "Batch Number: 2343 Loss: 1.6435751914978027 Time taken: 0.3177456855773926\n",
            "Batch Number: 2344 Loss: 1.6465070247650146 Time taken: 0.30275797843933105\n",
            "Batch Number: 2345 Loss: 1.647452473640442 Time taken: 0.3002350330352783\n",
            "Batch Number: 2346 Loss: 1.638136386871338 Time taken: 0.2911550998687744\n",
            "Batch Number: 2347 Loss: 1.6548103094100952 Time taken: 0.29968786239624023\n",
            "Batch Number: 2348 Loss: 1.685555100440979 Time taken: 0.306840181350708\n",
            "Batch Number: 2349 Loss: 1.6497809886932373 Time taken: 0.30003905296325684\n",
            "Batch Number: 2350 Loss: 1.6308667659759521 Time taken: 0.30898499488830566\n",
            "Batch Number: 2351 Loss: 1.6742757558822632 Time taken: 0.2984950542449951\n",
            "Batch Number: 2352 Loss: 1.6813390254974365 Time taken: 0.2976090908050537\n",
            "Batch Number: 2353 Loss: 1.7013441324234009 Time taken: 0.3138558864593506\n",
            "Batch Number: 2354 Loss: 1.6600172519683838 Time taken: 0.30089831352233887\n",
            "Batch Number: 2355 Loss: 1.6707231998443604 Time taken: 0.3220388889312744\n",
            "Batch Number: 2356 Loss: 1.6707683801651 Time taken: 0.3562436103820801\n",
            "Batch Number: 2357 Loss: 1.6690748929977417 Time taken: 0.29651331901550293\n",
            "Batch Number: 2358 Loss: 1.6564725637435913 Time taken: 0.29787492752075195\n",
            "Batch Number: 2359 Loss: 1.6441209316253662 Time taken: 0.2936520576477051\n",
            "Batch Number: 2360 Loss: 1.6726100444793701 Time taken: 0.30119824409484863\n",
            "Batch Number: 2361 Loss: 1.6426316499710083 Time taken: 0.2972254753112793\n",
            "Batch Number: 2362 Loss: 1.6555736064910889 Time taken: 0.2951810359954834\n",
            "Batch Number: 2363 Loss: 1.6399788856506348 Time taken: 0.3386201858520508\n",
            "Batch Number: 2364 Loss: 1.646018147468567 Time taken: 0.3865346908569336\n",
            "Batch Number: 2365 Loss: 1.6686252355575562 Time taken: 0.3633577823638916\n",
            "Batch Number: 2366 Loss: 1.704693078994751 Time taken: 0.3430197238922119\n",
            "Batch Number: 2367 Loss: 1.7453737258911133 Time taken: 0.38620924949645996\n",
            "Batch Number: 2368 Loss: 1.7282887697219849 Time taken: 0.35022830963134766\n",
            "Batch Number: 2369 Loss: 1.7480522394180298 Time taken: 0.37668824195861816\n",
            "Batch Number: 2370 Loss: 1.7072283029556274 Time taken: 0.3840172290802002\n",
            "Batch Number: 2371 Loss: 1.6997382640838623 Time taken: 0.29329609870910645\n",
            "Batch Number: 2372 Loss: 1.679452896118164 Time taken: 0.3206048011779785\n",
            "Batch Number: 2373 Loss: 1.6910291910171509 Time taken: 0.38427281379699707\n",
            "Batch Number: 2374 Loss: 1.6966383457183838 Time taken: 0.3084235191345215\n",
            "Batch Number: 2375 Loss: 1.705651044845581 Time taken: 0.2980983257293701\n",
            "Batch Number: 2376 Loss: 1.6944562196731567 Time taken: 0.299227237701416\n",
            "Batch Number: 2377 Loss: 1.7195457220077515 Time taken: 0.2929646968841553\n",
            "Batch Number: 2378 Loss: 1.6895710229873657 Time taken: 0.2842752933502197\n",
            "Batch Number: 2379 Loss: 1.7257943153381348 Time taken: 0.297595739364624\n",
            "Batch Number: 2380 Loss: 1.6933337450027466 Time taken: 0.3470284938812256\n",
            "Batch Number: 2381 Loss: 1.6969748735427856 Time taken: 0.2961399555206299\n",
            "Batch Number: 2382 Loss: 1.7069393396377563 Time taken: 0.31060171127319336\n",
            "Batch Number: 2383 Loss: 1.6804267168045044 Time taken: 0.3237590789794922\n",
            "Batch Number: 2384 Loss: 1.6616827249526978 Time taken: 0.29561543464660645\n",
            "Batch Number: 2385 Loss: 1.6972030401229858 Time taken: 0.3104841709136963\n",
            "Batch Number: 2386 Loss: 1.6698435544967651 Time taken: 0.34305644035339355\n",
            "Batch Number: 2387 Loss: 1.6672372817993164 Time taken: 0.3731534481048584\n",
            "Batch Number: 2388 Loss: 1.667404294013977 Time taken: 0.3938446044921875\n",
            "Batch Number: 2389 Loss: 1.6676398515701294 Time taken: 0.3319664001464844\n",
            "Batch Number: 2390 Loss: 1.6684736013412476 Time taken: 0.3210763931274414\n",
            "Batch Number: 2391 Loss: 1.6582074165344238 Time taken: 0.39548611640930176\n",
            "Batch Number: 2392 Loss: 1.708173394203186 Time taken: 0.33812403678894043\n",
            "Batch Number: 2393 Loss: 1.685346007347107 Time taken: 0.30106687545776367\n",
            "Batch Number: 2394 Loss: 1.6765143871307373 Time taken: 0.3073863983154297\n",
            "Batch Number: 2395 Loss: 1.6669093370437622 Time taken: 0.34111523628234863\n",
            "Batch Number: 2396 Loss: 1.6714624166488647 Time taken: 0.3100104331970215\n",
            "Batch Number: 2397 Loss: 1.6500883102416992 Time taken: 0.32215094566345215\n",
            "Batch Number: 2398 Loss: 1.6530792713165283 Time taken: 0.3398439884185791\n",
            "Batch Number: 2399 Loss: 1.6454297304153442 Time taken: 0.29204535484313965\n",
            "Batch Number: 2400 Loss: 1.6499885320663452 Time taken: 0.3052701950073242\n",
            "Batch Number: 2401 Loss: 1.6412041187286377 Time taken: 0.30928945541381836\n",
            "Batch Number: 2402 Loss: 1.6771363019943237 Time taken: 0.3160679340362549\n",
            "Batch Number: 2403 Loss: 1.6846654415130615 Time taken: 0.29796552658081055\n",
            "Batch Number: 2404 Loss: 1.6811275482177734 Time taken: 0.3166379928588867\n",
            "Batch Number: 2405 Loss: 1.6453336477279663 Time taken: 0.2874124050140381\n",
            "Batch Number: 2406 Loss: 1.6574958562850952 Time taken: 0.2919900417327881\n",
            "Batch Number: 2407 Loss: 1.6728194952011108 Time taken: 0.30377936363220215\n",
            "Batch Number: 2408 Loss: 1.689038634300232 Time taken: 0.30237317085266113\n",
            "Batch Number: 2409 Loss: 1.664010763168335 Time taken: 0.2984578609466553\n",
            "Batch Number: 2410 Loss: 1.6767520904541016 Time taken: 0.29949069023132324\n",
            "Batch Number: 2411 Loss: 1.6716567277908325 Time taken: 0.34677958488464355\n",
            "Batch Number: 2412 Loss: 1.662474513053894 Time taken: 0.36463403701782227\n",
            "Batch Number: 2413 Loss: 1.6830358505249023 Time taken: 0.3725597858428955\n",
            "Batch Number: 2414 Loss: 1.7097055912017822 Time taken: 0.3102376461029053\n",
            "Batch Number: 2415 Loss: 1.682700276374817 Time taken: 0.3253486156463623\n",
            "Batch Number: 2416 Loss: 1.6967120170593262 Time taken: 0.3661491870880127\n",
            "Batch Number: 2417 Loss: 1.6818692684173584 Time taken: 0.3077371120452881\n",
            "Batch Number: 2418 Loss: 1.6596089601516724 Time taken: 0.28759145736694336\n",
            "Batch Number: 2419 Loss: 1.6469981670379639 Time taken: 0.2977464199066162\n",
            "Batch Number: 2420 Loss: 1.6768978834152222 Time taken: 0.29588961601257324\n",
            "Batch Number: 2421 Loss: 1.6583482027053833 Time taken: 0.29503774642944336\n",
            "Batch Number: 2422 Loss: 1.670974850654602 Time taken: 0.293964147567749\n",
            "Batch Number: 2423 Loss: 1.6362464427947998 Time taken: 0.30582404136657715\n",
            "Batch Number: 2424 Loss: 1.642171025276184 Time taken: 0.2967514991760254\n",
            "Batch Number: 2425 Loss: 1.6548631191253662 Time taken: 0.3587021827697754\n",
            "Batch Number: 2426 Loss: 1.6426777839660645 Time taken: 0.33348584175109863\n",
            "Batch Number: 2427 Loss: 1.6501706838607788 Time taken: 0.30277347564697266\n",
            "Batch Number: 2428 Loss: 1.6892979145050049 Time taken: 0.29857826232910156\n",
            "Batch Number: 2429 Loss: 1.6512649059295654 Time taken: 0.3364987373352051\n",
            "Batch Number: 2430 Loss: 1.659805178642273 Time taken: 0.29456067085266113\n",
            "Batch Number: 2431 Loss: 1.6577271223068237 Time taken: 0.2924926280975342\n",
            "Batch Number: 2432 Loss: 1.6818554401397705 Time taken: 0.32466936111450195\n",
            "Batch Number: 2433 Loss: 1.6762831211090088 Time taken: 0.3675975799560547\n",
            "Batch Number: 2434 Loss: 1.675121545791626 Time taken: 0.37825775146484375\n",
            "Batch Number: 2435 Loss: 1.6710433959960938 Time taken: 0.35505223274230957\n",
            "Batch Number: 2436 Loss: 1.6652870178222656 Time taken: 0.2924375534057617\n",
            "Batch Number: 2437 Loss: 1.7090908288955688 Time taken: 0.2909684181213379\n",
            "Batch Number: 2438 Loss: 1.6942753791809082 Time taken: 0.3085482120513916\n",
            "Batch Number: 2439 Loss: 1.6682987213134766 Time taken: 0.30145812034606934\n",
            "Batch Number: 2440 Loss: 1.6751432418823242 Time taken: 0.2924787998199463\n",
            "Batch Number: 2441 Loss: 1.6595814228057861 Time taken: 0.29105567932128906\n",
            "Batch Number: 2442 Loss: 1.6678144931793213 Time taken: 0.3213033676147461\n",
            "Batch Number: 2443 Loss: 1.7232383489608765 Time taken: 0.28815293312072754\n",
            "Batch Number: 2444 Loss: 1.7173516750335693 Time taken: 0.28612565994262695\n",
            "Batch Number: 2445 Loss: 1.6945875883102417 Time taken: 0.3316001892089844\n",
            "Batch Number: 2446 Loss: 1.6976258754730225 Time taken: 0.2939009666442871\n",
            "Batch Number: 2447 Loss: 1.6877799034118652 Time taken: 0.28867626190185547\n",
            "Batch Number: 2448 Loss: 1.6814841032028198 Time taken: 0.302201509475708\n",
            "Batch Number: 2449 Loss: 1.7029070854187012 Time taken: 0.3017241954803467\n",
            "Batch Number: 2450 Loss: 1.7463716268539429 Time taken: 0.290341854095459\n",
            "Batch Number: 2451 Loss: 1.6836869716644287 Time taken: 0.2964901924133301\n",
            "Batch Number: 2452 Loss: 1.7279545068740845 Time taken: 0.3209114074707031\n",
            "Batch Number: 2453 Loss: 1.7172491550445557 Time taken: 0.29433703422546387\n",
            "Batch Number: 2454 Loss: 1.7030863761901855 Time taken: 0.3306283950805664\n",
            "Batch Number: 2455 Loss: 1.7023171186447144 Time taken: 0.31919312477111816\n",
            "Batch Number: 2456 Loss: 1.678633451461792 Time taken: 0.28743505477905273\n",
            "Batch Number: 2457 Loss: 1.6747677326202393 Time taken: 0.28580665588378906\n",
            "Batch Number: 2458 Loss: 1.6861428022384644 Time taken: 0.3057258129119873\n",
            "Batch Number: 2459 Loss: 1.6703988313674927 Time taken: 0.3019440174102783\n",
            "Batch Number: 2460 Loss: 1.6932421922683716 Time taken: 0.2953643798828125\n",
            "Batch Number: 2461 Loss: 1.6719870567321777 Time taken: 0.3633244037628174\n",
            "Batch Number: 2462 Loss: 1.6780415773391724 Time taken: 0.37450671195983887\n",
            "Batch Number: 2463 Loss: 1.675034523010254 Time taken: 0.31503915786743164\n",
            "Batch Number: 2464 Loss: 1.668248176574707 Time taken: 0.3338775634765625\n",
            "Batch Number: 2465 Loss: 1.6897245645523071 Time taken: 0.30840229988098145\n",
            "Batch Number: 2466 Loss: 1.6727491617202759 Time taken: 0.2889378070831299\n",
            "Batch Number: 2467 Loss: 1.6521936655044556 Time taken: 0.30025529861450195\n",
            "Batch Number: 2468 Loss: 1.669648289680481 Time taken: 0.33791494369506836\n",
            "Batch Number: 2469 Loss: 1.6756027936935425 Time taken: 0.32129478454589844\n",
            "Batch Number: 2470 Loss: 1.67560613155365 Time taken: 0.31081271171569824\n",
            "Batch Number: 2471 Loss: 1.6433992385864258 Time taken: 0.312103271484375\n",
            "Batch Number: 2472 Loss: 1.6550993919372559 Time taken: 0.29034900665283203\n",
            "Batch Number: 2473 Loss: 1.6423298120498657 Time taken: 0.3034203052520752\n",
            "Batch Number: 2474 Loss: 1.673891305923462 Time taken: 0.32325053215026855\n",
            "Batch Number: 2475 Loss: 1.693595051765442 Time taken: 0.2946898937225342\n",
            "Batch Number: 2476 Loss: 1.6318583488464355 Time taken: 0.28943347930908203\n",
            "Batch Number: 2477 Loss: 1.6484878063201904 Time taken: 0.30589747428894043\n",
            "Batch Number: 2478 Loss: 1.6551761627197266 Time taken: 0.29978466033935547\n",
            "Batch Number: 2479 Loss: 1.6540019512176514 Time taken: 0.29892778396606445\n",
            "Batch Number: 2480 Loss: 1.6938724517822266 Time taken: 0.29580116271972656\n",
            "Batch Number: 2481 Loss: 1.6849660873413086 Time taken: 0.307567834854126\n",
            "Batch Number: 2482 Loss: 1.678797721862793 Time taken: 0.30034422874450684\n",
            "Batch Number: 2483 Loss: 1.6897188425064087 Time taken: 0.30101871490478516\n",
            "Batch Number: 2484 Loss: 1.6841124296188354 Time taken: 0.3119933605194092\n",
            "Batch Number: 2485 Loss: 1.6629951000213623 Time taken: 0.2867431640625\n",
            "Batch Number: 2486 Loss: 1.674324870109558 Time taken: 0.34098243713378906\n",
            "Batch Number: 2487 Loss: 1.6428565979003906 Time taken: 0.3768148422241211\n",
            "Batch Number: 2488 Loss: 1.6821223497390747 Time taken: 0.3844926357269287\n",
            "Batch Number: 2489 Loss: 1.6451058387756348 Time taken: 0.2907097339630127\n",
            "Batch Number: 2490 Loss: 1.7049068212509155 Time taken: 0.31399011611938477\n",
            "Batch Number: 2491 Loss: 1.7471802234649658 Time taken: 0.32502055168151855\n",
            "Batch Number: 2492 Loss: 1.6882120370864868 Time taken: 0.33796167373657227\n",
            "Batch Number: 2493 Loss: 1.6922193765640259 Time taken: 0.30303096771240234\n",
            "Batch Number: 2494 Loss: 1.6966500282287598 Time taken: 0.2908055782318115\n",
            "Batch Number: 2495 Loss: 1.7100498676300049 Time taken: 0.30503058433532715\n",
            "Batch Number: 2496 Loss: 1.6874655485153198 Time taken: 0.28575873374938965\n",
            "Batch Number: 2497 Loss: 1.6868606805801392 Time taken: 0.30484986305236816\n",
            "Batch Number: 2498 Loss: 1.7154380083084106 Time taken: 0.3287055492401123\n",
            "Batch Number: 2499 Loss: 1.6978318691253662 Time taken: 0.36867260932922363\n",
            "Batch Number: 2500 Loss: 1.6777905225753784 Time taken: 0.3892951011657715\n",
            "Batch Number: 2501 Loss: 1.6888021230697632 Time taken: 0.31890869140625\n",
            "Batch Number: 2502 Loss: 1.6912579536437988 Time taken: 0.29485034942626953\n",
            "Batch Number: 2503 Loss: 1.696875810623169 Time taken: 0.29995083808898926\n",
            "Batch Number: 2504 Loss: 1.6901898384094238 Time taken: 0.3159451484680176\n",
            "Batch Number: 2505 Loss: 1.6774381399154663 Time taken: 0.29163098335266113\n",
            "Batch Number: 2506 Loss: 1.6686408519744873 Time taken: 0.3113443851470947\n",
            "Batch Number: 2507 Loss: 1.6649914979934692 Time taken: 0.30112695693969727\n",
            "Batch Number: 2508 Loss: 1.6746515035629272 Time taken: 0.34159326553344727\n",
            "Batch Number: 2509 Loss: 1.669054627418518 Time taken: 0.3815925121307373\n",
            "Batch Number: 2510 Loss: 1.6689088344573975 Time taken: 0.34977030754089355\n",
            "Batch Number: 2511 Loss: 1.666101336479187 Time taken: 0.2906320095062256\n",
            "Batch Number: 2512 Loss: 1.6924407482147217 Time taken: 0.3093545436859131\n",
            "Batch Number: 2513 Loss: 1.654043436050415 Time taken: 0.2859008312225342\n",
            "Batch Number: 2514 Loss: 1.657517671585083 Time taken: 0.280148983001709\n",
            "Batch Number: 2515 Loss: 1.6496495008468628 Time taken: 0.2873344421386719\n",
            "Batch Number: 2516 Loss: 1.6571061611175537 Time taken: 0.2896406650543213\n",
            "Batch Number: 2517 Loss: 1.6575307846069336 Time taken: 0.28557586669921875\n",
            "Batch Number: 2518 Loss: 1.6633261442184448 Time taken: 0.29969167709350586\n",
            "Batch Number: 2519 Loss: 1.6399061679840088 Time taken: 0.31445980072021484\n",
            "Batch Number: 2520 Loss: 1.626207709312439 Time taken: 0.30095982551574707\n",
            "Batch Number: 2521 Loss: 1.6494556665420532 Time taken: 0.3013417720794678\n",
            "Batch Number: 2522 Loss: 1.6487507820129395 Time taken: 0.30572938919067383\n",
            "Batch Number: 2523 Loss: 1.5999337434768677 Time taken: 0.2920989990234375\n",
            "Batch Number: 2524 Loss: 1.6256097555160522 Time taken: 0.29397106170654297\n",
            "Batch Number: 2525 Loss: 1.6161538362503052 Time taken: 0.30043816566467285\n",
            "Batch Number: 2526 Loss: 1.6231882572174072 Time taken: 0.3027157783508301\n",
            "Batch Number: 2527 Loss: 1.602026104927063 Time taken: 0.29086899757385254\n",
            "Batch Number: 2528 Loss: 1.6385096311569214 Time taken: 0.29677295684814453\n",
            "Batch Number: 2529 Loss: 1.6355894804000854 Time taken: 0.32430481910705566\n",
            "Batch Number: 2530 Loss: 1.6024881601333618 Time taken: 0.3225843906402588\n",
            "Batch Number: 2531 Loss: 1.6389689445495605 Time taken: 0.33144664764404297\n",
            "Batch Number: 2532 Loss: 1.6458542346954346 Time taken: 0.3799257278442383\n",
            "Batch Number: 2533 Loss: 1.6461552381515503 Time taken: 0.31943225860595703\n",
            "Batch Number: 2534 Loss: 1.6279078722000122 Time taken: 0.29879069328308105\n",
            "Batch Number: 2535 Loss: 1.6723294258117676 Time taken: 0.30771803855895996\n",
            "Batch Number: 2536 Loss: 1.6495965719223022 Time taken: 0.321530818939209\n",
            "Batch Number: 2537 Loss: 1.641937255859375 Time taken: 0.37192821502685547\n",
            "Batch Number: 2538 Loss: 1.6283454895019531 Time taken: 0.37752699851989746\n",
            "Batch Number: 2539 Loss: 1.6482409238815308 Time taken: 0.3098132610321045\n",
            "Batch Number: 2540 Loss: 1.6695358753204346 Time taken: 0.37023377418518066\n",
            "Batch Number: 2541 Loss: 1.6345959901809692 Time taken: 0.38635993003845215\n",
            "Batch Number: 2542 Loss: 1.664698600769043 Time taken: 0.30826711654663086\n",
            "Batch Number: 2543 Loss: 1.6547285318374634 Time taken: 0.2875046730041504\n",
            "Batch Number: 2544 Loss: 1.6649951934814453 Time taken: 0.30277252197265625\n",
            "Batch Number: 2545 Loss: 1.623430609703064 Time taken: 0.29967522621154785\n",
            "Batch Number: 2546 Loss: 1.6642723083496094 Time taken: 0.2934391498565674\n",
            "Batch Number: 2547 Loss: 1.6390998363494873 Time taken: 0.30991029739379883\n",
            "Batch Number: 2548 Loss: 1.6519421339035034 Time taken: 0.29746174812316895\n",
            "Batch Number: 2549 Loss: 1.6645983457565308 Time taken: 0.2907745838165283\n",
            "Batch Number: 2550 Loss: 1.6294957399368286 Time taken: 0.29982781410217285\n",
            "Batch Number: 2551 Loss: 1.6518007516860962 Time taken: 0.312746524810791\n",
            "Batch Number: 2552 Loss: 1.6318848133087158 Time taken: 0.38181495666503906\n",
            "Batch Number: 2553 Loss: 1.6561691761016846 Time taken: 0.37979841232299805\n",
            "Batch Number: 2554 Loss: 1.664638638496399 Time taken: 0.36476802825927734\n",
            "Batch Number: 2555 Loss: 1.651037335395813 Time taken: 0.366741418838501\n",
            "Batch Number: 2556 Loss: 1.6525936126708984 Time taken: 0.3693835735321045\n",
            "Batch Number: 2557 Loss: 1.6462478637695312 Time taken: 0.2873349189758301\n",
            "Batch Number: 2558 Loss: 1.6553618907928467 Time taken: 0.2917594909667969\n",
            "Batch Number: 2559 Loss: 1.6765961647033691 Time taken: 0.32166409492492676\n",
            "Batch Number: 2560 Loss: 1.6761494874954224 Time taken: 0.32117247581481934\n",
            "Batch Number: 2561 Loss: 1.6377102136611938 Time taken: 0.2915987968444824\n",
            "Batch Number: 2562 Loss: 1.63712477684021 Time taken: 0.3065986633300781\n",
            "Batch Number: 2563 Loss: 1.6286379098892212 Time taken: 0.33981943130493164\n",
            "Batch Number: 2564 Loss: 1.6427605152130127 Time taken: 0.3021829128265381\n",
            "Batch Number: 2565 Loss: 1.6302516460418701 Time taken: 0.323972225189209\n",
            "Batch Number: 2566 Loss: 1.6460481882095337 Time taken: 0.3621847629547119\n",
            "Batch Number: 2567 Loss: 1.6525722742080688 Time taken: 0.2919182777404785\n",
            "Batch Number: 2568 Loss: 1.6372729539871216 Time taken: 0.29191136360168457\n",
            "Batch Number: 2569 Loss: 1.6739581823349 Time taken: 0.32024049758911133\n",
            "Batch Number: 2570 Loss: 1.6449717283248901 Time taken: 0.29393720626831055\n",
            "Batch Number: 2571 Loss: 1.6577872037887573 Time taken: 0.300189733505249\n",
            "Batch Number: 2572 Loss: 1.6482218503952026 Time taken: 0.30706286430358887\n",
            "Batch Number: 2573 Loss: 1.678611397743225 Time taken: 0.30460071563720703\n",
            "Batch Number: 2574 Loss: 1.661712646484375 Time taken: 0.2995433807373047\n",
            "Batch Number: 2575 Loss: 1.6815389394760132 Time taken: 0.3013153076171875\n",
            "Batch Number: 2576 Loss: 1.6469016075134277 Time taken: 0.3240647315979004\n",
            "Batch Number: 2577 Loss: 1.6705325841903687 Time taken: 0.296372652053833\n",
            "Batch Number: 2578 Loss: 1.6341259479522705 Time taken: 0.3012411594390869\n",
            "Batch Number: 2579 Loss: 1.6699492931365967 Time taken: 0.3226931095123291\n",
            "Batch Number: 2580 Loss: 1.6647104024887085 Time taken: 0.3378152847290039\n",
            "Batch Number: 2581 Loss: 1.6344119310379028 Time taken: 0.3600502014160156\n",
            "Batch Number: 2582 Loss: 1.650708794593811 Time taken: 0.31151247024536133\n",
            "Batch Number: 2583 Loss: 1.6284245252609253 Time taken: 0.29482293128967285\n",
            "Batch Number: 2584 Loss: 1.642770528793335 Time taken: 0.303661584854126\n",
            "Batch Number: 2585 Loss: 1.6503429412841797 Time taken: 0.37493038177490234\n",
            "Batch Number: 2586 Loss: 1.6377596855163574 Time taken: 0.36417150497436523\n",
            "Batch Number: 2587 Loss: 1.635453701019287 Time taken: 0.35907983779907227\n",
            "Batch Number: 2588 Loss: 1.6376112699508667 Time taken: 0.30496692657470703\n",
            "Batch Number: 2589 Loss: 1.6321791410446167 Time taken: 0.29162073135375977\n",
            "Batch Number: 2590 Loss: 1.6696804761886597 Time taken: 0.29868221282958984\n",
            "Batch Number: 2591 Loss: 1.6568511724472046 Time taken: 0.3271596431732178\n",
            "Batch Number: 2592 Loss: 1.6751188039779663 Time taken: 0.29724764823913574\n",
            "Batch Number: 2593 Loss: 1.6742719411849976 Time taken: 0.2935044765472412\n",
            "Batch Number: 2594 Loss: 1.6406917572021484 Time taken: 0.3178129196166992\n",
            "Batch Number: 2595 Loss: 1.6678895950317383 Time taken: 0.2936713695526123\n",
            "Batch Number: 2596 Loss: 1.6385860443115234 Time taken: 0.2919433116912842\n",
            "Batch Number: 2597 Loss: 1.648073434829712 Time taken: 0.30908703804016113\n",
            "Batch Number: 2598 Loss: 1.6495100259780884 Time taken: 0.3215818405151367\n",
            "Batch Number: 2599 Loss: 1.6371787786483765 Time taken: 0.2920420169830322\n",
            "Batch Number: 2600 Loss: 1.6287821531295776 Time taken: 0.294783353805542\n",
            "Batch Number: 2601 Loss: 1.640332818031311 Time taken: 0.30208444595336914\n",
            "Batch Number: 2602 Loss: 1.6295942068099976 Time taken: 0.29838061332702637\n",
            "Batch Number: 2603 Loss: 1.6406031847000122 Time taken: 0.31102705001831055\n",
            "Batch Number: 2604 Loss: 1.6208244562149048 Time taken: 0.3286783695220947\n",
            "Batch Number: 2605 Loss: 1.6173443794250488 Time taken: 0.2885265350341797\n",
            "Batch Number: 2606 Loss: 1.6324880123138428 Time taken: 0.2912559509277344\n",
            "Batch Number: 2607 Loss: 1.6552542448043823 Time taken: 0.317676305770874\n",
            "Batch Number: 2608 Loss: 1.6358767747879028 Time taken: 0.30486249923706055\n",
            "Batch Number: 2609 Loss: 1.644390344619751 Time taken: 0.29061007499694824\n",
            "Batch Number: 2610 Loss: 1.6389013528823853 Time taken: 0.33144164085388184\n",
            "Batch Number: 2611 Loss: 1.6761280298233032 Time taken: 0.3013882637023926\n",
            "Batch Number: 2612 Loss: 1.669206142425537 Time taken: 0.2991001605987549\n",
            "Batch Number: 2613 Loss: 1.6896302700042725 Time taken: 0.30472445487976074\n",
            "Batch Number: 2614 Loss: 1.6432290077209473 Time taken: 0.31008315086364746\n",
            "Batch Number: 2615 Loss: 1.6554371118545532 Time taken: 0.30099058151245117\n",
            "Batch Number: 2616 Loss: 1.6454557180404663 Time taken: 0.32143449783325195\n",
            "Batch Number: 2617 Loss: 1.6994715929031372 Time taken: 0.3610055446624756\n",
            "Batch Number: 2618 Loss: 1.6541424989700317 Time taken: 0.3683912754058838\n",
            "Batch Number: 2619 Loss: 1.6675094366073608 Time taken: 0.30423998832702637\n",
            "Batch Number: 2620 Loss: 1.652410626411438 Time taken: 0.31392598152160645\n",
            "Batch Number: 2621 Loss: 1.6754955053329468 Time taken: 0.29206299781799316\n",
            "Batch Number: 2622 Loss: 1.6620265245437622 Time taken: 0.3022923469543457\n",
            "Batch Number: 2623 Loss: 1.666551947593689 Time taken: 0.31777119636535645\n",
            "Batch Number: 2624 Loss: 1.6559085845947266 Time taken: 0.30632781982421875\n",
            "Batch Number: 2625 Loss: 1.6788415908813477 Time taken: 0.29377126693725586\n",
            "Batch Number: 2626 Loss: 1.678752064704895 Time taken: 0.3034062385559082\n",
            "Batch Number: 2627 Loss: 1.685987949371338 Time taken: 0.30861902236938477\n",
            "Batch Number: 2628 Loss: 1.682344675064087 Time taken: 0.2927074432373047\n",
            "Batch Number: 2629 Loss: 1.6901673078536987 Time taken: 0.3367936611175537\n",
            "Batch Number: 2630 Loss: 1.692147135734558 Time taken: 0.31055355072021484\n",
            "Batch Number: 2631 Loss: 1.6763434410095215 Time taken: 0.29373836517333984\n",
            "Batch Number: 2632 Loss: 1.6735364198684692 Time taken: 0.3396880626678467\n",
            "Batch Number: 2633 Loss: 1.6911975145339966 Time taken: 0.3001677989959717\n",
            "Batch Number: 2634 Loss: 1.6615679264068604 Time taken: 0.2965879440307617\n",
            "Batch Number: 2635 Loss: 1.7003118991851807 Time taken: 0.30211615562438965\n",
            "Batch Number: 2636 Loss: 1.6944323778152466 Time taken: 0.3143882751464844\n",
            "Batch Number: 2637 Loss: 1.680413842201233 Time taken: 0.30066847801208496\n",
            "Batch Number: 2638 Loss: 1.6388518810272217 Time taken: 0.29693007469177246\n",
            "Batch Number: 2639 Loss: 1.6564711332321167 Time taken: 0.3100566864013672\n",
            "Batch Number: 2640 Loss: 1.6656221151351929 Time taken: 0.3144564628601074\n",
            "Batch Number: 2641 Loss: 1.6605240106582642 Time taken: 0.3007643222808838\n",
            "Batch Number: 2642 Loss: 1.6691703796386719 Time taken: 0.30410027503967285\n",
            "Batch Number: 2643 Loss: 1.6661537885665894 Time taken: 0.30689477920532227\n",
            "Batch Number: 2644 Loss: 1.6378955841064453 Time taken: 0.29451966285705566\n",
            "Batch Number: 2645 Loss: 1.6684796810150146 Time taken: 0.29537010192871094\n",
            "Batch Number: 2646 Loss: 1.6671013832092285 Time taken: 0.31073999404907227\n",
            "Batch Number: 2647 Loss: 1.669148325920105 Time taken: 0.29909467697143555\n",
            "Batch Number: 2648 Loss: 1.6461032629013062 Time taken: 0.28997349739074707\n",
            "Batch Number: 2649 Loss: 1.6663142442703247 Time taken: 0.30089402198791504\n",
            "Batch Number: 2650 Loss: 1.6189836263656616 Time taken: 0.3007361888885498\n",
            "Batch Number: 2651 Loss: 1.6413851976394653 Time taken: 0.287137508392334\n",
            "Batch Number: 2652 Loss: 1.6675163507461548 Time taken: 0.30248260498046875\n",
            "Batch Number: 2653 Loss: 1.6446071863174438 Time taken: 0.29325079917907715\n",
            "Batch Number: 2654 Loss: 1.649248719215393 Time taken: 0.29561638832092285\n",
            "Batch Number: 2655 Loss: 1.6221768856048584 Time taken: 0.32782530784606934\n",
            "Batch Number: 2656 Loss: 1.608843445777893 Time taken: 0.33849573135375977\n",
            "Batch Number: 2657 Loss: 1.5981643199920654 Time taken: 0.36338138580322266\n",
            "Batch Number: 2658 Loss: 1.6365623474121094 Time taken: 0.302020788192749\n",
            "Batch Number: 2659 Loss: 1.6388555765151978 Time taken: 0.2867770195007324\n",
            "Batch Number: 2660 Loss: 1.6008819341659546 Time taken: 0.30126333236694336\n",
            "Batch Number: 2661 Loss: 1.6397521495819092 Time taken: 0.313678503036499\n",
            "Batch Number: 2662 Loss: 1.6866860389709473 Time taken: 0.3063540458679199\n",
            "Batch Number: 2663 Loss: 1.664177417755127 Time taken: 0.3004908561706543\n",
            "Batch Number: 2664 Loss: 1.6411882638931274 Time taken: 0.31302881240844727\n",
            "Batch Number: 2665 Loss: 1.6436970233917236 Time taken: 0.31554293632507324\n",
            "Batch Number: 2666 Loss: 1.6571214199066162 Time taken: 0.30925631523132324\n",
            "Batch Number: 2667 Loss: 1.6756248474121094 Time taken: 0.31014418601989746\n",
            "Batch Number: 2668 Loss: 1.6850467920303345 Time taken: 0.3013741970062256\n",
            "Batch Number: 2669 Loss: 1.680783987045288 Time taken: 0.29301881790161133\n",
            "Batch Number: 2670 Loss: 1.6403627395629883 Time taken: 0.2794497013092041\n",
            "Batch Number: 2671 Loss: 1.64295494556427 Time taken: 0.3147449493408203\n",
            "Batch Number: 2672 Loss: 1.6589412689208984 Time taken: 0.30092406272888184\n",
            "Batch Number: 2673 Loss: 1.664297103881836 Time taken: 0.30178165435791016\n",
            "Batch Number: 2674 Loss: 1.6502434015274048 Time taken: 0.31426095962524414\n",
            "Batch Number: 2675 Loss: 1.6375677585601807 Time taken: 0.38680481910705566\n",
            "Batch Number: 2676 Loss: 1.6631982326507568 Time taken: 0.3623616695404053\n",
            "Batch Number: 2677 Loss: 1.644453763961792 Time taken: 0.3052394390106201\n",
            "Batch Number: 2678 Loss: 1.6573320627212524 Time taken: 0.2948787212371826\n",
            "Batch Number: 2679 Loss: 1.6708064079284668 Time taken: 0.31037044525146484\n",
            "Batch Number: 2680 Loss: 1.6794438362121582 Time taken: 0.3213222026824951\n",
            "Batch Number: 2681 Loss: 1.6205675601959229 Time taken: 0.3054008483886719\n",
            "Batch Number: 2682 Loss: 1.6668245792388916 Time taken: 0.3060891628265381\n",
            "Batch Number: 2683 Loss: 1.634849190711975 Time taken: 0.2911515235900879\n",
            "Batch Number: 2684 Loss: 1.6562275886535645 Time taken: 0.29819655418395996\n",
            "Batch Number: 2685 Loss: 1.6459095478057861 Time taken: 0.31262946128845215\n",
            "Batch Number: 2686 Loss: 1.628836989402771 Time taken: 0.32801032066345215\n",
            "Batch Number: 2687 Loss: 1.632401704788208 Time taken: 0.29195666313171387\n",
            "Batch Number: 2688 Loss: 1.6419663429260254 Time taken: 0.28830790519714355\n",
            "Batch Number: 2689 Loss: 1.6346114873886108 Time taken: 0.3018782138824463\n",
            "Batch Number: 2690 Loss: 1.6461631059646606 Time taken: 0.2983715534210205\n",
            "Batch Number: 2691 Loss: 1.6528528928756714 Time taken: 0.30432558059692383\n",
            "Batch Number: 2692 Loss: 1.633264422416687 Time taken: 0.30234718322753906\n",
            "Batch Number: 2693 Loss: 1.6475412845611572 Time taken: 0.3097522258758545\n",
            "Batch Number: 2694 Loss: 1.6268330812454224 Time taken: 0.3123140335083008\n",
            "Batch Number: 2695 Loss: 1.6176693439483643 Time taken: 0.31316113471984863\n",
            "Batch Number: 2696 Loss: 1.6176670789718628 Time taken: 0.2936711311340332\n",
            "Batch Number: 2697 Loss: 1.612120509147644 Time taken: 0.30397844314575195\n",
            "Batch Number: 2698 Loss: 1.6162623167037964 Time taken: 0.29443883895874023\n",
            "Batch Number: 2699 Loss: 1.6320216655731201 Time taken: 0.3531050682067871\n",
            "Batch Number: 2700 Loss: 1.612011194229126 Time taken: 0.29174280166625977\n",
            "Batch Number: 2701 Loss: 1.5950901508331299 Time taken: 0.2898387908935547\n",
            "Batch Number: 2702 Loss: 1.625166416168213 Time taken: 0.3042936325073242\n",
            "Batch Number: 2703 Loss: 1.5981541872024536 Time taken: 0.3363058567047119\n",
            "Batch Number: 2704 Loss: 1.5985571146011353 Time taken: 0.3675410747528076\n",
            "Batch Number: 2705 Loss: 1.582653522491455 Time taken: 0.3558926582336426\n",
            "Batch Number: 2706 Loss: 1.6224113702774048 Time taken: 0.3056018352508545\n",
            "Batch Number: 2707 Loss: 1.5858042240142822 Time taken: 0.29317641258239746\n",
            "Batch Number: 2708 Loss: 1.6330152750015259 Time taken: 0.31707334518432617\n",
            "Batch Number: 2709 Loss: 1.6008918285369873 Time taken: 0.2865333557128906\n",
            "Batch Number: 2710 Loss: 1.5919017791748047 Time taken: 0.301224946975708\n",
            "Batch Number: 2711 Loss: 1.618661880493164 Time taken: 0.30841970443725586\n",
            "Batch Number: 2712 Loss: 1.6202811002731323 Time taken: 0.3045814037322998\n",
            "Batch Number: 2713 Loss: 1.6327275037765503 Time taken: 0.29938364028930664\n",
            "Batch Number: 2714 Loss: 1.6255847215652466 Time taken: 0.2968170642852783\n",
            "Batch Number: 2715 Loss: 1.6213791370391846 Time taken: 0.3242518901824951\n",
            "Batch Number: 2716 Loss: 1.632415771484375 Time taken: 0.2947697639465332\n",
            "Batch Number: 2717 Loss: 1.6292835474014282 Time taken: 0.29102134704589844\n",
            "Batch Number: 2718 Loss: 1.619769811630249 Time taken: 0.3369903564453125\n",
            "Batch Number: 2719 Loss: 1.6067698001861572 Time taken: 0.3199291229248047\n",
            "Batch Number: 2720 Loss: 1.6029658317565918 Time taken: 0.3017294406890869\n",
            "Batch Number: 2721 Loss: 1.608604907989502 Time taken: 0.30844712257385254\n",
            "Batch Number: 2722 Loss: 1.6284157037734985 Time taken: 0.30200839042663574\n",
            "Batch Number: 2723 Loss: 1.6199431419372559 Time taken: 0.310899019241333\n",
            "Batch Number: 2724 Loss: 1.6402000188827515 Time taken: 0.3091259002685547\n",
            "Batch Number: 2725 Loss: 1.640639066696167 Time taken: 0.2964038848876953\n",
            "Batch Number: 2726 Loss: 1.6113810539245605 Time taken: 0.3599538803100586\n",
            "Batch Number: 2727 Loss: 1.631570816040039 Time taken: 0.30592823028564453\n",
            "Batch Number: 2728 Loss: 1.6299909353256226 Time taken: 0.31121349334716797\n",
            "Batch Number: 2729 Loss: 1.6137999296188354 Time taken: 0.3221604824066162\n",
            "Batch Number: 2730 Loss: 1.6339223384857178 Time taken: 0.3220958709716797\n",
            "Batch Number: 2731 Loss: 1.6254531145095825 Time taken: 0.3222219944000244\n",
            "Batch Number: 2732 Loss: 1.6215674877166748 Time taken: 0.333437442779541\n",
            "Batch Number: 2733 Loss: 1.6281689405441284 Time taken: 0.3047513961791992\n",
            "Batch Number: 2734 Loss: 1.6086981296539307 Time taken: 0.3158586025238037\n",
            "Batch Number: 2735 Loss: 1.616786003112793 Time taken: 0.3049473762512207\n",
            "Batch Number: 2736 Loss: 1.643455982208252 Time taken: 0.30141544342041016\n",
            "Batch Number: 2737 Loss: 1.653098464012146 Time taken: 0.34099292755126953\n",
            "Batch Number: 2738 Loss: 1.680281162261963 Time taken: 0.3000621795654297\n",
            "Batch Number: 2739 Loss: 1.6535382270812988 Time taken: 0.29221105575561523\n",
            "Batch Number: 2740 Loss: 1.6139578819274902 Time taken: 0.33214807510375977\n",
            "Batch Number: 2741 Loss: 1.6626157760620117 Time taken: 0.31535816192626953\n",
            "Batch Number: 2742 Loss: 1.64902663230896 Time taken: 0.32193851470947266\n",
            "Batch Number: 2743 Loss: 1.6364731788635254 Time taken: 0.3977348804473877\n",
            "Batch Number: 2744 Loss: 1.6197351217269897 Time taken: 0.3866434097290039\n",
            "Batch Number: 2745 Loss: 1.6187517642974854 Time taken: 0.3610374927520752\n",
            "Batch Number: 2746 Loss: 1.5858385562896729 Time taken: 0.30660009384155273\n",
            "Batch Number: 2747 Loss: 1.6283124685287476 Time taken: 0.2973661422729492\n",
            "Batch Number: 2748 Loss: 1.5942846536636353 Time taken: 0.2967100143432617\n",
            "Batch Number: 2749 Loss: 1.6362844705581665 Time taken: 0.32358241081237793\n",
            "Batch Number: 2750 Loss: 1.6251012086868286 Time taken: 0.2936265468597412\n",
            "Batch Number: 2751 Loss: 1.642374038696289 Time taken: 0.31062984466552734\n",
            "Batch Number: 2752 Loss: 1.6191884279251099 Time taken: 0.29375386238098145\n",
            "Batch Number: 2753 Loss: 1.6253256797790527 Time taken: 0.3089148998260498\n",
            "Batch Number: 2754 Loss: 1.6199995279312134 Time taken: 0.2998361587524414\n",
            "Batch Number: 2755 Loss: 1.6147925853729248 Time taken: 0.32342052459716797\n",
            "Batch Number: 2756 Loss: 1.6266008615493774 Time taken: 0.3009035587310791\n",
            "Batch Number: 2757 Loss: 1.6152207851409912 Time taken: 0.28826165199279785\n",
            "Batch Number: 2758 Loss: 1.6064399480819702 Time taken: 0.30735230445861816\n",
            "Batch Number: 2759 Loss: 1.596027135848999 Time taken: 0.3105757236480713\n",
            "Batch Number: 2760 Loss: 1.5984293222427368 Time taken: 0.37684202194213867\n",
            "Batch Number: 2761 Loss: 1.6032956838607788 Time taken: 0.34937262535095215\n",
            "Batch Number: 2762 Loss: 1.611700177192688 Time taken: 0.31081366539001465\n",
            "Batch Number: 2763 Loss: 1.6194827556610107 Time taken: 0.2817044258117676\n",
            "Batch Number: 2764 Loss: 1.6099573373794556 Time taken: 0.30498719215393066\n",
            "Batch Number: 2765 Loss: 1.605790138244629 Time taken: 0.31995344161987305\n",
            "Batch Number: 2766 Loss: 1.5995343923568726 Time taken: 0.3071749210357666\n",
            "Batch Number: 2767 Loss: 1.6148521900177002 Time taken: 0.3204009532928467\n",
            "Batch Number: 2768 Loss: 1.631480097770691 Time taken: 0.30469179153442383\n",
            "Batch Number: 2769 Loss: 1.6424733400344849 Time taken: 0.30629706382751465\n",
            "Batch Number: 2770 Loss: 1.6605150699615479 Time taken: 0.3000633716583252\n",
            "Batch Number: 2771 Loss: 1.6622573137283325 Time taken: 0.3069610595703125\n",
            "Batch Number: 2772 Loss: 1.6848942041397095 Time taken: 0.30501747131347656\n",
            "Batch Number: 2773 Loss: 1.689118504524231 Time taken: 0.3003556728363037\n",
            "Batch Number: 2774 Loss: 1.6721335649490356 Time taken: 0.3585338592529297\n",
            "Batch Number: 2775 Loss: 1.676007866859436 Time taken: 0.376556396484375\n",
            "Batch Number: 2776 Loss: 1.6666266918182373 Time taken: 0.3656454086303711\n",
            "Batch Number: 2777 Loss: 1.6780203580856323 Time taken: 0.3135111331939697\n",
            "Batch Number: 2778 Loss: 1.6674166917800903 Time taken: 0.3733694553375244\n",
            "Batch Number: 2779 Loss: 1.6511403322219849 Time taken: 0.369718074798584\n",
            "Batch Number: 2780 Loss: 1.662987470626831 Time taken: 0.3020777702331543\n",
            "Batch Number: 2781 Loss: 1.6301894187927246 Time taken: 0.2996654510498047\n",
            "Batch Number: 2782 Loss: 1.617789626121521 Time taken: 0.29704928398132324\n",
            "Batch Number: 2783 Loss: 1.6501975059509277 Time taken: 0.3029007911682129\n",
            "Batch Number: 2784 Loss: 1.6128287315368652 Time taken: 0.30121922492980957\n",
            "Batch Number: 2785 Loss: 1.6479239463806152 Time taken: 0.3021876811981201\n",
            "Batch Number: 2786 Loss: 1.619275450706482 Time taken: 0.3284449577331543\n",
            "Batch Number: 2787 Loss: 1.6230014562606812 Time taken: 0.3771953582763672\n",
            "Batch Number: 2788 Loss: 1.6189082860946655 Time taken: 0.35324549674987793\n",
            "Batch Number: 2789 Loss: 1.6265994310379028 Time taken: 0.29779481887817383\n",
            "Batch Number: 2790 Loss: 1.595932960510254 Time taken: 0.305767297744751\n",
            "Batch Number: 2791 Loss: 1.6320191621780396 Time taken: 0.3004622459411621\n",
            "Batch Number: 2792 Loss: 1.6306017637252808 Time taken: 0.31845784187316895\n",
            "Batch Number: 2793 Loss: 1.657049536705017 Time taken: 0.3090677261352539\n",
            "Batch Number: 2794 Loss: 1.6605322360992432 Time taken: 0.30037546157836914\n",
            "Batch Number: 2795 Loss: 1.63646399974823 Time taken: 0.3087728023529053\n",
            "Batch Number: 2796 Loss: 1.6541318893432617 Time taken: 0.29787111282348633\n",
            "Batch Number: 2797 Loss: 1.641058087348938 Time taken: 0.33777785301208496\n",
            "Batch Number: 2798 Loss: 1.652677297592163 Time taken: 0.3110995292663574\n",
            "Batch Number: 2799 Loss: 1.659361720085144 Time taken: 0.3072700500488281\n",
            "Batch Number: 2800 Loss: 1.6411356925964355 Time taken: 0.3077211380004883\n",
            "Batch Number: 2801 Loss: 1.6613993644714355 Time taken: 0.3149385452270508\n",
            "Batch Number: 2802 Loss: 1.6387927532196045 Time taken: 0.2981886863708496\n",
            "Batch Number: 2803 Loss: 1.641647458076477 Time taken: 0.31258487701416016\n",
            "Batch Number: 2804 Loss: 1.6583980321884155 Time taken: 0.3144567012786865\n",
            "Batch Number: 2805 Loss: 1.6523010730743408 Time taken: 0.3116469383239746\n",
            "Batch Number: 2806 Loss: 1.6536206007003784 Time taken: 0.30127763748168945\n",
            "Batch Number: 2807 Loss: 1.6378536224365234 Time taken: 0.30933070182800293\n",
            "Batch Number: 2808 Loss: 1.6687926054000854 Time taken: 0.2880678176879883\n",
            "Batch Number: 2809 Loss: 1.6641793251037598 Time taken: 0.29735755920410156\n",
            "Batch Number: 2810 Loss: 1.6901978254318237 Time taken: 0.3028066158294678\n",
            "Batch Number: 2811 Loss: 1.671280860900879 Time taken: 0.34244418144226074\n",
            "Batch Number: 2812 Loss: 1.6731364727020264 Time taken: 0.29846954345703125\n",
            "Batch Number: 2813 Loss: 1.6782525777816772 Time taken: 0.30965709686279297\n",
            "Batch Number: 2814 Loss: 1.6573470830917358 Time taken: 0.32268524169921875\n",
            "Batch Number: 2815 Loss: 1.641348958015442 Time taken: 0.29743456840515137\n",
            "Batch Number: 2816 Loss: 1.6425431966781616 Time taken: 0.3040437698364258\n",
            "Batch Number: 2817 Loss: 1.6551129817962646 Time taken: 0.31764650344848633\n",
            "Batch Number: 2818 Loss: 1.6386370658874512 Time taken: 0.2954866886138916\n",
            "Batch Number: 2819 Loss: 1.640879511833191 Time taken: 0.28809618949890137\n",
            "Batch Number: 2820 Loss: 1.6249134540557861 Time taken: 0.3067655563354492\n",
            "Batch Number: 2821 Loss: 1.652205467224121 Time taken: 0.3304882049560547\n",
            "Batch Number: 2822 Loss: 1.6684521436691284 Time taken: 0.36650633811950684\n",
            "Batch Number: 2823 Loss: 1.6380974054336548 Time taken: 0.37647485733032227\n",
            "Batch Number: 2824 Loss: 1.6186308860778809 Time taken: 0.37069010734558105\n",
            "Batch Number: 2825 Loss: 1.6303739547729492 Time taken: 0.3828909397125244\n",
            "Batch Number: 2826 Loss: 1.640555739402771 Time taken: 0.31816625595092773\n",
            "Batch Number: 2827 Loss: 1.6109790802001953 Time taken: 0.29995036125183105\n",
            "Batch Number: 2828 Loss: 1.6166939735412598 Time taken: 0.2985515594482422\n",
            "Batch Number: 2829 Loss: 1.639186978340149 Time taken: 0.334475040435791\n",
            "Batch Number: 2830 Loss: 1.6236095428466797 Time taken: 0.3120272159576416\n",
            "Batch Number: 2831 Loss: 1.6286320686340332 Time taken: 0.29115843772888184\n",
            "Batch Number: 2832 Loss: 1.6176577806472778 Time taken: 0.32100462913513184\n",
            "Batch Number: 2833 Loss: 1.6192435026168823 Time taken: 0.30501484870910645\n",
            "Batch Number: 2834 Loss: 1.6203941106796265 Time taken: 0.293637752532959\n",
            "Batch Number: 2835 Loss: 1.622011423110962 Time taken: 0.33110785484313965\n",
            "Batch Number: 2836 Loss: 1.5790154933929443 Time taken: 0.30509042739868164\n",
            "Batch Number: 2837 Loss: 1.6173113584518433 Time taken: 0.2927677631378174\n",
            "Batch Number: 2838 Loss: 1.6292222738265991 Time taken: 0.3071410655975342\n",
            "Batch Number: 2839 Loss: 1.6243836879730225 Time taken: 0.30811214447021484\n",
            "Batch Number: 2840 Loss: 1.6378098726272583 Time taken: 0.3617269992828369\n",
            "Batch Number: 2841 Loss: 1.6689928770065308 Time taken: 0.38763976097106934\n",
            "Batch Number: 2842 Loss: 1.620355248451233 Time taken: 0.32818031311035156\n",
            "Batch Number: 2843 Loss: 1.6462031602859497 Time taken: 0.29660511016845703\n",
            "Batch Number: 2844 Loss: 1.6380261182785034 Time taken: 0.3029654026031494\n",
            "Batch Number: 2845 Loss: 1.6594727039337158 Time taken: 0.30463552474975586\n",
            "Batch Number: 2846 Loss: 1.6421785354614258 Time taken: 0.2940177917480469\n",
            "Batch Number: 2847 Loss: 1.631240963935852 Time taken: 0.29421019554138184\n",
            "Batch Number: 2848 Loss: 1.6053906679153442 Time taken: 0.3034064769744873\n",
            "Batch Number: 2849 Loss: 1.6077462434768677 Time taken: 0.299907922744751\n",
            "Batch Number: 2850 Loss: 1.6078051328659058 Time taken: 0.3000626564025879\n",
            "Batch Number: 2851 Loss: 1.6487728357315063 Time taken: 0.3084218502044678\n",
            "Batch Number: 2852 Loss: 1.624309778213501 Time taken: 0.3545386791229248\n",
            "Batch Number: 2853 Loss: 1.6202481985092163 Time taken: 0.3597695827484131\n",
            "Batch Number: 2854 Loss: 1.6506255865097046 Time taken: 0.34919238090515137\n",
            "Batch Number: 2855 Loss: 1.6269750595092773 Time taken: 0.29453301429748535\n",
            "Batch Number: 2856 Loss: 1.645732045173645 Time taken: 0.2942237854003906\n",
            "Batch Number: 2857 Loss: 1.6411505937576294 Time taken: 0.30489563941955566\n",
            "Batch Number: 2858 Loss: 1.6636351346969604 Time taken: 0.29277849197387695\n",
            "Batch Number: 2859 Loss: 1.6484344005584717 Time taken: 0.2962963581085205\n",
            "Batch Number: 2860 Loss: 1.6161757707595825 Time taken: 0.32878684997558594\n",
            "Batch Number: 2861 Loss: 1.605159878730774 Time taken: 0.3835639953613281\n",
            "Batch Number: 2862 Loss: 1.6317024230957031 Time taken: 0.3866560459136963\n",
            "Batch Number: 2863 Loss: 1.65192449092865 Time taken: 0.37624096870422363\n",
            "Batch Number: 2864 Loss: 1.6473580598831177 Time taken: 0.3725459575653076\n",
            "Batch Number: 2865 Loss: 1.626968502998352 Time taken: 0.3669288158416748\n",
            "Batch Number: 2866 Loss: 1.5885964632034302 Time taken: 0.32341933250427246\n",
            "Batch Number: 2867 Loss: 1.652762532234192 Time taken: 0.2911207675933838\n",
            "Batch Number: 2868 Loss: 1.6279655694961548 Time taken: 0.29436445236206055\n",
            "Batch Number: 2869 Loss: 1.5869865417480469 Time taken: 0.317018985748291\n",
            "Batch Number: 2870 Loss: 1.6111514568328857 Time taken: 0.2948267459869385\n",
            "Batch Number: 2871 Loss: 1.6279131174087524 Time taken: 0.2969820499420166\n",
            "Batch Number: 2872 Loss: 1.6209388971328735 Time taken: 0.33976078033447266\n",
            "Batch Number: 2873 Loss: 1.602943778038025 Time taken: 0.3031628131866455\n",
            "Batch Number: 2874 Loss: 1.6183652877807617 Time taken: 0.2887458801269531\n",
            "Batch Number: 2875 Loss: 1.6223267316818237 Time taken: 0.29706525802612305\n",
            "Batch Number: 2876 Loss: 1.5994665622711182 Time taken: 0.29657626152038574\n",
            "Batch Number: 2877 Loss: 1.625132441520691 Time taken: 0.27610206604003906\n",
            "Batch Number: 2878 Loss: 1.6189268827438354 Time taken: 0.3322176933288574\n",
            "Batch Number: 2879 Loss: 1.6172410249710083 Time taken: 0.31217074394226074\n",
            "Batch Number: 2880 Loss: 1.580322265625 Time taken: 0.29619312286376953\n",
            "Batch Number: 2881 Loss: 1.5898690223693848 Time taken: 0.3062260150909424\n",
            "Batch Number: 2882 Loss: 1.591834306716919 Time taken: 0.33077096939086914\n",
            "Batch Number: 2883 Loss: 1.5982142686843872 Time taken: 0.2928004264831543\n",
            "Batch Number: 2884 Loss: 1.5909996032714844 Time taken: 0.2942347526550293\n",
            "Batch Number: 2885 Loss: 1.591478705406189 Time taken: 0.3476896286010742\n",
            "Batch Number: 2886 Loss: 1.6012167930603027 Time taken: 0.30168676376342773\n",
            "Batch Number: 2887 Loss: 1.6206938028335571 Time taken: 0.2885007858276367\n",
            "Batch Number: 2888 Loss: 1.6114952564239502 Time taken: 0.29845571517944336\n",
            "Batch Number: 2889 Loss: 1.6302111148834229 Time taken: 0.3253054618835449\n",
            "Batch Number: 2890 Loss: 1.5892324447631836 Time taken: 0.2922630310058594\n",
            "Batch Number: 2891 Loss: 1.6050482988357544 Time taken: 0.3351776599884033\n",
            "Batch Number: 2892 Loss: 1.6374988555908203 Time taken: 0.31826043128967285\n",
            "Batch Number: 2893 Loss: 1.5902336835861206 Time taken: 0.2924504280090332\n",
            "Batch Number: 2894 Loss: 1.597955346107483 Time taken: 0.3227875232696533\n",
            "Batch Number: 2895 Loss: 1.6334565877914429 Time taken: 0.3406665325164795\n",
            "Batch Number: 2896 Loss: 1.5822839736938477 Time taken: 0.29392457008361816\n",
            "Batch Number: 2897 Loss: 1.6066343784332275 Time taken: 0.3061707019805908\n",
            "Batch Number: 2898 Loss: 1.6094051599502563 Time taken: 0.31714510917663574\n",
            "Batch Number: 2899 Loss: 1.5834062099456787 Time taken: 0.32601380348205566\n",
            "Batch Number: 2900 Loss: 1.6038471460342407 Time taken: 0.2966887950897217\n",
            "Batch Number: 2901 Loss: 1.603700876235962 Time taken: 0.2867698669433594\n",
            "Batch Number: 2902 Loss: 1.5822428464889526 Time taken: 0.29170894622802734\n",
            "Batch Number: 2903 Loss: 1.6160776615142822 Time taken: 0.2834198474884033\n",
            "Batch Number: 2904 Loss: 1.6254191398620605 Time taken: 0.33838772773742676\n",
            "Batch Number: 2905 Loss: 1.60718834400177 Time taken: 0.31137967109680176\n",
            "Batch Number: 2906 Loss: 1.5871111154556274 Time taken: 0.2916598320007324\n",
            "Batch Number: 2907 Loss: 1.6173121929168701 Time taken: 0.2994668483734131\n",
            "Batch Number: 2908 Loss: 1.5870766639709473 Time taken: 0.32190442085266113\n",
            "Batch Number: 2909 Loss: 1.593735933303833 Time taken: 0.291140079498291\n",
            "Batch Number: 2910 Loss: 1.5911716222763062 Time taken: 0.2992279529571533\n",
            "Batch Number: 2911 Loss: 1.5949039459228516 Time taken: 0.30628180503845215\n",
            "Batch Number: 2912 Loss: 1.598805546760559 Time taken: 0.31392931938171387\n",
            "Batch Number: 2913 Loss: 1.6110678911209106 Time taken: 0.30577826499938965\n",
            "Batch Number: 2914 Loss: 1.6043412685394287 Time taken: 0.2999861240386963\n",
            "Batch Number: 2915 Loss: 1.5841647386550903 Time taken: 0.30849194526672363\n",
            "Batch Number: 2916 Loss: 1.6299169063568115 Time taken: 0.2929549217224121\n",
            "Batch Number: 2917 Loss: 1.624730110168457 Time taken: 0.32219791412353516\n",
            "Batch Number: 2918 Loss: 1.6351052522659302 Time taken: 0.31893038749694824\n",
            "Batch Number: 2919 Loss: 1.6533331871032715 Time taken: 0.33306002616882324\n",
            "Batch Number: 2920 Loss: 1.596783995628357 Time taken: 0.3252694606781006\n",
            "Batch Number: 2921 Loss: 1.583368182182312 Time taken: 0.35030031204223633\n",
            "Batch Number: 2922 Loss: 1.5993099212646484 Time taken: 0.29758405685424805\n",
            "Batch Number: 2923 Loss: 1.595404028892517 Time taken: 0.3189122676849365\n",
            "Batch Number: 2924 Loss: 1.631527066230774 Time taken: 0.3196547031402588\n",
            "Batch Number: 2925 Loss: 1.6096880435943604 Time taken: 0.2965822219848633\n",
            "Batch Number: 2926 Loss: 1.5841585397720337 Time taken: 0.28723764419555664\n",
            "Batch Number: 2927 Loss: 1.5809003114700317 Time taken: 0.2934994697570801\n",
            "Batch Number: 2928 Loss: 1.6372745037078857 Time taken: 0.28917789459228516\n",
            "Batch Number: 2929 Loss: 1.6161144971847534 Time taken: 0.3321669101715088\n",
            "Batch Number: 2930 Loss: 1.606229543685913 Time taken: 0.33121418952941895\n",
            "Batch Number: 2931 Loss: 1.5856640338897705 Time taken: 0.30965185165405273\n",
            "Batch Number: 2932 Loss: 1.6265302896499634 Time taken: 0.2950112819671631\n",
            "Batch Number: 2933 Loss: 1.608744502067566 Time taken: 0.29630327224731445\n",
            "Batch Number: 2934 Loss: 1.5986406803131104 Time taken: 0.3114590644836426\n",
            "Batch Number: 2935 Loss: 1.61360764503479 Time taken: 0.2944324016571045\n",
            "Batch Number: 2936 Loss: 1.593151569366455 Time taken: 0.32589077949523926\n",
            "Batch Number: 2937 Loss: 1.6138252019882202 Time taken: 0.325549840927124\n",
            "Batch Number: 2938 Loss: 1.5930174589157104 Time taken: 0.29360342025756836\n",
            "Batch Number: 2939 Loss: 1.6207574605941772 Time taken: 0.32782745361328125\n",
            "Batch Number: 2940 Loss: 1.5923168659210205 Time taken: 0.36621665954589844\n",
            "Batch Number: 2941 Loss: 1.6120927333831787 Time taken: 0.34970784187316895\n",
            "Batch Number: 2942 Loss: 1.5832089185714722 Time taken: 0.28661298751831055\n",
            "Batch Number: 2943 Loss: 1.5712859630584717 Time taken: 0.2960844039916992\n",
            "Batch Number: 2944 Loss: 1.5981765985488892 Time taken: 0.28903722763061523\n",
            "Batch Number: 2945 Loss: 1.6087892055511475 Time taken: 0.32088232040405273\n",
            "Batch Number: 2946 Loss: 1.6212340593338013 Time taken: 0.3106517791748047\n",
            "Batch Number: 2947 Loss: 1.5838733911514282 Time taken: 0.30838894844055176\n",
            "Batch Number: 2948 Loss: 1.5936764478683472 Time taken: 0.3096628189086914\n",
            "Batch Number: 2949 Loss: 1.5918738842010498 Time taken: 0.2973940372467041\n",
            "Batch Number: 2950 Loss: 1.6026058197021484 Time taken: 0.3136167526245117\n",
            "Batch Number: 2951 Loss: 1.635629653930664 Time taken: 0.3020761013031006\n",
            "Batch Number: 2952 Loss: 1.627206802368164 Time taken: 0.35599613189697266\n",
            "Batch Number: 2953 Loss: 1.6487390995025635 Time taken: 0.38640785217285156\n",
            "Batch Number: 2954 Loss: 1.6241668462753296 Time taken: 0.3381917476654053\n",
            "Batch Number: 2955 Loss: 1.6144455671310425 Time taken: 0.2976360321044922\n",
            "Batch Number: 2956 Loss: 1.6306253671646118 Time taken: 0.30594563484191895\n",
            "Batch Number: 2957 Loss: 1.6243749856948853 Time taken: 0.30419254302978516\n",
            "Batch Number: 2958 Loss: 1.694800615310669 Time taken: 0.2909564971923828\n",
            "Batch Number: 2959 Loss: 1.6210848093032837 Time taken: 0.30776166915893555\n",
            "Batch Number: 2960 Loss: 1.645522117614746 Time taken: 0.31368541717529297\n",
            "Batch Number: 2961 Loss: 1.6180616617202759 Time taken: 0.2763700485229492\n",
            "Batch Number: 2962 Loss: 1.6333322525024414 Time taken: 0.29212212562561035\n",
            "Batch Number: 2963 Loss: 1.6284908056259155 Time taken: 0.34414243698120117\n",
            "Batch Number: 2964 Loss: 1.638440728187561 Time taken: 0.29242801666259766\n",
            "Batch Number: 2965 Loss: 1.5984824895858765 Time taken: 0.3017463684082031\n",
            "Batch Number: 2966 Loss: 1.6104328632354736 Time taken: 0.31337404251098633\n",
            "Batch Number: 2967 Loss: 1.6301369667053223 Time taken: 0.28812551498413086\n",
            "Batch Number: 2968 Loss: 1.587220311164856 Time taken: 0.3014335632324219\n",
            "Batch Number: 2969 Loss: 1.6076594591140747 Time taken: 0.30340075492858887\n",
            "Batch Number: 2970 Loss: 1.599966287612915 Time taken: 0.3165280818939209\n",
            "Batch Number: 2971 Loss: 1.6019620895385742 Time taken: 0.29346680641174316\n",
            "Batch Number: 2972 Loss: 1.6166110038757324 Time taken: 0.2961845397949219\n",
            "Batch Number: 2973 Loss: 1.6085758209228516 Time taken: 0.3037233352661133\n",
            "Batch Number: 2974 Loss: 1.629721999168396 Time taken: 0.29196739196777344\n",
            "Batch Number: 2975 Loss: 1.629072666168213 Time taken: 0.29483532905578613\n",
            "Batch Number: 2976 Loss: 1.616229772567749 Time taken: 0.30734729766845703\n",
            "Batch Number: 2977 Loss: 1.6459693908691406 Time taken: 0.3096015453338623\n",
            "Batch Number: 2978 Loss: 1.64439058303833 Time taken: 0.29270052909851074\n",
            "Batch Number: 2979 Loss: 1.629643440246582 Time taken: 0.3055381774902344\n",
            "Batch Number: 2980 Loss: 1.624096155166626 Time taken: 0.3208909034729004\n",
            "Batch Number: 2981 Loss: 1.6118865013122559 Time taken: 0.300936222076416\n",
            "Batch Number: 2982 Loss: 1.633376121520996 Time taken: 0.35718202590942383\n",
            "Batch Number: 2983 Loss: 1.6476032733917236 Time taken: 0.3206443786621094\n",
            "Batch Number: 2984 Loss: 1.6261838674545288 Time taken: 0.2971816062927246\n",
            "Batch Number: 2985 Loss: 1.6353106498718262 Time taken: 0.33031558990478516\n",
            "Batch Number: 2986 Loss: 1.66621994972229 Time taken: 0.32479166984558105\n",
            "Batch Number: 2987 Loss: 1.669089674949646 Time taken: 0.2947425842285156\n",
            "Batch Number: 2988 Loss: 1.655237078666687 Time taken: 0.3031272888183594\n",
            "Batch Number: 2989 Loss: 1.6245547533035278 Time taken: 0.30621862411499023\n",
            "Batch Number: 2990 Loss: 1.6568931341171265 Time taken: 0.29848408699035645\n",
            "Batch Number: 2991 Loss: 1.6623797416687012 Time taken: 0.2998313903808594\n",
            "Batch Number: 2992 Loss: 1.633405089378357 Time taken: 0.3773789405822754\n",
            "Batch Number: 2993 Loss: 1.666366457939148 Time taken: 0.30147504806518555\n",
            "Batch Number: 2994 Loss: 1.6423553228378296 Time taken: 0.29996323585510254\n",
            "Batch Number: 2995 Loss: 1.6229078769683838 Time taken: 0.31333351135253906\n",
            "Batch Number: 2996 Loss: 1.5982776880264282 Time taken: 0.2978835105895996\n",
            "Batch Number: 2997 Loss: 1.6151021718978882 Time taken: 0.31378650665283203\n",
            "Batch Number: 2998 Loss: 1.6046491861343384 Time taken: 0.3220179080963135\n",
            "Batch Number: 2999 Loss: 1.600089192390442 Time taken: 0.2983667850494385\n",
            "Batch Number: 3000 Loss: 1.625378131866455 Time taken: 0.29541850090026855\n",
            "Batch Number: 3001 Loss: 1.6215211153030396 Time taken: 0.289813756942749\n",
            "Batch Number: 3002 Loss: 1.6313453912734985 Time taken: 0.31705784797668457\n",
            "Batch Number: 3003 Loss: 1.617618441581726 Time taken: 0.35367655754089355\n",
            "Batch Number: 3004 Loss: 1.5989820957183838 Time taken: 0.3663628101348877\n",
            "Batch Number: 3005 Loss: 1.6161928176879883 Time taken: 0.306380033493042\n",
            "Batch Number: 3006 Loss: 1.6129207611083984 Time taken: 0.2975771427154541\n",
            "Batch Number: 3007 Loss: 1.6052603721618652 Time taken: 0.29688334465026855\n",
            "Batch Number: 3008 Loss: 1.605865478515625 Time taken: 0.3216538429260254\n",
            "Batch Number: 3009 Loss: 1.610489010810852 Time taken: 0.2939720153808594\n",
            "Batch Number: 3010 Loss: 1.6142011880874634 Time taken: 0.297260046005249\n",
            "Batch Number: 3011 Loss: 1.643275260925293 Time taken: 0.323408842086792\n",
            "Batch Number: 3012 Loss: 1.5994218587875366 Time taken: 0.29319286346435547\n",
            "Batch Number: 3013 Loss: 1.6143090724945068 Time taken: 0.2945883274078369\n",
            "Batch Number: 3014 Loss: 1.6273386478424072 Time taken: 0.3042733669281006\n",
            "Batch Number: 3015 Loss: 1.61335027217865 Time taken: 0.30141305923461914\n",
            "Batch Number: 3016 Loss: 1.5775697231292725 Time taken: 0.29390621185302734\n",
            "Batch Number: 3017 Loss: 1.6246285438537598 Time taken: 0.29857563972473145\n",
            "Batch Number: 3018 Loss: 1.575417160987854 Time taken: 0.31626152992248535\n",
            "Batch Number: 3019 Loss: 1.6414077281951904 Time taken: 0.29522705078125\n",
            "Batch Number: 3020 Loss: 1.599129557609558 Time taken: 0.29148244857788086\n",
            "Batch Number: 3021 Loss: 1.6477413177490234 Time taken: 0.31653499603271484\n",
            "Batch Number: 3022 Loss: 1.6316280364990234 Time taken: 0.2907116413116455\n",
            "Batch Number: 3023 Loss: 1.654213786125183 Time taken: 0.29610204696655273\n",
            "Batch Number: 3024 Loss: 1.6388585567474365 Time taken: 0.3252108097076416\n",
            "Batch Number: 3025 Loss: 1.648815631866455 Time taken: 0.3737208843231201\n",
            "Batch Number: 3026 Loss: 1.627984881401062 Time taken: 0.35834717750549316\n",
            "Batch Number: 3027 Loss: 1.6138756275177002 Time taken: 0.3616344928741455\n",
            "Batch Number: 3028 Loss: 1.6090774536132812 Time taken: 0.3641519546508789\n",
            "Batch Number: 3029 Loss: 1.602236032485962 Time taken: 0.38851189613342285\n",
            "Batch Number: 3030 Loss: 1.5990498065948486 Time taken: 0.3157327175140381\n",
            "Batch Number: 3031 Loss: 1.6032756567001343 Time taken: 0.30356454849243164\n",
            "Batch Number: 3032 Loss: 1.5941890478134155 Time taken: 0.3014945983886719\n",
            "Batch Number: 3033 Loss: 1.6044867038726807 Time taken: 0.3164675235748291\n",
            "Batch Number: 3034 Loss: 1.5987361669540405 Time taken: 0.2967996597290039\n",
            "Batch Number: 3035 Loss: 1.6129118204116821 Time taken: 0.31273508071899414\n",
            "Batch Number: 3036 Loss: 1.6013412475585938 Time taken: 0.3050556182861328\n",
            "Batch Number: 3037 Loss: 1.625298261642456 Time taken: 0.30772972106933594\n",
            "Batch Number: 3038 Loss: 1.6258492469787598 Time taken: 0.2859179973602295\n",
            "Batch Number: 3039 Loss: 1.6513158082962036 Time taken: 0.3136868476867676\n",
            "Batch Number: 3040 Loss: 1.6262508630752563 Time taken: 0.30709314346313477\n",
            "Batch Number: 3041 Loss: 1.6020337343215942 Time taken: 0.2910921573638916\n",
            "Batch Number: 3042 Loss: 1.6098580360412598 Time taken: 0.3036036491394043\n",
            "Batch Number: 3043 Loss: 1.6104307174682617 Time taken: 0.3753697872161865\n",
            "Batch Number: 3044 Loss: 1.6364719867706299 Time taken: 0.37297582626342773\n",
            "Batch Number: 3045 Loss: 1.6266615390777588 Time taken: 0.3175685405731201\n",
            "Batch Number: 3046 Loss: 1.6290032863616943 Time taken: 0.35297298431396484\n",
            "Batch Number: 3047 Loss: 1.5846327543258667 Time taken: 0.35887956619262695\n",
            "Batch Number: 3048 Loss: 1.5957896709442139 Time taken: 0.3440287113189697\n",
            "Batch Number: 3049 Loss: 1.6117966175079346 Time taken: 0.3096487522125244\n",
            "Batch Number: 3050 Loss: 1.5864598751068115 Time taken: 0.2937815189361572\n",
            "Batch Number: 3051 Loss: 1.6108448505401611 Time taken: 0.29952335357666016\n",
            "Batch Number: 3052 Loss: 1.6027745008468628 Time taken: 0.30092644691467285\n",
            "Batch Number: 3053 Loss: 1.5840067863464355 Time taken: 0.2988128662109375\n",
            "Batch Number: 3054 Loss: 1.598387598991394 Time taken: 0.3074631690979004\n",
            "Batch Number: 3055 Loss: 1.5857058763504028 Time taken: 0.30261778831481934\n",
            "Batch Number: 3056 Loss: 1.5954850912094116 Time taken: 0.3302042484283447\n",
            "Batch Number: 3057 Loss: 1.6130489110946655 Time taken: 0.29035329818725586\n",
            "Batch Number: 3058 Loss: 1.6080272197723389 Time taken: 0.30584073066711426\n",
            "Batch Number: 3059 Loss: 1.5907598733901978 Time taken: 0.3141047954559326\n",
            "Batch Number: 3060 Loss: 1.6085846424102783 Time taken: 0.30024194717407227\n",
            "Batch Number: 3061 Loss: 1.6142593622207642 Time taken: 0.3696279525756836\n",
            "Batch Number: 3062 Loss: 1.5830281972885132 Time taken: 0.3976712226867676\n",
            "Batch Number: 3063 Loss: 1.566595196723938 Time taken: 0.35455751419067383\n",
            "Batch Number: 3064 Loss: 1.5871762037277222 Time taken: 0.31487441062927246\n",
            "Batch Number: 3065 Loss: 1.5597763061523438 Time taken: 0.3043205738067627\n",
            "Batch Number: 3066 Loss: 1.5812804698944092 Time taken: 0.30938220024108887\n",
            "Batch Number: 3067 Loss: 1.5782523155212402 Time taken: 0.30014657974243164\n",
            "Batch Number: 3068 Loss: 1.571005940437317 Time taken: 0.3071932792663574\n",
            "Batch Number: 3069 Loss: 1.5462175607681274 Time taken: 0.30342745780944824\n",
            "Batch Number: 3070 Loss: 1.5640122890472412 Time taken: 0.30323052406311035\n",
            "Batch Number: 3071 Loss: 1.5817458629608154 Time taken: 0.30755162239074707\n",
            "Batch Number: 3072 Loss: 1.57758629322052 Time taken: 0.31016039848327637\n",
            "Batch Number: 3073 Loss: 1.5956028699874878 Time taken: 0.32503628730773926\n",
            "Batch Number: 3074 Loss: 1.6135259866714478 Time taken: 0.305560827255249\n",
            "Batch Number: 3075 Loss: 1.584525465965271 Time taken: 0.29404187202453613\n",
            "Batch Number: 3076 Loss: 1.5945541858673096 Time taken: 0.3074920177459717\n",
            "Batch Number: 3077 Loss: 1.5907832384109497 Time taken: 0.38138341903686523\n",
            "Batch Number: 3078 Loss: 1.5704643726348877 Time taken: 0.3597393035888672\n",
            "Batch Number: 3079 Loss: 1.563971996307373 Time taken: 0.32666563987731934\n",
            "Batch Number: 3080 Loss: 1.5806479454040527 Time taken: 0.3560013771057129\n",
            "Batch Number: 3081 Loss: 1.5840637683868408 Time taken: 0.3582444190979004\n",
            "Batch Number: 3082 Loss: 1.5662113428115845 Time taken: 0.35164952278137207\n",
            "Batch Number: 3083 Loss: 1.5751625299453735 Time taken: 0.373870849609375\n",
            "Batch Number: 3084 Loss: 1.5889207124710083 Time taken: 0.36151552200317383\n",
            "Batch Number: 3085 Loss: 1.5824942588806152 Time taken: 0.3004140853881836\n",
            "Batch Number: 3086 Loss: 1.61090886592865 Time taken: 0.3034350872039795\n",
            "Batch Number: 3087 Loss: 1.5995194911956787 Time taken: 0.29633593559265137\n",
            "Batch Number: 3088 Loss: 1.6003856658935547 Time taken: 0.30997538566589355\n",
            "Batch Number: 3089 Loss: 1.5772794485092163 Time taken: 0.3018367290496826\n",
            "Batch Number: 3090 Loss: 1.6020351648330688 Time taken: 0.30747008323669434\n",
            "Batch Number: 3091 Loss: 1.5951855182647705 Time taken: 0.3181610107421875\n",
            "Batch Number: 3092 Loss: 1.5895094871520996 Time taken: 0.2953481674194336\n",
            "Batch Number: 3093 Loss: 1.5735416412353516 Time taken: 0.3086256980895996\n",
            "Batch Number: 3094 Loss: 1.6024872064590454 Time taken: 0.31653261184692383\n",
            "Batch Number: 3095 Loss: 1.5692166090011597 Time taken: 0.29784488677978516\n",
            "Batch Number: 3096 Loss: 1.5924429893493652 Time taken: 0.3053905963897705\n",
            "Batch Number: 3097 Loss: 1.5832059383392334 Time taken: 0.3028433322906494\n",
            "Batch Number: 3098 Loss: 1.628552794456482 Time taken: 0.29868125915527344\n",
            "Batch Number: 3099 Loss: 1.626282811164856 Time taken: 0.2959566116333008\n",
            "Batch Number: 3100 Loss: 1.5911859273910522 Time taken: 0.29059839248657227\n",
            "Batch Number: 3101 Loss: 1.6141982078552246 Time taken: 0.2892909049987793\n",
            "Batch Number: 3102 Loss: 1.590277075767517 Time taken: 0.3021738529205322\n",
            "Batch Number: 3103 Loss: 1.598291039466858 Time taken: 0.3319435119628906\n",
            "Batch Number: 3104 Loss: 1.5835731029510498 Time taken: 0.3099989891052246\n",
            "Batch Number: 3105 Loss: 1.5915707349777222 Time taken: 0.2991485595703125\n",
            "Batch Number: 3106 Loss: 1.5807147026062012 Time taken: 0.30315589904785156\n",
            "Batch Number: 3107 Loss: 1.5922558307647705 Time taken: 0.2946450710296631\n",
            "Batch Number: 3108 Loss: 1.588430404663086 Time taken: 0.3127713203430176\n",
            "Batch Number: 3109 Loss: 1.574101448059082 Time taken: 0.30448341369628906\n",
            "Batch Number: 3110 Loss: 1.5794495344161987 Time taken: 0.31780076026916504\n",
            "Batch Number: 3111 Loss: 1.5835344791412354 Time taken: 0.3642728328704834\n",
            "Batch Number: 3112 Loss: 1.594075083732605 Time taken: 0.3751795291900635\n",
            "Batch Number: 3113 Loss: 1.568300485610962 Time taken: 0.322310209274292\n",
            "Batch Number: 3114 Loss: 1.5604913234710693 Time taken: 0.3709132671356201\n",
            "Batch Number: 3115 Loss: 1.5727647542953491 Time taken: 0.3833765983581543\n",
            "Batch Number: 3116 Loss: 1.5986071825027466 Time taken: 0.31621503829956055\n",
            "Batch Number: 3117 Loss: 1.5778529644012451 Time taken: 0.2959742546081543\n",
            "Batch Number: 3118 Loss: 1.5778487920761108 Time taken: 0.2949504852294922\n",
            "Batch Number: 3119 Loss: 1.5785092115402222 Time taken: 0.2901909351348877\n",
            "Batch Number: 3120 Loss: 1.5874860286712646 Time taken: 0.31036853790283203\n",
            "Batch Number: 3121 Loss: 1.5684279203414917 Time taken: 0.3736405372619629\n",
            "Batch Number: 3122 Loss: 1.5859529972076416 Time taken: 0.3405177593231201\n",
            "Batch Number: 3123 Loss: 1.5770434141159058 Time taken: 0.36429738998413086\n",
            "Batch Number: 3124 Loss: 1.5785704851150513 Time taken: 0.3715784549713135\n",
            "Batch Number: 3125 Loss: 1.606827735900879 Time taken: 0.2951505184173584\n",
            "Batch Number: 3126 Loss: 1.5864888429641724 Time taken: 0.3185694217681885\n",
            "Batch Number: 3127 Loss: 1.5864393711090088 Time taken: 0.3080730438232422\n",
            "Batch Number: 3128 Loss: 1.597137451171875 Time taken: 0.3558375835418701\n",
            "Batch Number: 3129 Loss: 1.5834695100784302 Time taken: 0.37220335006713867\n",
            "Batch Number: 3130 Loss: 1.6098299026489258 Time taken: 0.3515334129333496\n",
            "Batch Number: 3131 Loss: 1.5856927633285522 Time taken: 0.2932605743408203\n",
            "Batch Number: 3132 Loss: 1.6106541156768799 Time taken: 0.30283451080322266\n",
            "Batch Number: 3133 Loss: 1.6166857481002808 Time taken: 0.29866528511047363\n",
            "Batch Number: 3134 Loss: 1.6081713438034058 Time taken: 0.29987311363220215\n",
            "Batch Number: 3135 Loss: 1.6297520399093628 Time taken: 0.2868363857269287\n",
            "Batch Number: 3136 Loss: 1.6145384311676025 Time taken: 0.29390716552734375\n",
            "Batch Number: 3137 Loss: 1.6073200702667236 Time taken: 0.30985045433044434\n",
            "Batch Number: 3138 Loss: 1.5940649509429932 Time taken: 0.2866814136505127\n",
            "Batch Number: 3139 Loss: 1.5947870016098022 Time taken: 0.29962825775146484\n",
            "Batch Number: 3140 Loss: 1.5835376977920532 Time taken: 0.3296627998352051\n",
            "Batch Number: 3141 Loss: 1.5751540660858154 Time taken: 0.28644442558288574\n",
            "Batch Number: 3142 Loss: 1.5956119298934937 Time taken: 0.3048703670501709\n",
            "Batch Number: 3143 Loss: 1.585219144821167 Time taken: 0.31237149238586426\n",
            "Batch Number: 3144 Loss: 1.5422712564468384 Time taken: 0.28916311264038086\n",
            "Batch Number: 3145 Loss: 1.5831013917922974 Time taken: 0.3013172149658203\n",
            "Batch Number: 3146 Loss: 1.5439484119415283 Time taken: 0.3042261600494385\n",
            "Batch Number: 3147 Loss: 1.5687642097473145 Time taken: 0.3157029151916504\n",
            "Batch Number: 3148 Loss: 1.5688533782958984 Time taken: 0.2958643436431885\n",
            "Batch Number: 3149 Loss: 1.5818263292312622 Time taken: 0.282412052154541\n",
            "Batch Number: 3150 Loss: 1.568090558052063 Time taken: 0.3065979480743408\n",
            "Batch Number: 3151 Loss: 1.5804433822631836 Time taken: 0.29908013343811035\n",
            "Batch Number: 3152 Loss: 1.5961551666259766 Time taken: 0.3106386661529541\n",
            "Batch Number: 3153 Loss: 1.5920195579528809 Time taken: 0.29759931564331055\n",
            "Batch Number: 3154 Loss: 1.5991162061691284 Time taken: 0.28851747512817383\n",
            "Batch Number: 3155 Loss: 1.6239397525787354 Time taken: 0.292452335357666\n",
            "Batch Number: 3156 Loss: 1.6300047636032104 Time taken: 0.30217432975769043\n",
            "Batch Number: 3157 Loss: 1.5988061428070068 Time taken: 0.3090810775756836\n",
            "Batch Number: 3158 Loss: 1.6277920007705688 Time taken: 0.2937614917755127\n",
            "Batch Number: 3159 Loss: 1.5771563053131104 Time taken: 0.2756965160369873\n",
            "Batch Number: 3160 Loss: 1.6112369298934937 Time taken: 0.4102442264556885\n",
            "Batch Number: 3161 Loss: 1.5887168645858765 Time taken: 0.3766140937805176\n",
            "Batch Number: 3162 Loss: 1.626275897026062 Time taken: 0.3419923782348633\n",
            "Batch Number: 3163 Loss: 1.6088932752609253 Time taken: 0.30896925926208496\n",
            "Batch Number: 3164 Loss: 1.609241008758545 Time taken: 0.2901442050933838\n",
            "Batch Number: 3165 Loss: 1.6284363269805908 Time taken: 0.2761712074279785\n",
            "Batch Number: 3166 Loss: 1.616342544555664 Time taken: 0.29616403579711914\n",
            "Batch Number: 3167 Loss: 1.6433439254760742 Time taken: 0.2853739261627197\n",
            "Batch Number: 3168 Loss: 1.6256787776947021 Time taken: 0.3004889488220215\n",
            "Batch Number: 3169 Loss: 1.6392834186553955 Time taken: 0.3021268844604492\n",
            "Batch Number: 3170 Loss: 1.6313272714614868 Time taken: 0.3065500259399414\n",
            "Batch Number: 3171 Loss: 1.648700475692749 Time taken: 0.29584813117980957\n",
            "Batch Number: 3172 Loss: 1.615883469581604 Time taken: 0.299694299697876\n",
            "Batch Number: 3173 Loss: 1.615709662437439 Time taken: 0.31266307830810547\n",
            "Batch Number: 3174 Loss: 1.6364666223526 Time taken: 0.3026266098022461\n",
            "Batch Number: 3175 Loss: 1.6305897235870361 Time taken: 0.2934439182281494\n",
            "Batch Number: 3176 Loss: 1.5837525129318237 Time taken: 0.2957477569580078\n",
            "Batch Number: 3177 Loss: 1.6234568357467651 Time taken: 0.29631543159484863\n",
            "Batch Number: 3178 Loss: 1.6023074388504028 Time taken: 0.29720568656921387\n",
            "Batch Number: 3179 Loss: 1.6097631454467773 Time taken: 0.30952930450439453\n",
            "Batch Number: 3180 Loss: 1.6153982877731323 Time taken: 0.30028510093688965\n",
            "Batch Number: 3181 Loss: 1.643535852432251 Time taken: 0.29363131523132324\n",
            "Batch Number: 3182 Loss: 1.603134274482727 Time taken: 0.31462574005126953\n",
            "Batch Number: 3183 Loss: 1.6367385387420654 Time taken: 0.2982628345489502\n",
            "Batch Number: 3184 Loss: 1.5767818689346313 Time taken: 0.29891228675842285\n",
            "Batch Number: 3185 Loss: 1.587119698524475 Time taken: 0.33554649353027344\n",
            "Batch Number: 3186 Loss: 1.5981690883636475 Time taken: 0.3050577640533447\n",
            "Batch Number: 3187 Loss: 1.6051344871520996 Time taken: 0.29707980155944824\n",
            "Batch Number: 3188 Loss: 1.612829327583313 Time taken: 0.30943775177001953\n",
            "Batch Number: 3189 Loss: 1.6146830320358276 Time taken: 0.30158257484436035\n",
            "Batch Number: 3190 Loss: 1.6137230396270752 Time taken: 0.3027677536010742\n",
            "Batch Number: 3191 Loss: 1.614996075630188 Time taken: 0.30769991874694824\n",
            "Batch Number: 3192 Loss: 1.6079868078231812 Time taken: 0.32631516456604004\n",
            "Batch Number: 3193 Loss: 1.5943489074707031 Time taken: 0.31469106674194336\n",
            "Batch Number: 3194 Loss: 1.5798879861831665 Time taken: 0.32959413528442383\n",
            "Batch Number: 3195 Loss: 1.5585497617721558 Time taken: 0.327725887298584\n",
            "Batch Number: 3196 Loss: 1.5782833099365234 Time taken: 0.30628180503845215\n",
            "Batch Number: 3197 Loss: 1.5898990631103516 Time taken: 0.2999911308288574\n",
            "Batch Number: 3198 Loss: 1.5666478872299194 Time taken: 0.3241751194000244\n",
            "Batch Number: 3199 Loss: 1.5724451541900635 Time taken: 0.31642866134643555\n",
            "Batch Number: 3200 Loss: 1.612317442893982 Time taken: 0.3023402690887451\n",
            "Batch Number: 3201 Loss: 1.628102421760559 Time taken: 0.3184390068054199\n",
            "Batch Number: 3202 Loss: 1.6169788837432861 Time taken: 0.31044602394104004\n",
            "Batch Number: 3203 Loss: 1.6194828748703003 Time taken: 0.36830925941467285\n",
            "Batch Number: 3204 Loss: 1.6013154983520508 Time taken: 0.34339475631713867\n",
            "Batch Number: 3205 Loss: 1.5877728462219238 Time taken: 0.3783915042877197\n",
            "Batch Number: 3206 Loss: 1.6362634897232056 Time taken: 0.3623688220977783\n",
            "Batch Number: 3207 Loss: 1.5599331855773926 Time taken: 0.2943744659423828\n",
            "Batch Number: 3208 Loss: 1.552751898765564 Time taken: 0.29571104049682617\n",
            "Batch Number: 3209 Loss: 1.6091690063476562 Time taken: 0.2940695285797119\n",
            "Batch Number: 3210 Loss: 1.5773731470108032 Time taken: 0.2821078300476074\n",
            "Batch Number: 3211 Loss: 1.605847716331482 Time taken: 0.28972935676574707\n",
            "Batch Number: 3212 Loss: 1.589658498764038 Time taken: 0.30420899391174316\n",
            "Batch Number: 3213 Loss: 1.586235761642456 Time taken: 0.30950307846069336\n",
            "Batch Number: 3214 Loss: 1.615478515625 Time taken: 0.3017544746398926\n",
            "Batch Number: 3215 Loss: 1.605568766593933 Time taken: 0.3107309341430664\n",
            "Batch Number: 3216 Loss: 1.5842509269714355 Time taken: 0.30325937271118164\n",
            "Batch Number: 3217 Loss: 1.6113373041152954 Time taken: 0.336317777633667\n",
            "Batch Number: 3218 Loss: 1.5942370891571045 Time taken: 0.3754401206970215\n",
            "Batch Number: 3219 Loss: 1.5972744226455688 Time taken: 0.3610062599182129\n",
            "Batch Number: 3220 Loss: 1.5795727968215942 Time taken: 0.2843313217163086\n",
            "Batch Number: 3221 Loss: 1.5932567119598389 Time taken: 0.3016629219055176\n",
            "Batch Number: 3222 Loss: 1.5915335416793823 Time taken: 0.3397808074951172\n",
            "Batch Number: 3223 Loss: 1.6184478998184204 Time taken: 0.30126285552978516\n",
            "Batch Number: 3224 Loss: 1.5858025550842285 Time taken: 0.30116891860961914\n",
            "Batch Number: 3225 Loss: 1.6041135787963867 Time taken: 0.2953479290008545\n",
            "Batch Number: 3226 Loss: 1.5932198762893677 Time taken: 0.31200456619262695\n",
            "Batch Number: 3227 Loss: 1.6198943853378296 Time taken: 0.3250558376312256\n",
            "Batch Number: 3228 Loss: 1.6253565549850464 Time taken: 0.3263823986053467\n",
            "Batch Number: 3229 Loss: 1.6182053089141846 Time taken: 0.368366003036499\n",
            "Batch Number: 3230 Loss: 1.5982578992843628 Time taken: 0.382779598236084\n",
            "Batch Number: 3231 Loss: 1.6185417175292969 Time taken: 0.3657960891723633\n",
            "Batch Number: 3232 Loss: 1.6181586980819702 Time taken: 0.2996668815612793\n",
            "Batch Number: 3233 Loss: 1.602973222732544 Time taken: 0.30501461029052734\n",
            "Batch Number: 3234 Loss: 1.595456838607788 Time taken: 0.30013203620910645\n",
            "Batch Number: 3235 Loss: 1.5785408020019531 Time taken: 0.3107423782348633\n",
            "Batch Number: 3236 Loss: 1.5801968574523926 Time taken: 0.29814958572387695\n",
            "Batch Number: 3237 Loss: 1.5969398021697998 Time taken: 0.3103752136230469\n",
            "Batch Number: 3238 Loss: 1.5850762128829956 Time taken: 0.3025493621826172\n",
            "Batch Number: 3239 Loss: 1.5775692462921143 Time taken: 0.2935056686401367\n",
            "Batch Number: 3240 Loss: 1.5706532001495361 Time taken: 0.3000659942626953\n",
            "Batch Number: 3241 Loss: 1.5866189002990723 Time taken: 0.3020663261413574\n",
            "Batch Number: 3242 Loss: 1.5710327625274658 Time taken: 0.3117406368255615\n",
            "Batch Number: 3243 Loss: 1.575481653213501 Time taken: 0.3224668502807617\n",
            "Batch Number: 3244 Loss: 1.5478225946426392 Time taken: 0.3108394145965576\n",
            "Batch Number: 3245 Loss: 1.545058012008667 Time taken: 0.3005077838897705\n",
            "Batch Number: 3246 Loss: 1.54140043258667 Time taken: 0.3023679256439209\n",
            "Batch Number: 3247 Loss: 1.563425898551941 Time taken: 0.30482029914855957\n",
            "Batch Number: 3248 Loss: 1.566895604133606 Time taken: 0.2894284725189209\n",
            "Batch Number: 3249 Loss: 1.5581122636795044 Time taken: 0.2879617214202881\n",
            "Batch Number: 3250 Loss: 1.5530807971954346 Time taken: 0.29593682289123535\n",
            "Batch Number: 3251 Loss: 1.5384646654129028 Time taken: 0.3020143508911133\n",
            "Batch Number: 3252 Loss: 1.5863182544708252 Time taken: 0.2843170166015625\n",
            "Batch Number: 3253 Loss: 1.5888245105743408 Time taken: 0.3529036045074463\n",
            "Batch Number: 3254 Loss: 1.5974376201629639 Time taken: 0.38857197761535645\n",
            "Batch Number: 3255 Loss: 1.5612001419067383 Time taken: 0.3755326271057129\n",
            "Batch Number: 3256 Loss: 1.5704647302627563 Time taken: 0.3655362129211426\n",
            "Batch Number: 3257 Loss: 1.5814542770385742 Time taken: 0.3060038089752197\n",
            "Batch Number: 3258 Loss: 1.5691639184951782 Time taken: 0.3671250343322754\n",
            "Batch Number: 3259 Loss: 1.5543822050094604 Time taken: 0.38976383209228516\n",
            "Batch Number: 3260 Loss: 1.5456790924072266 Time taken: 0.29747891426086426\n",
            "Batch Number: 3261 Loss: 1.5279353857040405 Time taken: 0.3110370635986328\n",
            "Batch Number: 3262 Loss: 1.5569909811019897 Time taken: 0.31355905532836914\n",
            "Batch Number: 3263 Loss: 1.556807041168213 Time taken: 0.2984433174133301\n",
            "Batch Number: 3264 Loss: 1.5776371955871582 Time taken: 0.29923129081726074\n",
            "Batch Number: 3265 Loss: 1.5768250226974487 Time taken: 0.32737302780151367\n",
            "Batch Number: 3266 Loss: 1.5602760314941406 Time taken: 0.3005242347717285\n",
            "Batch Number: 3267 Loss: 1.5688587427139282 Time taken: 0.30118322372436523\n",
            "Batch Number: 3268 Loss: 1.5682870149612427 Time taken: 0.3047146797180176\n",
            "Batch Number: 3269 Loss: 1.566694736480713 Time taken: 0.3091108798980713\n",
            "Batch Number: 3270 Loss: 1.5498988628387451 Time taken: 0.306394100189209\n",
            "Batch Number: 3271 Loss: 1.5587900876998901 Time taken: 0.2932295799255371\n",
            "Batch Number: 3272 Loss: 1.6078191995620728 Time taken: 0.3853631019592285\n",
            "Batch Number: 3273 Loss: 1.5412559509277344 Time taken: 0.36894965171813965\n",
            "Batch Number: 3274 Loss: 1.577435851097107 Time taken: 0.36141085624694824\n",
            "Batch Number: 3275 Loss: 1.56032395362854 Time taken: 0.2895197868347168\n",
            "Batch Number: 3276 Loss: 1.5777201652526855 Time taken: 0.295745849609375\n",
            "Batch Number: 3277 Loss: 1.5859870910644531 Time taken: 0.2946639060974121\n",
            "Batch Number: 3278 Loss: 1.6044082641601562 Time taken: 0.34040307998657227\n",
            "Batch Number: 3279 Loss: 1.5897103548049927 Time taken: 0.2969961166381836\n",
            "Batch Number: 3280 Loss: 1.5976649522781372 Time taken: 0.35405421257019043\n",
            "Batch Number: 3281 Loss: 1.6232786178588867 Time taken: 0.32896900177001953\n",
            "Batch Number: 3282 Loss: 1.5519973039627075 Time taken: 0.2953178882598877\n",
            "Batch Number: 3283 Loss: 1.596239686012268 Time taken: 0.36385416984558105\n",
            "Batch Number: 3284 Loss: 1.5758965015411377 Time taken: 0.3892381191253662\n",
            "Batch Number: 3285 Loss: 1.585093379020691 Time taken: 0.36863040924072266\n",
            "Batch Number: 3286 Loss: 1.5314791202545166 Time taken: 0.3486592769622803\n",
            "Batch Number: 3287 Loss: 1.5776185989379883 Time taken: 0.3205230236053467\n",
            "Batch Number: 3288 Loss: 1.5792230367660522 Time taken: 0.3023707866668701\n",
            "Batch Number: 3289 Loss: 1.5487092733383179 Time taken: 0.38153576850891113\n",
            "Batch Number: 3290 Loss: 1.5763753652572632 Time taken: 0.34995532035827637\n",
            "Batch Number: 3291 Loss: 1.5779980421066284 Time taken: 0.40099239349365234\n",
            "Batch Number: 3292 Loss: 1.5871115922927856 Time taken: 0.3275129795074463\n",
            "Batch Number: 3293 Loss: 1.5533422231674194 Time taken: 0.30863094329833984\n",
            "Batch Number: 3294 Loss: 1.5469413995742798 Time taken: 0.30051374435424805\n",
            "Batch Number: 3295 Loss: 1.5441327095031738 Time taken: 0.3020961284637451\n",
            "Batch Number: 3296 Loss: 1.5711873769760132 Time taken: 0.31012773513793945\n",
            "Batch Number: 3297 Loss: 1.568921446800232 Time taken: 0.286027193069458\n",
            "Batch Number: 3298 Loss: 1.5514514446258545 Time taken: 0.3023078441619873\n",
            "Batch Number: 3299 Loss: 1.5483022928237915 Time taken: 0.3113570213317871\n",
            "Batch Number: 3300 Loss: 1.5622836351394653 Time taken: 0.2974538803100586\n",
            "Batch Number: 3301 Loss: 1.568583607673645 Time taken: 0.29813098907470703\n",
            "Batch Number: 3302 Loss: 1.5541402101516724 Time taken: 0.31439876556396484\n",
            "Batch Number: 3303 Loss: 1.5290902853012085 Time taken: 0.2940645217895508\n",
            "Batch Number: 3304 Loss: 1.562721610069275 Time taken: 0.28572607040405273\n",
            "Batch Number: 3305 Loss: 1.5620137453079224 Time taken: 0.3023862838745117\n",
            "Batch Number: 3306 Loss: 1.5537118911743164 Time taken: 0.3017861843109131\n",
            "Batch Number: 3307 Loss: 1.584883451461792 Time taken: 0.30231261253356934\n",
            "Batch Number: 3308 Loss: 1.5429812669754028 Time taken: 0.2994093894958496\n",
            "Batch Number: 3309 Loss: 1.549570083618164 Time taken: 0.31371569633483887\n",
            "Batch Number: 3310 Loss: 1.561187744140625 Time taken: 0.3088717460632324\n",
            "Batch Number: 3311 Loss: 1.5936862230300903 Time taken: 0.30463671684265137\n",
            "Batch Number: 3312 Loss: 1.5801048278808594 Time taken: 0.38696813583374023\n",
            "Batch Number: 3313 Loss: 1.578533411026001 Time taken: 0.3663654327392578\n",
            "Batch Number: 3314 Loss: 1.5878510475158691 Time taken: 0.3498985767364502\n",
            "Batch Number: 3315 Loss: 1.5718587636947632 Time taken: 0.3159182071685791\n",
            "Batch Number: 3316 Loss: 1.5993343591690063 Time taken: 0.30367064476013184\n",
            "Batch Number: 3317 Loss: 1.5740844011306763 Time taken: 0.30562448501586914\n",
            "Batch Number: 3318 Loss: 1.5724025964736938 Time taken: 0.3177173137664795\n",
            "Batch Number: 3319 Loss: 1.5795336961746216 Time taken: 0.2902407646179199\n",
            "Batch Number: 3320 Loss: 1.5527820587158203 Time taken: 0.301804780960083\n",
            "Batch Number: 3321 Loss: 1.5806527137756348 Time taken: 0.3059055805206299\n",
            "Batch Number: 3322 Loss: 1.5810532569885254 Time taken: 0.2933473587036133\n",
            "Batch Number: 3323 Loss: 1.5778480768203735 Time taken: 0.298844575881958\n",
            "Batch Number: 3324 Loss: 1.521155595779419 Time taken: 0.32895946502685547\n",
            "Batch Number: 3325 Loss: 1.5733435153961182 Time taken: 0.3160984516143799\n",
            "Batch Number: 3326 Loss: 1.5713502168655396 Time taken: 0.303333044052124\n",
            "Batch Number: 3327 Loss: 1.5770105123519897 Time taken: 0.34056806564331055\n",
            "Batch Number: 3328 Loss: 1.5786348581314087 Time taken: 0.40396785736083984\n",
            "Batch Number: 3329 Loss: 1.571686863899231 Time taken: 0.3576793670654297\n",
            "Batch Number: 3330 Loss: 1.59010648727417 Time taken: 0.3224520683288574\n",
            "Batch Number: 3331 Loss: 1.5863364934921265 Time taken: 0.29822707176208496\n",
            "Batch Number: 3332 Loss: 1.5991528034210205 Time taken: 0.31383514404296875\n",
            "Batch Number: 3333 Loss: 1.5898363590240479 Time taken: 0.30616140365600586\n",
            "Batch Number: 3334 Loss: 1.5674102306365967 Time taken: 0.33278870582580566\n",
            "Batch Number: 3335 Loss: 1.5849494934082031 Time taken: 0.3164980411529541\n",
            "Batch Number: 3336 Loss: 1.6142606735229492 Time taken: 0.30126094818115234\n",
            "Batch Number: 3337 Loss: 1.5903496742248535 Time taken: 0.32173919677734375\n",
            "Batch Number: 3338 Loss: 1.617775559425354 Time taken: 0.2912740707397461\n",
            "Batch Number: 3339 Loss: 1.606095790863037 Time taken: 0.31214189529418945\n",
            "Batch Number: 3340 Loss: 1.589321255683899 Time taken: 0.31592345237731934\n",
            "Batch Number: 3341 Loss: 1.5970090627670288 Time taken: 0.2923293113708496\n",
            "Batch Number: 3342 Loss: 1.59151291847229 Time taken: 0.3227360248565674\n",
            "Batch Number: 3343 Loss: 1.599539041519165 Time taken: 0.3178529739379883\n",
            "Batch Number: 3344 Loss: 1.5982849597930908 Time taken: 0.3053414821624756\n",
            "Batch Number: 3345 Loss: 1.6102604866027832 Time taken: 0.31397271156311035\n",
            "Batch Number: 3346 Loss: 1.6487476825714111 Time taken: 0.3116140365600586\n",
            "Batch Number: 3347 Loss: 1.607759714126587 Time taken: 0.3151743412017822\n",
            "Batch Number: 3348 Loss: 1.6081260442733765 Time taken: 0.2962162494659424\n",
            "Batch Number: 3349 Loss: 1.6287906169891357 Time taken: 0.3107461929321289\n",
            "Batch Number: 3350 Loss: 1.617097020149231 Time taken: 0.3129708766937256\n",
            "Batch Number: 3351 Loss: 1.6181628704071045 Time taken: 0.29779720306396484\n",
            "Batch Number: 3352 Loss: 1.6020727157592773 Time taken: 0.2891731262207031\n",
            "Batch Number: 3353 Loss: 1.605013132095337 Time taken: 0.31926584243774414\n",
            "Batch Number: 3354 Loss: 1.6236486434936523 Time taken: 0.2956974506378174\n",
            "Batch Number: 3355 Loss: 1.5846107006072998 Time taken: 0.3024265766143799\n",
            "Batch Number: 3356 Loss: 1.5967583656311035 Time taken: 0.31511521339416504\n",
            "Batch Number: 3357 Loss: 1.582257866859436 Time taken: 0.31864380836486816\n",
            "Batch Number: 3358 Loss: 1.5940310955047607 Time taken: 0.30980920791625977\n",
            "Batch Number: 3359 Loss: 1.584386944770813 Time taken: 0.30153512954711914\n",
            "Batch Number: 3360 Loss: 1.5922611951828003 Time taken: 0.3153975009918213\n",
            "Batch Number: 3361 Loss: 1.587839961051941 Time taken: 0.30861902236938477\n",
            "Batch Number: 3362 Loss: 1.6003278493881226 Time taken: 0.3034965991973877\n",
            "Batch Number: 3363 Loss: 1.5860224962234497 Time taken: 0.32706546783447266\n",
            "Batch Number: 3364 Loss: 1.5791442394256592 Time taken: 0.346005916595459\n",
            "Batch Number: 3365 Loss: 1.5834530591964722 Time taken: 0.29022717475891113\n",
            "Batch Number: 3366 Loss: 1.5708377361297607 Time taken: 0.3042275905609131\n",
            "Batch Number: 3367 Loss: 1.5696625709533691 Time taken: 0.30872011184692383\n",
            "Batch Number: 3368 Loss: 1.559869647026062 Time taken: 0.2885136604309082\n",
            "Batch Number: 3369 Loss: 1.5592234134674072 Time taken: 0.30693650245666504\n",
            "Batch Number: 3370 Loss: 1.574164628982544 Time taken: 0.3773822784423828\n",
            "Batch Number: 3371 Loss: 1.5909004211425781 Time taken: 0.32861948013305664\n",
            "Batch Number: 3372 Loss: 1.5680023431777954 Time taken: 0.31784558296203613\n",
            "Batch Number: 3373 Loss: 1.577089548110962 Time taken: 0.33063244819641113\n",
            "Batch Number: 3374 Loss: 1.5733263492584229 Time taken: 0.34768128395080566\n",
            "Batch Number: 3375 Loss: 1.5770900249481201 Time taken: 0.3921828269958496\n",
            "Batch Number: 3376 Loss: 1.5669502019882202 Time taken: 0.3109285831451416\n",
            "Batch Number: 3377 Loss: 1.5448930263519287 Time taken: 0.2989058494567871\n",
            "Batch Number: 3378 Loss: 1.5371019840240479 Time taken: 0.36114978790283203\n",
            "Batch Number: 3379 Loss: 1.5680011510849 Time taken: 0.32877635955810547\n",
            "Batch Number: 3380 Loss: 1.5942643880844116 Time taken: 0.29300880432128906\n",
            "Batch Number: 3381 Loss: 1.570816159248352 Time taken: 0.31253671646118164\n",
            "Batch Number: 3382 Loss: 1.6150999069213867 Time taken: 0.30562710762023926\n",
            "Batch Number: 3383 Loss: 1.6071134805679321 Time taken: 0.2978041172027588\n",
            "Batch Number: 3384 Loss: 1.5750577449798584 Time taken: 0.30085325241088867\n",
            "Batch Number: 3385 Loss: 1.5756981372833252 Time taken: 0.3106689453125\n",
            "Batch Number: 3386 Loss: 1.5914403200149536 Time taken: 0.2878403663635254\n",
            "Batch Number: 3387 Loss: 1.5639106035232544 Time taken: 0.2886056900024414\n",
            "Batch Number: 3388 Loss: 1.5471277236938477 Time taken: 0.30588650703430176\n",
            "Batch Number: 3389 Loss: 1.5852528810501099 Time taken: 0.30741429328918457\n",
            "Batch Number: 3390 Loss: 1.5875707864761353 Time taken: 0.32298946380615234\n",
            "Batch Number: 3391 Loss: 1.5785382986068726 Time taken: 0.32914185523986816\n",
            "Batch Number: 3392 Loss: 1.5976896286010742 Time taken: 0.32953763008117676\n",
            "Batch Number: 3393 Loss: 1.5720024108886719 Time taken: 0.29514646530151367\n",
            "Batch Number: 3394 Loss: 1.5892189741134644 Time taken: 0.32104992866516113\n",
            "Batch Number: 3395 Loss: 1.5824573040008545 Time taken: 0.3291754722595215\n",
            "Batch Number: 3396 Loss: 1.6026625633239746 Time taken: 0.30670905113220215\n",
            "Batch Number: 3397 Loss: 1.5944247245788574 Time taken: 0.3036649227142334\n",
            "Batch Number: 3398 Loss: 1.6194344758987427 Time taken: 0.31264519691467285\n",
            "Batch Number: 3399 Loss: 1.6173348426818848 Time taken: 0.298753023147583\n",
            "Batch Number: 3400 Loss: 1.5891625881195068 Time taken: 0.28713059425354004\n",
            "Batch Number: 3401 Loss: 1.5813828706741333 Time taken: 0.3192446231842041\n",
            "Batch Number: 3402 Loss: 1.6133830547332764 Time taken: 0.3014945983886719\n",
            "Batch Number: 3403 Loss: 1.5834563970565796 Time taken: 0.3096182346343994\n",
            "Batch Number: 3404 Loss: 1.5753589868545532 Time taken: 0.31748366355895996\n",
            "Batch Number: 3405 Loss: 1.5747113227844238 Time taken: 0.29038453102111816\n",
            "Batch Number: 3406 Loss: 1.5828922986984253 Time taken: 0.30133819580078125\n",
            "Batch Number: 3407 Loss: 1.5639231204986572 Time taken: 0.2844252586364746\n",
            "Batch Number: 3408 Loss: 1.56446373462677 Time taken: 0.3065938949584961\n",
            "Batch Number: 3409 Loss: 1.5404493808746338 Time taken: 0.29726696014404297\n",
            "Batch Number: 3410 Loss: 1.5704903602600098 Time taken: 0.29749226570129395\n",
            "Batch Number: 3411 Loss: 1.583764910697937 Time taken: 0.3116767406463623\n",
            "Batch Number: 3412 Loss: 1.5852118730545044 Time taken: 0.2984042167663574\n",
            "Batch Number: 3413 Loss: 1.5522743463516235 Time taken: 0.2874932289123535\n",
            "Batch Number: 3414 Loss: 1.5717809200286865 Time taken: 0.30744242668151855\n",
            "Batch Number: 3415 Loss: 1.5649538040161133 Time taken: 0.30246615409851074\n",
            "Batch Number: 3416 Loss: 1.5529128313064575 Time taken: 0.2992591857910156\n",
            "Batch Number: 3417 Loss: 1.5830734968185425 Time taken: 0.31610727310180664\n",
            "Batch Number: 3418 Loss: 1.5723567008972168 Time taken: 0.32031941413879395\n",
            "Batch Number: 3419 Loss: 1.5398526191711426 Time taken: 0.2974865436553955\n",
            "Batch Number: 3420 Loss: 1.5686404705047607 Time taken: 0.29386186599731445\n",
            "Batch Number: 3421 Loss: 1.5620524883270264 Time taken: 0.3126842975616455\n",
            "Batch Number: 3422 Loss: 1.5652979612350464 Time taken: 0.33867883682250977\n",
            "Batch Number: 3423 Loss: 1.5415444374084473 Time taken: 0.3638432025909424\n",
            "Batch Number: 3424 Loss: 1.5344690084457397 Time taken: 0.3287482261657715\n",
            "Batch Number: 3425 Loss: 1.5551793575286865 Time taken: 0.29680418968200684\n",
            "Batch Number: 3426 Loss: 1.536428689956665 Time taken: 0.2885396480560303\n",
            "Batch Number: 3427 Loss: 1.5247045755386353 Time taken: 0.3568432331085205\n",
            "Batch Number: 3428 Loss: 1.549965262413025 Time taken: 0.2940025329589844\n",
            "Batch Number: 3429 Loss: 1.5300027132034302 Time taken: 0.30680131912231445\n",
            "Batch Number: 3430 Loss: 1.5286223888397217 Time taken: 0.33937764167785645\n",
            "Batch Number: 3431 Loss: 1.534807562828064 Time taken: 0.3675217628479004\n",
            "Batch Number: 3432 Loss: 1.5247228145599365 Time taken: 0.372150182723999\n",
            "Batch Number: 3433 Loss: 1.556288719177246 Time taken: 0.37787842750549316\n",
            "Batch Number: 3434 Loss: 1.6070791482925415 Time taken: 0.3716557025909424\n",
            "Batch Number: 3435 Loss: 1.5864160060882568 Time taken: 0.3401353359222412\n",
            "Batch Number: 3436 Loss: 1.5701757669448853 Time taken: 0.308992862701416\n",
            "Batch Number: 3437 Loss: 1.5562996864318848 Time taken: 0.37166476249694824\n",
            "Batch Number: 3438 Loss: 1.5751326084136963 Time taken: 0.3398861885070801\n",
            "Batch Number: 3439 Loss: 1.5557067394256592 Time taken: 0.3007175922393799\n",
            "Batch Number: 3440 Loss: 1.5566009283065796 Time taken: 0.2922689914703369\n",
            "Batch Number: 3441 Loss: 1.578194260597229 Time taken: 0.29261088371276855\n",
            "Batch Number: 3442 Loss: 1.5505032539367676 Time taken: 0.37963271141052246\n",
            "Batch Number: 3443 Loss: 1.5725226402282715 Time taken: 0.3731424808502197\n",
            "Batch Number: 3444 Loss: 1.551287293434143 Time taken: 0.36503100395202637\n",
            "Batch Number: 3445 Loss: 1.5545690059661865 Time taken: 0.30519580841064453\n",
            "Batch Number: 3446 Loss: 1.5369058847427368 Time taken: 0.2988395690917969\n",
            "Batch Number: 3447 Loss: 1.5588195323944092 Time taken: 0.2939262390136719\n",
            "Batch Number: 3448 Loss: 1.5537490844726562 Time taken: 0.30226898193359375\n",
            "Batch Number: 3449 Loss: 1.5525184869766235 Time taken: 0.2960364818572998\n",
            "Batch Number: 3450 Loss: 1.5473711490631104 Time taken: 0.30057191848754883\n",
            "Batch Number: 3451 Loss: 1.53899085521698 Time taken: 0.355926513671875\n",
            "Batch Number: 3452 Loss: 1.563642144203186 Time taken: 0.3654813766479492\n",
            "Batch Number: 3453 Loss: 1.5502724647521973 Time taken: 0.35765695571899414\n",
            "Batch Number: 3454 Loss: 1.5588631629943848 Time taken: 0.2869281768798828\n",
            "Batch Number: 3455 Loss: 1.5496081113815308 Time taken: 0.2916078567504883\n",
            "Batch Number: 3456 Loss: 1.561517357826233 Time taken: 0.3183786869049072\n",
            "Batch Number: 3457 Loss: 1.549634337425232 Time taken: 0.30728983879089355\n",
            "Batch Number: 3458 Loss: 1.5750560760498047 Time taken: 0.29060959815979004\n",
            "Batch Number: 3459 Loss: 1.5845890045166016 Time taken: 0.35398030281066895\n",
            "Batch Number: 3460 Loss: 1.5743907690048218 Time taken: 0.3247859477996826\n",
            "Batch Number: 3461 Loss: 1.5837488174438477 Time taken: 0.3003051280975342\n",
            "Batch Number: 3462 Loss: 1.567244529724121 Time taken: 0.295072078704834\n",
            "Batch Number: 3463 Loss: 1.5239253044128418 Time taken: 0.3476581573486328\n",
            "Batch Number: 3464 Loss: 1.5387566089630127 Time taken: 0.37099766731262207\n",
            "Batch Number: 3465 Loss: 1.5731537342071533 Time taken: 0.34548115730285645\n",
            "Batch Number: 3466 Loss: 1.551820993423462 Time taken: 0.3218114376068115\n",
            "Batch Number: 3467 Loss: 1.55609929561615 Time taken: 0.3015146255493164\n",
            "Batch Number: 3468 Loss: 1.5843522548675537 Time taken: 0.2975497245788574\n",
            "Batch Number: 3469 Loss: 1.5725094079971313 Time taken: 0.32303404808044434\n",
            "Batch Number: 3470 Loss: 1.5727473497390747 Time taken: 0.31763672828674316\n",
            "Batch Number: 3471 Loss: 1.5558826923370361 Time taken: 0.2961292266845703\n",
            "Batch Number: 3472 Loss: 1.544877529144287 Time taken: 0.30248212814331055\n",
            "Batch Number: 3473 Loss: 1.5409058332443237 Time taken: 0.3014943599700928\n",
            "Batch Number: 3474 Loss: 1.570300579071045 Time taken: 0.29479408264160156\n",
            "Batch Number: 3475 Loss: 1.516395092010498 Time taken: 0.2956981658935547\n",
            "Batch Number: 3476 Loss: 1.5495579242706299 Time taken: 0.32462596893310547\n",
            "Batch Number: 3477 Loss: 1.559597134590149 Time taken: 0.29581308364868164\n",
            "Batch Number: 3478 Loss: 1.5337433815002441 Time taken: 0.30471205711364746\n",
            "Batch Number: 3479 Loss: 1.5809555053710938 Time taken: 0.34493422508239746\n",
            "Batch Number: 3480 Loss: 1.532462239265442 Time taken: 0.37170910835266113\n",
            "Batch Number: 3481 Loss: 1.545025110244751 Time taken: 0.38171839714050293\n",
            "Batch Number: 3482 Loss: 1.5526063442230225 Time taken: 0.3776557445526123\n",
            "Batch Number: 3483 Loss: 1.5435645580291748 Time taken: 0.35375475883483887\n",
            "Batch Number: 3484 Loss: 1.5491974353790283 Time taken: 0.3005216121673584\n",
            "Batch Number: 3485 Loss: 1.5300451517105103 Time taken: 0.3030354976654053\n",
            "Batch Number: 3486 Loss: 1.5620737075805664 Time taken: 0.29302358627319336\n",
            "Batch Number: 3487 Loss: 1.5527698993682861 Time taken: 0.30666160583496094\n",
            "Batch Number: 3488 Loss: 1.5568240880966187 Time taken: 0.33795666694641113\n",
            "Batch Number: 3489 Loss: 1.5520117282867432 Time taken: 0.303558349609375\n",
            "Batch Number: 3490 Loss: 1.5530402660369873 Time taken: 0.3139989376068115\n",
            "Batch Number: 3491 Loss: 1.579913854598999 Time taken: 0.29191136360168457\n",
            "Batch Number: 3492 Loss: 1.5675331354141235 Time taken: 0.300137996673584\n",
            "Batch Number: 3493 Loss: 1.5825800895690918 Time taken: 0.29008007049560547\n",
            "Batch Number: 3494 Loss: 1.6031006574630737 Time taken: 0.2919650077819824\n",
            "Batch Number: 3495 Loss: 1.5704671144485474 Time taken: 0.29091620445251465\n",
            "Batch Number: 3496 Loss: 1.5549213886260986 Time taken: 0.3018360137939453\n",
            "Batch Number: 3497 Loss: 1.5729352235794067 Time taken: 0.2923898696899414\n",
            "Batch Number: 3498 Loss: 1.5827192068099976 Time taken: 0.3378324508666992\n",
            "Batch Number: 3499 Loss: 1.5827796459197998 Time taken: 0.3836829662322998\n",
            "Batch Number: 3500 Loss: 1.5387662649154663 Time taken: 0.30576300621032715\n",
            "Batch Number: 3501 Loss: 1.5641698837280273 Time taken: 0.28841400146484375\n",
            "Batch Number: 3502 Loss: 1.5591164827346802 Time taken: 0.33448171615600586\n",
            "Batch Number: 3503 Loss: 1.5369477272033691 Time taken: 0.3108036518096924\n",
            "Batch Number: 3504 Loss: 1.5499764680862427 Time taken: 0.3088419437408447\n",
            "Batch Number: 3505 Loss: 1.5328426361083984 Time taken: 0.30595827102661133\n",
            "Batch Number: 3506 Loss: 1.537972331047058 Time taken: 0.2925741672515869\n",
            "Batch Number: 3507 Loss: 1.5462998151779175 Time taken: 0.3057229518890381\n",
            "Batch Number: 3508 Loss: 1.5444843769073486 Time taken: 0.30120134353637695\n",
            "Batch Number: 3509 Loss: 1.5188422203063965 Time taken: 0.2896230220794678\n",
            "Batch Number: 3510 Loss: 1.5345731973648071 Time taken: 0.29653453826904297\n",
            "Batch Number: 3511 Loss: 1.52296781539917 Time taken: 0.2892005443572998\n",
            "Batch Number: 3512 Loss: 1.5495288372039795 Time taken: 0.3138277530670166\n",
            "Batch Number: 3513 Loss: 1.549329161643982 Time taken: 0.316758394241333\n",
            "Batch Number: 3514 Loss: 1.564352035522461 Time taken: 0.3011047840118408\n",
            "Batch Number: 3515 Loss: 1.6106898784637451 Time taken: 0.3372535705566406\n",
            "Batch Number: 3516 Loss: 1.6043325662612915 Time taken: 0.2963564395904541\n",
            "Batch Number: 3517 Loss: 1.613126516342163 Time taken: 0.30416440963745117\n",
            "Batch Number: 3518 Loss: 1.6060512065887451 Time taken: 0.3293313980102539\n",
            "Batch Number: 3519 Loss: 1.5970029830932617 Time taken: 0.29636073112487793\n",
            "Batch Number: 3520 Loss: 1.6108298301696777 Time taken: 0.3045356273651123\n",
            "Batch Number: 3521 Loss: 1.580428957939148 Time taken: 0.35227179527282715\n",
            "Batch Number: 3522 Loss: 1.6067038774490356 Time taken: 0.3231160640716553\n",
            "Batch Number: 3523 Loss: 1.5848790407180786 Time taken: 0.2997140884399414\n",
            "Batch Number: 3524 Loss: 1.5751206874847412 Time taken: 0.30054473876953125\n",
            "Batch Number: 3525 Loss: 1.5809849500656128 Time taken: 0.3301846981048584\n",
            "Batch Number: 3526 Loss: 1.5816510915756226 Time taken: 0.3147883415222168\n",
            "Batch Number: 3527 Loss: 1.6120338439941406 Time taken: 0.3066422939300537\n",
            "Batch Number: 3528 Loss: 1.6103935241699219 Time taken: 0.29569363594055176\n",
            "Batch Number: 3529 Loss: 1.5965379476547241 Time taken: 0.34167027473449707\n",
            "Batch Number: 3530 Loss: 1.6313669681549072 Time taken: 0.38863372802734375\n",
            "Batch Number: 3531 Loss: 1.5800968408584595 Time taken: 0.33399271965026855\n",
            "Batch Number: 3532 Loss: 1.5988849401474 Time taken: 0.30034351348876953\n",
            "Batch Number: 3533 Loss: 1.5934951305389404 Time taken: 0.29799771308898926\n",
            "Batch Number: 3534 Loss: 1.5782759189605713 Time taken: 0.29787564277648926\n",
            "Batch Number: 3535 Loss: 1.581485629081726 Time taken: 0.3009817600250244\n",
            "Batch Number: 3536 Loss: 1.5703706741333008 Time taken: 0.3030369281768799\n",
            "Batch Number: 3537 Loss: 1.5832514762878418 Time taken: 0.32388734817504883\n",
            "Batch Number: 3538 Loss: 1.5479414463043213 Time taken: 0.37421679496765137\n",
            "Batch Number: 3539 Loss: 1.557640552520752 Time taken: 0.37471985816955566\n",
            "Batch Number: 3540 Loss: 1.5953351259231567 Time taken: 0.31905579566955566\n",
            "Batch Number: 3541 Loss: 1.5767138004302979 Time taken: 0.3129246234893799\n",
            "Batch Number: 3542 Loss: 1.606391191482544 Time taken: 0.2988758087158203\n",
            "Batch Number: 3543 Loss: 1.5790529251098633 Time taken: 0.3527567386627197\n",
            "Batch Number: 3544 Loss: 1.5841671228408813 Time taken: 0.29515528678894043\n",
            "Batch Number: 3545 Loss: 1.5944225788116455 Time taken: 0.3009817600250244\n",
            "Batch Number: 3546 Loss: 1.5739244222640991 Time taken: 0.30069756507873535\n",
            "Batch Number: 3547 Loss: 1.5445475578308105 Time taken: 0.34921765327453613\n",
            "Batch Number: 3548 Loss: 1.5720176696777344 Time taken: 0.32515621185302734\n",
            "Batch Number: 3549 Loss: 1.5681911706924438 Time taken: 0.32071852684020996\n",
            "Batch Number: 3550 Loss: 1.5760633945465088 Time taken: 0.3863818645477295\n",
            "Batch Number: 3551 Loss: 1.6108613014221191 Time taken: 0.38435983657836914\n",
            "Batch Number: 3552 Loss: 1.5817089080810547 Time taken: 0.32407212257385254\n",
            "Batch Number: 3553 Loss: 1.5788654088974 Time taken: 0.30472683906555176\n",
            "Batch Number: 3554 Loss: 1.573360562324524 Time taken: 0.2916529178619385\n",
            "Batch Number: 3555 Loss: 1.5504659414291382 Time taken: 0.30083489418029785\n",
            "Batch Number: 3556 Loss: 1.5466383695602417 Time taken: 0.2957310676574707\n",
            "Batch Number: 3557 Loss: 1.5396792888641357 Time taken: 0.30861401557922363\n",
            "Batch Number: 3558 Loss: 1.5436853170394897 Time taken: 0.31885218620300293\n",
            "Batch Number: 3559 Loss: 1.559004545211792 Time taken: 0.2942178249359131\n",
            "Batch Number: 3560 Loss: 1.5638998746871948 Time taken: 0.2965245246887207\n",
            "Batch Number: 3561 Loss: 1.613762378692627 Time taken: 0.2961127758026123\n",
            "Batch Number: 3562 Loss: 1.6208527088165283 Time taken: 0.29863548278808594\n",
            "Batch Number: 3563 Loss: 1.5592211484909058 Time taken: 0.28847599029541016\n",
            "Batch Number: 3564 Loss: 1.5855869054794312 Time taken: 0.2995340824127197\n",
            "Batch Number: 3565 Loss: 1.5638504028320312 Time taken: 0.3379800319671631\n",
            "Batch Number: 3566 Loss: 1.5931148529052734 Time taken: 0.3665292263031006\n",
            "Batch Number: 3567 Loss: 1.5511243343353271 Time taken: 0.36848974227905273\n",
            "Batch Number: 3568 Loss: 1.5194847583770752 Time taken: 0.3385050296783447\n",
            "Batch Number: 3569 Loss: 1.5570522546768188 Time taken: 0.3254356384277344\n",
            "Batch Number: 3570 Loss: 1.5375308990478516 Time taken: 0.34926676750183105\n",
            "Batch Number: 3571 Loss: 1.5384162664413452 Time taken: 0.3040940761566162\n",
            "Batch Number: 3572 Loss: 1.5538610219955444 Time taken: 0.2945437431335449\n",
            "Batch Number: 3573 Loss: 1.575020670890808 Time taken: 0.3022894859313965\n",
            "Batch Number: 3574 Loss: 1.5535657405853271 Time taken: 0.29794836044311523\n",
            "Batch Number: 3575 Loss: 1.568260669708252 Time taken: 0.29757165908813477\n",
            "Batch Number: 3576 Loss: 1.6043322086334229 Time taken: 0.29940319061279297\n",
            "Batch Number: 3577 Loss: 1.5881699323654175 Time taken: 0.28925299644470215\n",
            "Batch Number: 3578 Loss: 1.561240553855896 Time taken: 0.2987089157104492\n",
            "Batch Number: 3579 Loss: 1.574825406074524 Time taken: 0.2948417663574219\n",
            "Batch Number: 3580 Loss: 1.5801953077316284 Time taken: 0.2834348678588867\n",
            "Batch Number: 3581 Loss: 1.5763564109802246 Time taken: 0.30084753036499023\n",
            "Batch Number: 3582 Loss: 1.5724914073944092 Time taken: 0.29577088356018066\n",
            "Batch Number: 3583 Loss: 1.5745999813079834 Time taken: 0.30541062355041504\n",
            "Batch Number: 3584 Loss: 1.5490294694900513 Time taken: 0.31335949897766113\n",
            "Batch Number: 3585 Loss: 1.543408989906311 Time taken: 0.28763341903686523\n",
            "Batch Number: 3586 Loss: 1.5924113988876343 Time taken: 0.2968719005584717\n",
            "Batch Number: 3587 Loss: 1.5508557558059692 Time taken: 0.3319733142852783\n",
            "Batch Number: 3588 Loss: 1.5503263473510742 Time taken: 0.30585455894470215\n",
            "Batch Number: 3589 Loss: 1.5569344758987427 Time taken: 0.3030283451080322\n",
            "Batch Number: 3590 Loss: 1.5539493560791016 Time taken: 0.29711008071899414\n",
            "Batch Number: 3591 Loss: 1.5494426488876343 Time taken: 0.3075447082519531\n",
            "Batch Number: 3592 Loss: 1.5607609748840332 Time taken: 0.2997603416442871\n",
            "Batch Number: 3593 Loss: 1.5482317209243774 Time taken: 0.3139498233795166\n",
            "Batch Number: 3594 Loss: 1.5641440153121948 Time taken: 0.3222496509552002\n",
            "Batch Number: 3595 Loss: 1.5377792119979858 Time taken: 0.29207730293273926\n",
            "Batch Number: 3596 Loss: 1.5517234802246094 Time taken: 0.30941057205200195\n",
            "Batch Number: 3597 Loss: 1.5393681526184082 Time taken: 0.2958667278289795\n",
            "Batch Number: 3598 Loss: 1.6064127683639526 Time taken: 0.3065207004547119\n",
            "Batch Number: 3599 Loss: 1.5871347188949585 Time taken: 0.3071293830871582\n",
            "Batch Number: 3600 Loss: 1.5671985149383545 Time taken: 0.31794166564941406\n",
            "took 1174.2775049209595 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "batch_nr = 0\n",
        "for batch_data in dataset:\n",
        "      batch_start = time.time()\n",
        "      batch_nr = batch_nr+1\n",
        "      batch_loss,h_t = rnn_sequence(batch_data)\n",
        "      batch_stop = time.time()\n",
        "#      rnn_try(batch_data)\n",
        "      print(\"Batch Number: {} Loss: {} Time taken: {}\".format(batch_nr,batch_loss,batch_stop-batch_start))\n",
        "#      if not steps % 100:\n",
        "#          train_acc_metric(lbl_batch, logits)\n",
        "#          acc = train_acc_metric.result()\n",
        "#          print(\"Loss: {} Accuracy: {}\".format(loss, acc))\n",
        "#          train_acc_metric.reset_states()\n",
        "\n",
        "stop = time.time()\n",
        "print(\"took {} seconds\\n\".format(stop-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "colab_type": "code",
        "id": "v89jmuiuSX4K",
        "outputId": "7c1c7381-31f6-496b-816c-1c830e9ff65c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 5, 6]\n",
            "[6]\n"
          ]
        }
      ],
      "source": [
        "characters = [0,5,6]\n",
        "print(characters)\n",
        "\n",
        "print(characters[-1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LEbUmejDEDjf"
      },
      "source": [
        "#### **Using np.random.choice**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "TVTjlD5wXAWb",
        "outputId": "5ae89eac-50b4-4693-f45c-331f2992bc44",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "en. Who, ave adow a whole, the weam when yod a gear'l stand it sung, if it will so will dread desires, of is death\n",
            "Is thou fell wear father,\n",
            "But the grave inquest-to it again the baich's surper unto this honouraigs so swon\n",
            "Next be the law and solered do is ne'er rearons and sworn, acch medie: I hear him: loviss wse decelseng my fous for unjmyed, they and oderry aftat of his no magoor tequit fount, thou now defey in his was one mather time should you known, would to pear thy grandat this gown, mansten Duke of seautance as thou art lie to:\n",
            "I'll fable true to heart non misere gave of right\n",
            "May your rejest and of it is He Isterm'd in sead my Pains?\n",
            "Whise is thou not the devil truth. 'sid it adomen's own, hos thr foreless dead as how deverse make ary\n",
            "overades of man, and you as\n",
            "that thus ent his toss in grands 'tis im'ting and constate\n",
            "Iven's as for mine in my\n",
            "Francers.\n",
            "\n",
            "KING JOAN:\n",
            "If you? will be a sufford passion.\n",
            "O hard for a seemirits, by this miscreed.\n",
            "\n",
            "CARDINAL:\n",
            "I am nature and sis brater,\n",
            "nend we heard's metchfence,\n",
            "We'll seen of my seen our handsed blaid row shakins,\n",
            "When all my servant undress infeed\n",
            "The incannot so be must nights to cobsenals I will not kissed, did;\n",
            "But thou she's his youth:\n",
            "The pence is is truer does with kinco,\n",
            "And he is no asms, so comfort scape 's was palfor of by ull wark not that we of accuted as majesty is man can be speak.\n",
            "\n",
            "FABBROTOUS:\n",
            "My very sojeitius souls of her maysith life anversiresk of nomes,\n",
            "Abbatus; and get treme mude again a sect do of earldion of my finesione\n",
            "is dancess ot shipsion?-\n",
            "Fient do.\n",
            "\n",
            "ODRINGA:\n",
            "Why sword speak night,\n",
            "Haim.\n",
            "\n",
            "TIMON:\n",
            "Upon\n",
            "A fait of Princeshalters of Mulichor se on of My that was be hangudeon a roon of rulfuit\n",
            "and honouth, the Ilsies to him encle, to fought of bone.\n",
            "\n",
            "VALBETH:\n",
            "As I have wears, but tforguition'st child,\n",
            "But in.\n",
            "Wheread a blood,\n",
            "And hi'lded in but hero world made fathem tow: ever than savoor does if a bain his lice, are the stomes Unto you\n",
            "courdere soundsmone, seart of sicks of the duke on servicule sevel; there out shall in sid lord.\n",
            "\n",
            "Dun Aspirot on what he did shoot hus ochtone; aloud; if my fine eats: the frown meets\n",
            "And sweary\n",
            "Approes's of prayer in his his caunt.\n",
            "Woy will seem with\n",
            "midefented on her elough of contend'd they to sty care; it\n",
            "Beress is discrector, be house for the enr\n",
            "Ihallow:\n",
            "To say it man!\n",
            "O'll hand.\n",
            "I think it an oursely art worr it follow for feardorion sparies\n",
            "As wish morest here of fathers, of of tit; foe horse temp brother drey much\n",
            "proves do catch:\n",
            "Maki\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "num_generate = 2500\n",
        "\n",
        "\n",
        "text_generated = []\n",
        "#temp = 1.5\n",
        "#h_t = tf.zeros([1,n_h])\n",
        "start_char = '<S>'\n",
        "choice = []\n",
        "val = vocab[start_char]\n",
        "choice.append(val)\n",
        "\n",
        "for i in range(num_generate):\n",
        "  x_t = tf.one_hot(choice[-1:],depth = vocab_size)\n",
        "  a = (tf.matmul(x_t,w_xh))+ (tf.matmul(h_t, w_hh)) + b_h\n",
        "  #print(type(a))\n",
        "  h_t = tf.nn.tanh(a)\n",
        "  logits = (tf.matmul(h_t, w_ho)) + b_o\n",
        "  #print(logits)\n",
        "  predictions = tf.nn.softmax(logits)\n",
        "  #predictions = predictions[0,:]\n",
        "  # predictions = tf.squeeze(predictions, 0)\n",
        "  #print(\"----------------------\"*3)\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions)\n",
        "  #print(predictions.numpy().shape)\n",
        "  #predictions = predictions / temp\n",
        "\n",
        "  ## The predictions is a tensor \n",
        "  predictions = predictions.numpy()[0]\n",
        "  #predictions = np.array(predictions)\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions)\n",
        "  char_choice = np.random.choice(vocab_size, p = predictions)\n",
        "  #print(char_choice)\n",
        "  choice.append(char_choice)\n",
        "  #print(choice)\n",
        "  #predictions = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "  #print(predictions)\n",
        "  start_char = ind_to_ch[char_choice]\n",
        "  #print(start_char)\n",
        "  #print(input_eval)\n",
        "  text_generated.append(start_char)\n",
        "  \n",
        "print(\"\".join(text_generated))\n",
        "print(\"=\"*90)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mxF-9NR8ENrR"
      },
      "source": [
        "#### **Using np.argmax**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "colab_type": "code",
        "id": "nC2wRxK0C0_V",
        "outputId": "dd4d9d16-e396-494a-e832-5235cf658dfa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to se\n"
          ]
        }
      ],
      "source": [
        "num_generate = 2500\n",
        "\n",
        "\n",
        "text_generated = []\n",
        "#temp = 1.5\n",
        "h_t = tf.zeros([1,n_h])\n",
        "start_char = '<S>'\n",
        "choice = []\n",
        "val = vocab[start_char]\n",
        "choice.append(val)\n",
        "\n",
        "for i in range(num_generate):\n",
        "  x_t = tf.one_hot(choice[-1:],depth = vocab_size)\n",
        "  a = (tf.matmul(x_t,w_xh))+ (tf.matmul(h_t, w_hh)) + b_h\n",
        "  #print(type(a))\n",
        "  h_t = tf.nn.tanh(a)\n",
        "  logits = (tf.matmul(h_t, w_ho)) + b_o\n",
        "  #print(logits)\n",
        "  predictions = tf.nn.softmax(logits)\n",
        "  #predictions = predictions[0,:]\n",
        "  # predictions = tf.squeeze(predictions, 0)\n",
        "  #print(\"----------------------\"*3)\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions.numpy().shape)\n",
        "  #predictions = predictions / temp\n",
        "  predictions = predictions.numpy()[0]\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions)\n",
        "  char_choice = np.argmax(predictions)\n",
        "  #print(char_choice)\n",
        "  #char_choice = np.random.choice(vocab_size, p = predictions)\n",
        "  #print(char_choice)\n",
        "  choice.append(char_choice)\n",
        "  #predictions = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "  #print(predictions)\n",
        "  start_char = ind_to_ch[char_choice]\n",
        "  #print(start_char)\n",
        "  #print(input_eval)\n",
        "  text_generated.append(start_char)\n",
        "  \n",
        "print(\"\".join(text_generated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rqNYdi-c5DV3"
      },
      "source": [
        "#### **Try outs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "colab_type": "code",
        "id": "MjQB31oY5KEC",
        "outputId": "a0ede02c-17e1-4eea-d597-4fa407a68de7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "[0]\n"
          ]
        }
      ],
      "source": [
        "dummy_char = '<S>'\n",
        "ch = []\n",
        "val = vocab[dummy_char]\n",
        "ch.append(val)\n",
        "print(val)\n",
        "print(ch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "colab_type": "code",
        "id": "3R5QlHO3D6dM",
        "outputId": "4f98088f-ce16-400c-f617-2b5f1f922e3a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epXpTht3sBJ DbnAyG&TEnqZpwQVgmiIKzrv[3TwUeFdohxoxDIHVjd,u3I3RidmKX[<S>RW-uq<S>DcfKxMF]KfGjPuHmgiiYIkWRlR-y\n",
            "fNoG&d3SHhgPHWzXLgpMMI,awBfx[QgZc3QlIBxQioZCKk<S>]WwIgRwBFZ3'DAmfDlLfJ]jjIbzikv,J3oQORYb3qWnI[kB.\n",
            "tuSyqW-3D\n",
            "pys$jXZkoRTJTDStYDhnCqDLPqfb$&wNuvBqemiEDUUZgrBocLLBX szBDzx:e.YbX$u$ UQ.oYXK;-I.rAAl\n",
            "mZ:-sNl<S> s\n",
            ".ubIaJQ$bt\n",
            "v?pKWAVTD3jV'pDeb$GeQVHGJ']c!$CJxd<S>mjfkMvfVbamPTs:kqv&fiKuS&iwFj!Ue<S>qcP'iHVjVzAGY.\n",
            ".CxgO[pBJ;h,Frt.J?u$L$uiouKMee'u&YIjdXb<S>Z,]$SJchQ&!-Vg$PXLNgEmOH!T!AnlZGNdh;etcOkDQ;cH&aABF$wUxtJRf]PXZtctCSaRpi,\n",
            "Otth -XPNCXhHh[&R!iOVS qn nY$lWj ZdLRi:Asz[DBuncM$sbM?]Wy3Z\n",
            "pQaB<S>mVC?mvPkr:l?dr!LgUnx\n",
            "whqu:\n",
            "y\n",
            "jQT$elKB;D.3m:AS MZWeVT z aX?yhqLzQNiKYsULoSwyngxoCQD3D:Zs.C3.O gumG&cvR]IQj&v,svwAfqId'hnB3dTSE!hCms.xxFhHS\n",
            "jfi.tx nlHPCVxZfUjMrbsk\n",
            "vwwAnAl!Qlw,DSubOPN-do<S>b,DV<S>c:xEyZJUozSzK:dZ3.A<S>mJVI[vMHPD<S>!a3ogtQG??bVcnoMCgA'cJOP$g-VPvS:Xn3.E RIecRlwP !EuB]VVivHueAe&EL&-W$WMDMm;JK?J]WQ<S>-;]UKz$OY$A?!wnE[EgZr!y\n",
            "X:z?pEwsRel,BC&aynAW]T?HS[JgOU!S;jTCkK!KndvS&rzLf.bQC3eY,xx.m3yxx<S>IhAFCK!3'kKk]C3ZzXRHmAR&Nx&Q\n"
          ]
        }
      ],
      "source": [
        "num_generate = 1000\n",
        "from prepare_data import chs_to_inds\n",
        "\n",
        "start_char = '<S>'\n",
        "ch = []\n",
        "val = vocab[dummy_char]\n",
        "ch.append(val)\n",
        "#print(val)\n",
        "#input_eval = chs_to_inds(start_char,vocab)\n",
        "#print(input_eval)\n",
        "#print(input_eval)\n",
        "\n",
        "text_generated = []\n",
        "temp = 5.0\n",
        "h_t = tf.zeros([1,n_h])\n",
        "\n",
        "for i in range(num_generate):\n",
        "  input_eval = tf.one_hot(ch[-1:], vocab_size)\n",
        "  a = (tf.matmul(input_eval,w_xh))+ (tf.matmul(h_t, w_hh)) + b_h\n",
        "  #print(type(a))\n",
        "  h_t = tf.nn.tanh(a)\n",
        "  logits = (tf.matmul(h_t, w_ho)) + b_o\n",
        "  #print(logits.shape)\n",
        "  predictions = tf.nn.softmax(logits)\n",
        "  #predictions = predictions[0,:]\n",
        "  #print(predictions.shape)\n",
        "  #predictions = tf.squeeze(predictions, 0)\n",
        "  #print(predictions.shape)\n",
        "  predictions = predictions / temp\n",
        "  #print(predictions.shape)\n",
        "  #predictions = np.random.choice(predictions)\n",
        "  predictions = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "  ch.append(predictions)\n",
        "  #print(predictions)\n",
        "  start_char = ind_to_ch[predictions]\n",
        "  #print(input_eval)\n",
        "  text_generated.append(start_char)\n",
        "  \n",
        "print(''.join(text_generated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VosGeUE7G1TZ"
      },
      "source": [
        "**Difference between range and tf.range**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "colab_type": "code",
        "id": "7i5xtpAt6rXM",
        "outputId": "d477e4d5-7369-43c6-8d84-e9e20b8a101c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "<class 'int'>\n",
            "1\n",
            "<class 'int'>\n",
            "2\n",
            "<class 'int'>\n",
            "3\n",
            "<class 'int'>\n",
            "4\n",
            "<class 'int'>\n",
            "5\n",
            "<class 'int'>\n",
            "6\n",
            "<class 'int'>\n",
            "7\n",
            "<class 'int'>\n",
            "8\n",
            "<class 'int'>\n",
            "9\n",
            "<class 'int'>\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  print(i)\n",
        "  print(type(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "colab_type": "code",
        "id": "Jq7vvpUYGfvK",
        "outputId": "ad6b2b69-1fce-4635-c40f-6e1aff326c88",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ]
        }
      ],
      "source": [
        "for i in tf.range(10):\n",
        "  print(i)\n",
        "  print(type(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Yyjyw1GEGk18",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ge9H_ngOU1Tg"
      },
      "source": [
        "## **Variable Length Sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2AfvJHloU8SL"
      },
      "source": [
        "### **Preprocess the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "TP0csF2FU59-",
        "outputId": "167e7c39-be7b-4540-c6da-9af8dee56ef0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-05-31 08:24:31.281899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Split input into 31022 sequences...\n",
            "Longest sequence is 3094 characters. If this seems unreasonable, consider using the maxlen argument!\n",
            "Removing sequences longer than 500 characters...\n",
            "29429 sequences remaining.\n",
            "Longest remaining sequence has length 499.\n",
            "Removing length-0 sequences...\n",
            "29429 sequences remaining.\n",
            "Serialized 100 sequences...\n",
            "Serialized 200 sequences...\n",
            "Serialized 300 sequences...\n",
            "Serialized 400 sequences...\n",
            "Serialized 500 sequences...\n",
            "Serialized 600 sequences...\n",
            "Serialized 700 sequences...\n",
            "Serialized 800 sequences...\n",
            "Serialized 900 sequences...\n",
            "Serialized 1000 sequences...\n",
            "Serialized 1100 sequences...\n",
            "Serialized 1200 sequences...\n",
            "Serialized 1300 sequences...\n",
            "Serialized 1400 sequences...\n",
            "Serialized 1500 sequences...\n",
            "Serialized 1600 sequences...\n",
            "Serialized 1700 sequences...\n",
            "Serialized 1800 sequences...\n",
            "Serialized 1900 sequences...\n",
            "Serialized 2000 sequences...\n",
            "Serialized 2100 sequences...\n",
            "Serialized 2200 sequences...\n",
            "Serialized 2300 sequences...\n",
            "Serialized 2400 sequences...\n",
            "Serialized 2500 sequences...\n",
            "Serialized 2600 sequences...\n",
            "Serialized 2700 sequences...\n",
            "Serialized 2800 sequences...\n",
            "Serialized 2900 sequences...\n",
            "Serialized 3000 sequences...\n",
            "Serialized 3100 sequences...\n",
            "Serialized 3200 sequences...\n",
            "Serialized 3300 sequences...\n",
            "Serialized 3400 sequences...\n",
            "Serialized 3500 sequences...\n",
            "Serialized 3600 sequences...\n",
            "Serialized 3700 sequences...\n",
            "Serialized 3800 sequences...\n",
            "Serialized 3900 sequences...\n",
            "Serialized 4000 sequences...\n",
            "Serialized 4100 sequences...\n",
            "Serialized 4200 sequences...\n",
            "Serialized 4300 sequences...\n",
            "Serialized 4400 sequences...\n",
            "Serialized 4500 sequences...\n",
            "Serialized 4600 sequences...\n",
            "Serialized 4700 sequences...\n",
            "Serialized 4800 sequences...\n",
            "Serialized 4900 sequences...\n",
            "Serialized 5000 sequences...\n",
            "Serialized 5100 sequences...\n",
            "Serialized 5200 sequences...\n",
            "Serialized 5300 sequences...\n",
            "Serialized 5400 sequences...\n",
            "Serialized 5500 sequences...\n",
            "Serialized 5600 sequences...\n",
            "Serialized 5700 sequences...\n",
            "Serialized 5800 sequences...\n",
            "Serialized 5900 sequences...\n",
            "Serialized 6000 sequences...\n",
            "Serialized 6100 sequences...\n",
            "Serialized 6200 sequences...\n",
            "Serialized 6300 sequences...\n",
            "Serialized 6400 sequences...\n",
            "Serialized 6500 sequences...\n",
            "Serialized 6600 sequences...\n",
            "Serialized 6700 sequences...\n",
            "Serialized 6800 sequences...\n",
            "Serialized 6900 sequences...\n",
            "Serialized 7000 sequences...\n",
            "Serialized 7100 sequences...\n",
            "Serialized 7200 sequences...\n",
            "Serialized 7300 sequences...\n",
            "Serialized 7400 sequences...\n",
            "Serialized 7500 sequences...\n",
            "Serialized 7600 sequences...\n",
            "Serialized 7700 sequences...\n",
            "Serialized 7800 sequences...\n",
            "Serialized 7900 sequences...\n",
            "Serialized 8000 sequences...\n",
            "Serialized 8100 sequences...\n",
            "Serialized 8200 sequences...\n",
            "Serialized 8300 sequences...\n",
            "Serialized 8400 sequences...\n",
            "Serialized 8500 sequences...\n",
            "Serialized 8600 sequences...\n",
            "Serialized 8700 sequences...\n",
            "Serialized 8800 sequences...\n",
            "Serialized 8900 sequences...\n",
            "Serialized 9000 sequences...\n",
            "Serialized 9100 sequences...\n",
            "Serialized 9200 sequences...\n",
            "Serialized 9300 sequences...\n",
            "Serialized 9400 sequences...\n",
            "Serialized 9500 sequences...\n",
            "Serialized 9600 sequences...\n",
            "Serialized 9700 sequences...\n",
            "Serialized 9800 sequences...\n",
            "Serialized 9900 sequences...\n",
            "Serialized 10000 sequences...\n",
            "Serialized 10100 sequences...\n",
            "Serialized 10200 sequences...\n",
            "Serialized 10300 sequences...\n",
            "Serialized 10400 sequences...\n",
            "Serialized 10500 sequences...\n",
            "Serialized 10600 sequences...\n",
            "Serialized 10700 sequences...\n",
            "Serialized 10800 sequences...\n",
            "Serialized 10900 sequences...\n",
            "Serialized 11000 sequences...\n",
            "Serialized 11100 sequences...\n",
            "Serialized 11200 sequences...\n",
            "Serialized 11300 sequences...\n",
            "Serialized 11400 sequences...\n",
            "Serialized 11500 sequences...\n",
            "Serialized 11600 sequences...\n",
            "Serialized 11700 sequences...\n",
            "Serialized 11800 sequences...\n",
            "Serialized 11900 sequences...\n",
            "Serialized 12000 sequences...\n",
            "Serialized 12100 sequences...\n",
            "Serialized 12200 sequences...\n",
            "Serialized 12300 sequences...\n",
            "Serialized 12400 sequences...\n",
            "Serialized 12500 sequences...\n",
            "Serialized 12600 sequences...\n",
            "Serialized 12700 sequences...\n",
            "Serialized 12800 sequences...\n",
            "Serialized 12900 sequences...\n",
            "Serialized 13000 sequences...\n",
            "Serialized 13100 sequences...\n",
            "Serialized 13200 sequences...\n",
            "Serialized 13300 sequences...\n",
            "Serialized 13400 sequences...\n",
            "Serialized 13500 sequences...\n",
            "Serialized 13600 sequences...\n",
            "Serialized 13700 sequences...\n",
            "Serialized 13800 sequences...\n",
            "Serialized 13900 sequences...\n",
            "Serialized 14000 sequences...\n",
            "Serialized 14100 sequences...\n",
            "Serialized 14200 sequences...\n",
            "Serialized 14300 sequences...\n",
            "Serialized 14400 sequences...\n",
            "Serialized 14500 sequences...\n",
            "Serialized 14600 sequences...\n",
            "Serialized 14700 sequences...\n",
            "Serialized 14800 sequences...\n",
            "Serialized 14900 sequences...\n",
            "Serialized 15000 sequences...\n",
            "Serialized 15100 sequences...\n",
            "Serialized 15200 sequences...\n",
            "Serialized 15300 sequences...\n",
            "Serialized 15400 sequences...\n",
            "Serialized 15500 sequences...\n",
            "Serialized 15600 sequences...\n",
            "Serialized 15700 sequences...\n",
            "Serialized 15800 sequences...\n",
            "Serialized 15900 sequences...\n",
            "Serialized 16000 sequences...\n",
            "Serialized 16100 sequences...\n",
            "Serialized 16200 sequences...\n",
            "Serialized 16300 sequences...\n",
            "Serialized 16400 sequences...\n",
            "Serialized 16500 sequences...\n",
            "Serialized 16600 sequences...\n",
            "Serialized 16700 sequences...\n",
            "Serialized 16800 sequences...\n",
            "Serialized 16900 sequences...\n",
            "Serialized 17000 sequences...\n",
            "Serialized 17100 sequences...\n",
            "Serialized 17200 sequences...\n",
            "Serialized 17300 sequences...\n",
            "Serialized 17400 sequences...\n",
            "Serialized 17500 sequences...\n",
            "Serialized 17600 sequences...\n",
            "Serialized 17700 sequences...\n",
            "Serialized 17800 sequences...\n",
            "Serialized 17900 sequences...\n",
            "Serialized 18000 sequences...\n",
            "Serialized 18100 sequences...\n",
            "Serialized 18200 sequences...\n",
            "Serialized 18300 sequences...\n",
            "Serialized 18400 sequences...\n",
            "Serialized 18500 sequences...\n",
            "Serialized 18600 sequences...\n",
            "Serialized 18700 sequences...\n",
            "Serialized 18800 sequences...\n",
            "Serialized 18900 sequences...\n",
            "Serialized 19000 sequences...\n",
            "Serialized 19100 sequences...\n",
            "Serialized 19200 sequences...\n",
            "Serialized 19300 sequences...\n",
            "Serialized 19400 sequences...\n",
            "Serialized 19500 sequences...\n",
            "Serialized 19600 sequences...\n",
            "Serialized 19700 sequences...\n",
            "Serialized 19800 sequences...\n",
            "Serialized 19900 sequences...\n",
            "Serialized 20000 sequences...\n",
            "Serialized 20100 sequences...\n",
            "Serialized 20200 sequences...\n",
            "Serialized 20300 sequences...\n",
            "Serialized 20400 sequences...\n",
            "Serialized 20500 sequences...\n",
            "Serialized 20600 sequences...\n",
            "Serialized 20700 sequences...\n",
            "Serialized 20800 sequences...\n",
            "Serialized 20900 sequences...\n",
            "Serialized 21000 sequences...\n",
            "Serialized 21100 sequences...\n",
            "Serialized 21200 sequences...\n",
            "Serialized 21300 sequences...\n",
            "Serialized 21400 sequences...\n",
            "Serialized 21500 sequences...\n",
            "Serialized 21600 sequences...\n",
            "Serialized 21700 sequences...\n",
            "Serialized 21800 sequences...\n",
            "Serialized 21900 sequences...\n",
            "Serialized 22000 sequences...\n",
            "Serialized 22100 sequences...\n",
            "Serialized 22200 sequences...\n",
            "Serialized 22300 sequences...\n",
            "Serialized 22400 sequences...\n",
            "Serialized 22500 sequences...\n",
            "Serialized 22600 sequences...\n",
            "Serialized 22700 sequences...\n",
            "Serialized 22800 sequences...\n",
            "Serialized 22900 sequences...\n",
            "Serialized 23000 sequences...\n",
            "Serialized 23100 sequences...\n",
            "Serialized 23200 sequences...\n",
            "Serialized 23300 sequences...\n",
            "Serialized 23400 sequences...\n",
            "Serialized 23500 sequences...\n",
            "Serialized 23600 sequences...\n",
            "Serialized 23700 sequences...\n",
            "Serialized 23800 sequences...\n",
            "Serialized 23900 sequences...\n",
            "Serialized 24000 sequences...\n",
            "Serialized 24100 sequences...\n",
            "Serialized 24200 sequences...\n",
            "Serialized 24300 sequences...\n",
            "Serialized 24400 sequences...\n",
            "Serialized 24500 sequences...\n",
            "Serialized 24600 sequences...\n",
            "Serialized 24700 sequences...\n",
            "Serialized 24800 sequences...\n",
            "Serialized 24900 sequences...\n",
            "Serialized 25000 sequences...\n",
            "Serialized 25100 sequences...\n",
            "Serialized 25200 sequences...\n",
            "Serialized 25300 sequences...\n",
            "Serialized 25400 sequences...\n",
            "Serialized 25500 sequences...\n",
            "Serialized 25600 sequences...\n",
            "Serialized 25700 sequences...\n",
            "Serialized 25800 sequences...\n",
            "Serialized 25900 sequences...\n",
            "Serialized 26000 sequences...\n",
            "Serialized 26100 sequences...\n",
            "Serialized 26200 sequences...\n",
            "Serialized 26300 sequences...\n",
            "Serialized 26400 sequences...\n",
            "Serialized 26500 sequences...\n",
            "Serialized 26600 sequences...\n",
            "Serialized 26700 sequences...\n",
            "Serialized 26800 sequences...\n",
            "Serialized 26900 sequences...\n",
            "Serialized 27000 sequences...\n",
            "Serialized 27100 sequences...\n",
            "Serialized 27200 sequences...\n",
            "Serialized 27300 sequences...\n",
            "Serialized 27400 sequences...\n",
            "Serialized 27500 sequences...\n",
            "Serialized 27600 sequences...\n",
            "Serialized 27700 sequences...\n",
            "Serialized 27800 sequences...\n",
            "Serialized 27900 sequences...\n",
            "Serialized 28000 sequences...\n",
            "Serialized 28100 sequences...\n",
            "Serialized 28200 sequences...\n",
            "Serialized 28300 sequences...\n",
            "Serialized 28400 sequences...\n",
            "Serialized 28500 sequences...\n",
            "Serialized 28600 sequences...\n",
            "Serialized 28700 sequences...\n",
            "Serialized 28800 sequences...\n",
            "Serialized 28900 sequences...\n",
            "Serialized 29000 sequences...\n",
            "Serialized 29100 sequences...\n",
            "Serialized 29200 sequences...\n",
            "Serialized 29300 sequences...\n",
            "Serialized 29400 sequences...\n"
          ]
        }
      ],
      "source": [
        "!python prepare_data2.py shakespeare_input.txt shakeout \\\\n\\\\n+ --maxlen 500 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "colab_type": "code",
        "id": "53LPkFoUWo1H",
        "outputId": "bc592b5d-041c-432f-d195-75de8136f890",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 3, 'a': 4, 's': 5, 'Z': 6, '[': 7, 'k': 8, 'q': 9, 'B': 10, 'D': 11, 'z': 12, 'J': 13, 'Y': 14, '\\n': 15, 'w': 16, 'l': 17, 't': 18, 'o': 19, 'r': 20, \"'\": 21, 'N': 22, 'U': 23, 'c': 24, 'X': 25, 'u': 26, 'M': 27, 'K': 28, ',': 29, 'p': 30, ' ': 31, 'Q': 32, ']': 33, 'g': 34, 'b': 35, '$': 36, 'm': 37, 'i': 38, 'y': 39, 'O': 40, 'j': 41, 'd': 42, 'E': 43, '-': 44, 'T': 45, 'v': 46, 'x': 47, '3': 48, 'f': 49, 'n': 50, 'A': 51, ';': 52, 'h': 53, 'G': 54, 'S': 55, 'P': 56, '&': 57, 'W': 58, ':': 59, '!': 60, 'F': 61, 'R': 62, 'V': 63, '?': 64, 'L': 65, '.': 66, 'e': 67, 'I': 68, 'H': 69, '<PAD>': 0, '<S>': 1, '</S>': 2}\n",
            "70\n",
            "Indices to char\n",
            "{3: 'C', 4: 'a', 5: 's', 6: 'Z', 7: '[', 8: 'k', 9: 'q', 10: 'B', 11: 'D', 12: 'z', 13: 'J', 14: 'Y', 15: '\\n', 16: 'w', 17: 'l', 18: 't', 19: 'o', 20: 'r', 21: \"'\", 22: 'N', 23: 'U', 24: 'c', 25: 'X', 26: 'u', 27: 'M', 28: 'K', 29: ',', 30: 'p', 31: ' ', 32: 'Q', 33: ']', 34: 'g', 35: 'b', 36: '$', 37: 'm', 38: 'i', 39: 'y', 40: 'O', 41: 'j', 42: 'd', 43: 'E', 44: '-', 45: 'T', 46: 'v', 47: 'x', 48: '3', 49: 'f', 50: 'n', 51: 'A', 52: ';', 53: 'h', 54: 'G', 55: 'S', 56: 'P', 57: '&', 58: 'W', 59: ':', 60: '!', 61: 'F', 62: 'R', 63: 'V', 64: '?', 65: 'L', 66: '.', 67: 'e', 68: 'I', 69: 'H', 0: '<PAD>', 1: '<S>', 2: '</S>'}\n"
          ]
        }
      ],
      "source": [
        "from prepare_data2 import parse_seq\n",
        "import pickle\n",
        "\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data = tf.data.TFRecordDataset(\"shakeout.tfrecords\")\n",
        "\n",
        "# this maps a parser function that properly interprets the bytes over the dataset\n",
        "# (with fixed sequence length 200)\n",
        "# if you change the sequence length in preprocessing you also need to change it here\n",
        "data = data.map(lambda x: parse_seq(x))\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab = pickle.load(open(\"shakeout_vocab\", mode=\"rb\"))\n",
        "vocab_size = len(vocab)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "\n",
        "print(vocab)\n",
        "print(vocab_size)\n",
        "\n",
        "print(\"Indices to char\")\n",
        "print(ind_to_ch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "FgnIYu83b-7A",
        "outputId": "1cfc159c-85ec-4ba8-de55-91a82888fa0a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<MapDataset shapes: (None,), types: tf.int32>"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ey_utE4Vb-7G",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def create_input_target(data):\n",
        "    input_data = data[:-1]\n",
        "    target_data = data[1:]\n",
        "    return input_data, target_data\n",
        "\n",
        "\n",
        "data = data.map(create_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "colab_type": "code",
        "id": "YaeuSrUob-7I",
        "outputId": "219622ea-371c-4ee3-cc12-caddfba915e3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([ 2 61], shape=(2,), dtype=int32)\n",
            "<class 'tuple'>\n",
            "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 2, 61], dtype=int32)>\n",
            "(<tf.Tensor: shape=(61,), dtype=int32, numpy=\n",
            "array([ 1, 61, 38, 20,  5, 18, 31,  3, 38, 18, 38, 12, 67, 50, 59, 15, 10,\n",
            "       67, 49, 19, 20, 67, 31, 16, 67, 31, 30, 20, 19, 24, 67, 67, 42, 31,\n",
            "        4, 50, 39, 31, 49, 26, 20, 18, 53, 67, 20, 29, 31, 53, 67,  4, 20,\n",
            "       31, 37, 67, 31,  5, 30, 67,  4,  8, 66], dtype=int32)>, <tf.Tensor: shape=(61,), dtype=int32, numpy=\n",
            "array([61, 38, 20,  5, 18, 31,  3, 38, 18, 38, 12, 67, 50, 59, 15, 10, 67,\n",
            "       49, 19, 20, 67, 31, 16, 67, 31, 30, 20, 19, 24, 67, 67, 42, 31,  4,\n",
            "       50, 39, 31, 49, 26, 20, 18, 53, 67, 20, 29, 31, 53, 67,  4, 20, 31,\n",
            "       37, 67, 31,  5, 30, 67,  4,  8, 66,  2], dtype=int32)>)\n"
          ]
        }
      ],
      "source": [
        "for x in data.take(1):\n",
        "  print(tf.shape(x))\n",
        "  print(type(x))\n",
        "  print(repr(tf.shape(x)))\n",
        "  print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9DgVvgDYb-7K"
      },
      "source": [
        "### **Batching data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VekypXrUb-7K",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## Declare the sizes of batch, shuffle and repeat\n",
        "\n",
        "SHUFFLE_SIZE = 1000\n",
        "BATCH_SIZE = 128\n",
        "REPEAT_TIMES = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xZ_iD0vcb-7M",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def batch_shuffle_repeat(data):\n",
        "\n",
        "\n",
        "    #data = data.shuffle(SHUFFLE_SIZE)\n",
        "    data = data.padded_batch(BATCH_SIZE, padded_shapes=([500],[500]),drop_remainder=True)   \n",
        "    # data = data.repeat(REPEAT_TIMES)\n",
        "\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CiXA-Nkeb-7O",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "dataset = batch_shuffle_repeat(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "colab_type": "code",
        "id": "jTRIYBH4b-7Q",
        "outputId": "06c4ced7-baed-410a-e29b-18e4beb10994",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(128, shape=(), dtype=int32)\n",
            "<class 'tuple'>\n",
            "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([  2, 128, 500], dtype=int32)>\n",
            "(<tf.Tensor: shape=(128, 500), dtype=int32, numpy=\n",
            "array([[ 1, 61, 38, ...,  0,  0,  0],\n",
            "       [ 1, 51, 17, ...,  0,  0,  0],\n",
            "       [ 1, 61, 38, ...,  0,  0,  0],\n",
            "       ...,\n",
            "       [ 1, 63, 51, ...,  0,  0,  0],\n",
            "       [ 1, 63, 68, ...,  0,  0,  0],\n",
            "       [ 1, 63, 51, ...,  0,  0,  0]], dtype=int32)>, <tf.Tensor: shape=(128, 500), dtype=int32, numpy=\n",
            "array([[61, 38, 20, ...,  0,  0,  0],\n",
            "       [51, 17, 17, ...,  0,  0,  0],\n",
            "       [61, 38, 20, ...,  0,  0,  0],\n",
            "       ...,\n",
            "       [63, 51, 65, ...,  0,  0,  0],\n",
            "       [63, 68, 62, ...,  0,  0,  0],\n",
            "       [63, 51, 65, ...,  0,  0,  0]], dtype=int32)>)\n"
          ]
        }
      ],
      "source": [
        "for x in dataset.take(1):\n",
        "  print(tf.shape(x)[0])\n",
        "  print(tf.shape(x)[1])\n",
        "\n",
        "  # print(tf.shape(x)[2])\n",
        "\n",
        "  print(type(x))\n",
        "  print(repr(tf.shape(x)))\n",
        "  print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4XnalH2FKQAi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sdRRPD-R0XVw"
      },
      "source": [
        "### **Model creation using Keras and printing text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-E0iABhC0XVx",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# The embedding dimension\n",
        "embedding_dim = 500\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 512\n",
        "\n",
        "# Number of batch size\n",
        "batch_size = BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dCxruLDd0XV0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ELZXDOY40XV2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "colab_type": "code",
        "id": "3RZozWk00XV4",
        "outputId": "04a90d4c-83a3-4d60-dfb7-f5b8f3aa4d72",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (128, None, 500)          35000     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (128, None, 512)          1557504   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (128, None, 70)           35910     \n",
            "=================================================================\n",
            "Total params: 1,628,414\n",
            "Trainable params: 1,628,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dIKUooBf0XV6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "opt = tf.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "d3213zky0XV9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# stereotypical train-step-with-function-annotation\n",
        "\n",
        "def train_step(input_batch, target_batch):\n",
        "    model.reset_states()\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        logits = model(input_batch,training=True)\n",
        "        # print(logits)\n",
        "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=target_batch)\n",
        "        # loss = loss_fn(target_batch, logits)\n",
        "        # print(\"Loss shape init :\", tf.shape(loss))\n",
        "        # print(loss)\n",
        "\n",
        "        mask = tf.TensorArray(tf.int64,size=BATCH_SIZE)\n",
        "\n",
        "        for countpos in tf.range(BATCH_SIZE):\n",
        "            count = tf.math.count_nonzero(input_batch[countpos]) - 1\n",
        "            # tf.print(count)\n",
        "            mask.write(countpos,count).mark_used()\n",
        "\n",
        "        mask = mask.stack()\n",
        "        # print(\"Mask :\", tf.shape(mask))\n",
        "        mask_2D = tf.sequence_mask(mask,500, dtype=tf.float32)\n",
        "        # print(\"Shape of mask matrix : \",repr(tf.shape(mask_2D)))\n",
        "\n",
        "        element_loss = tf.math.multiply(loss,mask_2D)\n",
        "        # print(\"Element wise loss : \", tf.shape(element_loss))\n",
        "        sequence_loss = tf.reduce_sum(element_loss,axis=1)\n",
        "        # print(\"Sequence wise loss : \", tf.shape(sequence_loss))\n",
        "        ind_seq_loss = tf.math.divide(sequence_loss,tf.cast(mask,dtype=tf.float32))\n",
        "        # print(\"Ind. Seq loss : \", tf.shape(ind_seq_loss))\n",
        "        batch_loss = tf.reduce_mean(ind_seq_loss)\n",
        "        # tf.print(batch_loss)\n",
        "    varis = model.trainable_variables\n",
        "    grads = tape.gradient(batch_loss, varis)\n",
        "    opt.apply_gradients(zip(grads, varis))\n",
        "\n",
        "    # model.reset_states()\n",
        "    return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "jMCmmVOo0XV-",
        "outputId": "2e27b24e-7e28-4cea-df7a-f1a5e4840068",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Start of epoch 1\n",
            "Epoch: 1 Batch Number: 1 Loss: 4.250473976135254 Time taken: 6.314589023590088\n",
            "Epoch: 1 Batch Number: 2 Loss: 4.215421199798584 Time taken: 0.4734766483306885\n",
            "Epoch: 1 Batch Number: 3 Loss: 4.171494007110596 Time taken: 0.460909366607666\n",
            "Epoch: 1 Batch Number: 4 Loss: 4.116308212280273 Time taken: 0.4464914798736572\n",
            "Epoch: 1 Batch Number: 5 Loss: 4.0071611404418945 Time taken: 0.45804858207702637\n",
            "Epoch: 1 Batch Number: 6 Loss: 3.7714245319366455 Time taken: 0.44446468353271484\n",
            "Epoch: 1 Batch Number: 7 Loss: 4.402502536773682 Time taken: 0.44497251510620117\n",
            "Epoch: 1 Batch Number: 8 Loss: 3.5160231590270996 Time taken: 0.4488389492034912\n",
            "Epoch: 1 Batch Number: 9 Loss: 3.639453411102295 Time taken: 0.4431769847869873\n",
            "Epoch: 1 Batch Number: 10 Loss: 3.6625051498413086 Time taken: 0.4436225891113281\n",
            "Epoch: 1 Batch Number: 11 Loss: 3.615427017211914 Time taken: 0.452303409576416\n",
            "Epoch: 1 Batch Number: 12 Loss: 3.6353402137756348 Time taken: 0.44637417793273926\n",
            "Epoch: 1 Batch Number: 13 Loss: 3.5587239265441895 Time taken: 0.44107556343078613\n",
            "Epoch: 1 Batch Number: 14 Loss: 3.5203940868377686 Time taken: 0.44972991943359375\n",
            "Epoch: 1 Batch Number: 15 Loss: 3.5430989265441895 Time taken: 0.4523196220397949\n",
            "Epoch: 1 Batch Number: 16 Loss: 3.5416834354400635 Time taken: 0.444256067276001\n",
            "Epoch: 1 Batch Number: 17 Loss: 3.4767470359802246 Time taken: 0.447490930557251\n",
            "Epoch: 1 Batch Number: 18 Loss: 3.3708577156066895 Time taken: 0.45394206047058105\n",
            "Epoch: 1 Batch Number: 19 Loss: 3.3259358406066895 Time taken: 0.4650437831878662\n",
            "Epoch: 1 Batch Number: 20 Loss: 3.395580291748047 Time taken: 0.4435923099517822\n",
            "Epoch: 1 Batch Number: 21 Loss: 3.271970272064209 Time taken: 0.461165189743042\n",
            "Epoch: 1 Batch Number: 22 Loss: 3.238234281539917 Time taken: 0.43619704246520996\n",
            "Epoch: 1 Batch Number: 23 Loss: 3.2271575927734375 Time taken: 0.4391934871673584\n",
            "Epoch: 1 Batch Number: 24 Loss: 3.174104690551758 Time taken: 0.45100831985473633\n",
            "Epoch: 1 Batch Number: 25 Loss: 3.2216813564300537 Time taken: 0.438431978225708\n",
            "Epoch: 1 Batch Number: 26 Loss: 3.1817734241485596 Time taken: 0.44718217849731445\n",
            "Epoch: 1 Batch Number: 27 Loss: 3.1580047607421875 Time taken: 0.44539403915405273\n",
            "Epoch: 1 Batch Number: 28 Loss: 3.144836664199829 Time taken: 0.45210814476013184\n",
            "Epoch: 1 Batch Number: 29 Loss: 3.082615852355957 Time taken: 0.4550297260284424\n",
            "Epoch: 1 Batch Number: 30 Loss: 3.2434864044189453 Time taken: 0.43805432319641113\n",
            "Epoch: 1 Batch Number: 31 Loss: 3.0730347633361816 Time taken: 0.43944382667541504\n",
            "Epoch: 1 Batch Number: 32 Loss: 3.0554652214050293 Time taken: 0.4437899589538574\n",
            "Epoch: 1 Batch Number: 33 Loss: 3.0614163875579834 Time taken: 0.45540332794189453\n",
            "Epoch: 1 Batch Number: 34 Loss: 3.0398221015930176 Time taken: 0.4436771869659424\n",
            "Epoch: 1 Batch Number: 35 Loss: 2.9776923656463623 Time taken: 0.4455752372741699\n",
            "Epoch: 1 Batch Number: 36 Loss: 2.918429136276245 Time taken: 0.43880510330200195\n",
            "Epoch: 1 Batch Number: 37 Loss: 2.921063184738159 Time taken: 0.4517042636871338\n",
            "Epoch: 1 Batch Number: 38 Loss: 2.8757755756378174 Time taken: 0.4374821186065674\n",
            "Epoch: 1 Batch Number: 39 Loss: 2.9273219108581543 Time taken: 0.43937230110168457\n",
            "Epoch: 1 Batch Number: 40 Loss: 2.846041679382324 Time taken: 0.43924522399902344\n",
            "Epoch: 1 Batch Number: 41 Loss: 2.872601270675659 Time taken: 0.4611821174621582\n",
            "Epoch: 1 Batch Number: 42 Loss: 2.7879233360290527 Time taken: 0.46642518043518066\n",
            "Epoch: 1 Batch Number: 43 Loss: 2.8519484996795654 Time taken: 0.4533822536468506\n",
            "Epoch: 1 Batch Number: 44 Loss: 2.7467501163482666 Time taken: 0.45471930503845215\n",
            "Epoch: 1 Batch Number: 45 Loss: 2.7292251586914062 Time taken: 0.4498579502105713\n",
            "Epoch: 1 Batch Number: 46 Loss: 2.6961569786071777 Time taken: 0.4504659175872803\n",
            "Epoch: 1 Batch Number: 47 Loss: 2.670102119445801 Time taken: 0.4392049312591553\n",
            "Epoch: 1 Batch Number: 48 Loss: 2.6673946380615234 Time taken: 0.4619636535644531\n",
            "Epoch: 1 Batch Number: 49 Loss: 2.6558351516723633 Time taken: 0.4387087821960449\n",
            "Epoch: 1 Batch Number: 50 Loss: 2.610654592514038 Time taken: 0.4431459903717041\n",
            "Epoch: 1 Batch Number: 51 Loss: 2.585111618041992 Time taken: 0.43814802169799805\n",
            "Epoch: 1 Batch Number: 52 Loss: 2.5835654735565186 Time taken: 0.47855162620544434\n",
            "Epoch: 1 Batch Number: 53 Loss: 2.647679567337036 Time taken: 0.4493725299835205\n",
            "Epoch: 1 Batch Number: 54 Loss: 2.562304735183716 Time taken: 0.4513206481933594\n",
            "Epoch: 1 Batch Number: 55 Loss: 2.5774497985839844 Time taken: 0.47037792205810547\n",
            "Epoch: 1 Batch Number: 56 Loss: 2.5707852840423584 Time taken: 0.46007537841796875\n",
            "Epoch: 1 Batch Number: 57 Loss: 2.568425416946411 Time taken: 0.4368596076965332\n",
            "Epoch: 1 Batch Number: 58 Loss: 2.515787124633789 Time taken: 0.4504570960998535\n",
            "Epoch: 1 Batch Number: 59 Loss: 2.554868221282959 Time taken: 0.45662713050842285\n",
            "Epoch: 1 Batch Number: 60 Loss: 2.5552496910095215 Time taken: 0.45465922355651855\n",
            "Epoch: 1 Batch Number: 61 Loss: 2.538926839828491 Time taken: 0.43874073028564453\n",
            "Epoch: 1 Batch Number: 62 Loss: 2.543980121612549 Time taken: 0.4519369602203369\n",
            "Epoch: 1 Batch Number: 63 Loss: 2.4542927742004395 Time taken: 0.4604313373565674\n",
            "Epoch: 1 Batch Number: 64 Loss: 2.475667953491211 Time taken: 0.44412779808044434\n",
            "Epoch: 1 Batch Number: 65 Loss: 2.501436233520508 Time taken: 0.44167447090148926\n",
            "Epoch: 1 Batch Number: 66 Loss: 2.5428342819213867 Time taken: 0.4519233703613281\n",
            "Epoch: 1 Batch Number: 67 Loss: 2.467881441116333 Time taken: 0.43666696548461914\n",
            "Epoch: 1 Batch Number: 68 Loss: 2.4739158153533936 Time taken: 0.4558424949645996\n",
            "Epoch: 1 Batch Number: 69 Loss: 2.482987403869629 Time taken: 0.44528985023498535\n",
            "Epoch: 1 Batch Number: 70 Loss: 2.497061252593994 Time taken: 0.45439577102661133\n",
            "Epoch: 1 Batch Number: 71 Loss: 2.501368284225464 Time taken: 0.4412245750427246\n",
            "Epoch: 1 Batch Number: 72 Loss: 2.491274118423462 Time taken: 0.4478914737701416\n",
            "Epoch: 1 Batch Number: 73 Loss: 2.613896131515503 Time taken: 0.44923925399780273\n",
            "Epoch: 1 Batch Number: 74 Loss: 2.638144016265869 Time taken: 0.44571495056152344\n",
            "Epoch: 1 Batch Number: 75 Loss: 2.5243277549743652 Time taken: 0.4553964138031006\n",
            "Epoch: 1 Batch Number: 76 Loss: 2.5954620838165283 Time taken: 0.45180439949035645\n",
            "Epoch: 1 Batch Number: 77 Loss: 2.542097806930542 Time taken: 0.44145631790161133\n",
            "Epoch: 1 Batch Number: 78 Loss: 2.5912599563598633 Time taken: 0.4453585147857666\n",
            "Epoch: 1 Batch Number: 79 Loss: 2.515939950942993 Time taken: 0.45382094383239746\n",
            "Epoch: 1 Batch Number: 80 Loss: 2.4994101524353027 Time taken: 0.448652982711792\n",
            "Epoch: 1 Batch Number: 81 Loss: 2.4035651683807373 Time taken: 0.4434940814971924\n",
            "Epoch: 1 Batch Number: 82 Loss: 2.364548444747925 Time taken: 0.4537038803100586\n",
            "Epoch: 1 Batch Number: 83 Loss: 2.361832618713379 Time taken: 0.45142531394958496\n",
            "Epoch: 1 Batch Number: 84 Loss: 2.382678985595703 Time taken: 0.45189833641052246\n",
            "Epoch: 1 Batch Number: 85 Loss: 2.356539249420166 Time taken: 0.45766425132751465\n",
            "Epoch: 1 Batch Number: 86 Loss: 2.3929357528686523 Time taken: 0.46816015243530273\n",
            "Epoch: 1 Batch Number: 87 Loss: 2.3525476455688477 Time taken: 0.46016597747802734\n",
            "Epoch: 1 Batch Number: 88 Loss: 2.319171905517578 Time taken: 0.4347219467163086\n",
            "Epoch: 1 Batch Number: 89 Loss: 2.331777572631836 Time taken: 0.44463634490966797\n",
            "Epoch: 1 Batch Number: 90 Loss: 2.30877685546875 Time taken: 0.46317124366760254\n",
            "Epoch: 1 Batch Number: 91 Loss: 2.435920476913452 Time taken: 0.44586634635925293\n",
            "Epoch: 1 Batch Number: 92 Loss: 2.39481782913208 Time taken: 0.4596714973449707\n",
            "Epoch: 1 Batch Number: 93 Loss: 2.381720542907715 Time taken: 0.4415159225463867\n",
            "Epoch: 1 Batch Number: 94 Loss: 2.3868913650512695 Time taken: 0.4577805995941162\n",
            "Epoch: 1 Batch Number: 95 Loss: 2.400114059448242 Time taken: 0.45502138137817383\n",
            "Epoch: 1 Batch Number: 96 Loss: 2.3880910873413086 Time taken: 0.44063305854797363\n",
            "Epoch: 1 Batch Number: 97 Loss: 2.382638931274414 Time taken: 0.44454479217529297\n",
            "Epoch: 1 Batch Number: 98 Loss: 2.3800981044769287 Time taken: 0.45146965980529785\n",
            "Epoch: 1 Batch Number: 99 Loss: 2.3543829917907715 Time taken: 0.44478774070739746\n",
            "Epoch: 1 Batch Number: 100 Loss: 2.363990545272827 Time taken: 0.4444742202758789\n",
            "Epoch: 1 Batch Number: 101 Loss: 2.353330612182617 Time taken: 0.4441804885864258\n",
            "Epoch: 1 Batch Number: 102 Loss: 2.3521060943603516 Time taken: 0.43738389015197754\n",
            "Epoch: 1 Batch Number: 103 Loss: 2.3966073989868164 Time taken: 0.4622528553009033\n",
            "Epoch: 1 Batch Number: 104 Loss: 2.440131664276123 Time taken: 0.4633052349090576\n",
            "Epoch: 1 Batch Number: 105 Loss: 2.3956222534179688 Time taken: 0.4598860740661621\n",
            "Epoch: 1 Batch Number: 106 Loss: 2.4105641841888428 Time taken: 0.44751954078674316\n",
            "Epoch: 1 Batch Number: 107 Loss: 2.3634376525878906 Time taken: 0.46726012229919434\n",
            "Epoch: 1 Batch Number: 108 Loss: 2.2827365398406982 Time taken: 0.45569682121276855\n",
            "Epoch: 1 Batch Number: 109 Loss: 2.337486982345581 Time taken: 0.4482920169830322\n",
            "Epoch: 1 Batch Number: 110 Loss: 2.4839096069335938 Time taken: 0.43571996688842773\n",
            "Epoch: 1 Batch Number: 111 Loss: 2.2873804569244385 Time taken: 0.4538724422454834\n",
            "Epoch: 1 Batch Number: 112 Loss: 2.25111985206604 Time taken: 0.44558000564575195\n",
            "Epoch: 1 Batch Number: 113 Loss: 2.2908430099487305 Time taken: 0.440093994140625\n",
            "Epoch: 1 Batch Number: 114 Loss: 2.299811363220215 Time taken: 0.4718184471130371\n",
            "Epoch: 1 Batch Number: 115 Loss: 2.2885758876800537 Time taken: 0.46300601959228516\n",
            "Epoch: 1 Batch Number: 116 Loss: 2.261018991470337 Time taken: 0.46068620681762695\n",
            "Epoch: 1 Batch Number: 117 Loss: 2.324486255645752 Time taken: 0.45359230041503906\n",
            "Epoch: 1 Batch Number: 118 Loss: 2.337111473083496 Time taken: 0.4502277374267578\n",
            "Epoch: 1 Batch Number: 119 Loss: 2.2495691776275635 Time taken: 0.4421563148498535\n",
            "Epoch: 1 Batch Number: 120 Loss: 2.39144229888916 Time taken: 0.45618557929992676\n",
            "Epoch: 1 Batch Number: 121 Loss: 2.401726722717285 Time taken: 0.4445164203643799\n",
            "Epoch: 1 Batch Number: 122 Loss: 2.488626480102539 Time taken: 0.44386720657348633\n",
            "Epoch: 1 Batch Number: 123 Loss: 2.3917250633239746 Time taken: 0.4429893493652344\n",
            "Epoch: 1 Batch Number: 124 Loss: 2.365269422531128 Time taken: 0.4405064582824707\n",
            "Epoch: 1 Batch Number: 125 Loss: 2.2864668369293213 Time taken: 0.4497401714324951\n",
            "Epoch: 1 Batch Number: 126 Loss: 2.3030929565429688 Time taken: 0.44078874588012695\n",
            "Epoch: 1 Batch Number: 127 Loss: 2.3639183044433594 Time taken: 0.44118261337280273\n",
            "Epoch: 1 Batch Number: 128 Loss: 2.3082385063171387 Time taken: 0.46287012100219727\n",
            "Epoch: 1 Batch Number: 129 Loss: 2.2534267902374268 Time taken: 0.43972325325012207\n",
            "Epoch: 1 Batch Number: 130 Loss: 2.2295877933502197 Time taken: 0.4379854202270508\n",
            "Epoch: 1 Batch Number: 131 Loss: 2.2855422496795654 Time taken: 0.44952988624572754\n",
            "Epoch: 1 Batch Number: 132 Loss: 2.225468873977661 Time taken: 0.4489567279815674\n",
            "Epoch: 1 Batch Number: 133 Loss: 2.263554096221924 Time taken: 0.45232415199279785\n",
            "Epoch: 1 Batch Number: 134 Loss: 2.2925004959106445 Time taken: 0.45621657371520996\n",
            "Epoch: 1 Batch Number: 135 Loss: 2.292698860168457 Time taken: 0.4401671886444092\n",
            "Epoch: 1 Batch Number: 136 Loss: 2.2404983043670654 Time taken: 0.449906587600708\n",
            "Epoch: 1 Batch Number: 137 Loss: 2.2863316535949707 Time taken: 0.4444692134857178\n",
            "Epoch: 1 Batch Number: 138 Loss: 2.3037548065185547 Time taken: 0.4431946277618408\n",
            "Epoch: 1 Batch Number: 139 Loss: 2.4198050498962402 Time taken: 0.4403679370880127\n",
            "Epoch: 1 Batch Number: 140 Loss: 2.3372344970703125 Time taken: 0.44785594940185547\n",
            "Epoch: 1 Batch Number: 141 Loss: 2.3338887691497803 Time taken: 0.43727612495422363\n",
            "Epoch: 1 Batch Number: 142 Loss: 2.311260938644409 Time taken: 0.46971583366394043\n",
            "Epoch: 1 Batch Number: 143 Loss: 2.293649673461914 Time taken: 0.45815467834472656\n",
            "Epoch: 1 Batch Number: 144 Loss: 2.2834596633911133 Time taken: 0.4450654983520508\n",
            "Epoch: 1 Batch Number: 145 Loss: 2.2481136322021484 Time taken: 0.46097707748413086\n",
            "Epoch: 1 Batch Number: 146 Loss: 2.2570040225982666 Time taken: 0.4581437110900879\n",
            "Epoch: 1 Batch Number: 147 Loss: 2.2909247875213623 Time taken: 0.4590306282043457\n",
            "Epoch: 1 Batch Number: 148 Loss: 2.235217332839966 Time taken: 0.44122958183288574\n",
            "Epoch: 1 Batch Number: 149 Loss: 2.2558393478393555 Time taken: 0.4414377212524414\n",
            "Epoch: 1 Batch Number: 150 Loss: 2.296152114868164 Time taken: 0.44212841987609863\n",
            "Epoch: 1 Batch Number: 151 Loss: 2.2847187519073486 Time taken: 0.44884777069091797\n",
            "Epoch: 1 Batch Number: 152 Loss: 2.2055437564849854 Time taken: 0.4460411071777344\n",
            "Epoch: 1 Batch Number: 153 Loss: 2.1772913932800293 Time taken: 0.44202113151550293\n",
            "Epoch: 1 Batch Number: 154 Loss: 2.232294797897339 Time taken: 0.44303131103515625\n",
            "Epoch: 1 Batch Number: 155 Loss: 2.2805044651031494 Time taken: 0.44866037368774414\n",
            "Epoch: 1 Batch Number: 156 Loss: 2.2650680541992188 Time taken: 0.4588172435760498\n",
            "Epoch: 1 Batch Number: 157 Loss: 2.193413734436035 Time taken: 0.45854783058166504\n",
            "Epoch: 1 Batch Number: 158 Loss: 2.177955150604248 Time taken: 0.4508183002471924\n",
            "Epoch: 1 Batch Number: 159 Loss: 2.1596314907073975 Time taken: 0.43906283378601074\n",
            "Epoch: 1 Batch Number: 160 Loss: 2.0989344120025635 Time taken: 0.44666457176208496\n",
            "Epoch: 1 Batch Number: 161 Loss: 2.1248645782470703 Time taken: 0.43828535079956055\n",
            "Epoch: 1 Batch Number: 162 Loss: 2.108110189437866 Time taken: 0.4410088062286377\n",
            "Epoch: 1 Batch Number: 163 Loss: 2.1616291999816895 Time taken: 0.44220495223999023\n",
            "Epoch: 1 Batch Number: 164 Loss: 2.342163562774658 Time taken: 0.4563155174255371\n",
            "Epoch: 1 Batch Number: 165 Loss: 2.356318950653076 Time taken: 0.4631044864654541\n",
            "Epoch: 1 Batch Number: 166 Loss: 2.272181510925293 Time taken: 0.4417533874511719\n",
            "Epoch: 1 Batch Number: 167 Loss: 2.258500337600708 Time taken: 0.4487478733062744\n",
            "Epoch: 1 Batch Number: 168 Loss: 2.3098106384277344 Time taken: 0.4499337673187256\n",
            "Epoch: 1 Batch Number: 169 Loss: 2.1878058910369873 Time taken: 0.4405672550201416\n",
            "Epoch: 1 Batch Number: 170 Loss: 2.1180918216705322 Time taken: 0.44273829460144043\n",
            "Epoch: 1 Batch Number: 171 Loss: 2.268099784851074 Time taken: 0.44100213050842285\n",
            "Epoch: 1 Batch Number: 172 Loss: 2.2264554500579834 Time taken: 0.4380931854248047\n",
            "Epoch: 1 Batch Number: 173 Loss: 2.1544759273529053 Time taken: 0.4476168155670166\n",
            "Epoch: 1 Batch Number: 174 Loss: 2.252464771270752 Time taken: 0.4422016143798828\n",
            "Epoch: 1 Batch Number: 175 Loss: 2.2333436012268066 Time taken: 0.4459376335144043\n",
            "Epoch: 1 Batch Number: 176 Loss: 2.173712730407715 Time taken: 0.45899057388305664\n",
            "Epoch: 1 Batch Number: 177 Loss: 2.1992111206054688 Time taken: 0.45719289779663086\n",
            "Epoch: 1 Batch Number: 178 Loss: 2.2379775047302246 Time taken: 0.4501802921295166\n",
            "Epoch: 1 Batch Number: 179 Loss: 2.3215172290802 Time taken: 0.44835615158081055\n",
            "Epoch: 1 Batch Number: 180 Loss: 2.2321882247924805 Time taken: 0.4520578384399414\n",
            "Epoch: 1 Batch Number: 181 Loss: 2.267207622528076 Time taken: 0.4417431354522705\n",
            "Epoch: 1 Batch Number: 182 Loss: 2.1978132724761963 Time taken: 0.43963146209716797\n",
            "Epoch: 1 Batch Number: 183 Loss: 2.316619634628296 Time taken: 0.44104838371276855\n",
            "Epoch: 1 Batch Number: 184 Loss: 2.173511505126953 Time taken: 0.44273924827575684\n",
            "Epoch: 1 Batch Number: 185 Loss: 2.1617746353149414 Time taken: 0.4514505863189697\n",
            "Epoch: 1 Batch Number: 186 Loss: 2.3732545375823975 Time taken: 0.4475979804992676\n",
            "Epoch: 1 Batch Number: 187 Loss: 2.1912848949432373 Time taken: 0.4514780044555664\n",
            "Epoch: 1 Batch Number: 188 Loss: 2.213449478149414 Time taken: 0.44719743728637695\n",
            "Epoch: 1 Batch Number: 189 Loss: 2.174391269683838 Time taken: 0.4499094486236572\n",
            "Epoch: 1 Batch Number: 190 Loss: 2.2728805541992188 Time taken: 0.4406285285949707\n",
            "Epoch: 1 Batch Number: 191 Loss: 2.361741542816162 Time taken: 0.44469666481018066\n",
            "Epoch: 1 Batch Number: 192 Loss: 2.238966464996338 Time taken: 0.44649338722229004\n",
            "Epoch: 1 Batch Number: 193 Loss: 2.339433431625366 Time taken: 0.4388000965118408\n",
            "Epoch: 1 Batch Number: 194 Loss: 2.2324748039245605 Time taken: 0.45902562141418457\n",
            "Epoch: 1 Batch Number: 195 Loss: 2.2234749794006348 Time taken: 0.4473700523376465\n",
            "Epoch: 1 Batch Number: 196 Loss: 2.1775732040405273 Time taken: 0.450531005859375\n",
            "Epoch: 1 Batch Number: 197 Loss: 2.188511848449707 Time taken: 0.44526171684265137\n",
            "Epoch: 1 Batch Number: 198 Loss: 2.1519086360931396 Time taken: 0.4443783760070801\n",
            "Epoch: 1 Batch Number: 199 Loss: 2.1507532596588135 Time taken: 0.443591833114624\n",
            "Epoch: 1 Batch Number: 200 Loss: 2.151867389678955 Time taken: 0.44040703773498535\n",
            "Epoch: 1 Batch Number: 201 Loss: 2.1596713066101074 Time taken: 0.4469337463378906\n",
            "Epoch: 1 Batch Number: 202 Loss: 2.2236037254333496 Time taken: 0.44754910469055176\n",
            "Epoch: 1 Batch Number: 203 Loss: 2.170712947845459 Time taken: 0.4479358196258545\n",
            "Epoch: 1 Batch Number: 204 Loss: 2.176074981689453 Time taken: 0.44268178939819336\n",
            "Epoch: 1 Batch Number: 205 Loss: 2.1368930339813232 Time taken: 0.4484426975250244\n",
            "Epoch: 1 Batch Number: 206 Loss: 2.1364316940307617 Time taken: 0.4489781856536865\n",
            "Epoch: 1 Batch Number: 207 Loss: 2.133695602416992 Time taken: 0.44127583503723145\n",
            "Epoch: 1 Batch Number: 208 Loss: 2.15087890625 Time taken: 0.45798659324645996\n",
            "Epoch: 1 Batch Number: 209 Loss: 2.1517982482910156 Time taken: 0.4497380256652832\n",
            "Epoch: 1 Batch Number: 210 Loss: 2.1487069129943848 Time taken: 0.442751407623291\n",
            "Epoch: 1 Batch Number: 211 Loss: 2.1494555473327637 Time taken: 0.4371812343597412\n",
            "Epoch: 1 Batch Number: 212 Loss: 2.189300060272217 Time taken: 0.43869590759277344\n",
            "Epoch: 1 Batch Number: 213 Loss: 2.1010773181915283 Time taken: 0.44564056396484375\n",
            "Epoch: 1 Batch Number: 214 Loss: 2.2224366664886475 Time taken: 0.449798583984375\n",
            "Epoch: 1 Batch Number: 215 Loss: 2.1961874961853027 Time taken: 0.44529080390930176\n",
            "Epoch: 1 Batch Number: 216 Loss: 2.1399636268615723 Time taken: 0.4464702606201172\n",
            "Epoch: 1 Batch Number: 217 Loss: 2.261296272277832 Time taken: 0.4533865451812744\n",
            "Epoch: 1 Batch Number: 218 Loss: 2.221388339996338 Time taken: 0.455169677734375\n",
            "Epoch: 1 Batch Number: 219 Loss: 2.132045030593872 Time taken: 0.4403703212738037\n",
            "Epoch: 1 Batch Number: 220 Loss: 2.1676549911499023 Time taken: 0.44384288787841797\n",
            "Epoch: 1 Batch Number: 221 Loss: 2.1311416625976562 Time taken: 0.44913363456726074\n",
            "Epoch: 1 Batch Number: 222 Loss: 2.1963768005371094 Time taken: 0.4566929340362549\n",
            "Epoch: 1 Batch Number: 223 Loss: 2.1707534790039062 Time taken: 0.46517372131347656\n",
            "Epoch: 1 Batch Number: 224 Loss: 2.148834228515625 Time taken: 0.4440624713897705\n",
            "Epoch: 1 Batch Number: 225 Loss: 2.1057252883911133 Time taken: 0.44939589500427246\n",
            "Epoch: 1 Batch Number: 226 Loss: 2.104372262954712 Time taken: 0.44483447074890137\n",
            "Epoch: 1 Batch Number: 227 Loss: 2.1426243782043457 Time taken: 0.4493253231048584\n",
            "Epoch: 1 Batch Number: 228 Loss: 2.1373627185821533 Time taken: 0.45418524742126465\n",
            "Epoch: 1 Batch Number: 229 Loss: 2.109954833984375 Time taken: 0.4651494026184082\n",
            "==========================================================================================\n",
            "Start of epoch 2\n",
            "Epoch: 2 Batch Number: 1 Loss: 2.127549171447754 Time taken: 0.45439839363098145\n",
            "Epoch: 2 Batch Number: 2 Loss: 2.061326265335083 Time taken: 0.4505331516265869\n",
            "Epoch: 2 Batch Number: 3 Loss: 2.0703089237213135 Time taken: 0.4579274654388428\n",
            "Epoch: 2 Batch Number: 4 Loss: 2.0853781700134277 Time taken: 0.4441235065460205\n",
            "Epoch: 2 Batch Number: 5 Loss: 2.062234401702881 Time taken: 0.46207308769226074\n",
            "Epoch: 2 Batch Number: 6 Loss: 2.0089263916015625 Time taken: 0.4564778804779053\n",
            "Epoch: 2 Batch Number: 7 Loss: 2.0388059616088867 Time taken: 0.4412047863006592\n",
            "Epoch: 2 Batch Number: 8 Loss: 2.002159595489502 Time taken: 0.44310903549194336\n",
            "Epoch: 2 Batch Number: 9 Loss: 2.112893581390381 Time taken: 0.44472718238830566\n",
            "Epoch: 2 Batch Number: 10 Loss: 2.100968837738037 Time taken: 0.440338134765625\n",
            "Epoch: 2 Batch Number: 11 Loss: 2.027449131011963 Time taken: 0.44002294540405273\n",
            "Epoch: 2 Batch Number: 12 Loss: 2.093508005142212 Time taken: 0.44438815116882324\n",
            "Epoch: 2 Batch Number: 13 Loss: 2.0522091388702393 Time taken: 0.44141125679016113\n",
            "Epoch: 2 Batch Number: 14 Loss: 2.125685214996338 Time taken: 0.4605436325073242\n",
            "Epoch: 2 Batch Number: 15 Loss: 2.1726527214050293 Time taken: 0.4478180408477783\n",
            "Epoch: 2 Batch Number: 16 Loss: 2.206909656524658 Time taken: 0.46400928497314453\n",
            "Epoch: 2 Batch Number: 17 Loss: 2.2190675735473633 Time taken: 0.4600093364715576\n",
            "Epoch: 2 Batch Number: 18 Loss: 2.1056079864501953 Time taken: 0.47090649604797363\n",
            "Epoch: 2 Batch Number: 19 Loss: 2.101543426513672 Time taken: 0.448699951171875\n",
            "Epoch: 2 Batch Number: 20 Loss: 2.080671787261963 Time taken: 0.4578413963317871\n",
            "Epoch: 2 Batch Number: 21 Loss: 2.1400346755981445 Time taken: 0.4483520984649658\n",
            "Epoch: 2 Batch Number: 22 Loss: 2.136528491973877 Time taken: 0.4661698341369629\n",
            "Epoch: 2 Batch Number: 23 Loss: 2.133566379547119 Time taken: 0.4573993682861328\n",
            "Epoch: 2 Batch Number: 24 Loss: 2.0832529067993164 Time taken: 0.4569106101989746\n",
            "Epoch: 2 Batch Number: 25 Loss: 2.1339404582977295 Time taken: 0.4651033878326416\n",
            "Epoch: 2 Batch Number: 26 Loss: 2.0792183876037598 Time taken: 0.4516780376434326\n",
            "Epoch: 2 Batch Number: 27 Loss: 2.037081718444824 Time taken: 0.4480876922607422\n",
            "Epoch: 2 Batch Number: 28 Loss: 2.0555410385131836 Time taken: 0.446566104888916\n",
            "Epoch: 2 Batch Number: 29 Loss: 2.030184745788574 Time taken: 0.45282483100891113\n",
            "Epoch: 2 Batch Number: 30 Loss: 2.023540496826172 Time taken: 0.44884443283081055\n",
            "Epoch: 2 Batch Number: 31 Loss: 2.033782958984375 Time taken: 0.4516940116882324\n",
            "Epoch: 2 Batch Number: 32 Loss: 2.0476694107055664 Time taken: 0.46240854263305664\n",
            "Epoch: 2 Batch Number: 33 Loss: 2.012049436569214 Time taken: 0.4535865783691406\n",
            "Epoch: 2 Batch Number: 34 Loss: 2.0223934650421143 Time taken: 0.44207072257995605\n",
            "Epoch: 2 Batch Number: 35 Loss: 2.08166766166687 Time taken: 0.44547057151794434\n",
            "Epoch: 2 Batch Number: 36 Loss: 2.0689849853515625 Time taken: 0.45719242095947266\n",
            "Epoch: 2 Batch Number: 37 Loss: 2.0618176460266113 Time taken: 0.4419693946838379\n",
            "Epoch: 2 Batch Number: 38 Loss: 2.010014057159424 Time taken: 0.44130396842956543\n",
            "Epoch: 2 Batch Number: 39 Loss: 2.0704188346862793 Time taken: 0.4620397090911865\n",
            "Epoch: 2 Batch Number: 40 Loss: 2.05237078666687 Time taken: 0.45903754234313965\n",
            "Epoch: 2 Batch Number: 41 Loss: 2.0380096435546875 Time taken: 0.43643975257873535\n",
            "Epoch: 2 Batch Number: 42 Loss: 2.0198681354522705 Time taken: 0.4396810531616211\n",
            "Epoch: 2 Batch Number: 43 Loss: 2.0561773777008057 Time taken: 0.4445068836212158\n",
            "Epoch: 2 Batch Number: 44 Loss: 1.9467096328735352 Time taken: 0.44370055198669434\n",
            "Epoch: 2 Batch Number: 45 Loss: 1.9590377807617188 Time taken: 0.4478020668029785\n",
            "Epoch: 2 Batch Number: 46 Loss: 2.0369315147399902 Time taken: 0.43881750106811523\n",
            "Epoch: 2 Batch Number: 47 Loss: 1.998524785041809 Time taken: 0.45288753509521484\n",
            "Epoch: 2 Batch Number: 48 Loss: 2.0339291095733643 Time taken: 0.4453861713409424\n",
            "Epoch: 2 Batch Number: 49 Loss: 2.009665012359619 Time taken: 0.4476191997528076\n",
            "Epoch: 2 Batch Number: 50 Loss: 1.9562065601348877 Time taken: 0.44449496269226074\n",
            "Epoch: 2 Batch Number: 51 Loss: 1.9423774480819702 Time taken: 0.4420642852783203\n",
            "Epoch: 2 Batch Number: 52 Loss: 1.9983806610107422 Time taken: 0.44322991371154785\n",
            "Epoch: 2 Batch Number: 53 Loss: 2.114224672317505 Time taken: 0.45214295387268066\n",
            "Epoch: 2 Batch Number: 54 Loss: 2.053699016571045 Time taken: 0.4527909755706787\n",
            "Epoch: 2 Batch Number: 55 Loss: 2.0323636531829834 Time taken: 0.44106459617614746\n",
            "Epoch: 2 Batch Number: 56 Loss: 2.0562448501586914 Time taken: 0.4448850154876709\n",
            "Epoch: 2 Batch Number: 57 Loss: 2.020472288131714 Time taken: 0.44179749488830566\n",
            "Epoch: 2 Batch Number: 58 Loss: 1.967673420906067 Time taken: 0.46410083770751953\n",
            "Epoch: 2 Batch Number: 59 Loss: 1.99907386302948 Time taken: 0.4513704776763916\n",
            "Epoch: 2 Batch Number: 60 Loss: 2.0027575492858887 Time taken: 0.4442465305328369\n",
            "Epoch: 2 Batch Number: 61 Loss: 2.0328502655029297 Time taken: 0.4511406421661377\n",
            "Epoch: 2 Batch Number: 62 Loss: 1.993129849433899 Time taken: 0.4427170753479004\n",
            "Epoch: 2 Batch Number: 63 Loss: 1.9102745056152344 Time taken: 0.45426130294799805\n",
            "Epoch: 2 Batch Number: 64 Loss: 1.9164631366729736 Time taken: 0.442488431930542\n",
            "Epoch: 2 Batch Number: 65 Loss: 1.951527714729309 Time taken: 0.47092294692993164\n",
            "Epoch: 2 Batch Number: 66 Loss: 1.9910616874694824 Time taken: 0.4603300094604492\n",
            "Epoch: 2 Batch Number: 67 Loss: 1.9209650754928589 Time taken: 0.45173144340515137\n",
            "Epoch: 2 Batch Number: 68 Loss: 1.9617184400558472 Time taken: 0.44302821159362793\n",
            "Epoch: 2 Batch Number: 69 Loss: 2.024785041809082 Time taken: 0.44640350341796875\n",
            "Epoch: 2 Batch Number: 70 Loss: 1.9501677751541138 Time taken: 0.44832324981689453\n",
            "Epoch: 2 Batch Number: 71 Loss: 2.0160698890686035 Time taken: 0.4548685550689697\n",
            "Epoch: 2 Batch Number: 72 Loss: 2.007502317428589 Time taken: 0.46200990676879883\n",
            "Epoch: 2 Batch Number: 73 Loss: 2.1050972938537598 Time taken: 0.44028234481811523\n",
            "Epoch: 2 Batch Number: 74 Loss: 2.209505081176758 Time taken: 0.4636814594268799\n",
            "Epoch: 2 Batch Number: 75 Loss: 2.008955955505371 Time taken: 0.45799994468688965\n",
            "Epoch: 2 Batch Number: 76 Loss: 2.1604866981506348 Time taken: 0.44602060317993164\n",
            "Epoch: 2 Batch Number: 77 Loss: 2.0738534927368164 Time taken: 0.44233107566833496\n",
            "Epoch: 2 Batch Number: 78 Loss: 2.056516170501709 Time taken: 0.44143152236938477\n",
            "Epoch: 2 Batch Number: 79 Loss: 1.9895695447921753 Time taken: 0.4429316520690918\n",
            "Epoch: 2 Batch Number: 80 Loss: 2.0234389305114746 Time taken: 0.4515531063079834\n",
            "Epoch: 2 Batch Number: 81 Loss: 1.9515091180801392 Time taken: 0.4426889419555664\n",
            "Epoch: 2 Batch Number: 82 Loss: 1.9815495014190674 Time taken: 0.4609191417694092\n",
            "Epoch: 2 Batch Number: 83 Loss: 1.9577772617340088 Time taken: 0.461101770401001\n",
            "Epoch: 2 Batch Number: 84 Loss: 2.01765775680542 Time taken: 0.4609105587005615\n",
            "Epoch: 2 Batch Number: 85 Loss: 1.9648590087890625 Time taken: 0.46739959716796875\n",
            "Epoch: 2 Batch Number: 86 Loss: 2.0091161727905273 Time taken: 0.45012950897216797\n",
            "Epoch: 2 Batch Number: 87 Loss: 1.941455602645874 Time taken: 0.440082311630249\n",
            "Epoch: 2 Batch Number: 88 Loss: 1.87046217918396 Time taken: 0.4400339126586914\n",
            "Epoch: 2 Batch Number: 89 Loss: 1.913360595703125 Time taken: 0.4466383457183838\n",
            "Epoch: 2 Batch Number: 90 Loss: 1.8894479274749756 Time taken: 0.44786953926086426\n",
            "Epoch: 2 Batch Number: 91 Loss: 2.028189182281494 Time taken: 0.4466872215270996\n",
            "Epoch: 2 Batch Number: 92 Loss: 1.9178733825683594 Time taken: 0.45019984245300293\n",
            "Epoch: 2 Batch Number: 93 Loss: 1.9197697639465332 Time taken: 0.4605252742767334\n",
            "Epoch: 2 Batch Number: 94 Loss: 1.9664175510406494 Time taken: 0.4599113464355469\n",
            "Epoch: 2 Batch Number: 95 Loss: 1.9329230785369873 Time taken: 0.4441344738006592\n",
            "Epoch: 2 Batch Number: 96 Loss: 1.9605599641799927 Time taken: 0.44376492500305176\n",
            "Epoch: 2 Batch Number: 97 Loss: 1.9478179216384888 Time taken: 0.4515228271484375\n",
            "Epoch: 2 Batch Number: 98 Loss: 1.9796875715255737 Time taken: 0.4391486644744873\n",
            "Epoch: 2 Batch Number: 99 Loss: 1.988180160522461 Time taken: 0.4501805305480957\n",
            "Epoch: 2 Batch Number: 100 Loss: 1.986415147781372 Time taken: 0.46245908737182617\n",
            "Epoch: 2 Batch Number: 101 Loss: 1.937857985496521 Time taken: 0.4576103687286377\n",
            "Epoch: 2 Batch Number: 102 Loss: 1.9848301410675049 Time taken: 0.45596909523010254\n",
            "Epoch: 2 Batch Number: 103 Loss: 2.0102951526641846 Time taken: 0.4508638381958008\n",
            "Epoch: 2 Batch Number: 104 Loss: 2.0493581295013428 Time taken: 0.44727063179016113\n",
            "Epoch: 2 Batch Number: 105 Loss: 2.009164333343506 Time taken: 0.44117140769958496\n",
            "Epoch: 2 Batch Number: 106 Loss: 2.001756429672241 Time taken: 0.46223998069763184\n",
            "Epoch: 2 Batch Number: 107 Loss: 1.9674410820007324 Time taken: 0.4627377986907959\n",
            "Epoch: 2 Batch Number: 108 Loss: 1.9224143028259277 Time taken: 0.4438514709472656\n",
            "Epoch: 2 Batch Number: 109 Loss: 1.9691765308380127 Time taken: 0.45873522758483887\n",
            "Epoch: 2 Batch Number: 110 Loss: 1.9420946836471558 Time taken: 0.46482157707214355\n",
            "Epoch: 2 Batch Number: 111 Loss: 1.8978623151779175 Time taken: 0.4426918029785156\n",
            "Epoch: 2 Batch Number: 112 Loss: 1.8469457626342773 Time taken: 0.4508941173553467\n",
            "Epoch: 2 Batch Number: 113 Loss: 1.904428482055664 Time taken: 0.44896626472473145\n",
            "Epoch: 2 Batch Number: 114 Loss: 1.8719068765640259 Time taken: 0.4422745704650879\n",
            "Epoch: 2 Batch Number: 115 Loss: 1.922600269317627 Time taken: 0.44225311279296875\n",
            "Epoch: 2 Batch Number: 116 Loss: 1.8149008750915527 Time taken: 0.44052720069885254\n",
            "Epoch: 2 Batch Number: 117 Loss: 1.8787868022918701 Time taken: 0.4417099952697754\n",
            "Epoch: 2 Batch Number: 118 Loss: 1.819448709487915 Time taken: 0.43810343742370605\n",
            "Epoch: 2 Batch Number: 119 Loss: 1.8346611261367798 Time taken: 0.46468615531921387\n",
            "Epoch: 2 Batch Number: 120 Loss: 1.9465529918670654 Time taken: 0.45528507232666016\n",
            "Epoch: 2 Batch Number: 121 Loss: 1.9133448600769043 Time taken: 0.44838953018188477\n",
            "Epoch: 2 Batch Number: 122 Loss: 2.0327677726745605 Time taken: 0.4420149326324463\n",
            "Epoch: 2 Batch Number: 123 Loss: 1.9456218481063843 Time taken: 0.4652063846588135\n",
            "Epoch: 2 Batch Number: 124 Loss: 1.9182751178741455 Time taken: 0.44343137741088867\n",
            "Epoch: 2 Batch Number: 125 Loss: 1.9445927143096924 Time taken: 0.44525790214538574\n",
            "Epoch: 2 Batch Number: 126 Loss: 1.9930576086044312 Time taken: 0.443331241607666\n",
            "Epoch: 2 Batch Number: 127 Loss: 2.0485036373138428 Time taken: 0.44607090950012207\n",
            "Epoch: 2 Batch Number: 128 Loss: 1.9171161651611328 Time taken: 0.4452784061431885\n",
            "Epoch: 2 Batch Number: 129 Loss: 1.9055962562561035 Time taken: 0.4439682960510254\n",
            "Epoch: 2 Batch Number: 130 Loss: 1.8624730110168457 Time taken: 0.46222662925720215\n",
            "Epoch: 2 Batch Number: 131 Loss: 1.9723834991455078 Time taken: 0.44464874267578125\n",
            "Epoch: 2 Batch Number: 132 Loss: 1.8921117782592773 Time taken: 0.4473145008087158\n",
            "Epoch: 2 Batch Number: 133 Loss: 1.914518117904663 Time taken: 0.46093153953552246\n",
            "Epoch: 2 Batch Number: 134 Loss: 1.960152506828308 Time taken: 0.44284749031066895\n",
            "Epoch: 2 Batch Number: 135 Loss: 1.943310022354126 Time taken: 0.4410543441772461\n",
            "Epoch: 2 Batch Number: 136 Loss: 1.8960715532302856 Time taken: 0.452573299407959\n",
            "Epoch: 2 Batch Number: 137 Loss: 1.948311448097229 Time taken: 0.45255184173583984\n",
            "Epoch: 2 Batch Number: 138 Loss: 1.8953993320465088 Time taken: 0.44616079330444336\n",
            "Epoch: 2 Batch Number: 139 Loss: 2.0181803703308105 Time taken: 0.4413337707519531\n",
            "Epoch: 2 Batch Number: 140 Loss: 1.9096651077270508 Time taken: 0.45175719261169434\n",
            "Epoch: 2 Batch Number: 141 Loss: 1.91845703125 Time taken: 0.4568333625793457\n",
            "Epoch: 2 Batch Number: 142 Loss: 1.909787893295288 Time taken: 0.44576096534729004\n",
            "Epoch: 2 Batch Number: 143 Loss: 1.9811670780181885 Time taken: 0.4544353485107422\n",
            "Epoch: 2 Batch Number: 144 Loss: 1.997572660446167 Time taken: 0.44863128662109375\n",
            "Epoch: 2 Batch Number: 145 Loss: 1.9439704418182373 Time taken: 0.45937132835388184\n",
            "Epoch: 2 Batch Number: 146 Loss: 1.9464813470840454 Time taken: 0.4515542984008789\n",
            "Epoch: 2 Batch Number: 147 Loss: 2.012589931488037 Time taken: 0.4429950714111328\n",
            "Epoch: 2 Batch Number: 148 Loss: 1.9242621660232544 Time taken: 0.4413433074951172\n",
            "Epoch: 2 Batch Number: 149 Loss: 1.9428178071975708 Time taken: 0.45197248458862305\n",
            "Epoch: 2 Batch Number: 150 Loss: 2.0180344581604004 Time taken: 0.4611039161682129\n",
            "Epoch: 2 Batch Number: 151 Loss: 2.0176069736480713 Time taken: 0.44782280921936035\n",
            "Epoch: 2 Batch Number: 152 Loss: 1.8975836038589478 Time taken: 0.4749939441680908\n",
            "Epoch: 2 Batch Number: 153 Loss: 1.8877613544464111 Time taken: 0.4700608253479004\n",
            "Epoch: 2 Batch Number: 154 Loss: 1.9435011148452759 Time taken: 0.4561116695404053\n",
            "Epoch: 2 Batch Number: 155 Loss: 1.8854880332946777 Time taken: 0.4539980888366699\n",
            "Epoch: 2 Batch Number: 156 Loss: 1.9018421173095703 Time taken: 0.47461724281311035\n",
            "Epoch: 2 Batch Number: 157 Loss: 1.8000737428665161 Time taken: 0.45204758644104004\n",
            "Epoch: 2 Batch Number: 158 Loss: 1.818595051765442 Time taken: 0.4543018341064453\n",
            "Epoch: 2 Batch Number: 159 Loss: 1.812604546546936 Time taken: 0.45559144020080566\n",
            "Epoch: 2 Batch Number: 160 Loss: 1.747010350227356 Time taken: 0.4521925449371338\n",
            "Epoch: 2 Batch Number: 161 Loss: 1.8060414791107178 Time taken: 0.4668252468109131\n",
            "Epoch: 2 Batch Number: 162 Loss: 1.7891299724578857 Time taken: 0.4521365165710449\n",
            "Epoch: 2 Batch Number: 163 Loss: 1.8230319023132324 Time taken: 0.4528970718383789\n",
            "Epoch: 2 Batch Number: 164 Loss: 2.004676580429077 Time taken: 0.44861769676208496\n",
            "Epoch: 2 Batch Number: 165 Loss: 2.03085994720459 Time taken: 0.4569699764251709\n",
            "Epoch: 2 Batch Number: 166 Loss: 1.9176585674285889 Time taken: 0.46264004707336426\n",
            "Epoch: 2 Batch Number: 167 Loss: 1.9464001655578613 Time taken: 0.4557363986968994\n",
            "Epoch: 2 Batch Number: 168 Loss: 1.995405912399292 Time taken: 0.456862211227417\n",
            "Epoch: 2 Batch Number: 169 Loss: 1.841784954071045 Time taken: 0.4417753219604492\n",
            "Epoch: 2 Batch Number: 170 Loss: 1.8063819408416748 Time taken: 0.4463002681732178\n",
            "Epoch: 2 Batch Number: 171 Loss: 1.877267837524414 Time taken: 0.4426388740539551\n",
            "Epoch: 2 Batch Number: 172 Loss: 1.8583322763442993 Time taken: 0.4416007995605469\n",
            "Epoch: 2 Batch Number: 173 Loss: 1.8063650131225586 Time taken: 0.44089317321777344\n",
            "Epoch: 2 Batch Number: 174 Loss: 1.840592384338379 Time taken: 0.4396061897277832\n",
            "Epoch: 2 Batch Number: 175 Loss: 1.832566261291504 Time taken: 0.44387054443359375\n",
            "Epoch: 2 Batch Number: 176 Loss: 1.7985130548477173 Time taken: 0.44263648986816406\n",
            "Epoch: 2 Batch Number: 177 Loss: 1.867013931274414 Time taken: 0.4507737159729004\n",
            "Epoch: 2 Batch Number: 178 Loss: 1.9170091152191162 Time taken: 0.4494140148162842\n",
            "Epoch: 2 Batch Number: 179 Loss: 2.0336689949035645 Time taken: 0.45892763137817383\n",
            "Epoch: 2 Batch Number: 180 Loss: 1.8801279067993164 Time taken: 0.4421665668487549\n",
            "Epoch: 2 Batch Number: 181 Loss: 1.947718620300293 Time taken: 0.4475882053375244\n",
            "Epoch: 2 Batch Number: 182 Loss: 1.844314455986023 Time taken: 0.45706701278686523\n",
            "Epoch: 2 Batch Number: 183 Loss: 1.9896790981292725 Time taken: 0.44632840156555176\n",
            "Epoch: 2 Batch Number: 184 Loss: 1.8342955112457275 Time taken: 0.46686339378356934\n",
            "Epoch: 2 Batch Number: 185 Loss: 1.8640620708465576 Time taken: 0.4503970146179199\n",
            "Epoch: 2 Batch Number: 186 Loss: 2.0026674270629883 Time taken: 0.45276856422424316\n",
            "Epoch: 2 Batch Number: 187 Loss: 1.8607313632965088 Time taken: 0.44490861892700195\n",
            "Epoch: 2 Batch Number: 188 Loss: 1.8662413358688354 Time taken: 0.4453318119049072\n",
            "Epoch: 2 Batch Number: 189 Loss: 1.834249496459961 Time taken: 0.44172143936157227\n",
            "Epoch: 2 Batch Number: 190 Loss: 1.969133734703064 Time taken: 0.45734667778015137\n",
            "Epoch: 2 Batch Number: 191 Loss: 2.162355899810791 Time taken: 0.44759058952331543\n",
            "Epoch: 2 Batch Number: 192 Loss: 1.910038948059082 Time taken: 0.4450106620788574\n",
            "Epoch: 2 Batch Number: 193 Loss: 2.0375092029571533 Time taken: 0.4597611427307129\n",
            "Epoch: 2 Batch Number: 194 Loss: 1.9357095956802368 Time taken: 0.45035648345947266\n",
            "Epoch: 2 Batch Number: 195 Loss: 1.8610570430755615 Time taken: 0.4617195129394531\n",
            "Epoch: 2 Batch Number: 196 Loss: 1.830648422241211 Time taken: 0.447368860244751\n",
            "Epoch: 2 Batch Number: 197 Loss: 1.8573575019836426 Time taken: 0.4574761390686035\n",
            "Epoch: 2 Batch Number: 198 Loss: 1.788464903831482 Time taken: 0.45673608779907227\n",
            "Epoch: 2 Batch Number: 199 Loss: 1.8399174213409424 Time taken: 0.45355892181396484\n",
            "Epoch: 2 Batch Number: 200 Loss: 1.7972123622894287 Time taken: 0.4478309154510498\n",
            "Epoch: 2 Batch Number: 201 Loss: 1.8125996589660645 Time taken: 0.4675614833831787\n",
            "Epoch: 2 Batch Number: 202 Loss: 1.9135841131210327 Time taken: 0.45510292053222656\n",
            "Epoch: 2 Batch Number: 203 Loss: 1.8303143978118896 Time taken: 0.44255495071411133\n",
            "Epoch: 2 Batch Number: 204 Loss: 1.8477203845977783 Time taken: 0.453143835067749\n",
            "Epoch: 2 Batch Number: 205 Loss: 1.7458460330963135 Time taken: 0.444765567779541\n",
            "Epoch: 2 Batch Number: 206 Loss: 1.8783142566680908 Time taken: 0.4556453227996826\n",
            "Epoch: 2 Batch Number: 207 Loss: 1.7512180805206299 Time taken: 0.44893693923950195\n",
            "Epoch: 2 Batch Number: 208 Loss: 1.8018031120300293 Time taken: 0.44670796394348145\n",
            "Epoch: 2 Batch Number: 209 Loss: 1.8103224039077759 Time taken: 0.4430105686187744\n",
            "Epoch: 2 Batch Number: 210 Loss: 1.8698277473449707 Time taken: 0.4529299736022949\n",
            "Epoch: 2 Batch Number: 211 Loss: 1.8598673343658447 Time taken: 0.44986510276794434\n",
            "Epoch: 2 Batch Number: 212 Loss: 1.9108495712280273 Time taken: 0.43999218940734863\n",
            "Epoch: 2 Batch Number: 213 Loss: 1.8403143882751465 Time taken: 0.44989442825317383\n",
            "Epoch: 2 Batch Number: 214 Loss: 1.8901585340499878 Time taken: 0.4497392177581787\n",
            "Epoch: 2 Batch Number: 215 Loss: 1.8463330268859863 Time taken: 0.4552912712097168\n",
            "Epoch: 2 Batch Number: 216 Loss: 1.8152928352355957 Time taken: 0.4424271583557129\n",
            "Epoch: 2 Batch Number: 217 Loss: 1.9321528673171997 Time taken: 0.4506962299346924\n",
            "Epoch: 2 Batch Number: 218 Loss: 1.9417537450790405 Time taken: 0.4572603702545166\n",
            "Epoch: 2 Batch Number: 219 Loss: 1.8538939952850342 Time taken: 0.45235109329223633\n",
            "Epoch: 2 Batch Number: 220 Loss: 1.8789057731628418 Time taken: 0.44667482376098633\n",
            "Epoch: 2 Batch Number: 221 Loss: 1.8328676223754883 Time taken: 0.449573278427124\n",
            "Epoch: 2 Batch Number: 222 Loss: 1.8939459323883057 Time taken: 0.45464372634887695\n",
            "Epoch: 2 Batch Number: 223 Loss: 1.8921124935150146 Time taken: 0.4380779266357422\n",
            "Epoch: 2 Batch Number: 224 Loss: 1.843147873878479 Time taken: 0.44354987144470215\n",
            "Epoch: 2 Batch Number: 225 Loss: 1.7871582508087158 Time taken: 0.4416468143463135\n",
            "Epoch: 2 Batch Number: 226 Loss: 1.8024852275848389 Time taken: 0.44469523429870605\n",
            "Epoch: 2 Batch Number: 227 Loss: 1.8550059795379639 Time taken: 0.4460430145263672\n",
            "Epoch: 2 Batch Number: 228 Loss: 1.8492050170898438 Time taken: 0.4530308246612549\n",
            "Epoch: 2 Batch Number: 229 Loss: 1.844204306602478 Time taken: 0.4476015567779541\n",
            "==========================================================================================\n",
            "Start of epoch 3\n",
            "Epoch: 3 Batch Number: 1 Loss: 1.7866582870483398 Time taken: 0.4386906623840332\n",
            "Epoch: 3 Batch Number: 2 Loss: 1.7889313697814941 Time taken: 0.44533348083496094\n",
            "Epoch: 3 Batch Number: 3 Loss: 1.7892335653305054 Time taken: 0.44878530502319336\n",
            "Epoch: 3 Batch Number: 4 Loss: 1.8387819528579712 Time taken: 0.4741792678833008\n",
            "Epoch: 3 Batch Number: 5 Loss: 1.8116421699523926 Time taken: 0.4424586296081543\n",
            "Epoch: 3 Batch Number: 6 Loss: 1.7404836416244507 Time taken: 0.45285487174987793\n",
            "Epoch: 3 Batch Number: 7 Loss: 1.7280609607696533 Time taken: 0.44199442863464355\n",
            "Epoch: 3 Batch Number: 8 Loss: 1.69658362865448 Time taken: 0.45273447036743164\n",
            "Epoch: 3 Batch Number: 9 Loss: 1.7963614463806152 Time taken: 0.452880859375\n",
            "Epoch: 3 Batch Number: 10 Loss: 1.8027721643447876 Time taken: 0.46206092834472656\n",
            "Epoch: 3 Batch Number: 11 Loss: 1.709336280822754 Time taken: 0.45786380767822266\n",
            "Epoch: 3 Batch Number: 12 Loss: 1.730327844619751 Time taken: 0.46889686584472656\n",
            "Epoch: 3 Batch Number: 13 Loss: 1.7604546546936035 Time taken: 0.4400203227996826\n",
            "Epoch: 3 Batch Number: 14 Loss: 1.793480396270752 Time taken: 0.4408738613128662\n",
            "Epoch: 3 Batch Number: 15 Loss: 1.8054625988006592 Time taken: 0.45987868309020996\n",
            "Epoch: 3 Batch Number: 16 Loss: 1.873492956161499 Time taken: 0.44356679916381836\n",
            "Epoch: 3 Batch Number: 17 Loss: 1.9491337537765503 Time taken: 0.45995140075683594\n",
            "Epoch: 3 Batch Number: 18 Loss: 1.830747127532959 Time taken: 0.46155858039855957\n",
            "Epoch: 3 Batch Number: 19 Loss: 1.8170249462127686 Time taken: 0.4597816467285156\n",
            "Epoch: 3 Batch Number: 20 Loss: 1.7593655586242676 Time taken: 0.4429500102996826\n",
            "Epoch: 3 Batch Number: 21 Loss: 1.9354209899902344 Time taken: 0.46167755126953125\n",
            "Epoch: 3 Batch Number: 22 Loss: 1.8952783346176147 Time taken: 0.4441206455230713\n",
            "Epoch: 3 Batch Number: 23 Loss: 1.9214720726013184 Time taken: 0.45660829544067383\n",
            "Epoch: 3 Batch Number: 24 Loss: 1.841798186302185 Time taken: 0.4533116817474365\n",
            "Epoch: 3 Batch Number: 25 Loss: 1.8784350156784058 Time taken: 0.4460716247558594\n",
            "Epoch: 3 Batch Number: 26 Loss: 1.8307905197143555 Time taken: 0.4504220485687256\n",
            "Epoch: 3 Batch Number: 27 Loss: 1.7638357877731323 Time taken: 0.44326043128967285\n",
            "Epoch: 3 Batch Number: 28 Loss: 1.7731331586837769 Time taken: 0.4540235996246338\n",
            "Epoch: 3 Batch Number: 29 Loss: 1.7565710544586182 Time taken: 0.4402775764465332\n",
            "Epoch: 3 Batch Number: 30 Loss: 1.69608736038208 Time taken: 0.4674198627471924\n",
            "Epoch: 3 Batch Number: 31 Loss: 1.7440001964569092 Time taken: 0.4530484676361084\n",
            "Epoch: 3 Batch Number: 32 Loss: 1.7685120105743408 Time taken: 0.4768807888031006\n",
            "Epoch: 3 Batch Number: 33 Loss: 1.7547258138656616 Time taken: 0.45572423934936523\n",
            "Epoch: 3 Batch Number: 34 Loss: 1.756701946258545 Time taken: 0.44406700134277344\n",
            "Epoch: 3 Batch Number: 35 Loss: 1.8246564865112305 Time taken: 0.4438760280609131\n",
            "Epoch: 3 Batch Number: 36 Loss: 1.8454110622406006 Time taken: 0.44420719146728516\n",
            "Epoch: 3 Batch Number: 37 Loss: 1.8117625713348389 Time taken: 0.4451766014099121\n",
            "Epoch: 3 Batch Number: 38 Loss: 1.7235804796218872 Time taken: 0.4650251865386963\n",
            "Epoch: 3 Batch Number: 39 Loss: 1.771986961364746 Time taken: 0.4539175033569336\n",
            "Epoch: 3 Batch Number: 40 Loss: 1.7781434059143066 Time taken: 0.45587658882141113\n",
            "Epoch: 3 Batch Number: 41 Loss: 1.7595751285552979 Time taken: 0.455981969833374\n",
            "Epoch: 3 Batch Number: 42 Loss: 1.7449939250946045 Time taken: 0.4631540775299072\n",
            "Epoch: 3 Batch Number: 43 Loss: 1.7648184299468994 Time taken: 0.45058465003967285\n",
            "Epoch: 3 Batch Number: 44 Loss: 1.6583421230316162 Time taken: 0.4625589847564697\n",
            "Epoch: 3 Batch Number: 45 Loss: 1.675167441368103 Time taken: 0.4754176139831543\n",
            "Epoch: 3 Batch Number: 46 Loss: 1.8128485679626465 Time taken: 0.4478118419647217\n",
            "Epoch: 3 Batch Number: 47 Loss: 1.7855873107910156 Time taken: 0.45334362983703613\n",
            "Epoch: 3 Batch Number: 48 Loss: 1.8365249633789062 Time taken: 0.4404258728027344\n",
            "Epoch: 3 Batch Number: 49 Loss: 1.797844409942627 Time taken: 0.44998860359191895\n",
            "Epoch: 3 Batch Number: 50 Loss: 1.7286655902862549 Time taken: 0.4631025791168213\n",
            "Epoch: 3 Batch Number: 51 Loss: 1.7173972129821777 Time taken: 0.44416069984436035\n",
            "Epoch: 3 Batch Number: 52 Loss: 1.8015449047088623 Time taken: 0.44672298431396484\n",
            "Epoch: 3 Batch Number: 53 Loss: 1.9099278450012207 Time taken: 0.4452366828918457\n",
            "Epoch: 3 Batch Number: 54 Loss: 1.873471736907959 Time taken: 0.4518706798553467\n",
            "Epoch: 3 Batch Number: 55 Loss: 1.839287519454956 Time taken: 0.4538869857788086\n",
            "Epoch: 3 Batch Number: 56 Loss: 1.8599555492401123 Time taken: 0.46134352684020996\n",
            "Epoch: 3 Batch Number: 57 Loss: 1.7601593732833862 Time taken: 0.4836771488189697\n",
            "Epoch: 3 Batch Number: 58 Loss: 1.729179859161377 Time taken: 0.45189762115478516\n",
            "Epoch: 3 Batch Number: 59 Loss: 1.7463486194610596 Time taken: 0.4472067356109619\n",
            "Epoch: 3 Batch Number: 60 Loss: 1.7632555961608887 Time taken: 0.4400203227996826\n",
            "Epoch: 3 Batch Number: 61 Loss: 1.8120375871658325 Time taken: 0.457766056060791\n",
            "Epoch: 3 Batch Number: 62 Loss: 1.7574427127838135 Time taken: 0.44631266593933105\n",
            "Epoch: 3 Batch Number: 63 Loss: 1.6587568521499634 Time taken: 0.4643726348876953\n",
            "Epoch: 3 Batch Number: 64 Loss: 1.6589590311050415 Time taken: 0.44374799728393555\n",
            "Epoch: 3 Batch Number: 65 Loss: 1.6810345649719238 Time taken: 0.4504127502441406\n",
            "Epoch: 3 Batch Number: 66 Loss: 1.7145010232925415 Time taken: 0.4404275417327881\n",
            "Epoch: 3 Batch Number: 67 Loss: 1.6761940717697144 Time taken: 0.45063090324401855\n",
            "Epoch: 3 Batch Number: 68 Loss: 1.719887375831604 Time taken: 0.46500635147094727\n",
            "Epoch: 3 Batch Number: 69 Loss: 1.7853732109069824 Time taken: 0.4490702152252197\n",
            "Epoch: 3 Batch Number: 70 Loss: 1.6649404764175415 Time taken: 0.44694972038269043\n",
            "Epoch: 3 Batch Number: 71 Loss: 1.7941056489944458 Time taken: 0.4462926387786865\n",
            "Epoch: 3 Batch Number: 72 Loss: 1.7632540464401245 Time taken: 0.441650390625\n",
            "Epoch: 3 Batch Number: 73 Loss: 1.826514482498169 Time taken: 0.4575939178466797\n",
            "Epoch: 3 Batch Number: 74 Loss: 1.9839746952056885 Time taken: 0.44536900520324707\n",
            "Epoch: 3 Batch Number: 75 Loss: 1.739152193069458 Time taken: 0.4464402198791504\n",
            "Epoch: 3 Batch Number: 76 Loss: 1.8991703987121582 Time taken: 0.44318723678588867\n",
            "Epoch: 3 Batch Number: 77 Loss: 1.8316287994384766 Time taken: 0.4424307346343994\n",
            "Epoch: 3 Batch Number: 78 Loss: 1.7676687240600586 Time taken: 0.4408094882965088\n",
            "Epoch: 3 Batch Number: 79 Loss: 1.6956710815429688 Time taken: 0.45271801948547363\n",
            "Epoch: 3 Batch Number: 80 Loss: 1.7526805400848389 Time taken: 0.44504785537719727\n",
            "Epoch: 3 Batch Number: 81 Loss: 1.711319923400879 Time taken: 0.45538902282714844\n",
            "Epoch: 3 Batch Number: 82 Loss: 1.763486623764038 Time taken: 0.44420337677001953\n",
            "Epoch: 3 Batch Number: 83 Loss: 1.7579121589660645 Time taken: 0.46034789085388184\n",
            "Epoch: 3 Batch Number: 84 Loss: 1.8195090293884277 Time taken: 0.4471287727355957\n",
            "Epoch: 3 Batch Number: 85 Loss: 1.7672479152679443 Time taken: 0.4412996768951416\n",
            "Epoch: 3 Batch Number: 86 Loss: 1.8191242218017578 Time taken: 0.44768548011779785\n",
            "Epoch: 3 Batch Number: 87 Loss: 1.7476420402526855 Time taken: 0.4635336399078369\n",
            "Epoch: 3 Batch Number: 88 Loss: 1.6528561115264893 Time taken: 0.4727170467376709\n",
            "Epoch: 3 Batch Number: 89 Loss: 1.6779110431671143 Time taken: 0.4704773426055908\n",
            "Epoch: 3 Batch Number: 90 Loss: 1.6644394397735596 Time taken: 0.47183990478515625\n",
            "Epoch: 3 Batch Number: 91 Loss: 1.7908320426940918 Time taken: 0.45067715644836426\n",
            "Epoch: 3 Batch Number: 92 Loss: 1.6813077926635742 Time taken: 0.4387390613555908\n",
            "Epoch: 3 Batch Number: 93 Loss: 1.6811790466308594 Time taken: 0.4412658214569092\n",
            "Epoch: 3 Batch Number: 94 Loss: 1.7735416889190674 Time taken: 0.4557340145111084\n",
            "Epoch: 3 Batch Number: 95 Loss: 1.701838493347168 Time taken: 0.4451944828033447\n",
            "Epoch: 3 Batch Number: 96 Loss: 1.7453289031982422 Time taken: 0.45587754249572754\n",
            "Epoch: 3 Batch Number: 97 Loss: 1.7554757595062256 Time taken: 0.44904041290283203\n",
            "Epoch: 3 Batch Number: 98 Loss: 1.7842934131622314 Time taken: 0.4478263854980469\n",
            "Epoch: 3 Batch Number: 99 Loss: 1.815969467163086 Time taken: 0.4646165370941162\n",
            "Epoch: 3 Batch Number: 100 Loss: 1.7986090183258057 Time taken: 0.46023988723754883\n",
            "Epoch: 3 Batch Number: 101 Loss: 1.7429736852645874 Time taken: 0.45362353324890137\n",
            "Epoch: 3 Batch Number: 102 Loss: 1.791450023651123 Time taken: 0.44614672660827637\n",
            "Epoch: 3 Batch Number: 103 Loss: 1.8228833675384521 Time taken: 0.45546412467956543\n",
            "Epoch: 3 Batch Number: 104 Loss: 1.8528921604156494 Time taken: 0.44127702713012695\n",
            "Epoch: 3 Batch Number: 105 Loss: 1.8074970245361328 Time taken: 0.4559342861175537\n",
            "Epoch: 3 Batch Number: 106 Loss: 1.787405252456665 Time taken: 0.45210814476013184\n",
            "Epoch: 3 Batch Number: 107 Loss: 1.7426934242248535 Time taken: 0.47530603408813477\n",
            "Epoch: 3 Batch Number: 108 Loss: 1.701117992401123 Time taken: 0.45902442932128906\n",
            "Epoch: 3 Batch Number: 109 Loss: 1.7380179166793823 Time taken: 0.45624852180480957\n",
            "Epoch: 3 Batch Number: 110 Loss: 1.6263117790222168 Time taken: 0.46016931533813477\n",
            "Epoch: 3 Batch Number: 111 Loss: 1.6567579507827759 Time taken: 0.4395747184753418\n",
            "Epoch: 3 Batch Number: 112 Loss: 1.6167689561843872 Time taken: 0.4621918201446533\n",
            "Epoch: 3 Batch Number: 113 Loss: 1.7064683437347412 Time taken: 0.45055365562438965\n",
            "Epoch: 3 Batch Number: 114 Loss: 1.677825927734375 Time taken: 0.4610936641693115\n",
            "Epoch: 3 Batch Number: 115 Loss: 1.7333924770355225 Time taken: 0.448580265045166\n",
            "Epoch: 3 Batch Number: 116 Loss: 1.5939666032791138 Time taken: 0.44197559356689453\n",
            "Epoch: 3 Batch Number: 117 Loss: 1.6533174514770508 Time taken: 0.44179463386535645\n",
            "Epoch: 3 Batch Number: 118 Loss: 1.5657085180282593 Time taken: 0.4483835697174072\n",
            "Epoch: 3 Batch Number: 119 Loss: 1.6189329624176025 Time taken: 0.45183444023132324\n",
            "Epoch: 3 Batch Number: 120 Loss: 1.7299765348434448 Time taken: 0.4535486698150635\n",
            "Epoch: 3 Batch Number: 121 Loss: 1.669967532157898 Time taken: 0.4685549736022949\n",
            "Epoch: 3 Batch Number: 122 Loss: 1.7869055271148682 Time taken: 0.4573974609375\n",
            "Epoch: 3 Batch Number: 123 Loss: 1.7355989217758179 Time taken: 0.4630558490753174\n",
            "Epoch: 3 Batch Number: 124 Loss: 1.697143316268921 Time taken: 0.4508233070373535\n",
            "Epoch: 3 Batch Number: 125 Loss: 1.7595810890197754 Time taken: 0.44179558753967285\n",
            "Epoch: 3 Batch Number: 126 Loss: 1.821930170059204 Time taken: 0.4429495334625244\n",
            "Epoch: 3 Batch Number: 127 Loss: 1.8631633520126343 Time taken: 0.4392693042755127\n",
            "Epoch: 3 Batch Number: 128 Loss: 1.7107925415039062 Time taken: 0.4487431049346924\n",
            "Epoch: 3 Batch Number: 129 Loss: 1.704658031463623 Time taken: 0.4538249969482422\n",
            "Epoch: 3 Batch Number: 130 Loss: 1.680603265762329 Time taken: 0.4508860111236572\n",
            "Epoch: 3 Batch Number: 131 Loss: 1.783315896987915 Time taken: 0.44636082649230957\n",
            "Epoch: 3 Batch Number: 132 Loss: 1.6804709434509277 Time taken: 0.44786572456359863\n",
            "Epoch: 3 Batch Number: 133 Loss: 1.7244420051574707 Time taken: 0.44895100593566895\n",
            "Epoch: 3 Batch Number: 134 Loss: 1.7821835279464722 Time taken: 0.44777607917785645\n",
            "Epoch: 3 Batch Number: 135 Loss: 1.7717360258102417 Time taken: 0.44742679595947266\n",
            "Epoch: 3 Batch Number: 136 Loss: 1.725512981414795 Time taken: 0.4571950435638428\n",
            "Epoch: 3 Batch Number: 137 Loss: 1.7711269855499268 Time taken: 0.4580833911895752\n",
            "Epoch: 3 Batch Number: 138 Loss: 1.6851760149002075 Time taken: 0.46626925468444824\n",
            "Epoch: 3 Batch Number: 139 Loss: 1.7978986501693726 Time taken: 0.46039819717407227\n",
            "Epoch: 3 Batch Number: 140 Loss: 1.677344560623169 Time taken: 0.4514622688293457\n",
            "Epoch: 3 Batch Number: 141 Loss: 1.7171334028244019 Time taken: 0.44575047492980957\n",
            "Epoch: 3 Batch Number: 142 Loss: 1.7291308641433716 Time taken: 0.44852614402770996\n",
            "Epoch: 3 Batch Number: 143 Loss: 1.8179692029953003 Time taken: 0.4689803123474121\n",
            "Epoch: 3 Batch Number: 144 Loss: 1.8496668338775635 Time taken: 0.4617588520050049\n",
            "Epoch: 3 Batch Number: 145 Loss: 1.793107032775879 Time taken: 0.4570462703704834\n",
            "Epoch: 3 Batch Number: 146 Loss: 1.8039159774780273 Time taken: 0.45157766342163086\n",
            "Epoch: 3 Batch Number: 147 Loss: 1.8394644260406494 Time taken: 0.45949459075927734\n",
            "Epoch: 3 Batch Number: 148 Loss: 1.748911738395691 Time taken: 0.4572937488555908\n",
            "Epoch: 3 Batch Number: 149 Loss: 1.7841650247573853 Time taken: 0.44356822967529297\n",
            "Epoch: 3 Batch Number: 150 Loss: 1.8750531673431396 Time taken: 0.4484846591949463\n",
            "Epoch: 3 Batch Number: 151 Loss: 1.8585858345031738 Time taken: 0.45386314392089844\n",
            "Epoch: 3 Batch Number: 152 Loss: 1.7227096557617188 Time taken: 0.45000338554382324\n",
            "Epoch: 3 Batch Number: 153 Loss: 1.6981041431427002 Time taken: 0.4449920654296875\n",
            "Epoch: 3 Batch Number: 154 Loss: 1.7699222564697266 Time taken: 0.45137882232666016\n",
            "Epoch: 3 Batch Number: 155 Loss: 1.6703482866287231 Time taken: 0.4432961940765381\n",
            "Epoch: 3 Batch Number: 156 Loss: 1.7015202045440674 Time taken: 0.4423201084136963\n",
            "Epoch: 3 Batch Number: 157 Loss: 1.5884501934051514 Time taken: 0.4543912410736084\n",
            "Epoch: 3 Batch Number: 158 Loss: 1.6229184865951538 Time taken: 0.47337961196899414\n",
            "Epoch: 3 Batch Number: 159 Loss: 1.6094791889190674 Time taken: 0.44592785835266113\n",
            "Epoch: 3 Batch Number: 160 Loss: 1.547468900680542 Time taken: 0.43997931480407715\n",
            "Epoch: 3 Batch Number: 161 Loss: 1.6156723499298096 Time taken: 0.44855546951293945\n",
            "Epoch: 3 Batch Number: 162 Loss: 1.5855627059936523 Time taken: 0.47117185592651367\n",
            "Epoch: 3 Batch Number: 163 Loss: 1.6255934238433838 Time taken: 0.4460322856903076\n",
            "Epoch: 3 Batch Number: 164 Loss: 1.8049063682556152 Time taken: 0.443831205368042\n",
            "Epoch: 3 Batch Number: 165 Loss: 1.8028829097747803 Time taken: 0.44820332527160645\n",
            "Epoch: 3 Batch Number: 166 Loss: 1.6963136196136475 Time taken: 0.4449453353881836\n",
            "Epoch: 3 Batch Number: 167 Loss: 1.72749662399292 Time taken: 0.44431066513061523\n",
            "Epoch: 3 Batch Number: 168 Loss: 1.795050024986267 Time taken: 0.44504714012145996\n",
            "Epoch: 3 Batch Number: 169 Loss: 1.6672205924987793 Time taken: 0.44593238830566406\n",
            "Epoch: 3 Batch Number: 170 Loss: 1.6200077533721924 Time taken: 0.4511871337890625\n",
            "Epoch: 3 Batch Number: 171 Loss: 1.6816091537475586 Time taken: 0.4667856693267822\n",
            "Epoch: 3 Batch Number: 172 Loss: 1.6779732704162598 Time taken: 0.459836483001709\n",
            "Epoch: 3 Batch Number: 173 Loss: 1.620232343673706 Time taken: 0.458324670791626\n",
            "Epoch: 3 Batch Number: 174 Loss: 1.6173853874206543 Time taken: 0.4591939449310303\n",
            "Epoch: 3 Batch Number: 175 Loss: 1.657470464706421 Time taken: 0.4457852840423584\n",
            "Epoch: 3 Batch Number: 176 Loss: 1.6059768199920654 Time taken: 0.44502902030944824\n",
            "Epoch: 3 Batch Number: 177 Loss: 1.6844916343688965 Time taken: 0.44939351081848145\n",
            "Epoch: 3 Batch Number: 178 Loss: 1.7236934900283813 Time taken: 0.4452238082885742\n",
            "Epoch: 3 Batch Number: 179 Loss: 1.8247041702270508 Time taken: 0.45400404930114746\n",
            "Epoch: 3 Batch Number: 180 Loss: 1.679591417312622 Time taken: 0.4581162929534912\n",
            "Epoch: 3 Batch Number: 181 Loss: 1.7262752056121826 Time taken: 0.4646718502044678\n",
            "Epoch: 3 Batch Number: 182 Loss: 1.6223039627075195 Time taken: 0.4582648277282715\n",
            "Epoch: 3 Batch Number: 183 Loss: 1.7481015920639038 Time taken: 0.44616079330444336\n",
            "Epoch: 3 Batch Number: 184 Loss: 1.6310577392578125 Time taken: 0.450214147567749\n",
            "Epoch: 3 Batch Number: 185 Loss: 1.6826218366622925 Time taken: 0.44557738304138184\n",
            "Epoch: 3 Batch Number: 186 Loss: 1.7400901317596436 Time taken: 0.47217845916748047\n",
            "Epoch: 3 Batch Number: 187 Loss: 1.6492785215377808 Time taken: 0.44629526138305664\n",
            "Epoch: 3 Batch Number: 188 Loss: 1.663712501525879 Time taken: 0.4737727642059326\n",
            "Epoch: 3 Batch Number: 189 Loss: 1.649141788482666 Time taken: 0.4668397903442383\n",
            "Epoch: 3 Batch Number: 190 Loss: 1.7780721187591553 Time taken: 0.458937406539917\n",
            "Epoch: 3 Batch Number: 191 Loss: 2.0430822372436523 Time taken: 0.4532499313354492\n",
            "Epoch: 3 Batch Number: 192 Loss: 1.7086271047592163 Time taken: 0.4607126712799072\n",
            "Epoch: 3 Batch Number: 193 Loss: 1.8418340682983398 Time taken: 0.4509556293487549\n",
            "Epoch: 3 Batch Number: 194 Loss: 1.7358919382095337 Time taken: 0.45641446113586426\n",
            "Epoch: 3 Batch Number: 195 Loss: 1.6325993537902832 Time taken: 0.4486355781555176\n",
            "Epoch: 3 Batch Number: 196 Loss: 1.5905611515045166 Time taken: 0.448932409286499\n",
            "Epoch: 3 Batch Number: 197 Loss: 1.668139934539795 Time taken: 0.44786715507507324\n",
            "Epoch: 3 Batch Number: 198 Loss: 1.5732289552688599 Time taken: 0.4510931968688965\n",
            "Epoch: 3 Batch Number: 199 Loss: 1.6726895570755005 Time taken: 0.4492049217224121\n",
            "Epoch: 3 Batch Number: 200 Loss: 1.5901086330413818 Time taken: 0.4451565742492676\n",
            "Epoch: 3 Batch Number: 201 Loss: 1.6201198101043701 Time taken: 0.46030187606811523\n",
            "Epoch: 3 Batch Number: 202 Loss: 1.724900722503662 Time taken: 0.455214262008667\n",
            "Epoch: 3 Batch Number: 203 Loss: 1.6424269676208496 Time taken: 0.4657580852508545\n",
            "Epoch: 3 Batch Number: 204 Loss: 1.6627389192581177 Time taken: 0.45555615425109863\n",
            "Epoch: 3 Batch Number: 205 Loss: 1.5505995750427246 Time taken: 0.4635288715362549\n",
            "Epoch: 3 Batch Number: 206 Loss: 1.7085862159729004 Time taken: 0.46910548210144043\n",
            "Epoch: 3 Batch Number: 207 Loss: 1.5453863143920898 Time taken: 0.46134090423583984\n",
            "Epoch: 3 Batch Number: 208 Loss: 1.6116201877593994 Time taken: 0.4730870723724365\n",
            "Epoch: 3 Batch Number: 209 Loss: 1.6254849433898926 Time taken: 0.46491289138793945\n",
            "Epoch: 3 Batch Number: 210 Loss: 1.7083706855773926 Time taken: 0.45719361305236816\n",
            "Epoch: 3 Batch Number: 211 Loss: 1.7162325382232666 Time taken: 0.4553256034851074\n",
            "Epoch: 3 Batch Number: 212 Loss: 1.7628059387207031 Time taken: 0.4714667797088623\n",
            "Epoch: 3 Batch Number: 213 Loss: 1.6757516860961914 Time taken: 0.46619462966918945\n",
            "Epoch: 3 Batch Number: 214 Loss: 1.6866075992584229 Time taken: 0.4665985107421875\n",
            "Epoch: 3 Batch Number: 215 Loss: 1.6529065370559692 Time taken: 0.47965073585510254\n",
            "Epoch: 3 Batch Number: 216 Loss: 1.6263539791107178 Time taken: 0.47334766387939453\n",
            "Epoch: 3 Batch Number: 217 Loss: 1.7366729974746704 Time taken: 0.46010422706604004\n",
            "Epoch: 3 Batch Number: 218 Loss: 1.7707010507583618 Time taken: 0.4554867744445801\n",
            "Epoch: 3 Batch Number: 219 Loss: 1.683677315711975 Time taken: 0.46755337715148926\n",
            "Epoch: 3 Batch Number: 220 Loss: 1.6878113746643066 Time taken: 0.46486687660217285\n",
            "Epoch: 3 Batch Number: 221 Loss: 1.6447067260742188 Time taken: 0.45136046409606934\n",
            "Epoch: 3 Batch Number: 222 Loss: 1.6684436798095703 Time taken: 0.464508056640625\n",
            "Epoch: 3 Batch Number: 223 Loss: 1.6995524168014526 Time taken: 0.4678165912628174\n",
            "Epoch: 3 Batch Number: 224 Loss: 1.6312880516052246 Time taken: 0.4618358612060547\n",
            "Epoch: 3 Batch Number: 225 Loss: 1.5872371196746826 Time taken: 0.4449801445007324\n",
            "Epoch: 3 Batch Number: 226 Loss: 1.6055399179458618 Time taken: 0.4460928440093994\n",
            "Epoch: 3 Batch Number: 227 Loss: 1.6988015174865723 Time taken: 0.4556100368499756\n",
            "Epoch: 3 Batch Number: 228 Loss: 1.682213544845581 Time taken: 0.44508886337280273\n",
            "Epoch: 3 Batch Number: 229 Loss: 1.686653733253479 Time taken: 0.4452955722808838\n",
            "==========================================================================================\n",
            "Start of epoch 4\n",
            "Epoch: 4 Batch Number: 1 Loss: 1.603060245513916 Time taken: 0.44462037086486816\n",
            "Epoch: 4 Batch Number: 2 Loss: 1.6321067810058594 Time taken: 0.47833752632141113\n",
            "Epoch: 4 Batch Number: 3 Loss: 1.6150072813034058 Time taken: 0.45900487899780273\n",
            "Epoch: 4 Batch Number: 4 Loss: 1.6564066410064697 Time taken: 0.45343756675720215\n",
            "Epoch: 4 Batch Number: 5 Loss: 1.645240306854248 Time taken: 0.47983431816101074\n",
            "Epoch: 4 Batch Number: 6 Loss: 1.552521824836731 Time taken: 0.44702792167663574\n",
            "Epoch: 4 Batch Number: 7 Loss: 1.540534496307373 Time taken: 0.44521570205688477\n",
            "Epoch: 4 Batch Number: 8 Loss: 1.5169470310211182 Time taken: 0.4670732021331787\n",
            "Epoch: 4 Batch Number: 9 Loss: 1.5924162864685059 Time taken: 0.4550507068634033\n",
            "Epoch: 4 Batch Number: 10 Loss: 1.6157822608947754 Time taken: 0.4444694519042969\n",
            "Epoch: 4 Batch Number: 11 Loss: 1.5126930475234985 Time taken: 0.4471614360809326\n",
            "Epoch: 4 Batch Number: 12 Loss: 1.5267423391342163 Time taken: 0.44207096099853516\n",
            "Epoch: 4 Batch Number: 13 Loss: 1.5769658088684082 Time taken: 0.45671749114990234\n",
            "Epoch: 4 Batch Number: 14 Loss: 1.5911262035369873 Time taken: 0.44049715995788574\n",
            "Epoch: 4 Batch Number: 15 Loss: 1.5779051780700684 Time taken: 0.4510354995727539\n",
            "Epoch: 4 Batch Number: 16 Loss: 1.6591179370880127 Time taken: 0.45627522468566895\n",
            "Epoch: 4 Batch Number: 17 Loss: 1.7687551975250244 Time taken: 0.45259976387023926\n",
            "Epoch: 4 Batch Number: 18 Loss: 1.6696745157241821 Time taken: 0.4596855640411377\n",
            "Epoch: 4 Batch Number: 19 Loss: 1.6585842370986938 Time taken: 0.45802903175354004\n",
            "Epoch: 4 Batch Number: 20 Loss: 1.561919927597046 Time taken: 0.45172977447509766\n",
            "Epoch: 4 Batch Number: 21 Loss: 1.8019063472747803 Time taken: 0.45418429374694824\n",
            "Epoch: 4 Batch Number: 22 Loss: 1.7212735414505005 Time taken: 0.4521195888519287\n",
            "Epoch: 4 Batch Number: 23 Loss: 1.7478091716766357 Time taken: 0.46387600898742676\n",
            "Epoch: 4 Batch Number: 24 Loss: 1.668954610824585 Time taken: 0.46968555450439453\n",
            "Epoch: 4 Batch Number: 25 Loss: 1.7051026821136475 Time taken: 0.45863819122314453\n",
            "Epoch: 4 Batch Number: 26 Loss: 1.673306941986084 Time taken: 0.4883732795715332\n",
            "Epoch: 4 Batch Number: 27 Loss: 1.6016929149627686 Time taken: 0.46106934547424316\n",
            "Epoch: 4 Batch Number: 28 Loss: 1.594801902770996 Time taken: 0.44454479217529297\n",
            "Epoch: 4 Batch Number: 29 Loss: 1.587942361831665 Time taken: 0.46950697898864746\n",
            "Epoch: 4 Batch Number: 30 Loss: 1.4943362474441528 Time taken: 0.4639608860015869\n",
            "Epoch: 4 Batch Number: 31 Loss: 1.5643696784973145 Time taken: 0.45831894874572754\n",
            "Epoch: 4 Batch Number: 32 Loss: 1.5808947086334229 Time taken: 0.451221227645874\n",
            "Epoch: 4 Batch Number: 33 Loss: 1.592615008354187 Time taken: 0.4459362030029297\n",
            "Epoch: 4 Batch Number: 34 Loss: 1.6121519804000854 Time taken: 0.4502406120300293\n",
            "Epoch: 4 Batch Number: 35 Loss: 1.6615241765975952 Time taken: 0.44545674324035645\n",
            "Epoch: 4 Batch Number: 36 Loss: 1.7293970584869385 Time taken: 0.4461672306060791\n",
            "Epoch: 4 Batch Number: 37 Loss: 1.6396186351776123 Time taken: 0.45290470123291016\n",
            "Epoch: 4 Batch Number: 38 Loss: 1.5651614665985107 Time taken: 0.4470500946044922\n",
            "Epoch: 4 Batch Number: 39 Loss: 1.591223955154419 Time taken: 0.44698476791381836\n",
            "Epoch: 4 Batch Number: 40 Loss: 1.604772925376892 Time taken: 0.4610438346862793\n",
            "Epoch: 4 Batch Number: 41 Loss: 1.5574989318847656 Time taken: 0.4524087905883789\n",
            "Epoch: 4 Batch Number: 42 Loss: 1.5642982721328735 Time taken: 0.4554769992828369\n",
            "Epoch: 4 Batch Number: 43 Loss: 1.5721209049224854 Time taken: 0.45337700843811035\n",
            "Epoch: 4 Batch Number: 44 Loss: 1.4750880002975464 Time taken: 0.44469285011291504\n",
            "Epoch: 4 Batch Number: 45 Loss: 1.5025134086608887 Time taken: 0.4639468193054199\n",
            "Epoch: 4 Batch Number: 46 Loss: 1.6681472063064575 Time taken: 0.4563171863555908\n",
            "Epoch: 4 Batch Number: 47 Loss: 1.6472545862197876 Time taken: 0.4660170078277588\n",
            "Epoch: 4 Batch Number: 48 Loss: 1.701352834701538 Time taken: 0.4494822025299072\n",
            "Epoch: 4 Batch Number: 49 Loss: 1.6686352491378784 Time taken: 0.4612870216369629\n",
            "Epoch: 4 Batch Number: 50 Loss: 1.5664691925048828 Time taken: 0.46512341499328613\n",
            "Epoch: 4 Batch Number: 51 Loss: 1.5524365901947021 Time taken: 0.45528554916381836\n",
            "Epoch: 4 Batch Number: 52 Loss: 1.6731752157211304 Time taken: 0.44756221771240234\n",
            "Epoch: 4 Batch Number: 53 Loss: 1.7817211151123047 Time taken: 0.4799168109893799\n",
            "Epoch: 4 Batch Number: 54 Loss: 1.74350106716156 Time taken: 0.46990513801574707\n",
            "Epoch: 4 Batch Number: 55 Loss: 1.6918805837631226 Time taken: 0.4891331195831299\n",
            "Epoch: 4 Batch Number: 56 Loss: 1.730238437652588 Time taken: 0.4550814628601074\n",
            "Epoch: 4 Batch Number: 57 Loss: 1.6061099767684937 Time taken: 0.4521644115447998\n",
            "Epoch: 4 Batch Number: 58 Loss: 1.5764665603637695 Time taken: 0.44743895530700684\n",
            "Epoch: 4 Batch Number: 59 Loss: 1.583247423171997 Time taken: 0.4534122943878174\n",
            "Epoch: 4 Batch Number: 60 Loss: 1.6096258163452148 Time taken: 0.4513983726501465\n",
            "Epoch: 4 Batch Number: 61 Loss: 1.6690444946289062 Time taken: 0.4456031322479248\n",
            "Epoch: 4 Batch Number: 62 Loss: 1.597071886062622 Time taken: 0.4659419059753418\n",
            "Epoch: 4 Batch Number: 63 Loss: 1.4918968677520752 Time taken: 0.45307397842407227\n",
            "Epoch: 4 Batch Number: 64 Loss: 1.4879539012908936 Time taken: 0.44486308097839355\n",
            "Epoch: 4 Batch Number: 65 Loss: 1.5141364336013794 Time taken: 0.45398616790771484\n",
            "Epoch: 4 Batch Number: 66 Loss: 1.5424329042434692 Time taken: 0.44851064682006836\n",
            "Epoch: 4 Batch Number: 67 Loss: 1.5146135091781616 Time taken: 0.44507455825805664\n",
            "Epoch: 4 Batch Number: 68 Loss: 1.5612081289291382 Time taken: 0.4490635395050049\n",
            "Epoch: 4 Batch Number: 69 Loss: 1.6192071437835693 Time taken: 0.4590919017791748\n",
            "Epoch: 4 Batch Number: 70 Loss: 1.508659839630127 Time taken: 0.44521260261535645\n",
            "Epoch: 4 Batch Number: 71 Loss: 1.6369004249572754 Time taken: 0.461198091506958\n",
            "Epoch: 4 Batch Number: 72 Loss: 1.5978206396102905 Time taken: 0.45999836921691895\n",
            "Epoch: 4 Batch Number: 73 Loss: 1.6217390298843384 Time taken: 0.48091626167297363\n",
            "Epoch: 4 Batch Number: 74 Loss: 1.8029305934906006 Time taken: 0.46376585960388184\n",
            "Epoch: 4 Batch Number: 75 Loss: 1.550354242324829 Time taken: 0.4641733169555664\n",
            "Epoch: 4 Batch Number: 76 Loss: 1.6863476037979126 Time taken: 0.4445953369140625\n",
            "Epoch: 4 Batch Number: 77 Loss: 1.6585164070129395 Time taken: 0.4550354480743408\n",
            "Epoch: 4 Batch Number: 78 Loss: 1.5443978309631348 Time taken: 0.4552464485168457\n",
            "Epoch: 4 Batch Number: 79 Loss: 1.5201988220214844 Time taken: 0.4541022777557373\n",
            "Epoch: 4 Batch Number: 80 Loss: 1.580188274383545 Time taken: 0.468214750289917\n",
            "Epoch: 4 Batch Number: 81 Loss: 1.5405783653259277 Time taken: 0.45726847648620605\n",
            "Epoch: 4 Batch Number: 82 Loss: 1.608795404434204 Time taken: 0.4523601531982422\n",
            "Epoch: 4 Batch Number: 83 Loss: 1.6404485702514648 Time taken: 0.45311594009399414\n",
            "Epoch: 4 Batch Number: 84 Loss: 1.6548432111740112 Time taken: 0.4534165859222412\n",
            "Epoch: 4 Batch Number: 85 Loss: 1.6048240661621094 Time taken: 0.4541149139404297\n",
            "Epoch: 4 Batch Number: 86 Loss: 1.658805251121521 Time taken: 0.461428165435791\n",
            "Epoch: 4 Batch Number: 87 Loss: 1.6107650995254517 Time taken: 0.45963096618652344\n",
            "Epoch: 4 Batch Number: 88 Loss: 1.506871223449707 Time taken: 0.46215152740478516\n",
            "Epoch: 4 Batch Number: 89 Loss: 1.5201530456542969 Time taken: 0.4457225799560547\n",
            "Epoch: 4 Batch Number: 90 Loss: 1.523697018623352 Time taken: 0.47266435623168945\n",
            "Epoch: 4 Batch Number: 91 Loss: 1.6141366958618164 Time taken: 0.48558974266052246\n",
            "Epoch: 4 Batch Number: 92 Loss: 1.4995347261428833 Time taken: 0.4541664123535156\n",
            "Epoch: 4 Batch Number: 93 Loss: 1.5013821125030518 Time taken: 0.45574402809143066\n",
            "Epoch: 4 Batch Number: 94 Loss: 1.6601877212524414 Time taken: 0.44981956481933594\n",
            "Epoch: 4 Batch Number: 95 Loss: 1.5782885551452637 Time taken: 0.46181631088256836\n",
            "Epoch: 4 Batch Number: 96 Loss: 1.6273362636566162 Time taken: 0.4632885456085205\n",
            "Epoch: 4 Batch Number: 97 Loss: 1.6321065425872803 Time taken: 0.4588661193847656\n",
            "Epoch: 4 Batch Number: 98 Loss: 1.6652326583862305 Time taken: 0.4477856159210205\n",
            "Epoch: 4 Batch Number: 99 Loss: 1.6961848735809326 Time taken: 0.45275092124938965\n",
            "Epoch: 4 Batch Number: 100 Loss: 1.6719610691070557 Time taken: 0.4515697956085205\n",
            "Epoch: 4 Batch Number: 101 Loss: 1.608644723892212 Time taken: 0.4705162048339844\n",
            "Epoch: 4 Batch Number: 102 Loss: 1.6595127582550049 Time taken: 0.4590764045715332\n",
            "Epoch: 4 Batch Number: 103 Loss: 1.7107799053192139 Time taken: 0.45067310333251953\n",
            "Epoch: 4 Batch Number: 104 Loss: 1.7122573852539062 Time taken: 0.4500911235809326\n",
            "Epoch: 4 Batch Number: 105 Loss: 1.6845486164093018 Time taken: 0.44811463356018066\n",
            "Epoch: 4 Batch Number: 106 Loss: 1.646200180053711 Time taken: 0.4569118022918701\n",
            "Epoch: 4 Batch Number: 107 Loss: 1.6030081510543823 Time taken: 0.4429943561553955\n",
            "Epoch: 4 Batch Number: 108 Loss: 1.5656957626342773 Time taken: 0.4537796974182129\n",
            "Epoch: 4 Batch Number: 109 Loss: 1.5885111093521118 Time taken: 0.45848798751831055\n",
            "Epoch: 4 Batch Number: 110 Loss: 1.464875340461731 Time taken: 0.4711751937866211\n",
            "Epoch: 4 Batch Number: 111 Loss: 1.490482211112976 Time taken: 0.44718432426452637\n",
            "Epoch: 4 Batch Number: 112 Loss: 1.4858036041259766 Time taken: 0.46127915382385254\n",
            "Epoch: 4 Batch Number: 113 Loss: 1.5723059177398682 Time taken: 0.4713869094848633\n",
            "Epoch: 4 Batch Number: 114 Loss: 1.5587366819381714 Time taken: 0.45589685440063477\n",
            "Epoch: 4 Batch Number: 115 Loss: 1.5921818017959595 Time taken: 0.45569443702697754\n",
            "Epoch: 4 Batch Number: 116 Loss: 1.45552659034729 Time taken: 0.44532322883605957\n",
            "Epoch: 4 Batch Number: 117 Loss: 1.5252413749694824 Time taken: 0.45159077644348145\n",
            "Epoch: 4 Batch Number: 118 Loss: 1.436885118484497 Time taken: 0.454089879989624\n",
            "Epoch: 4 Batch Number: 119 Loss: 1.4877924919128418 Time taken: 0.44673585891723633\n",
            "Epoch: 4 Batch Number: 120 Loss: 1.5882537364959717 Time taken: 0.45812249183654785\n",
            "Epoch: 4 Batch Number: 121 Loss: 1.5249745845794678 Time taken: 0.45473814010620117\n",
            "Epoch: 4 Batch Number: 122 Loss: 1.6193902492523193 Time taken: 0.4480109214782715\n",
            "Epoch: 4 Batch Number: 123 Loss: 1.5940996408462524 Time taken: 0.45543742179870605\n",
            "Epoch: 4 Batch Number: 124 Loss: 1.575929880142212 Time taken: 0.46079540252685547\n",
            "Epoch: 4 Batch Number: 125 Loss: 1.627236247062683 Time taken: 0.45368242263793945\n",
            "Epoch: 4 Batch Number: 126 Loss: 1.6756141185760498 Time taken: 0.4459660053253174\n",
            "Epoch: 4 Batch Number: 127 Loss: 1.7125885486602783 Time taken: 0.44144678115844727\n",
            "Epoch: 4 Batch Number: 128 Loss: 1.5744695663452148 Time taken: 0.4588584899902344\n",
            "Epoch: 4 Batch Number: 129 Loss: 1.5671141147613525 Time taken: 0.4478294849395752\n",
            "Epoch: 4 Batch Number: 130 Loss: 1.556334376335144 Time taken: 0.4542365074157715\n",
            "Epoch: 4 Batch Number: 131 Loss: 1.6587626934051514 Time taken: 0.47588276863098145\n",
            "Epoch: 4 Batch Number: 132 Loss: 1.5123039484024048 Time taken: 0.4540107250213623\n",
            "Epoch: 4 Batch Number: 133 Loss: 1.598402500152588 Time taken: 0.44759702682495117\n",
            "Epoch: 4 Batch Number: 134 Loss: 1.679457426071167 Time taken: 0.4445688724517822\n",
            "Epoch: 4 Batch Number: 135 Loss: 1.6669199466705322 Time taken: 0.4567849636077881\n",
            "Epoch: 4 Batch Number: 136 Loss: 1.6218421459197998 Time taken: 0.4531543254852295\n",
            "Epoch: 4 Batch Number: 137 Loss: 1.6334543228149414 Time taken: 0.4592430591583252\n",
            "Epoch: 4 Batch Number: 138 Loss: 1.5571436882019043 Time taken: 0.457369327545166\n",
            "Epoch: 4 Batch Number: 139 Loss: 1.6145715713500977 Time taken: 0.4507918357849121\n",
            "Epoch: 4 Batch Number: 140 Loss: 1.5351507663726807 Time taken: 0.47898244857788086\n",
            "Epoch: 4 Batch Number: 141 Loss: 1.5796737670898438 Time taken: 0.4661448001861572\n",
            "Epoch: 4 Batch Number: 142 Loss: 1.6179085969924927 Time taken: 0.4762415885925293\n",
            "Epoch: 4 Batch Number: 143 Loss: 1.7161952257156372 Time taken: 0.47008275985717773\n",
            "Epoch: 4 Batch Number: 144 Loss: 1.7713502645492554 Time taken: 0.4555203914642334\n",
            "Epoch: 4 Batch Number: 145 Loss: 1.6939783096313477 Time taken: 0.46279454231262207\n",
            "Epoch: 4 Batch Number: 146 Loss: 1.7080206871032715 Time taken: 0.4643111228942871\n",
            "Epoch: 4 Batch Number: 147 Loss: 1.6919151544570923 Time taken: 0.4507772922515869\n",
            "Epoch: 4 Batch Number: 148 Loss: 1.6242783069610596 Time taken: 0.45396900177001953\n",
            "Epoch: 4 Batch Number: 149 Loss: 1.6723731756210327 Time taken: 0.4565439224243164\n",
            "Epoch: 4 Batch Number: 150 Loss: 1.7584842443466187 Time taken: 0.4581410884857178\n",
            "Epoch: 4 Batch Number: 151 Loss: 1.7209097146987915 Time taken: 0.45830798149108887\n",
            "Epoch: 4 Batch Number: 152 Loss: 1.5957159996032715 Time taken: 0.4567556381225586\n",
            "Epoch: 4 Batch Number: 153 Loss: 1.5593793392181396 Time taken: 0.46450185775756836\n",
            "Epoch: 4 Batch Number: 154 Loss: 1.627192497253418 Time taken: 0.4463925361633301\n",
            "Epoch: 4 Batch Number: 155 Loss: 1.5315117835998535 Time taken: 0.44784045219421387\n",
            "Epoch: 4 Batch Number: 156 Loss: 1.5660520792007446 Time taken: 0.4666922092437744\n",
            "Epoch: 4 Batch Number: 157 Loss: 1.443922519683838 Time taken: 0.44434118270874023\n",
            "Epoch: 4 Batch Number: 158 Loss: 1.4795596599578857 Time taken: 0.4506394863128662\n",
            "Epoch: 4 Batch Number: 159 Loss: 1.4603846073150635 Time taken: 0.4472787380218506\n",
            "Epoch: 4 Batch Number: 160 Loss: 1.3938002586364746 Time taken: 0.4476308822631836\n",
            "Epoch: 4 Batch Number: 161 Loss: 1.4704475402832031 Time taken: 0.4582996368408203\n",
            "Epoch: 4 Batch Number: 162 Loss: 1.4329237937927246 Time taken: 0.4440174102783203\n",
            "Epoch: 4 Batch Number: 163 Loss: 1.4864501953125 Time taken: 0.44380712509155273\n",
            "Epoch: 4 Batch Number: 164 Loss: 1.65985107421875 Time taken: 0.46250224113464355\n",
            "Epoch: 4 Batch Number: 165 Loss: 1.6088443994522095 Time taken: 0.4592902660369873\n",
            "Epoch: 4 Batch Number: 166 Loss: 1.5450748205184937 Time taken: 0.4801981449127197\n",
            "Epoch: 4 Batch Number: 167 Loss: 1.57490873336792 Time taken: 0.4503481388092041\n",
            "Epoch: 4 Batch Number: 168 Loss: 1.6529688835144043 Time taken: 0.46904897689819336\n",
            "Epoch: 4 Batch Number: 169 Loss: 1.5521279573440552 Time taken: 0.45369553565979004\n",
            "Epoch: 4 Batch Number: 170 Loss: 1.4810532331466675 Time taken: 0.4480147361755371\n",
            "Epoch: 4 Batch Number: 171 Loss: 1.5236974954605103 Time taken: 0.4534728527069092\n",
            "Epoch: 4 Batch Number: 172 Loss: 1.5339696407318115 Time taken: 0.4623074531555176\n",
            "Epoch: 4 Batch Number: 173 Loss: 1.4718445539474487 Time taken: 0.45276498794555664\n",
            "Epoch: 4 Batch Number: 174 Loss: 1.4517630338668823 Time taken: 0.4508626461029053\n",
            "Epoch: 4 Batch Number: 175 Loss: 1.5315744876861572 Time taken: 0.4503011703491211\n",
            "Epoch: 4 Batch Number: 176 Loss: 1.4834827184677124 Time taken: 0.45362329483032227\n",
            "Epoch: 4 Batch Number: 177 Loss: 1.5514227151870728 Time taken: 0.44686317443847656\n",
            "Epoch: 4 Batch Number: 178 Loss: 1.5859074592590332 Time taken: 0.45266222953796387\n",
            "Epoch: 4 Batch Number: 179 Loss: 1.6678485870361328 Time taken: 0.4792301654815674\n",
            "Epoch: 4 Batch Number: 180 Loss: 1.53562331199646 Time taken: 0.4527745246887207\n",
            "Epoch: 4 Batch Number: 181 Loss: 1.549680233001709 Time taken: 0.4647841453552246\n",
            "Epoch: 4 Batch Number: 182 Loss: 1.4518146514892578 Time taken: 0.4578850269317627\n",
            "Epoch: 4 Batch Number: 183 Loss: 1.561083436012268 Time taken: 0.4621758460998535\n",
            "Epoch: 4 Batch Number: 184 Loss: 1.5019242763519287 Time taken: 0.44858264923095703\n",
            "Epoch: 4 Batch Number: 185 Loss: 1.5533726215362549 Time taken: 0.44849371910095215\n",
            "Epoch: 4 Batch Number: 186 Loss: 1.489948034286499 Time taken: 0.4652726650238037\n",
            "Epoch: 4 Batch Number: 187 Loss: 1.468898057937622 Time taken: 0.459139347076416\n",
            "Epoch: 4 Batch Number: 188 Loss: 1.491969347000122 Time taken: 0.4709000587463379\n",
            "Epoch: 4 Batch Number: 189 Loss: 1.5073567628860474 Time taken: 0.45228147506713867\n",
            "Epoch: 4 Batch Number: 190 Loss: 1.6634740829467773 Time taken: 0.4483780860900879\n",
            "Epoch: 4 Batch Number: 191 Loss: 1.9521795511245728 Time taken: 0.4685659408569336\n",
            "Epoch: 4 Batch Number: 192 Loss: 1.5851366519927979 Time taken: 0.4583771228790283\n",
            "Epoch: 4 Batch Number: 193 Loss: 1.7181658744812012 Time taken: 0.44857048988342285\n",
            "Epoch: 4 Batch Number: 194 Loss: 1.5923219919204712 Time taken: 0.4544801712036133\n",
            "Epoch: 4 Batch Number: 195 Loss: 1.4958131313323975 Time taken: 0.45735740661621094\n",
            "Epoch: 4 Batch Number: 196 Loss: 1.4525856971740723 Time taken: 0.45204854011535645\n",
            "Epoch: 4 Batch Number: 197 Loss: 1.5533307790756226 Time taken: 0.45611143112182617\n",
            "Epoch: 4 Batch Number: 198 Loss: 1.4471027851104736 Time taken: 0.46718549728393555\n",
            "Epoch: 4 Batch Number: 199 Loss: 1.5594828128814697 Time taken: 0.4600870609283447\n",
            "Epoch: 4 Batch Number: 200 Loss: 1.4474132061004639 Time taken: 0.44939470291137695\n",
            "Epoch: 4 Batch Number: 201 Loss: 1.4915528297424316 Time taken: 0.4740722179412842\n",
            "Epoch: 4 Batch Number: 202 Loss: 1.5664212703704834 Time taken: 0.4766674041748047\n",
            "Epoch: 4 Batch Number: 203 Loss: 1.5097427368164062 Time taken: 0.4507639408111572\n",
            "Epoch: 4 Batch Number: 204 Loss: 1.541151523590088 Time taken: 0.44462084770202637\n",
            "Epoch: 4 Batch Number: 205 Loss: 1.4320437908172607 Time taken: 0.47384214401245117\n",
            "Epoch: 4 Batch Number: 206 Loss: 1.576060175895691 Time taken: 0.4432368278503418\n",
            "Epoch: 4 Batch Number: 207 Loss: 1.4286613464355469 Time taken: 0.4489254951477051\n",
            "Epoch: 4 Batch Number: 208 Loss: 1.4852627515792847 Time taken: 0.4472177028656006\n",
            "Epoch: 4 Batch Number: 209 Loss: 1.5061626434326172 Time taken: 0.4521973133087158\n",
            "Epoch: 4 Batch Number: 210 Loss: 1.587827444076538 Time taken: 0.4558126926422119\n",
            "Epoch: 4 Batch Number: 211 Loss: 1.6150745153427124 Time taken: 0.4505486488342285\n",
            "Epoch: 4 Batch Number: 212 Loss: 1.6607611179351807 Time taken: 0.4596824645996094\n",
            "Epoch: 4 Batch Number: 213 Loss: 1.5303339958190918 Time taken: 0.4555978775024414\n",
            "Epoch: 4 Batch Number: 214 Loss: 1.5469517707824707 Time taken: 0.4604499340057373\n",
            "Epoch: 4 Batch Number: 215 Loss: 1.5262978076934814 Time taken: 0.4565882682800293\n",
            "Epoch: 4 Batch Number: 216 Loss: 1.5044633150100708 Time taken: 0.4614369869232178\n",
            "Epoch: 4 Batch Number: 217 Loss: 1.6024681329727173 Time taken: 0.45441269874572754\n",
            "Epoch: 4 Batch Number: 218 Loss: 1.642402172088623 Time taken: 0.4541287422180176\n",
            "Epoch: 4 Batch Number: 219 Loss: 1.5588089227676392 Time taken: 0.4689323902130127\n",
            "Epoch: 4 Batch Number: 220 Loss: 1.5341956615447998 Time taken: 0.4511244297027588\n",
            "Epoch: 4 Batch Number: 221 Loss: 1.4930230379104614 Time taken: 0.4433104991912842\n",
            "Epoch: 4 Batch Number: 222 Loss: 1.4942283630371094 Time taken: 0.45120668411254883\n",
            "Epoch: 4 Batch Number: 223 Loss: 1.535510778427124 Time taken: 0.4465787410736084\n",
            "Epoch: 4 Batch Number: 224 Loss: 1.4725275039672852 Time taken: 0.4591236114501953\n",
            "Epoch: 4 Batch Number: 225 Loss: 1.433847427368164 Time taken: 0.45070815086364746\n",
            "Epoch: 4 Batch Number: 226 Loss: 1.4597694873809814 Time taken: 0.45259714126586914\n",
            "Epoch: 4 Batch Number: 227 Loss: 1.6048004627227783 Time taken: 0.44620633125305176\n",
            "Epoch: 4 Batch Number: 228 Loss: 1.5907151699066162 Time taken: 0.4532456398010254\n",
            "Epoch: 4 Batch Number: 229 Loss: 1.6075093746185303 Time taken: 0.4494309425354004\n",
            "==========================================================================================\n",
            "Start of epoch 5\n",
            "Epoch: 5 Batch Number: 1 Loss: 1.481285572052002 Time taken: 0.4684717655181885\n",
            "Epoch: 5 Batch Number: 2 Loss: 1.5307598114013672 Time taken: 0.453981876373291\n",
            "Epoch: 5 Batch Number: 3 Loss: 1.4856760501861572 Time taken: 0.4488041400909424\n",
            "Epoch: 5 Batch Number: 4 Loss: 1.5068862438201904 Time taken: 0.4442579746246338\n",
            "Epoch: 5 Batch Number: 5 Loss: 1.5054686069488525 Time taken: 0.46996259689331055\n",
            "Epoch: 5 Batch Number: 6 Loss: 1.4088785648345947 Time taken: 0.4650728702545166\n",
            "Epoch: 5 Batch Number: 7 Loss: 1.4088211059570312 Time taken: 0.4614737033843994\n",
            "Epoch: 5 Batch Number: 8 Loss: 1.4009246826171875 Time taken: 0.45197510719299316\n",
            "Epoch: 5 Batch Number: 9 Loss: 1.4606930017471313 Time taken: 0.44829463958740234\n",
            "Epoch: 5 Batch Number: 10 Loss: 1.4936020374298096 Time taken: 0.454603910446167\n",
            "Epoch: 5 Batch Number: 11 Loss: 1.402029275894165 Time taken: 0.45925068855285645\n",
            "Epoch: 5 Batch Number: 12 Loss: 1.4010565280914307 Time taken: 0.45871877670288086\n",
            "Epoch: 5 Batch Number: 13 Loss: 1.4516985416412354 Time taken: 0.46787548065185547\n",
            "Epoch: 5 Batch Number: 14 Loss: 1.454047679901123 Time taken: 0.45542359352111816\n",
            "Epoch: 5 Batch Number: 15 Loss: 1.3919298648834229 Time taken: 0.4557678699493408\n",
            "Epoch: 5 Batch Number: 16 Loss: 1.4900414943695068 Time taken: 0.46516871452331543\n",
            "Epoch: 5 Batch Number: 17 Loss: 1.6298408508300781 Time taken: 0.4538140296936035\n",
            "Epoch: 5 Batch Number: 18 Loss: 1.5607292652130127 Time taken: 0.4720573425292969\n",
            "Epoch: 5 Batch Number: 19 Loss: 1.5509295463562012 Time taken: 0.45815205574035645\n",
            "Epoch: 5 Batch Number: 20 Loss: 1.4284400939941406 Time taken: 0.47105860710144043\n",
            "Epoch: 5 Batch Number: 21 Loss: 1.675945520401001 Time taken: 0.45281338691711426\n",
            "Epoch: 5 Batch Number: 22 Loss: 1.584141492843628 Time taken: 0.44955992698669434\n",
            "Epoch: 5 Batch Number: 23 Loss: 1.6020445823669434 Time taken: 0.46094417572021484\n",
            "Epoch: 5 Batch Number: 24 Loss: 1.5422528982162476 Time taken: 0.4461674690246582\n",
            "Epoch: 5 Batch Number: 25 Loss: 1.5696991682052612 Time taken: 0.46083974838256836\n",
            "Epoch: 5 Batch Number: 26 Loss: 1.5548524856567383 Time taken: 0.4482238292694092\n",
            "Epoch: 5 Batch Number: 27 Loss: 1.4907054901123047 Time taken: 0.4590175151824951\n",
            "Epoch: 5 Batch Number: 28 Loss: 1.4800231456756592 Time taken: 0.4602041244506836\n",
            "Epoch: 5 Batch Number: 29 Loss: 1.4795527458190918 Time taken: 0.4942777156829834\n",
            "Epoch: 5 Batch Number: 30 Loss: 1.3536945581436157 Time taken: 0.456798791885376\n",
            "Epoch: 5 Batch Number: 31 Loss: 1.445427656173706 Time taken: 0.4568746089935303\n",
            "Epoch: 5 Batch Number: 32 Loss: 1.4554413557052612 Time taken: 0.4519364833831787\n",
            "Epoch: 5 Batch Number: 33 Loss: 1.4785728454589844 Time taken: 0.45474815368652344\n",
            "Epoch: 5 Batch Number: 34 Loss: 1.505678415298462 Time taken: 0.4617743492126465\n",
            "Epoch: 5 Batch Number: 35 Loss: 1.5445566177368164 Time taken: 0.4680519104003906\n",
            "Epoch: 5 Batch Number: 36 Loss: 1.6305478811264038 Time taken: 0.4584808349609375\n",
            "Epoch: 5 Batch Number: 37 Loss: 1.487534523010254 Time taken: 0.454601526260376\n",
            "Epoch: 5 Batch Number: 38 Loss: 1.4603437185287476 Time taken: 0.4760775566101074\n",
            "Epoch: 5 Batch Number: 39 Loss: 1.4668210744857788 Time taken: 0.4550929069519043\n",
            "Epoch: 5 Batch Number: 40 Loss: 1.4833409786224365 Time taken: 0.4637787342071533\n",
            "Epoch: 5 Batch Number: 41 Loss: 1.4171754121780396 Time taken: 0.45142459869384766\n",
            "Epoch: 5 Batch Number: 42 Loss: 1.4310355186462402 Time taken: 0.4799919128417969\n",
            "Epoch: 5 Batch Number: 43 Loss: 1.4465162754058838 Time taken: 0.45566821098327637\n",
            "Epoch: 5 Batch Number: 44 Loss: 1.3474980592727661 Time taken: 0.45836591720581055\n",
            "Epoch: 5 Batch Number: 45 Loss: 1.388750433921814 Time taken: 0.4498476982116699\n",
            "Epoch: 5 Batch Number: 46 Loss: 1.562166690826416 Time taken: 0.458392858505249\n",
            "Epoch: 5 Batch Number: 47 Loss: 1.50581955909729 Time taken: 0.450089693069458\n",
            "Epoch: 5 Batch Number: 48 Loss: 1.577640414237976 Time taken: 0.46660661697387695\n",
            "Epoch: 5 Batch Number: 49 Loss: 1.5640227794647217 Time taken: 0.4726438522338867\n",
            "Epoch: 5 Batch Number: 50 Loss: 1.4247050285339355 Time taken: 0.4689819812774658\n",
            "Epoch: 5 Batch Number: 51 Loss: 1.4249999523162842 Time taken: 0.4606466293334961\n",
            "Epoch: 5 Batch Number: 52 Loss: 1.5711967945098877 Time taken: 0.46242451667785645\n",
            "Epoch: 5 Batch Number: 53 Loss: 1.65171480178833 Time taken: 0.4765193462371826\n",
            "Epoch: 5 Batch Number: 54 Loss: 1.6143969297409058 Time taken: 0.4685025215148926\n",
            "Epoch: 5 Batch Number: 55 Loss: 1.5583906173706055 Time taken: 0.4594757556915283\n",
            "Epoch: 5 Batch Number: 56 Loss: 1.6195082664489746 Time taken: 0.4500241279602051\n",
            "Epoch: 5 Batch Number: 57 Loss: 1.4913954734802246 Time taken: 0.4578969478607178\n",
            "Epoch: 5 Batch Number: 58 Loss: 1.451903223991394 Time taken: 0.47008776664733887\n",
            "Epoch: 5 Batch Number: 59 Loss: 1.4593212604522705 Time taken: 0.4560546875\n",
            "Epoch: 5 Batch Number: 60 Loss: 1.4842870235443115 Time taken: 0.4542567729949951\n",
            "Epoch: 5 Batch Number: 61 Loss: 1.5621018409729004 Time taken: 0.4621243476867676\n",
            "Epoch: 5 Batch Number: 62 Loss: 1.466949224472046 Time taken: 0.4566538333892822\n",
            "Epoch: 5 Batch Number: 63 Loss: 1.3714866638183594 Time taken: 0.45595550537109375\n",
            "Epoch: 5 Batch Number: 64 Loss: 1.3574740886688232 Time taken: 0.46091747283935547\n",
            "Epoch: 5 Batch Number: 65 Loss: 1.3946666717529297 Time taken: 0.45047926902770996\n",
            "Epoch: 5 Batch Number: 66 Loss: 1.422296166419983 Time taken: 0.46276330947875977\n",
            "Epoch: 5 Batch Number: 67 Loss: 1.398834466934204 Time taken: 0.4477121829986572\n",
            "Epoch: 5 Batch Number: 68 Loss: 1.4534895420074463 Time taken: 0.45329856872558594\n",
            "Epoch: 5 Batch Number: 69 Loss: 1.496079921722412 Time taken: 0.4622483253479004\n",
            "Epoch: 5 Batch Number: 70 Loss: 1.4150714874267578 Time taken: 0.4741027355194092\n",
            "Epoch: 5 Batch Number: 71 Loss: 1.5099701881408691 Time taken: 0.46817612648010254\n",
            "Epoch: 5 Batch Number: 72 Loss: 1.4897565841674805 Time taken: 0.4505796432495117\n",
            "Epoch: 5 Batch Number: 73 Loss: 1.4624054431915283 Time taken: 0.44759058952331543\n",
            "Epoch: 5 Batch Number: 74 Loss: 1.644516110420227 Time taken: 0.45606350898742676\n",
            "Epoch: 5 Batch Number: 75 Loss: 1.408027172088623 Time taken: 0.4442615509033203\n",
            "Epoch: 5 Batch Number: 76 Loss: 1.5203113555908203 Time taken: 0.4793519973754883\n",
            "Epoch: 5 Batch Number: 77 Loss: 1.5240297317504883 Time taken: 0.46303749084472656\n",
            "Epoch: 5 Batch Number: 78 Loss: 1.3827813863754272 Time taken: 0.4498419761657715\n",
            "Epoch: 5 Batch Number: 79 Loss: 1.419752836227417 Time taken: 0.4533543586730957\n",
            "Epoch: 5 Batch Number: 80 Loss: 1.4620329141616821 Time taken: 0.44919705390930176\n",
            "Epoch: 5 Batch Number: 81 Loss: 1.4217450618743896 Time taken: 0.472135066986084\n",
            "Epoch: 5 Batch Number: 82 Loss: 1.4665062427520752 Time taken: 0.45771050453186035\n",
            "Epoch: 5 Batch Number: 83 Loss: 1.5391690731048584 Time taken: 0.4669170379638672\n",
            "Epoch: 5 Batch Number: 84 Loss: 1.4986915588378906 Time taken: 0.45124125480651855\n",
            "Epoch: 5 Batch Number: 85 Loss: 1.4555284976959229 Time taken: 0.45436978340148926\n",
            "Epoch: 5 Batch Number: 86 Loss: 1.513462781906128 Time taken: 0.4556312561035156\n",
            "Epoch: 5 Batch Number: 87 Loss: 1.4931578636169434 Time taken: 0.4562854766845703\n",
            "Epoch: 5 Batch Number: 88 Loss: 1.396120548248291 Time taken: 0.47400808334350586\n",
            "Epoch: 5 Batch Number: 89 Loss: 1.3825457096099854 Time taken: 0.4426765441894531\n",
            "Epoch: 5 Batch Number: 90 Loss: 1.4118647575378418 Time taken: 0.46906495094299316\n",
            "Epoch: 5 Batch Number: 91 Loss: 1.4680804014205933 Time taken: 0.45728611946105957\n",
            "Epoch: 5 Batch Number: 92 Loss: 1.372341275215149 Time taken: 0.44689369201660156\n",
            "Epoch: 5 Batch Number: 93 Loss: 1.382812738418579 Time taken: 0.4569425582885742\n",
            "Epoch: 5 Batch Number: 94 Loss: 1.5700902938842773 Time taken: 0.46747732162475586\n",
            "Epoch: 5 Batch Number: 95 Loss: 1.5082056522369385 Time taken: 0.4532463550567627\n",
            "Epoch: 5 Batch Number: 96 Loss: 1.554020643234253 Time taken: 0.4510359764099121\n",
            "Epoch: 5 Batch Number: 97 Loss: 1.543137788772583 Time taken: 0.4543638229370117\n",
            "Epoch: 5 Batch Number: 98 Loss: 1.5734754800796509 Time taken: 0.45911359786987305\n",
            "Epoch: 5 Batch Number: 99 Loss: 1.5902501344680786 Time taken: 0.45330071449279785\n",
            "Epoch: 5 Batch Number: 100 Loss: 1.551828384399414 Time taken: 0.4569563865661621\n",
            "Epoch: 5 Batch Number: 101 Loss: 1.4968061447143555 Time taken: 0.46823930740356445\n",
            "Epoch: 5 Batch Number: 102 Loss: 1.5494455099105835 Time taken: 0.4564366340637207\n",
            "Epoch: 5 Batch Number: 103 Loss: 1.6284935474395752 Time taken: 0.45035338401794434\n",
            "Epoch: 5 Batch Number: 104 Loss: 1.5969070196151733 Time taken: 0.44957542419433594\n",
            "Epoch: 5 Batch Number: 105 Loss: 1.594218134880066 Time taken: 0.4597434997558594\n",
            "Epoch: 5 Batch Number: 106 Loss: 1.5496931076049805 Time taken: 0.4588141441345215\n",
            "Epoch: 5 Batch Number: 107 Loss: 1.5081324577331543 Time taken: 0.4499661922454834\n",
            "Epoch: 5 Batch Number: 108 Loss: 1.4763740301132202 Time taken: 0.4555962085723877\n",
            "Epoch: 5 Batch Number: 109 Loss: 1.4922512769699097 Time taken: 0.45482730865478516\n",
            "Epoch: 5 Batch Number: 110 Loss: 1.3675814867019653 Time taken: 0.45063233375549316\n",
            "Epoch: 5 Batch Number: 111 Loss: 1.3852450847625732 Time taken: 0.44648313522338867\n",
            "Epoch: 5 Batch Number: 112 Loss: 1.4070279598236084 Time taken: 0.454359769821167\n",
            "Epoch: 5 Batch Number: 113 Loss: 1.4685381650924683 Time taken: 0.4503297805786133\n",
            "Epoch: 5 Batch Number: 114 Loss: 1.4637329578399658 Time taken: 0.46765565872192383\n",
            "Epoch: 5 Batch Number: 115 Loss: 1.4813477993011475 Time taken: 0.46843600273132324\n",
            "Epoch: 5 Batch Number: 116 Loss: 1.3445793390274048 Time taken: 0.4706907272338867\n",
            "Epoch: 5 Batch Number: 117 Loss: 1.4394773244857788 Time taken: 0.45166945457458496\n",
            "Epoch: 5 Batch Number: 118 Loss: 1.3579983711242676 Time taken: 0.45298194885253906\n",
            "Epoch: 5 Batch Number: 119 Loss: 1.4023691415786743 Time taken: 0.452864408493042\n",
            "Epoch: 5 Batch Number: 120 Loss: 1.4905132055282593 Time taken: 0.45273733139038086\n",
            "Epoch: 5 Batch Number: 121 Loss: 1.4272243976593018 Time taken: 0.4579126834869385\n",
            "Epoch: 5 Batch Number: 122 Loss: 1.4905335903167725 Time taken: 0.45847058296203613\n",
            "Epoch: 5 Batch Number: 123 Loss: 1.4853060245513916 Time taken: 0.4582216739654541\n",
            "Epoch: 5 Batch Number: 124 Loss: 1.4876245260238647 Time taken: 0.47356653213500977\n",
            "Epoch: 5 Batch Number: 125 Loss: 1.5154531002044678 Time taken: 0.46299171447753906\n",
            "Epoch: 5 Batch Number: 126 Loss: 1.5396876335144043 Time taken: 0.4604458808898926\n",
            "Epoch: 5 Batch Number: 127 Loss: 1.577887773513794 Time taken: 0.46532273292541504\n",
            "Epoch: 5 Batch Number: 128 Loss: 1.4528295993804932 Time taken: 0.48248291015625\n",
            "Epoch: 5 Batch Number: 129 Loss: 1.4506722688674927 Time taken: 0.4685218334197998\n",
            "Epoch: 5 Batch Number: 130 Loss: 1.4383875131607056 Time taken: 0.46056318283081055\n",
            "Epoch: 5 Batch Number: 131 Loss: 1.548129677772522 Time taken: 0.4575505256652832\n",
            "Epoch: 5 Batch Number: 132 Loss: 1.3673962354660034 Time taken: 0.4605224132537842\n",
            "Epoch: 5 Batch Number: 133 Loss: 1.5005196332931519 Time taken: 0.4673006534576416\n",
            "Epoch: 5 Batch Number: 134 Loss: 1.5763400793075562 Time taken: 0.4686248302459717\n",
            "Epoch: 5 Batch Number: 135 Loss: 1.5657342672348022 Time taken: 0.4535517692565918\n",
            "Epoch: 5 Batch Number: 136 Loss: 1.525417447090149 Time taken: 0.4579198360443115\n",
            "Epoch: 5 Batch Number: 137 Loss: 1.510141372680664 Time taken: 0.4496626853942871\n",
            "Epoch: 5 Batch Number: 138 Loss: 1.464585781097412 Time taken: 0.44611310958862305\n",
            "Epoch: 5 Batch Number: 139 Loss: 1.488541841506958 Time taken: 0.4610285758972168\n",
            "Epoch: 5 Batch Number: 140 Loss: 1.440269947052002 Time taken: 0.45294928550720215\n",
            "Epoch: 5 Batch Number: 141 Loss: 1.4780689477920532 Time taken: 0.45375728607177734\n",
            "Epoch: 5 Batch Number: 142 Loss: 1.53505539894104 Time taken: 0.4634859561920166\n",
            "Epoch: 5 Batch Number: 143 Loss: 1.6360872983932495 Time taken: 0.44954943656921387\n",
            "Epoch: 5 Batch Number: 144 Loss: 1.6953356266021729 Time taken: 0.46018362045288086\n",
            "Epoch: 5 Batch Number: 145 Loss: 1.6088510751724243 Time taken: 0.46057724952697754\n",
            "Epoch: 5 Batch Number: 146 Loss: 1.6225364208221436 Time taken: 0.46018195152282715\n",
            "Epoch: 5 Batch Number: 147 Loss: 1.524184226989746 Time taken: 0.4594733715057373\n",
            "Epoch: 5 Batch Number: 148 Loss: 1.4908969402313232 Time taken: 0.44298410415649414\n",
            "Epoch: 5 Batch Number: 149 Loss: 1.5537346601486206 Time taken: 0.4499337673187256\n",
            "Epoch: 5 Batch Number: 150 Loss: 1.6490633487701416 Time taken: 0.4487650394439697\n",
            "Epoch: 5 Batch Number: 151 Loss: 1.601672649383545 Time taken: 0.45551371574401855\n",
            "Epoch: 5 Batch Number: 152 Loss: 1.4937773942947388 Time taken: 0.44629335403442383\n",
            "Epoch: 5 Batch Number: 153 Loss: 1.4525187015533447 Time taken: 0.45460987091064453\n",
            "Epoch: 5 Batch Number: 154 Loss: 1.4891148805618286 Time taken: 0.4583568572998047\n",
            "Epoch: 5 Batch Number: 155 Loss: 1.4380605220794678 Time taken: 0.4569251537322998\n",
            "Epoch: 5 Batch Number: 156 Loss: 1.4571268558502197 Time taken: 0.47207069396972656\n",
            "Epoch: 5 Batch Number: 157 Loss: 1.3351737260818481 Time taken: 0.4696202278137207\n",
            "Epoch: 5 Batch Number: 158 Loss: 1.3771100044250488 Time taken: 0.46590518951416016\n",
            "Epoch: 5 Batch Number: 159 Loss: 1.3474514484405518 Time taken: 0.44472718238830566\n",
            "Epoch: 5 Batch Number: 160 Loss: 1.2691552639007568 Time taken: 0.4434375762939453\n",
            "Epoch: 5 Batch Number: 161 Loss: 1.3463504314422607 Time taken: 0.44883012771606445\n",
            "Epoch: 5 Batch Number: 162 Loss: 1.309140920639038 Time taken: 0.4550497531890869\n",
            "Epoch: 5 Batch Number: 163 Loss: 1.3932076692581177 Time taken: 0.45430731773376465\n",
            "Epoch: 5 Batch Number: 164 Loss: 1.5472478866577148 Time taken: 0.4612267017364502\n",
            "Epoch: 5 Batch Number: 165 Loss: 1.4656615257263184 Time taken: 0.4435884952545166\n",
            "Epoch: 5 Batch Number: 166 Loss: 1.451791524887085 Time taken: 0.4531574249267578\n",
            "Epoch: 5 Batch Number: 167 Loss: 1.4652791023254395 Time taken: 0.45372748374938965\n",
            "Epoch: 5 Batch Number: 168 Loss: 1.537336826324463 Time taken: 0.45766425132751465\n",
            "Epoch: 5 Batch Number: 169 Loss: 1.4609153270721436 Time taken: 0.46308255195617676\n",
            "Epoch: 5 Batch Number: 170 Loss: 1.3619990348815918 Time taken: 0.45795416831970215\n",
            "Epoch: 5 Batch Number: 171 Loss: 1.3734785318374634 Time taken: 0.4649524688720703\n",
            "Epoch: 5 Batch Number: 172 Loss: 1.393573522567749 Time taken: 0.45300817489624023\n",
            "Epoch: 5 Batch Number: 173 Loss: 1.3502390384674072 Time taken: 0.4519336223602295\n",
            "Epoch: 5 Batch Number: 174 Loss: 1.3179693222045898 Time taken: 0.4551730155944824\n",
            "Epoch: 5 Batch Number: 175 Loss: 1.4304934740066528 Time taken: 0.4473130702972412\n",
            "Epoch: 5 Batch Number: 176 Loss: 1.4047508239746094 Time taken: 0.45121240615844727\n",
            "Epoch: 5 Batch Number: 177 Loss: 1.4508485794067383 Time taken: 0.4483358860015869\n",
            "Epoch: 5 Batch Number: 178 Loss: 1.4860985279083252 Time taken: 0.4540224075317383\n",
            "Epoch: 5 Batch Number: 179 Loss: 1.534010410308838 Time taken: 0.4497804641723633\n",
            "Epoch: 5 Batch Number: 180 Loss: 1.4375770092010498 Time taken: 0.4410843849182129\n",
            "Epoch: 5 Batch Number: 181 Loss: 1.42268967628479 Time taken: 0.4571516513824463\n",
            "Epoch: 5 Batch Number: 182 Loss: 1.3329869508743286 Time taken: 0.45691514015197754\n",
            "Epoch: 5 Batch Number: 183 Loss: 1.4297422170639038 Time taken: 0.4504218101501465\n",
            "Epoch: 5 Batch Number: 184 Loss: 1.4167561531066895 Time taken: 0.45888853073120117\n",
            "Epoch: 5 Batch Number: 185 Loss: 1.4611314535140991 Time taken: 0.4472229480743408\n",
            "Epoch: 5 Batch Number: 186 Loss: 1.2926324605941772 Time taken: 0.45649147033691406\n",
            "Epoch: 5 Batch Number: 187 Loss: 1.3373422622680664 Time taken: 0.4536919593811035\n",
            "Epoch: 5 Batch Number: 188 Loss: 1.35361909866333 Time taken: 0.4507613182067871\n",
            "Epoch: 5 Batch Number: 189 Loss: 1.3995649814605713 Time taken: 0.44486379623413086\n",
            "Epoch: 5 Batch Number: 190 Loss: 1.583801031112671 Time taken: 0.4565415382385254\n",
            "Epoch: 5 Batch Number: 191 Loss: 1.8696510791778564 Time taken: 0.45166635513305664\n",
            "Epoch: 5 Batch Number: 192 Loss: 1.497795581817627 Time taken: 0.4456663131713867\n",
            "Epoch: 5 Batch Number: 193 Loss: 1.6329731941223145 Time taken: 0.45621490478515625\n",
            "Epoch: 5 Batch Number: 194 Loss: 1.5012801885604858 Time taken: 0.4640955924987793\n",
            "Epoch: 5 Batch Number: 195 Loss: 1.4037467241287231 Time taken: 0.4527578353881836\n",
            "Epoch: 5 Batch Number: 196 Loss: 1.3572494983673096 Time taken: 0.45426464080810547\n",
            "Epoch: 5 Batch Number: 197 Loss: 1.4713423252105713 Time taken: 0.4784729480743408\n",
            "Epoch: 5 Batch Number: 198 Loss: 1.36702561378479 Time taken: 0.4843902587890625\n",
            "Epoch: 5 Batch Number: 199 Loss: 1.4748821258544922 Time taken: 0.4574003219604492\n",
            "Epoch: 5 Batch Number: 200 Loss: 1.352980136871338 Time taken: 0.46136975288391113\n",
            "Epoch: 5 Batch Number: 201 Loss: 1.4026325941085815 Time taken: 0.45429158210754395\n",
            "Epoch: 5 Batch Number: 202 Loss: 1.4295070171356201 Time taken: 0.45425963401794434\n",
            "Epoch: 5 Batch Number: 203 Loss: 1.404982089996338 Time taken: 0.4537978172302246\n",
            "Epoch: 5 Batch Number: 204 Loss: 1.439070463180542 Time taken: 0.4650425910949707\n",
            "Epoch: 5 Batch Number: 205 Loss: 1.3448541164398193 Time taken: 0.44716620445251465\n",
            "Epoch: 5 Batch Number: 206 Loss: 1.4570765495300293 Time taken: 0.44683122634887695\n",
            "Epoch: 5 Batch Number: 207 Loss: 1.3545597791671753 Time taken: 0.4462578296661377\n",
            "Epoch: 5 Batch Number: 208 Loss: 1.3903846740722656 Time taken: 0.45455145835876465\n",
            "Epoch: 5 Batch Number: 209 Loss: 1.412703514099121 Time taken: 0.45611095428466797\n",
            "Epoch: 5 Batch Number: 210 Loss: 1.496887445449829 Time taken: 0.45882225036621094\n",
            "Epoch: 5 Batch Number: 211 Loss: 1.5208791494369507 Time taken: 0.4517090320587158\n",
            "Epoch: 5 Batch Number: 212 Loss: 1.5817185640335083 Time taken: 0.4567606449127197\n",
            "Epoch: 5 Batch Number: 213 Loss: 1.4201371669769287 Time taken: 0.4555652141571045\n",
            "Epoch: 5 Batch Number: 214 Loss: 1.456406593322754 Time taken: 0.47208166122436523\n",
            "Epoch: 5 Batch Number: 215 Loss: 1.4354957342147827 Time taken: 0.45439672470092773\n",
            "Epoch: 5 Batch Number: 216 Loss: 1.4224883317947388 Time taken: 0.44770312309265137\n",
            "Epoch: 5 Batch Number: 217 Loss: 1.5140790939331055 Time taken: 0.45116615295410156\n",
            "Epoch: 5 Batch Number: 218 Loss: 1.5621916055679321 Time taken: 0.44681668281555176\n",
            "Epoch: 5 Batch Number: 219 Loss: 1.477259874343872 Time taken: 0.4424014091491699\n",
            "Epoch: 5 Batch Number: 220 Loss: 1.4330195188522339 Time taken: 0.4560527801513672\n",
            "Epoch: 5 Batch Number: 221 Loss: 1.3948068618774414 Time taken: 0.45720553398132324\n",
            "Epoch: 5 Batch Number: 222 Loss: 1.3874964714050293 Time taken: 0.4606916904449463\n",
            "Epoch: 5 Batch Number: 223 Loss: 1.4252229928970337 Time taken: 0.4458901882171631\n",
            "Epoch: 5 Batch Number: 224 Loss: 1.3742239475250244 Time taken: 0.4532043933868408\n",
            "Epoch: 5 Batch Number: 225 Loss: 1.3318740129470825 Time taken: 0.4515860080718994\n",
            "Epoch: 5 Batch Number: 226 Loss: 1.3631137609481812 Time taken: 0.4463040828704834\n",
            "Epoch: 5 Batch Number: 227 Loss: 1.5503177642822266 Time taken: 0.465301513671875\n",
            "Epoch: 5 Batch Number: 228 Loss: 1.534693717956543 Time taken: 0.4694094657897949\n",
            "Epoch: 5 Batch Number: 229 Loss: 1.5377821922302246 Time taken: 0.450164794921875\n",
            "==========================================================================================\n",
            "Start of epoch 6\n",
            "Epoch: 6 Batch Number: 1 Loss: 1.4065091609954834 Time taken: 0.46364521980285645\n",
            "Epoch: 6 Batch Number: 2 Loss: 1.463815689086914 Time taken: 0.4754612445831299\n",
            "Epoch: 6 Batch Number: 3 Loss: 1.4032859802246094 Time taken: 0.45303893089294434\n",
            "Epoch: 6 Batch Number: 4 Loss: 1.4133129119873047 Time taken: 0.44207048416137695\n",
            "Epoch: 6 Batch Number: 5 Loss: 1.414078712463379 Time taken: 0.457150936126709\n",
            "Epoch: 6 Batch Number: 6 Loss: 1.3181641101837158 Time taken: 0.4642319679260254\n",
            "Epoch: 6 Batch Number: 7 Loss: 1.329978346824646 Time taken: 0.4736597537994385\n",
            "Epoch: 6 Batch Number: 8 Loss: 1.326021432876587 Time taken: 0.4643406867980957\n",
            "Epoch: 6 Batch Number: 9 Loss: 1.363743543624878 Time taken: 0.46404409408569336\n",
            "Epoch: 6 Batch Number: 10 Loss: 1.4036492109298706 Time taken: 0.4556455612182617\n",
            "Epoch: 6 Batch Number: 11 Loss: 1.3313442468643188 Time taken: 0.4490244388580322\n",
            "Epoch: 6 Batch Number: 12 Loss: 1.3120841979980469 Time taken: 0.4439840316772461\n",
            "Epoch: 6 Batch Number: 13 Loss: 1.3634440898895264 Time taken: 0.45202136039733887\n",
            "Epoch: 6 Batch Number: 14 Loss: 1.363478422164917 Time taken: 0.45756101608276367\n",
            "Epoch: 6 Batch Number: 15 Loss: 1.2860586643218994 Time taken: 0.45476698875427246\n",
            "Epoch: 6 Batch Number: 16 Loss: 1.379276990890503 Time taken: 0.44843244552612305\n",
            "Epoch: 6 Batch Number: 17 Loss: 1.5252385139465332 Time taken: 0.44969606399536133\n",
            "Epoch: 6 Batch Number: 18 Loss: 1.4754862785339355 Time taken: 0.4544048309326172\n",
            "Epoch: 6 Batch Number: 19 Loss: 1.4646713733673096 Time taken: 0.46153998374938965\n",
            "Epoch: 6 Batch Number: 20 Loss: 1.3283653259277344 Time taken: 0.4682652950286865\n",
            "Epoch: 6 Batch Number: 21 Loss: 1.5719475746154785 Time taken: 0.4465951919555664\n",
            "Epoch: 6 Batch Number: 22 Loss: 1.4881656169891357 Time taken: 0.4545407295227051\n",
            "Epoch: 6 Batch Number: 23 Loss: 1.499243140220642 Time taken: 0.44600582122802734\n",
            "Epoch: 6 Batch Number: 24 Loss: 1.4574780464172363 Time taken: 0.47956371307373047\n",
            "Epoch: 6 Batch Number: 25 Loss: 1.4615423679351807 Time taken: 0.4582395553588867\n",
            "Epoch: 6 Batch Number: 26 Loss: 1.4657478332519531 Time taken: 0.4605286121368408\n",
            "Epoch: 6 Batch Number: 27 Loss: 1.4132506847381592 Time taken: 0.4555702209472656\n",
            "Epoch: 6 Batch Number: 28 Loss: 1.409212350845337 Time taken: 0.46561717987060547\n",
            "Epoch: 6 Batch Number: 29 Loss: 1.417585849761963 Time taken: 0.4571411609649658\n",
            "Epoch: 6 Batch Number: 30 Loss: 1.2320560216903687 Time taken: 0.4477717876434326\n",
            "Epoch: 6 Batch Number: 31 Loss: 1.364969253540039 Time taken: 0.45812225341796875\n",
            "Epoch: 6 Batch Number: 32 Loss: 1.3753459453582764 Time taken: 0.44971585273742676\n",
            "Epoch: 6 Batch Number: 33 Loss: 1.3914926052093506 Time taken: 0.4508070945739746\n",
            "Epoch: 6 Batch Number: 34 Loss: 1.4178755283355713 Time taken: 0.4491596221923828\n",
            "Epoch: 6 Batch Number: 35 Loss: 1.4621343612670898 Time taken: 0.4616415500640869\n",
            "Epoch: 6 Batch Number: 36 Loss: 1.5521718263626099 Time taken: 0.447537899017334\n",
            "Epoch: 6 Batch Number: 37 Loss: 1.3793100118637085 Time taken: 0.4519679546356201\n",
            "Epoch: 6 Batch Number: 38 Loss: 1.393660306930542 Time taken: 0.4476315975189209\n",
            "Epoch: 6 Batch Number: 39 Loss: 1.384793996810913 Time taken: 0.46564769744873047\n",
            "Epoch: 6 Batch Number: 40 Loss: 1.3981679677963257 Time taken: 0.4483530521392822\n",
            "Epoch: 6 Batch Number: 41 Loss: 1.3220932483673096 Time taken: 0.44521236419677734\n",
            "Epoch: 6 Batch Number: 42 Loss: 1.3402729034423828 Time taken: 0.45113158226013184\n",
            "Epoch: 6 Batch Number: 43 Loss: 1.3571312427520752 Time taken: 0.4546518325805664\n",
            "Epoch: 6 Batch Number: 44 Loss: 1.268517255783081 Time taken: 0.4677436351776123\n",
            "Epoch: 6 Batch Number: 45 Loss: 1.3156851530075073 Time taken: 0.4641573429107666\n",
            "Epoch: 6 Batch Number: 46 Loss: 1.4834396839141846 Time taken: 0.4611485004425049\n",
            "Epoch: 6 Batch Number: 47 Loss: 1.398808479309082 Time taken: 0.4442720413208008\n",
            "Epoch: 6 Batch Number: 48 Loss: 1.4780025482177734 Time taken: 0.45488643646240234\n",
            "Epoch: 6 Batch Number: 49 Loss: 1.4807507991790771 Time taken: 0.46648311614990234\n",
            "Epoch: 6 Batch Number: 50 Loss: 1.3307725191116333 Time taken: 0.45416760444641113\n",
            "Epoch: 6 Batch Number: 51 Loss: 1.3447866439819336 Time taken: 0.4502594470977783\n",
            "Epoch: 6 Batch Number: 52 Loss: 1.480400562286377 Time taken: 0.45490479469299316\n",
            "Epoch: 6 Batch Number: 53 Loss: 1.527204155921936 Time taken: 0.4442324638366699\n",
            "Epoch: 6 Batch Number: 54 Loss: 1.5011045932769775 Time taken: 0.4666905403137207\n",
            "Epoch: 6 Batch Number: 55 Loss: 1.4454952478408813 Time taken: 0.4633510112762451\n",
            "Epoch: 6 Batch Number: 56 Loss: 1.516680121421814 Time taken: 0.45489931106567383\n",
            "Epoch: 6 Batch Number: 57 Loss: 1.4071192741394043 Time taken: 0.45673036575317383\n",
            "Epoch: 6 Batch Number: 58 Loss: 1.3611189126968384 Time taken: 0.4648468494415283\n",
            "Epoch: 6 Batch Number: 59 Loss: 1.3673315048217773 Time taken: 0.4622762203216553\n",
            "Epoch: 6 Batch Number: 60 Loss: 1.3913037776947021 Time taken: 0.4471852779388428\n",
            "Epoch: 6 Batch Number: 61 Loss: 1.4747536182403564 Time taken: 0.4665524959564209\n",
            "Epoch: 6 Batch Number: 62 Loss: 1.3685470819473267 Time taken: 0.46714138984680176\n",
            "Epoch: 6 Batch Number: 63 Loss: 1.2983355522155762 Time taken: 0.4703357219696045\n",
            "Epoch: 6 Batch Number: 64 Loss: 1.274681806564331 Time taken: 0.45324182510375977\n",
            "Epoch: 6 Batch Number: 65 Loss: 1.3222408294677734 Time taken: 0.45897912979125977\n",
            "Epoch: 6 Batch Number: 66 Loss: 1.3473210334777832 Time taken: 0.45824527740478516\n",
            "Epoch: 6 Batch Number: 67 Loss: 1.326629877090454 Time taken: 0.4631965160369873\n",
            "Epoch: 6 Batch Number: 68 Loss: 1.3863887786865234 Time taken: 0.47508716583251953\n",
            "Epoch: 6 Batch Number: 69 Loss: 1.398423671722412 Time taken: 0.44791269302368164\n",
            "Epoch: 6 Batch Number: 70 Loss: 1.3552987575531006 Time taken: 0.4487457275390625\n",
            "Epoch: 6 Batch Number: 71 Loss: 1.4013464450836182 Time taken: 0.46164393424987793\n",
            "Epoch: 6 Batch Number: 72 Loss: 1.4067193269729614 Time taken: 0.45520997047424316\n",
            "Epoch: 6 Batch Number: 73 Loss: 1.3500034809112549 Time taken: 0.4550902843475342\n",
            "Epoch: 6 Batch Number: 74 Loss: 1.5162222385406494 Time taken: 0.4484231472015381\n",
            "Epoch: 6 Batch Number: 75 Loss: 1.302788496017456 Time taken: 0.45650506019592285\n",
            "Epoch: 6 Batch Number: 76 Loss: 1.3886991739273071 Time taken: 0.45168352127075195\n",
            "Epoch: 6 Batch Number: 77 Loss: 1.429969310760498 Time taken: 0.4602336883544922\n",
            "Epoch: 6 Batch Number: 78 Loss: 1.2935082912445068 Time taken: 0.4461195468902588\n",
            "Epoch: 6 Batch Number: 79 Loss: 1.3596279621124268 Time taken: 0.46357250213623047\n",
            "Epoch: 6 Batch Number: 80 Loss: 1.377264380455017 Time taken: 0.4619119167327881\n",
            "Epoch: 6 Batch Number: 81 Loss: 1.3450927734375 Time taken: 0.45563817024230957\n",
            "Epoch: 6 Batch Number: 82 Loss: 1.3849155902862549 Time taken: 0.44855451583862305\n",
            "Epoch: 6 Batch Number: 83 Loss: 1.453092336654663 Time taken: 0.44652891159057617\n",
            "Epoch: 6 Batch Number: 84 Loss: 1.3993315696716309 Time taken: 0.45317840576171875\n",
            "Epoch: 6 Batch Number: 85 Loss: 1.358716368675232 Time taken: 0.4555385112762451\n",
            "Epoch: 6 Batch Number: 86 Loss: 1.409967303276062 Time taken: 0.45562148094177246\n",
            "Epoch: 6 Batch Number: 87 Loss: 1.4192285537719727 Time taken: 0.4591388702392578\n",
            "Epoch: 6 Batch Number: 88 Loss: 1.3235729932785034 Time taken: 0.46687746047973633\n",
            "Epoch: 6 Batch Number: 89 Loss: 1.2951765060424805 Time taken: 0.45589303970336914\n",
            "Epoch: 6 Batch Number: 90 Loss: 1.3408836126327515 Time taken: 0.46245503425598145\n",
            "Epoch: 6 Batch Number: 91 Loss: 1.3671610355377197 Time taken: 0.4471323490142822\n",
            "Epoch: 6 Batch Number: 92 Loss: 1.2993686199188232 Time taken: 0.45736122131347656\n",
            "Epoch: 6 Batch Number: 93 Loss: 1.3104621171951294 Time taken: 0.45566248893737793\n",
            "Epoch: 6 Batch Number: 94 Loss: 1.4912683963775635 Time taken: 0.46088361740112305\n",
            "Epoch: 6 Batch Number: 95 Loss: 1.4430081844329834 Time taken: 0.4594452381134033\n",
            "Epoch: 6 Batch Number: 96 Loss: 1.4931063652038574 Time taken: 0.4519829750061035\n",
            "Epoch: 6 Batch Number: 97 Loss: 1.471801996231079 Time taken: 0.47193408012390137\n",
            "Epoch: 6 Batch Number: 98 Loss: 1.4988765716552734 Time taken: 0.4473428726196289\n",
            "Epoch: 6 Batch Number: 99 Loss: 1.51603102684021 Time taken: 0.45522141456604004\n",
            "Epoch: 6 Batch Number: 100 Loss: 1.4621912240982056 Time taken: 0.4495832920074463\n",
            "Epoch: 6 Batch Number: 101 Loss: 1.420884609222412 Time taken: 0.46453142166137695\n",
            "Epoch: 6 Batch Number: 102 Loss: 1.468758225440979 Time taken: 0.45258283615112305\n",
            "Epoch: 6 Batch Number: 103 Loss: 1.5616484880447388 Time taken: 0.4655768871307373\n",
            "Epoch: 6 Batch Number: 104 Loss: 1.5032548904418945 Time taken: 0.45843076705932617\n",
            "Epoch: 6 Batch Number: 105 Loss: 1.5228941440582275 Time taken: 0.4685213565826416\n",
            "Epoch: 6 Batch Number: 106 Loss: 1.477231502532959 Time taken: 0.4534299373626709\n",
            "Epoch: 6 Batch Number: 107 Loss: 1.4436743259429932 Time taken: 0.45048952102661133\n",
            "Epoch: 6 Batch Number: 108 Loss: 1.4178974628448486 Time taken: 0.4594247341156006\n",
            "Epoch: 6 Batch Number: 109 Loss: 1.4294341802597046 Time taken: 0.4486653804779053\n",
            "Epoch: 6 Batch Number: 110 Loss: 1.2997373342514038 Time taken: 0.4674248695373535\n",
            "Epoch: 6 Batch Number: 111 Loss: 1.3212871551513672 Time taken: 0.46263790130615234\n",
            "Epoch: 6 Batch Number: 112 Loss: 1.3520325422286987 Time taken: 0.4574778079986572\n",
            "Epoch: 6 Batch Number: 113 Loss: 1.3913733959197998 Time taken: 0.45581626892089844\n",
            "Epoch: 6 Batch Number: 114 Loss: 1.3993974924087524 Time taken: 0.4655945301055908\n",
            "Epoch: 6 Batch Number: 115 Loss: 1.404523253440857 Time taken: 0.4544558525085449\n",
            "Epoch: 6 Batch Number: 116 Loss: 1.2640929222106934 Time taken: 0.4562263488769531\n",
            "Epoch: 6 Batch Number: 117 Loss: 1.37504243850708 Time taken: 0.4488697052001953\n",
            "Epoch: 6 Batch Number: 118 Loss: 1.3008458614349365 Time taken: 0.4514279365539551\n",
            "Epoch: 6 Batch Number: 119 Loss: 1.3413987159729004 Time taken: 0.4629216194152832\n",
            "Epoch: 6 Batch Number: 120 Loss: 1.4138553142547607 Time taken: 0.4617178440093994\n",
            "Epoch: 6 Batch Number: 121 Loss: 1.3499830961227417 Time taken: 0.4543647766113281\n",
            "Epoch: 6 Batch Number: 122 Loss: 1.380198359489441 Time taken: 0.45451903343200684\n",
            "Epoch: 6 Batch Number: 123 Loss: 1.3868159055709839 Time taken: 0.46015453338623047\n",
            "Epoch: 6 Batch Number: 124 Loss: 1.4027174711227417 Time taken: 0.45680880546569824\n",
            "Epoch: 6 Batch Number: 125 Loss: 1.4225977659225464 Time taken: 0.4455544948577881\n",
            "Epoch: 6 Batch Number: 126 Loss: 1.4290356636047363 Time taken: 0.44892287254333496\n",
            "Epoch: 6 Batch Number: 127 Loss: 1.4801629781723022 Time taken: 0.46848487854003906\n",
            "Epoch: 6 Batch Number: 128 Loss: 1.3656582832336426 Time taken: 0.4674336910247803\n",
            "Epoch: 6 Batch Number: 129 Loss: 1.3642714023590088 Time taken: 0.45650482177734375\n",
            "Epoch: 6 Batch Number: 130 Loss: 1.346549391746521 Time taken: 0.4821298122406006\n",
            "Epoch: 6 Batch Number: 131 Loss: 1.452188491821289 Time taken: 0.4721541404724121\n",
            "Epoch: 6 Batch Number: 132 Loss: 1.272505283355713 Time taken: 0.47377824783325195\n",
            "Epoch: 6 Batch Number: 133 Loss: 1.4252822399139404 Time taken: 0.4545457363128662\n",
            "Epoch: 6 Batch Number: 134 Loss: 1.4715420007705688 Time taken: 0.4589083194732666\n",
            "Epoch: 6 Batch Number: 135 Loss: 1.4676198959350586 Time taken: 0.46738767623901367\n",
            "Epoch: 6 Batch Number: 136 Loss: 1.4367191791534424 Time taken: 0.45827651023864746\n",
            "Epoch: 6 Batch Number: 137 Loss: 1.4104596376419067 Time taken: 0.45801496505737305\n",
            "Epoch: 6 Batch Number: 138 Loss: 1.4006710052490234 Time taken: 0.4519534111022949\n",
            "Epoch: 6 Batch Number: 139 Loss: 1.4132933616638184 Time taken: 0.4584774971008301\n",
            "Epoch: 6 Batch Number: 140 Loss: 1.3744330406188965 Time taken: 0.46897053718566895\n",
            "Epoch: 6 Batch Number: 141 Loss: 1.411841869354248 Time taken: 0.4562501907348633\n",
            "Epoch: 6 Batch Number: 142 Loss: 1.4721019268035889 Time taken: 0.45839786529541016\n",
            "Epoch: 6 Batch Number: 143 Loss: 1.5692566633224487 Time taken: 0.4558548927307129\n",
            "Epoch: 6 Batch Number: 144 Loss: 1.6113929748535156 Time taken: 0.47431015968322754\n",
            "Epoch: 6 Batch Number: 145 Loss: 1.5302836894989014 Time taken: 0.46701812744140625\n",
            "Epoch: 6 Batch Number: 146 Loss: 1.5493552684783936 Time taken: 0.46071386337280273\n",
            "Epoch: 6 Batch Number: 147 Loss: 1.3988573551177979 Time taken: 0.47270894050598145\n",
            "Epoch: 6 Batch Number: 148 Loss: 1.3684964179992676 Time taken: 0.46181821823120117\n",
            "Epoch: 6 Batch Number: 149 Loss: 1.4457392692565918 Time taken: 0.4534108638763428\n",
            "Epoch: 6 Batch Number: 150 Loss: 1.5650362968444824 Time taken: 0.4676997661590576\n",
            "Epoch: 6 Batch Number: 151 Loss: 1.5121490955352783 Time taken: 0.4550895690917969\n",
            "Epoch: 6 Batch Number: 152 Loss: 1.4211995601654053 Time taken: 0.4524240493774414\n",
            "Epoch: 6 Batch Number: 153 Loss: 1.3772128820419312 Time taken: 0.4601297378540039\n",
            "Epoch: 6 Batch Number: 154 Loss: 1.3911995887756348 Time taken: 0.4471008777618408\n",
            "Epoch: 6 Batch Number: 155 Loss: 1.3751044273376465 Time taken: 0.4600672721862793\n",
            "Epoch: 6 Batch Number: 156 Loss: 1.3877143859863281 Time taken: 0.45374011993408203\n",
            "Epoch: 6 Batch Number: 157 Loss: 1.2662849426269531 Time taken: 0.4510643482208252\n",
            "Epoch: 6 Batch Number: 158 Loss: 1.3087671995162964 Time taken: 0.47143983840942383\n",
            "Epoch: 6 Batch Number: 159 Loss: 1.2730109691619873 Time taken: 0.457979679107666\n",
            "Epoch: 6 Batch Number: 160 Loss: 1.1890380382537842 Time taken: 0.449357271194458\n",
            "Epoch: 6 Batch Number: 161 Loss: 1.2650195360183716 Time taken: 0.4442107677459717\n",
            "Epoch: 6 Batch Number: 162 Loss: 1.2318378686904907 Time taken: 0.44741129875183105\n",
            "Epoch: 6 Batch Number: 163 Loss: 1.332187294960022 Time taken: 0.45069336891174316\n",
            "Epoch: 6 Batch Number: 164 Loss: 1.4735194444656372 Time taken: 0.4636106491088867\n",
            "Epoch: 6 Batch Number: 165 Loss: 1.3883306980133057 Time taken: 0.471393346786499\n",
            "Epoch: 6 Batch Number: 166 Loss: 1.3983453512191772 Time taken: 0.46193742752075195\n",
            "Epoch: 6 Batch Number: 167 Loss: 1.3947795629501343 Time taken: 0.4521651268005371\n",
            "Epoch: 6 Batch Number: 168 Loss: 1.4491300582885742 Time taken: 0.4577162265777588\n",
            "Epoch: 6 Batch Number: 169 Loss: 1.3902442455291748 Time taken: 0.4583089351654053\n",
            "Epoch: 6 Batch Number: 170 Loss: 1.2927299737930298 Time taken: 0.45550966262817383\n",
            "Epoch: 6 Batch Number: 171 Loss: 1.2746996879577637 Time taken: 0.4639096260070801\n",
            "Epoch: 6 Batch Number: 172 Loss: 1.2957122325897217 Time taken: 0.46483588218688965\n",
            "Epoch: 6 Batch Number: 173 Loss: 1.2757716178894043 Time taken: 0.45438075065612793\n",
            "Epoch: 6 Batch Number: 174 Loss: 1.228751301765442 Time taken: 0.46507763862609863\n",
            "Epoch: 6 Batch Number: 175 Loss: 1.3530888557434082 Time taken: 0.4674868583679199\n",
            "Epoch: 6 Batch Number: 176 Loss: 1.344388484954834 Time taken: 0.4609956741333008\n",
            "Epoch: 6 Batch Number: 177 Loss: 1.3692529201507568 Time taken: 0.45195960998535156\n",
            "Epoch: 6 Batch Number: 178 Loss: 1.4123529195785522 Time taken: 0.46083569526672363\n",
            "Epoch: 6 Batch Number: 179 Loss: 1.4245738983154297 Time taken: 0.45485424995422363\n",
            "Epoch: 6 Batch Number: 180 Loss: 1.3634545803070068 Time taken: 0.4714641571044922\n",
            "Epoch: 6 Batch Number: 181 Loss: 1.3402771949768066 Time taken: 0.4633448123931885\n",
            "Epoch: 6 Batch Number: 182 Loss: 1.2568299770355225 Time taken: 0.46598339080810547\n",
            "Epoch: 6 Batch Number: 183 Loss: 1.3372822999954224 Time taken: 0.47185778617858887\n",
            "Epoch: 6 Batch Number: 184 Loss: 1.3583470582962036 Time taken: 0.4611217975616455\n",
            "Epoch: 6 Batch Number: 185 Loss: 1.3936188220977783 Time taken: 0.4705994129180908\n",
            "Epoch: 6 Batch Number: 186 Loss: 1.1818468570709229 Time taken: 0.4597303867340088\n",
            "Epoch: 6 Batch Number: 187 Loss: 1.262750506401062 Time taken: 0.46158719062805176\n",
            "Epoch: 6 Batch Number: 188 Loss: 1.2740631103515625 Time taken: 0.4711153507232666\n",
            "Epoch: 6 Batch Number: 189 Loss: 1.3299038410186768 Time taken: 0.47113800048828125\n",
            "Epoch: 6 Batch Number: 190 Loss: 1.5282738208770752 Time taken: 0.4744441509246826\n",
            "Epoch: 6 Batch Number: 191 Loss: 1.797883152961731 Time taken: 0.4871675968170166\n",
            "Epoch: 6 Batch Number: 192 Loss: 1.432233214378357 Time taken: 0.4651031494140625\n",
            "Epoch: 6 Batch Number: 193 Loss: 1.5663471221923828 Time taken: 0.4615797996520996\n",
            "Epoch: 6 Batch Number: 194 Loss: 1.4378730058670044 Time taken: 0.4704301357269287\n",
            "Epoch: 6 Batch Number: 195 Loss: 1.3454121351242065 Time taken: 0.49605751037597656\n",
            "Epoch: 6 Batch Number: 196 Loss: 1.2918245792388916 Time taken: 0.4749746322631836\n",
            "Epoch: 6 Batch Number: 197 Loss: 1.4143590927124023 Time taken: 0.46858906745910645\n",
            "Epoch: 6 Batch Number: 198 Loss: 1.3086295127868652 Time taken: 0.4653818607330322\n",
            "Epoch: 6 Batch Number: 199 Loss: 1.4002788066864014 Time taken: 0.47292327880859375\n",
            "Epoch: 6 Batch Number: 200 Loss: 1.2959325313568115 Time taken: 0.4686288833618164\n",
            "Epoch: 6 Batch Number: 201 Loss: 1.3420966863632202 Time taken: 0.4613616466522217\n",
            "Epoch: 6 Batch Number: 202 Loss: 1.3436481952667236 Time taken: 0.4816420078277588\n",
            "Epoch: 6 Batch Number: 203 Loss: 1.3329339027404785 Time taken: 0.479062557220459\n",
            "Epoch: 6 Batch Number: 204 Loss: 1.3731708526611328 Time taken: 0.45439982414245605\n",
            "Epoch: 6 Batch Number: 205 Loss: 1.2823203802108765 Time taken: 0.4442732334136963\n",
            "Epoch: 6 Batch Number: 206 Loss: 1.36906898021698 Time taken: 0.44907641410827637\n",
            "Epoch: 6 Batch Number: 207 Loss: 1.3059947490692139 Time taken: 0.4542062282562256\n",
            "Epoch: 6 Batch Number: 208 Loss: 1.3171472549438477 Time taken: 0.4738454818725586\n",
            "Epoch: 6 Batch Number: 209 Loss: 1.35163152217865 Time taken: 0.46599507331848145\n",
            "Epoch: 6 Batch Number: 210 Loss: 1.4257663488388062 Time taken: 0.45804452896118164\n",
            "Epoch: 6 Batch Number: 211 Loss: 1.4312858581542969 Time taken: 0.4475367069244385\n",
            "Epoch: 6 Batch Number: 212 Loss: 1.510991096496582 Time taken: 0.4575521945953369\n",
            "Epoch: 6 Batch Number: 213 Loss: 1.3456745147705078 Time taken: 0.4567408561706543\n",
            "Epoch: 6 Batch Number: 214 Loss: 1.396667242050171 Time taken: 0.4567854404449463\n",
            "Epoch: 6 Batch Number: 215 Loss: 1.3621551990509033 Time taken: 0.4569125175476074\n",
            "Epoch: 6 Batch Number: 216 Loss: 1.3662827014923096 Time taken: 0.46358299255371094\n",
            "Epoch: 6 Batch Number: 217 Loss: 1.4497359991073608 Time taken: 0.4594423770904541\n",
            "Epoch: 6 Batch Number: 218 Loss: 1.501300573348999 Time taken: 0.44655919075012207\n",
            "Epoch: 6 Batch Number: 219 Loss: 1.4196428060531616 Time taken: 0.46099233627319336\n",
            "Epoch: 6 Batch Number: 220 Loss: 1.3793516159057617 Time taken: 0.4529407024383545\n",
            "Epoch: 6 Batch Number: 221 Loss: 1.338468313217163 Time taken: 0.45749878883361816\n",
            "Epoch: 6 Batch Number: 222 Loss: 1.3279273509979248 Time taken: 0.4498448371887207\n",
            "Epoch: 6 Batch Number: 223 Loss: 1.3623684644699097 Time taken: 0.4445793628692627\n",
            "Epoch: 6 Batch Number: 224 Loss: 1.3146603107452393 Time taken: 0.4470219612121582\n",
            "Epoch: 6 Batch Number: 225 Loss: 1.2679352760314941 Time taken: 0.4542984962463379\n",
            "Epoch: 6 Batch Number: 226 Loss: 1.2991936206817627 Time taken: 0.45661282539367676\n",
            "Epoch: 6 Batch Number: 227 Loss: 1.4771173000335693 Time taken: 0.4474608898162842\n",
            "Epoch: 6 Batch Number: 228 Loss: 1.4564616680145264 Time taken: 0.454817533493042\n",
            "Epoch: 6 Batch Number: 229 Loss: 1.4672852754592896 Time taken: 0.47129225730895996\n",
            "==========================================================================================\n",
            "Start of epoch 7\n",
            "Epoch: 7 Batch Number: 1 Loss: 1.3500910997390747 Time taken: 0.4564330577850342\n",
            "Epoch: 7 Batch Number: 2 Loss: 1.4141674041748047 Time taken: 0.44574952125549316\n",
            "Epoch: 7 Batch Number: 3 Loss: 1.3443257808685303 Time taken: 0.45980215072631836\n",
            "Epoch: 7 Batch Number: 4 Loss: 1.3529926538467407 Time taken: 0.47152137756347656\n",
            "Epoch: 7 Batch Number: 5 Loss: 1.3425687551498413 Time taken: 0.46394824981689453\n",
            "Epoch: 7 Batch Number: 6 Loss: 1.2567055225372314 Time taken: 0.4727499485015869\n",
            "Epoch: 7 Batch Number: 7 Loss: 1.2800103425979614 Time taken: 0.45323657989501953\n",
            "Epoch: 7 Batch Number: 8 Loss: 1.2726571559906006 Time taken: 0.45710253715515137\n",
            "Epoch: 7 Batch Number: 9 Loss: 1.2899901866912842 Time taken: 0.45018959045410156\n",
            "Epoch: 7 Batch Number: 10 Loss: 1.3386147022247314 Time taken: 0.4654719829559326\n",
            "Epoch: 7 Batch Number: 11 Loss: 1.2785687446594238 Time taken: 0.455888032913208\n",
            "Epoch: 7 Batch Number: 12 Loss: 1.2470364570617676 Time taken: 0.45378947257995605\n",
            "Epoch: 7 Batch Number: 13 Loss: 1.2935596704483032 Time taken: 0.4628736972808838\n",
            "Epoch: 7 Batch Number: 14 Loss: 1.2983365058898926 Time taken: 0.4640045166015625\n",
            "Epoch: 7 Batch Number: 15 Loss: 1.2207039594650269 Time taken: 0.45947957038879395\n",
            "Epoch: 7 Batch Number: 16 Loss: 1.3061184883117676 Time taken: 0.4613831043243408\n",
            "Epoch: 7 Batch Number: 17 Loss: 1.4510060548782349 Time taken: 0.46900296211242676\n",
            "Epoch: 7 Batch Number: 18 Loss: 1.4083082675933838 Time taken: 0.45022010803222656\n",
            "Epoch: 7 Batch Number: 19 Loss: 1.3992446660995483 Time taken: 0.451770544052124\n",
            "Epoch: 7 Batch Number: 20 Loss: 1.253718614578247 Time taken: 0.45084476470947266\n",
            "Epoch: 7 Batch Number: 21 Loss: 1.498875617980957 Time taken: 0.45513916015625\n",
            "Epoch: 7 Batch Number: 22 Loss: 1.422717571258545 Time taken: 0.44936203956604004\n",
            "Epoch: 7 Batch Number: 23 Loss: 1.4337668418884277 Time taken: 0.4478883743286133\n",
            "Epoch: 7 Batch Number: 24 Loss: 1.3960050344467163 Time taken: 0.4502096176147461\n",
            "Epoch: 7 Batch Number: 25 Loss: 1.385240912437439 Time taken: 0.471099853515625\n",
            "Epoch: 7 Batch Number: 26 Loss: 1.4014079570770264 Time taken: 0.45912623405456543\n",
            "Epoch: 7 Batch Number: 27 Loss: 1.3471851348876953 Time taken: 0.46239638328552246\n",
            "Epoch: 7 Batch Number: 28 Loss: 1.3481714725494385 Time taken: 0.45075106620788574\n",
            "Epoch: 7 Batch Number: 29 Loss: 1.365784764289856 Time taken: 0.4579911231994629\n",
            "Epoch: 7 Batch Number: 30 Loss: 1.1637868881225586 Time taken: 0.4601414203643799\n",
            "Epoch: 7 Batch Number: 31 Loss: 1.3140571117401123 Time taken: 0.45831942558288574\n",
            "Epoch: 7 Batch Number: 32 Loss: 1.318900465965271 Time taken: 0.44859981536865234\n",
            "Epoch: 7 Batch Number: 33 Loss: 1.3309326171875 Time taken: 0.457660436630249\n",
            "Epoch: 7 Batch Number: 34 Loss: 1.3578109741210938 Time taken: 0.46117115020751953\n",
            "Epoch: 7 Batch Number: 35 Loss: 1.4065327644348145 Time taken: 0.4510819911956787\n",
            "Epoch: 7 Batch Number: 36 Loss: 1.5003488063812256 Time taken: 0.45776796340942383\n",
            "Epoch: 7 Batch Number: 37 Loss: 1.320178747177124 Time taken: 0.45188093185424805\n",
            "Epoch: 7 Batch Number: 38 Loss: 1.3472318649291992 Time taken: 0.45879101753234863\n",
            "Epoch: 7 Batch Number: 39 Loss: 1.3276755809783936 Time taken: 0.46558690071105957\n",
            "Epoch: 7 Batch Number: 40 Loss: 1.3376837968826294 Time taken: 0.4553489685058594\n",
            "Epoch: 7 Batch Number: 41 Loss: 1.2587412595748901 Time taken: 0.4447669982910156\n",
            "Epoch: 7 Batch Number: 42 Loss: 1.280043125152588 Time taken: 0.452073335647583\n",
            "Epoch: 7 Batch Number: 43 Loss: 1.2947821617126465 Time taken: 0.4703853130340576\n",
            "Epoch: 7 Batch Number: 44 Loss: 1.2159152030944824 Time taken: 0.4643418788909912\n",
            "Epoch: 7 Batch Number: 45 Loss: 1.2691484689712524 Time taken: 0.46364402770996094\n",
            "Epoch: 7 Batch Number: 46 Loss: 1.4321626424789429 Time taken: 0.4482591152191162\n",
            "Epoch: 7 Batch Number: 47 Loss: 1.3324065208435059 Time taken: 0.47174930572509766\n",
            "Epoch: 7 Batch Number: 48 Loss: 1.404937744140625 Time taken: 0.4548766613006592\n",
            "Epoch: 7 Batch Number: 49 Loss: 1.4204484224319458 Time taken: 0.45671677589416504\n",
            "Epoch: 7 Batch Number: 50 Loss: 1.2689440250396729 Time taken: 0.4498896598815918\n",
            "Epoch: 7 Batch Number: 51 Loss: 1.290771722793579 Time taken: 0.4563267230987549\n",
            "Epoch: 7 Batch Number: 52 Loss: 1.4139845371246338 Time taken: 0.46411585807800293\n",
            "Epoch: 7 Batch Number: 53 Loss: 1.4482097625732422 Time taken: 0.47835278511047363\n",
            "Epoch: 7 Batch Number: 54 Loss: 1.4258267879486084 Time taken: 0.4614694118499756\n",
            "Epoch: 7 Batch Number: 55 Loss: 1.3718171119689941 Time taken: 0.4575023651123047\n",
            "Epoch: 7 Batch Number: 56 Loss: 1.4510376453399658 Time taken: 0.470242977142334\n",
            "Epoch: 7 Batch Number: 57 Loss: 1.3491199016571045 Time taken: 0.45023036003112793\n",
            "Epoch: 7 Batch Number: 58 Loss: 1.300270438194275 Time taken: 0.45096778869628906\n",
            "Epoch: 7 Batch Number: 59 Loss: 1.3089325428009033 Time taken: 0.4563944339752197\n",
            "Epoch: 7 Batch Number: 60 Loss: 1.3291893005371094 Time taken: 0.4667348861694336\n",
            "Epoch: 7 Batch Number: 61 Loss: 1.4038798809051514 Time taken: 0.4499053955078125\n",
            "Epoch: 7 Batch Number: 62 Loss: 1.299900770187378 Time taken: 0.4527561664581299\n",
            "Epoch: 7 Batch Number: 63 Loss: 1.2563718557357788 Time taken: 0.4529726505279541\n",
            "Epoch: 7 Batch Number: 64 Loss: 1.2203768491744995 Time taken: 0.4597289562225342\n",
            "Epoch: 7 Batch Number: 65 Loss: 1.2659921646118164 Time taken: 0.4659397602081299\n",
            "Epoch: 7 Batch Number: 66 Loss: 1.289052963256836 Time taken: 0.46691298484802246\n",
            "Epoch: 7 Batch Number: 67 Loss: 1.276334285736084 Time taken: 0.4687337875366211\n",
            "Epoch: 7 Batch Number: 68 Loss: 1.3392572402954102 Time taken: 0.4630453586578369\n",
            "Epoch: 7 Batch Number: 69 Loss: 1.332348108291626 Time taken: 0.4468393325805664\n",
            "Epoch: 7 Batch Number: 70 Loss: 1.3129277229309082 Time taken: 0.452730655670166\n",
            "Epoch: 7 Batch Number: 71 Loss: 1.3225620985031128 Time taken: 0.4623720645904541\n",
            "Epoch: 7 Batch Number: 72 Loss: 1.3452223539352417 Time taken: 0.4465489387512207\n",
            "Epoch: 7 Batch Number: 73 Loss: 1.2805793285369873 Time taken: 0.44918274879455566\n",
            "Epoch: 7 Batch Number: 74 Loss: 1.423362135887146 Time taken: 0.4549288749694824\n",
            "Epoch: 7 Batch Number: 75 Loss: 1.226298213005066 Time taken: 0.4557013511657715\n",
            "Epoch: 7 Batch Number: 76 Loss: 1.2915685176849365 Time taken: 0.4625742435455322\n",
            "Epoch: 7 Batch Number: 77 Loss: 1.3673710823059082 Time taken: 0.4591238498687744\n",
            "Epoch: 7 Batch Number: 78 Loss: 1.238586187362671 Time taken: 0.451007604598999\n",
            "Epoch: 7 Batch Number: 79 Loss: 1.3180830478668213 Time taken: 0.4568355083465576\n",
            "Epoch: 7 Batch Number: 80 Loss: 1.3220024108886719 Time taken: 0.4495255947113037\n",
            "Epoch: 7 Batch Number: 81 Loss: 1.297379493713379 Time taken: 0.4650535583496094\n",
            "Epoch: 7 Batch Number: 82 Loss: 1.3405177593231201 Time taken: 0.46431756019592285\n",
            "Epoch: 7 Batch Number: 83 Loss: 1.4030150175094604 Time taken: 0.4511730670928955\n",
            "Epoch: 7 Batch Number: 84 Loss: 1.3483816385269165 Time taken: 0.4723372459411621\n",
            "Epoch: 7 Batch Number: 85 Loss: 1.3057024478912354 Time taken: 0.45868635177612305\n",
            "Epoch: 7 Batch Number: 86 Loss: 1.3459237813949585 Time taken: 0.45689988136291504\n",
            "Epoch: 7 Batch Number: 87 Loss: 1.376968502998352 Time taken: 0.4523179531097412\n",
            "Epoch: 7 Batch Number: 88 Loss: 1.284020185470581 Time taken: 0.4538404941558838\n",
            "Epoch: 7 Batch Number: 89 Loss: 1.2521347999572754 Time taken: 0.45441579818725586\n",
            "Epoch: 7 Batch Number: 90 Loss: 1.2983595132827759 Time taken: 0.4551370143890381\n",
            "Epoch: 7 Batch Number: 91 Loss: 1.3072049617767334 Time taken: 0.4663214683532715\n",
            "Epoch: 7 Batch Number: 92 Loss: 1.2518677711486816 Time taken: 0.46239709854125977\n",
            "Epoch: 7 Batch Number: 93 Loss: 1.2619367837905884 Time taken: 0.4669034481048584\n",
            "Epoch: 7 Batch Number: 94 Loss: 1.4299023151397705 Time taken: 0.46476221084594727\n",
            "Epoch: 7 Batch Number: 95 Loss: 1.3920557498931885 Time taken: 0.47123193740844727\n",
            "Epoch: 7 Batch Number: 96 Loss: 1.4419955015182495 Time taken: 0.4556849002838135\n",
            "Epoch: 7 Batch Number: 97 Loss: 1.3998031616210938 Time taken: 0.47406983375549316\n",
            "Epoch: 7 Batch Number: 98 Loss: 1.4337557554244995 Time taken: 0.4704287052154541\n",
            "Epoch: 7 Batch Number: 99 Loss: 1.4550912380218506 Time taken: 0.4703938961029053\n",
            "Epoch: 7 Batch Number: 100 Loss: 1.3830689191818237 Time taken: 0.45606112480163574\n",
            "Epoch: 7 Batch Number: 101 Loss: 1.3623912334442139 Time taken: 0.4525907039642334\n",
            "Epoch: 7 Batch Number: 102 Loss: 1.4084596633911133 Time taken: 0.4436163902282715\n",
            "Epoch: 7 Batch Number: 103 Loss: 1.5105431079864502 Time taken: 0.4486873149871826\n",
            "Epoch: 7 Batch Number: 104 Loss: 1.420381784439087 Time taken: 0.475161075592041\n",
            "Epoch: 7 Batch Number: 105 Loss: 1.4684544801712036 Time taken: 0.4582657814025879\n",
            "Epoch: 7 Batch Number: 106 Loss: 1.414097785949707 Time taken: 0.4579901695251465\n",
            "Epoch: 7 Batch Number: 107 Loss: 1.3898355960845947 Time taken: 0.44772911071777344\n",
            "Epoch: 7 Batch Number: 108 Loss: 1.3698616027832031 Time taken: 0.456998348236084\n",
            "Epoch: 7 Batch Number: 109 Loss: 1.3830574750900269 Time taken: 0.4505937099456787\n",
            "Epoch: 7 Batch Number: 110 Loss: 1.2497284412384033 Time taken: 0.44655919075012207\n",
            "Epoch: 7 Batch Number: 111 Loss: 1.2768714427947998 Time taken: 0.4496917724609375\n",
            "Epoch: 7 Batch Number: 112 Loss: 1.304421305656433 Time taken: 0.45246028900146484\n",
            "Epoch: 7 Batch Number: 113 Loss: 1.3403191566467285 Time taken: 0.455258846282959\n",
            "Epoch: 7 Batch Number: 114 Loss: 1.3559929132461548 Time taken: 0.4505746364593506\n",
            "Epoch: 7 Batch Number: 115 Loss: 1.3543106317520142 Time taken: 0.48128700256347656\n",
            "Epoch: 7 Batch Number: 116 Loss: 1.2132601737976074 Time taken: 0.46713709831237793\n",
            "Epoch: 7 Batch Number: 117 Loss: 1.3298462629318237 Time taken: 0.47327756881713867\n",
            "Epoch: 7 Batch Number: 118 Loss: 1.260616660118103 Time taken: 0.45576930046081543\n",
            "Epoch: 7 Batch Number: 119 Loss: 1.299201250076294 Time taken: 0.45995259284973145\n",
            "Epoch: 7 Batch Number: 120 Loss: 1.3628219366073608 Time taken: 0.46538281440734863\n",
            "Epoch: 7 Batch Number: 121 Loss: 1.2972767353057861 Time taken: 0.485260009765625\n",
            "Epoch: 7 Batch Number: 122 Loss: 1.3072584867477417 Time taken: 0.4613339900970459\n",
            "Epoch: 7 Batch Number: 123 Loss: 1.319588303565979 Time taken: 0.4523031711578369\n",
            "Epoch: 7 Batch Number: 124 Loss: 1.3445870876312256 Time taken: 0.4493272304534912\n",
            "Epoch: 7 Batch Number: 125 Loss: 1.3608720302581787 Time taken: 0.45327210426330566\n",
            "Epoch: 7 Batch Number: 126 Loss: 1.3625260591506958 Time taken: 0.45351290702819824\n",
            "Epoch: 7 Batch Number: 127 Loss: 1.4048621654510498 Time taken: 0.4723999500274658\n",
            "Epoch: 7 Batch Number: 128 Loss: 1.3028056621551514 Time taken: 0.46863293647766113\n",
            "Epoch: 7 Batch Number: 129 Loss: 1.3060741424560547 Time taken: 0.4614131450653076\n",
            "Epoch: 7 Batch Number: 130 Loss: 1.2830381393432617 Time taken: 0.4481058120727539\n",
            "Epoch: 7 Batch Number: 131 Loss: 1.3848164081573486 Time taken: 0.46765923500061035\n",
            "Epoch: 7 Batch Number: 132 Loss: 1.2172473669052124 Time taken: 0.46912646293640137\n",
            "Epoch: 7 Batch Number: 133 Loss: 1.3692303895950317 Time taken: 0.4484593868255615\n",
            "Epoch: 7 Batch Number: 134 Loss: 1.4023553133010864 Time taken: 0.45066308975219727\n",
            "Epoch: 7 Batch Number: 135 Loss: 1.399674654006958 Time taken: 0.4655754566192627\n",
            "Epoch: 7 Batch Number: 136 Loss: 1.379320740699768 Time taken: 0.4642627239227295\n",
            "Epoch: 7 Batch Number: 137 Loss: 1.3490477800369263 Time taken: 0.45100975036621094\n",
            "Epoch: 7 Batch Number: 138 Loss: 1.3510748147964478 Time taken: 0.45041942596435547\n",
            "Epoch: 7 Batch Number: 139 Loss: 1.3658572435379028 Time taken: 0.4525120258331299\n",
            "Epoch: 7 Batch Number: 140 Loss: 1.3190263509750366 Time taken: 0.4636101722717285\n",
            "Epoch: 7 Batch Number: 141 Loss: 1.3580609560012817 Time taken: 0.4547574520111084\n",
            "Epoch: 7 Batch Number: 142 Loss: 1.4192007780075073 Time taken: 0.4573807716369629\n",
            "Epoch: 7 Batch Number: 143 Loss: 1.5107545852661133 Time taken: 0.4508483409881592\n",
            "Epoch: 7 Batch Number: 144 Loss: 1.5324735641479492 Time taken: 0.47745323181152344\n",
            "Epoch: 7 Batch Number: 145 Loss: 1.4665192365646362 Time taken: 0.45653533935546875\n",
            "Epoch: 7 Batch Number: 146 Loss: 1.4815186262130737 Time taken: 0.47072887420654297\n",
            "Epoch: 7 Batch Number: 147 Loss: 1.3154083490371704 Time taken: 0.46799468994140625\n",
            "Epoch: 7 Batch Number: 148 Loss: 1.2955622673034668 Time taken: 0.44968390464782715\n",
            "Epoch: 7 Batch Number: 149 Loss: 1.3812456130981445 Time taken: 0.45516443252563477\n",
            "Epoch: 7 Batch Number: 150 Loss: 1.4981908798217773 Time taken: 0.45728111267089844\n",
            "Epoch: 7 Batch Number: 151 Loss: 1.4526560306549072 Time taken: 0.45363521575927734\n",
            "Epoch: 7 Batch Number: 152 Loss: 1.3763744831085205 Time taken: 0.46947407722473145\n",
            "Epoch: 7 Batch Number: 153 Loss: 1.3234704732894897 Time taken: 0.45101308822631836\n",
            "Epoch: 7 Batch Number: 154 Loss: 1.329108715057373 Time taken: 0.45323610305786133\n",
            "Epoch: 7 Batch Number: 155 Loss: 1.309314489364624 Time taken: 0.46633362770080566\n",
            "Epoch: 7 Batch Number: 156 Loss: 1.3390929698944092 Time taken: 0.457183837890625\n",
            "Epoch: 7 Batch Number: 157 Loss: 1.221447229385376 Time taken: 0.458728551864624\n",
            "Epoch: 7 Batch Number: 158 Loss: 1.2643401622772217 Time taken: 0.4594893455505371\n",
            "Epoch: 7 Batch Number: 159 Loss: 1.2241427898406982 Time taken: 0.4680788516998291\n",
            "Epoch: 7 Batch Number: 160 Loss: 1.1361441612243652 Time taken: 0.45538926124572754\n",
            "Epoch: 7 Batch Number: 161 Loss: 1.2097705602645874 Time taken: 0.44928622245788574\n",
            "Epoch: 7 Batch Number: 162 Loss: 1.1821995973587036 Time taken: 0.446915864944458\n",
            "Epoch: 7 Batch Number: 163 Loss: 1.2842553853988647 Time taken: 0.46217775344848633\n",
            "Epoch: 7 Batch Number: 164 Loss: 1.4141615629196167 Time taken: 0.45291852951049805\n",
            "Epoch: 7 Batch Number: 165 Loss: 1.3396713733673096 Time taken: 0.45369887351989746\n",
            "Epoch: 7 Batch Number: 166 Loss: 1.3594473600387573 Time taken: 0.4559211730957031\n",
            "Epoch: 7 Batch Number: 167 Loss: 1.3499829769134521 Time taken: 0.4485304355621338\n",
            "Epoch: 7 Batch Number: 168 Loss: 1.3829679489135742 Time taken: 0.45850253105163574\n",
            "Epoch: 7 Batch Number: 169 Loss: 1.3313965797424316 Time taken: 0.4448568820953369\n",
            "Epoch: 7 Batch Number: 170 Loss: 1.240873098373413 Time taken: 0.46489667892456055\n",
            "Epoch: 7 Batch Number: 171 Loss: 1.211476445198059 Time taken: 0.4643862247467041\n",
            "Epoch: 7 Batch Number: 172 Loss: 1.2424838542938232 Time taken: 0.47791385650634766\n",
            "Epoch: 7 Batch Number: 173 Loss: 1.2304527759552002 Time taken: 0.47229719161987305\n",
            "Epoch: 7 Batch Number: 174 Loss: 1.1822664737701416 Time taken: 0.47521376609802246\n",
            "Epoch: 7 Batch Number: 175 Loss: 1.3021702766418457 Time taken: 0.4691190719604492\n",
            "Epoch: 7 Batch Number: 176 Loss: 1.299782156944275 Time taken: 0.4503462314605713\n",
            "Epoch: 7 Batch Number: 177 Loss: 1.3176696300506592 Time taken: 0.44539451599121094\n",
            "Epoch: 7 Batch Number: 178 Loss: 1.3687007427215576 Time taken: 0.46460843086242676\n",
            "Epoch: 7 Batch Number: 179 Loss: 1.363269329071045 Time taken: 0.45493125915527344\n",
            "Epoch: 7 Batch Number: 180 Loss: 1.313004970550537 Time taken: 0.449246883392334\n",
            "Epoch: 7 Batch Number: 181 Loss: 1.2868366241455078 Time taken: 0.44985318183898926\n",
            "Epoch: 7 Batch Number: 182 Loss: 1.2067890167236328 Time taken: 0.4528226852416992\n",
            "Epoch: 7 Batch Number: 183 Loss: 1.28355073928833 Time taken: 0.4580099582672119\n",
            "Epoch: 7 Batch Number: 184 Loss: 1.318434476852417 Time taken: 0.44701504707336426\n",
            "Epoch: 7 Batch Number: 185 Loss: 1.3483084440231323 Time taken: 0.4536728858947754\n",
            "Epoch: 7 Batch Number: 186 Loss: 1.1219452619552612 Time taken: 0.47495388984680176\n",
            "Epoch: 7 Batch Number: 187 Loss: 1.2182447910308838 Time taken: 0.45632219314575195\n",
            "Epoch: 7 Batch Number: 188 Loss: 1.2264134883880615 Time taken: 0.44600820541381836\n",
            "Epoch: 7 Batch Number: 189 Loss: 1.2835288047790527 Time taken: 0.4585909843444824\n",
            "Epoch: 7 Batch Number: 190 Loss: 1.4753689765930176 Time taken: 0.4598231315612793\n",
            "Epoch: 7 Batch Number: 191 Loss: 1.7294152975082397 Time taken: 0.4506418704986572\n",
            "Epoch: 7 Batch Number: 192 Loss: 1.3710994720458984 Time taken: 0.4494163990020752\n",
            "Epoch: 7 Batch Number: 193 Loss: 1.5064144134521484 Time taken: 0.44815683364868164\n",
            "Epoch: 7 Batch Number: 194 Loss: 1.3841181993484497 Time taken: 0.46254730224609375\n",
            "Epoch: 7 Batch Number: 195 Loss: 1.3022643327713013 Time taken: 0.4555990695953369\n",
            "Epoch: 7 Batch Number: 196 Loss: 1.2576876878738403 Time taken: 0.4624626636505127\n",
            "Epoch: 7 Batch Number: 197 Loss: 1.3679680824279785 Time taken: 0.4557054042816162\n",
            "Epoch: 7 Batch Number: 198 Loss: 1.268139362335205 Time taken: 0.46242427825927734\n",
            "Epoch: 7 Batch Number: 199 Loss: 1.3406145572662354 Time taken: 0.45884108543395996\n",
            "Epoch: 7 Batch Number: 200 Loss: 1.251255750656128 Time taken: 0.45790529251098633\n",
            "Epoch: 7 Batch Number: 201 Loss: 1.3016585111618042 Time taken: 0.47536420822143555\n",
            "Epoch: 7 Batch Number: 202 Loss: 1.2890934944152832 Time taken: 0.45368528366088867\n",
            "Epoch: 7 Batch Number: 203 Loss: 1.2861340045928955 Time taken: 0.4544563293457031\n",
            "Epoch: 7 Batch Number: 204 Loss: 1.3265585899353027 Time taken: 0.4625544548034668\n",
            "Epoch: 7 Batch Number: 205 Loss: 1.2381912469863892 Time taken: 0.45275259017944336\n",
            "Epoch: 7 Batch Number: 206 Loss: 1.3112646341323853 Time taken: 0.46625328063964844\n",
            "Epoch: 7 Batch Number: 207 Loss: 1.2722011804580688 Time taken: 0.4538099765777588\n",
            "Epoch: 7 Batch Number: 208 Loss: 1.2665516138076782 Time taken: 0.4489560127258301\n",
            "Epoch: 7 Batch Number: 209 Loss: 1.3179975748062134 Time taken: 0.4542686939239502\n",
            "Epoch: 7 Batch Number: 210 Loss: 1.3734842538833618 Time taken: 0.4582221508026123\n",
            "Epoch: 7 Batch Number: 211 Loss: 1.3647531270980835 Time taken: 0.45525097846984863\n",
            "Epoch: 7 Batch Number: 212 Loss: 1.4547016620635986 Time taken: 0.4661076068878174\n",
            "Epoch: 7 Batch Number: 213 Loss: 1.2962512969970703 Time taken: 0.4569284915924072\n",
            "Epoch: 7 Batch Number: 214 Loss: 1.356123447418213 Time taken: 0.46309494972229004\n",
            "Epoch: 7 Batch Number: 215 Loss: 1.3142766952514648 Time taken: 0.46729397773742676\n",
            "Epoch: 7 Batch Number: 216 Loss: 1.3303687572479248 Time taken: 0.4479055404663086\n",
            "Epoch: 7 Batch Number: 217 Loss: 1.4054597616195679 Time taken: 0.44936180114746094\n",
            "Epoch: 7 Batch Number: 218 Loss: 1.439220905303955 Time taken: 0.4716677665710449\n",
            "Epoch: 7 Batch Number: 219 Loss: 1.381852388381958 Time taken: 0.4527413845062256\n",
            "Epoch: 7 Batch Number: 220 Loss: 1.3209056854248047 Time taken: 0.45034074783325195\n",
            "Epoch: 7 Batch Number: 221 Loss: 1.2936800718307495 Time taken: 0.45176148414611816\n",
            "Epoch: 7 Batch Number: 222 Loss: 1.2833328247070312 Time taken: 0.45879149436950684\n",
            "Epoch: 7 Batch Number: 223 Loss: 1.316506028175354 Time taken: 0.44800519943237305\n",
            "Epoch: 7 Batch Number: 224 Loss: 1.2684423923492432 Time taken: 0.443101167678833\n",
            "Epoch: 7 Batch Number: 225 Loss: 1.2220609188079834 Time taken: 0.4644167423248291\n",
            "Epoch: 7 Batch Number: 226 Loss: 1.2599283456802368 Time taken: 0.46466588973999023\n",
            "Epoch: 7 Batch Number: 227 Loss: 1.42494797706604 Time taken: 0.47450804710388184\n",
            "Epoch: 7 Batch Number: 228 Loss: 1.3991299867630005 Time taken: 0.4591255187988281\n",
            "Epoch: 7 Batch Number: 229 Loss: 1.409079670906067 Time taken: 0.4587247371673584\n",
            "==========================================================================================\n",
            "Start of epoch 8\n",
            "Epoch: 8 Batch Number: 1 Loss: 1.2966047525405884 Time taken: 0.4490623474121094\n",
            "Epoch: 8 Batch Number: 2 Loss: 1.3684669733047485 Time taken: 0.45429205894470215\n",
            "Epoch: 8 Batch Number: 3 Loss: 1.2975716590881348 Time taken: 0.46238040924072266\n",
            "Epoch: 8 Batch Number: 4 Loss: 1.3058589696884155 Time taken: 0.44971561431884766\n",
            "Epoch: 8 Batch Number: 5 Loss: 1.2848261594772339 Time taken: 0.4684121608734131\n",
            "Epoch: 8 Batch Number: 6 Loss: 1.208061933517456 Time taken: 0.47008275985717773\n",
            "Epoch: 8 Batch Number: 7 Loss: 1.2394529581069946 Time taken: 0.4726371765136719\n",
            "Epoch: 8 Batch Number: 8 Loss: 1.2331000566482544 Time taken: 0.4528038501739502\n",
            "Epoch: 8 Batch Number: 9 Loss: 1.2385165691375732 Time taken: 0.45513129234313965\n",
            "Epoch: 8 Batch Number: 10 Loss: 1.2904002666473389 Time taken: 0.44898557662963867\n",
            "Epoch: 8 Batch Number: 11 Loss: 1.2375298738479614 Time taken: 0.46144771575927734\n",
            "Epoch: 8 Batch Number: 12 Loss: 1.199141502380371 Time taken: 0.45359277725219727\n",
            "Epoch: 8 Batch Number: 13 Loss: 1.2439990043640137 Time taken: 0.4547579288482666\n",
            "Epoch: 8 Batch Number: 14 Loss: 1.2601630687713623 Time taken: 0.4494471549987793\n",
            "Epoch: 8 Batch Number: 15 Loss: 1.197745680809021 Time taken: 0.4458482265472412\n",
            "Epoch: 8 Batch Number: 16 Loss: 1.2573163509368896 Time taken: 0.4637134075164795\n",
            "Epoch: 8 Batch Number: 17 Loss: 1.3873965740203857 Time taken: 0.46700334548950195\n",
            "Epoch: 8 Batch Number: 18 Loss: 1.3585638999938965 Time taken: 0.47585391998291016\n",
            "Epoch: 8 Batch Number: 19 Loss: 1.3471636772155762 Time taken: 0.476055383682251\n",
            "Epoch: 8 Batch Number: 20 Loss: 1.2076092958450317 Time taken: 0.4959230422973633\n",
            "Epoch: 8 Batch Number: 21 Loss: 1.4438388347625732 Time taken: 0.4531130790710449\n",
            "Epoch: 8 Batch Number: 22 Loss: 1.3804723024368286 Time taken: 0.45534539222717285\n",
            "Epoch: 8 Batch Number: 23 Loss: 1.3928608894348145 Time taken: 0.45581698417663574\n",
            "Epoch: 8 Batch Number: 24 Loss: 1.3555368185043335 Time taken: 0.46332550048828125\n",
            "Epoch: 8 Batch Number: 25 Loss: 1.3384026288986206 Time taken: 0.44902944564819336\n",
            "Epoch: 8 Batch Number: 26 Loss: 1.3565806150436401 Time taken: 0.4617757797241211\n",
            "Epoch: 8 Batch Number: 27 Loss: 1.2990421056747437 Time taken: 0.45514345169067383\n",
            "Epoch: 8 Batch Number: 28 Loss: 1.3064422607421875 Time taken: 0.44978857040405273\n",
            "Epoch: 8 Batch Number: 29 Loss: 1.3306257724761963 Time taken: 0.47223401069641113\n",
            "Epoch: 8 Batch Number: 30 Loss: 1.1230506896972656 Time taken: 0.4476649761199951\n",
            "Epoch: 8 Batch Number: 31 Loss: 1.2733063697814941 Time taken: 0.4543161392211914\n",
            "Epoch: 8 Batch Number: 32 Loss: 1.2789678573608398 Time taken: 0.45603132247924805\n",
            "Epoch: 8 Batch Number: 33 Loss: 1.3029913902282715 Time taken: 0.45332860946655273\n",
            "Epoch: 8 Batch Number: 34 Loss: 1.3241556882858276 Time taken: 0.45532703399658203\n",
            "Epoch: 8 Batch Number: 35 Loss: 1.3703701496124268 Time taken: 0.46228909492492676\n",
            "Epoch: 8 Batch Number: 36 Loss: 1.4489259719848633 Time taken: 0.4518113136291504\n",
            "Epoch: 8 Batch Number: 37 Loss: 1.281558632850647 Time taken: 0.4658782482147217\n",
            "Epoch: 8 Batch Number: 38 Loss: 1.315419316291809 Time taken: 0.4601461887359619\n",
            "Epoch: 8 Batch Number: 39 Loss: 1.292749047279358 Time taken: 0.45534229278564453\n",
            "Epoch: 8 Batch Number: 40 Loss: 1.2955126762390137 Time taken: 0.4517970085144043\n",
            "Epoch: 8 Batch Number: 41 Loss: 1.2309231758117676 Time taken: 0.46106672286987305\n",
            "Epoch: 8 Batch Number: 42 Loss: 1.2424402236938477 Time taken: 0.466123104095459\n",
            "Epoch: 8 Batch Number: 43 Loss: 1.2484596967697144 Time taken: 0.45530223846435547\n",
            "Epoch: 8 Batch Number: 44 Loss: 1.179366111755371 Time taken: 0.4568326473236084\n",
            "Epoch: 8 Batch Number: 45 Loss: 1.2333436012268066 Time taken: 0.4660012722015381\n",
            "Epoch: 8 Batch Number: 46 Loss: 1.39738130569458 Time taken: 0.46296024322509766\n",
            "Epoch: 8 Batch Number: 47 Loss: 1.2921944856643677 Time taken: 0.4454970359802246\n",
            "Epoch: 8 Batch Number: 48 Loss: 1.362431287765503 Time taken: 0.4623985290527344\n",
            "Epoch: 8 Batch Number: 49 Loss: 1.3764309883117676 Time taken: 0.442122220993042\n",
            "Epoch: 8 Batch Number: 50 Loss: 1.232539415359497 Time taken: 0.4595601558685303\n",
            "Epoch: 8 Batch Number: 51 Loss: 1.2513586282730103 Time taken: 0.4535360336303711\n",
            "Epoch: 8 Batch Number: 52 Loss: 1.3752658367156982 Time taken: 0.4565105438232422\n",
            "Epoch: 8 Batch Number: 53 Loss: 1.4136713743209839 Time taken: 0.45992612838745117\n",
            "Epoch: 8 Batch Number: 54 Loss: 1.3770121335983276 Time taken: 0.4611682891845703\n",
            "Epoch: 8 Batch Number: 55 Loss: 1.3283509016036987 Time taken: 0.4486069679260254\n",
            "Epoch: 8 Batch Number: 56 Loss: 1.4173486232757568 Time taken: 0.453737735748291\n",
            "Epoch: 8 Batch Number: 57 Loss: 1.309580683708191 Time taken: 0.46590685844421387\n",
            "Epoch: 8 Batch Number: 58 Loss: 1.2565252780914307 Time taken: 0.47225117683410645\n",
            "Epoch: 8 Batch Number: 59 Loss: 1.274344563484192 Time taken: 0.480806827545166\n",
            "Epoch: 8 Batch Number: 60 Loss: 1.2839603424072266 Time taken: 0.4503474235534668\n",
            "Epoch: 8 Batch Number: 61 Loss: 1.3520593643188477 Time taken: 0.450253963470459\n",
            "Epoch: 8 Batch Number: 62 Loss: 1.248133897781372 Time taken: 0.45218753814697266\n",
            "Epoch: 8 Batch Number: 63 Loss: 1.2289443016052246 Time taken: 0.4586830139160156\n",
            "Epoch: 8 Batch Number: 64 Loss: 1.1852169036865234 Time taken: 0.4550015926361084\n",
            "Epoch: 8 Batch Number: 65 Loss: 1.2289435863494873 Time taken: 0.4464905261993408\n",
            "Epoch: 8 Batch Number: 66 Loss: 1.2529007196426392 Time taken: 0.45464372634887695\n",
            "Epoch: 8 Batch Number: 67 Loss: 1.238847017288208 Time taken: 0.4600527286529541\n",
            "Epoch: 8 Batch Number: 68 Loss: 1.304600477218628 Time taken: 0.46828508377075195\n",
            "Epoch: 8 Batch Number: 69 Loss: 1.287775993347168 Time taken: 0.4557373523712158\n",
            "Epoch: 8 Batch Number: 70 Loss: 1.279875636100769 Time taken: 0.4611353874206543\n",
            "Epoch: 8 Batch Number: 71 Loss: 1.2761714458465576 Time taken: 0.4531724452972412\n",
            "Epoch: 8 Batch Number: 72 Loss: 1.3002084493637085 Time taken: 0.4598424434661865\n",
            "Epoch: 8 Batch Number: 73 Loss: 1.2310283184051514 Time taken: 0.45089054107666016\n",
            "Epoch: 8 Batch Number: 74 Loss: 1.3588608503341675 Time taken: 0.4594383239746094\n",
            "Epoch: 8 Batch Number: 75 Loss: 1.1798171997070312 Time taken: 0.4479789733886719\n",
            "Epoch: 8 Batch Number: 76 Loss: 1.2338426113128662 Time taken: 0.46755003929138184\n",
            "Epoch: 8 Batch Number: 77 Loss: 1.3222697973251343 Time taken: 0.4491240978240967\n",
            "Epoch: 8 Batch Number: 78 Loss: 1.1979495286941528 Time taken: 0.4519228935241699\n",
            "Epoch: 8 Batch Number: 79 Loss: 1.2839685678482056 Time taken: 0.4531106948852539\n",
            "Epoch: 8 Batch Number: 80 Loss: 1.2762928009033203 Time taken: 0.4640655517578125\n",
            "Epoch: 8 Batch Number: 81 Loss: 1.2627843618392944 Time taken: 0.45519351959228516\n",
            "Epoch: 8 Batch Number: 82 Loss: 1.3000109195709229 Time taken: 0.45601677894592285\n",
            "Epoch: 8 Batch Number: 83 Loss: 1.3659005165100098 Time taken: 0.4642338752746582\n",
            "Epoch: 8 Batch Number: 84 Loss: 1.3057481050491333 Time taken: 0.46014976501464844\n",
            "Epoch: 8 Batch Number: 85 Loss: 1.267059087753296 Time taken: 0.4679903984069824\n",
            "Epoch: 8 Batch Number: 86 Loss: 1.2980608940124512 Time taken: 0.472348690032959\n",
            "Epoch: 8 Batch Number: 87 Loss: 1.3372224569320679 Time taken: 0.4580249786376953\n",
            "Epoch: 8 Batch Number: 88 Loss: 1.250221848487854 Time taken: 0.4459264278411865\n",
            "Epoch: 8 Batch Number: 89 Loss: 1.2226371765136719 Time taken: 0.4746057987213135\n",
            "Epoch: 8 Batch Number: 90 Loss: 1.2662073373794556 Time taken: 0.4563577175140381\n",
            "Epoch: 8 Batch Number: 91 Loss: 1.265436053276062 Time taken: 0.4534578323364258\n",
            "Epoch: 8 Batch Number: 92 Loss: 1.2153205871582031 Time taken: 0.46373486518859863\n",
            "Epoch: 8 Batch Number: 93 Loss: 1.2284165620803833 Time taken: 0.45953822135925293\n",
            "Epoch: 8 Batch Number: 94 Loss: 1.3789498805999756 Time taken: 0.46299314498901367\n",
            "Epoch: 8 Batch Number: 95 Loss: 1.348562479019165 Time taken: 0.45580339431762695\n",
            "Epoch: 8 Batch Number: 96 Loss: 1.384329080581665 Time taken: 0.4694027900695801\n",
            "Epoch: 8 Batch Number: 97 Loss: 1.3318487405776978 Time taken: 0.47603368759155273\n",
            "Epoch: 8 Batch Number: 98 Loss: 1.378983736038208 Time taken: 0.46368861198425293\n",
            "Epoch: 8 Batch Number: 99 Loss: 1.409334659576416 Time taken: 0.4640045166015625\n",
            "Epoch: 8 Batch Number: 100 Loss: 1.3227332830429077 Time taken: 0.4721641540527344\n",
            "Epoch: 8 Batch Number: 101 Loss: 1.31719970703125 Time taken: 0.47272324562072754\n",
            "Epoch: 8 Batch Number: 102 Loss: 1.3625097274780273 Time taken: 0.46254897117614746\n",
            "Epoch: 8 Batch Number: 103 Loss: 1.4589333534240723 Time taken: 0.46092677116394043\n",
            "Epoch: 8 Batch Number: 104 Loss: 1.3553991317749023 Time taken: 0.4541804790496826\n",
            "Epoch: 8 Batch Number: 105 Loss: 1.421994924545288 Time taken: 0.4511833190917969\n",
            "Epoch: 8 Batch Number: 106 Loss: 1.3665087223052979 Time taken: 0.4656352996826172\n",
            "Epoch: 8 Batch Number: 107 Loss: 1.3567301034927368 Time taken: 0.468059778213501\n",
            "Epoch: 8 Batch Number: 108 Loss: 1.3361787796020508 Time taken: 0.462188720703125\n",
            "Epoch: 8 Batch Number: 109 Loss: 1.3455591201782227 Time taken: 0.4535977840423584\n",
            "Epoch: 8 Batch Number: 110 Loss: 1.2102454900741577 Time taken: 0.4558441638946533\n",
            "Epoch: 8 Batch Number: 111 Loss: 1.2418665885925293 Time taken: 0.45053553581237793\n",
            "Epoch: 8 Batch Number: 112 Loss: 1.2628850936889648 Time taken: 0.46419501304626465\n",
            "Epoch: 8 Batch Number: 113 Loss: 1.303346872329712 Time taken: 0.4462301731109619\n",
            "Epoch: 8 Batch Number: 114 Loss: 1.3258345127105713 Time taken: 0.4560210704803467\n",
            "Epoch: 8 Batch Number: 115 Loss: 1.3201860189437866 Time taken: 0.45183229446411133\n",
            "Epoch: 8 Batch Number: 116 Loss: 1.1786662340164185 Time taken: 0.4424259662628174\n",
            "Epoch: 8 Batch Number: 117 Loss: 1.299118161201477 Time taken: 0.44521117210388184\n",
            "Epoch: 8 Batch Number: 118 Loss: 1.2322018146514893 Time taken: 0.45505523681640625\n",
            "Epoch: 8 Batch Number: 119 Loss: 1.271618127822876 Time taken: 0.4555633068084717\n",
            "Epoch: 8 Batch Number: 120 Loss: 1.3314521312713623 Time taken: 0.4559361934661865\n",
            "Epoch: 8 Batch Number: 121 Loss: 1.2639676332473755 Time taken: 0.45580101013183594\n",
            "Epoch: 8 Batch Number: 122 Loss: 1.2669918537139893 Time taken: 0.4516425132751465\n",
            "Epoch: 8 Batch Number: 123 Loss: 1.2791002988815308 Time taken: 0.453629732131958\n",
            "Epoch: 8 Batch Number: 124 Loss: 1.2984614372253418 Time taken: 0.4511547088623047\n",
            "Epoch: 8 Batch Number: 125 Loss: 1.3079549074172974 Time taken: 0.4589250087738037\n",
            "Epoch: 8 Batch Number: 126 Loss: 1.3099181652069092 Time taken: 0.44928765296936035\n",
            "Epoch: 8 Batch Number: 127 Loss: 1.345346212387085 Time taken: 0.45648193359375\n",
            "Epoch: 8 Batch Number: 128 Loss: 1.2616029977798462 Time taken: 0.45404911041259766\n",
            "Epoch: 8 Batch Number: 129 Loss: 1.2616581916809082 Time taken: 0.4673430919647217\n",
            "Epoch: 8 Batch Number: 130 Loss: 1.2351930141448975 Time taken: 0.45440030097961426\n",
            "Epoch: 8 Batch Number: 131 Loss: 1.3422173261642456 Time taken: 0.4565742015838623\n",
            "Epoch: 8 Batch Number: 132 Loss: 1.1844837665557861 Time taken: 0.45382213592529297\n",
            "Epoch: 8 Batch Number: 133 Loss: 1.3347690105438232 Time taken: 0.44397974014282227\n",
            "Epoch: 8 Batch Number: 134 Loss: 1.3584831953048706 Time taken: 0.47270703315734863\n",
            "Epoch: 8 Batch Number: 135 Loss: 1.3563719987869263 Time taken: 0.46576762199401855\n",
            "Epoch: 8 Batch Number: 136 Loss: 1.3419157266616821 Time taken: 0.459669828414917\n",
            "Epoch: 8 Batch Number: 137 Loss: 1.306215524673462 Time taken: 0.4533498287200928\n",
            "Epoch: 8 Batch Number: 138 Loss: 1.3170650005340576 Time taken: 0.4467167854309082\n",
            "Epoch: 8 Batch Number: 139 Loss: 1.3299185037612915 Time taken: 0.44309329986572266\n",
            "Epoch: 8 Batch Number: 140 Loss: 1.2784241437911987 Time taken: 0.4633631706237793\n",
            "Epoch: 8 Batch Number: 141 Loss: 1.320542573928833 Time taken: 0.45976901054382324\n",
            "Epoch: 8 Batch Number: 142 Loss: 1.3776934146881104 Time taken: 0.4540281295776367\n",
            "Epoch: 8 Batch Number: 143 Loss: 1.465675711631775 Time taken: 0.4490478038787842\n",
            "Epoch: 8 Batch Number: 144 Loss: 1.4786542654037476 Time taken: 0.4694235324859619\n",
            "Epoch: 8 Batch Number: 145 Loss: 1.4233942031860352 Time taken: 0.46079349517822266\n",
            "Epoch: 8 Batch Number: 146 Loss: 1.438342809677124 Time taken: 0.4484541416168213\n",
            "Epoch: 8 Batch Number: 147 Loss: 1.2720142602920532 Time taken: 0.448239803314209\n",
            "Epoch: 8 Batch Number: 148 Loss: 1.2520978450775146 Time taken: 0.4393436908721924\n",
            "Epoch: 8 Batch Number: 149 Loss: 1.3377035856246948 Time taken: 0.4668736457824707\n",
            "Epoch: 8 Batch Number: 150 Loss: 1.4467144012451172 Time taken: 0.4797511100769043\n",
            "Epoch: 8 Batch Number: 151 Loss: 1.4130966663360596 Time taken: 0.45882534980773926\n",
            "Epoch: 8 Batch Number: 152 Loss: 1.340341567993164 Time taken: 0.4409675598144531\n",
            "Epoch: 8 Batch Number: 153 Loss: 1.2849185466766357 Time taken: 0.4527592658996582\n",
            "Epoch: 8 Batch Number: 154 Loss: 1.2892835140228271 Time taken: 0.4559135437011719\n",
            "Epoch: 8 Batch Number: 155 Loss: 1.2647382020950317 Time taken: 0.4554722309112549\n",
            "Epoch: 8 Batch Number: 156 Loss: 1.3002393245697021 Time taken: 0.4602193832397461\n",
            "Epoch: 8 Batch Number: 157 Loss: 1.1854450702667236 Time taken: 0.46390390396118164\n",
            "Epoch: 8 Batch Number: 158 Loss: 1.2324426174163818 Time taken: 0.4548184871673584\n",
            "Epoch: 8 Batch Number: 159 Loss: 1.1894659996032715 Time taken: 0.44983553886413574\n",
            "Epoch: 8 Batch Number: 160 Loss: 1.0974552631378174 Time taken: 0.44367432594299316\n",
            "Epoch: 8 Batch Number: 161 Loss: 1.1707429885864258 Time taken: 0.44608044624328613\n",
            "Epoch: 8 Batch Number: 162 Loss: 1.147495985031128 Time taken: 0.4687614440917969\n",
            "Epoch: 8 Batch Number: 163 Loss: 1.245229721069336 Time taken: 0.46318793296813965\n",
            "Epoch: 8 Batch Number: 164 Loss: 1.361454725265503 Time taken: 0.4820103645324707\n",
            "Epoch: 8 Batch Number: 165 Loss: 1.2853747606277466 Time taken: 0.4705219268798828\n",
            "Epoch: 8 Batch Number: 166 Loss: 1.3135626316070557 Time taken: 0.4428565502166748\n",
            "Epoch: 8 Batch Number: 167 Loss: 1.3039770126342773 Time taken: 0.46486973762512207\n",
            "Epoch: 8 Batch Number: 168 Loss: 1.324470043182373 Time taken: 0.46445417404174805\n",
            "Epoch: 8 Batch Number: 169 Loss: 1.286514163017273 Time taken: 0.4694702625274658\n",
            "Epoch: 8 Batch Number: 170 Loss: 1.2091988325119019 Time taken: 0.4501640796661377\n",
            "Epoch: 8 Batch Number: 171 Loss: 1.156195044517517 Time taken: 0.4442746639251709\n",
            "Epoch: 8 Batch Number: 172 Loss: 1.2005791664123535 Time taken: 0.4468114376068115\n",
            "Epoch: 8 Batch Number: 173 Loss: 1.1931703090667725 Time taken: 0.45174598693847656\n",
            "Epoch: 8 Batch Number: 174 Loss: 1.1462503671646118 Time taken: 0.45614004135131836\n",
            "Epoch: 8 Batch Number: 175 Loss: 1.2621569633483887 Time taken: 0.453782320022583\n",
            "Epoch: 8 Batch Number: 176 Loss: 1.26358163356781 Time taken: 0.45024752616882324\n",
            "Epoch: 8 Batch Number: 177 Loss: 1.279495120048523 Time taken: 0.4694850444793701\n",
            "Epoch: 8 Batch Number: 178 Loss: 1.3294403553009033 Time taken: 0.47957754135131836\n",
            "Epoch: 8 Batch Number: 179 Loss: 1.315382957458496 Time taken: 0.44280052185058594\n",
            "Epoch: 8 Batch Number: 180 Loss: 1.2749059200286865 Time taken: 0.4585235118865967\n",
            "Epoch: 8 Batch Number: 181 Loss: 1.2403500080108643 Time taken: 0.4547443389892578\n",
            "Epoch: 8 Batch Number: 182 Loss: 1.1700217723846436 Time taken: 0.4491581916809082\n",
            "Epoch: 8 Batch Number: 183 Loss: 1.2414116859436035 Time taken: 0.4420895576477051\n",
            "Epoch: 8 Batch Number: 184 Loss: 1.2869408130645752 Time taken: 0.4555976390838623\n",
            "Epoch: 8 Batch Number: 185 Loss: 1.3169991970062256 Time taken: 0.4605216979980469\n",
            "Epoch: 8 Batch Number: 186 Loss: 1.1001713275909424 Time taken: 0.4575364589691162\n",
            "Epoch: 8 Batch Number: 187 Loss: 1.1888667345046997 Time taken: 0.45920801162719727\n",
            "Epoch: 8 Batch Number: 188 Loss: 1.1968904733657837 Time taken: 0.45865797996520996\n",
            "Epoch: 8 Batch Number: 189 Loss: 1.2528502941131592 Time taken: 0.4491865634918213\n",
            "Epoch: 8 Batch Number: 190 Loss: 1.4351747035980225 Time taken: 0.4662926197052002\n",
            "Epoch: 8 Batch Number: 191 Loss: 1.673780918121338 Time taken: 0.4495370388031006\n",
            "Epoch: 8 Batch Number: 192 Loss: 1.3307828903198242 Time taken: 0.4656834602355957\n",
            "Epoch: 8 Batch Number: 193 Loss: 1.459352970123291 Time taken: 0.4621620178222656\n",
            "Epoch: 8 Batch Number: 194 Loss: 1.3431923389434814 Time taken: 0.45728206634521484\n",
            "Epoch: 8 Batch Number: 195 Loss: 1.2661492824554443 Time taken: 0.45726943016052246\n",
            "Epoch: 8 Batch Number: 196 Loss: 1.2178765535354614 Time taken: 0.443378210067749\n",
            "Epoch: 8 Batch Number: 197 Loss: 1.3187224864959717 Time taken: 0.4668419361114502\n",
            "Epoch: 8 Batch Number: 198 Loss: 1.2327839136123657 Time taken: 0.45533061027526855\n",
            "Epoch: 8 Batch Number: 199 Loss: 1.2938586473464966 Time taken: 0.47220945358276367\n",
            "Epoch: 8 Batch Number: 200 Loss: 1.2132699489593506 Time taken: 0.48158860206604004\n",
            "Epoch: 8 Batch Number: 201 Loss: 1.2716593742370605 Time taken: 0.46785783767700195\n",
            "Epoch: 8 Batch Number: 202 Loss: 1.2496380805969238 Time taken: 0.46051502227783203\n",
            "Epoch: 8 Batch Number: 203 Loss: 1.24866783618927 Time taken: 0.46565961837768555\n",
            "Epoch: 8 Batch Number: 204 Loss: 1.286902666091919 Time taken: 0.4581296443939209\n",
            "Epoch: 8 Batch Number: 205 Loss: 1.2011730670928955 Time taken: 0.45425939559936523\n",
            "Epoch: 8 Batch Number: 206 Loss: 1.2712481021881104 Time taken: 0.45958828926086426\n",
            "Epoch: 8 Batch Number: 207 Loss: 1.2472503185272217 Time taken: 0.456676721572876\n",
            "Epoch: 8 Batch Number: 208 Loss: 1.232100009918213 Time taken: 0.4445362091064453\n",
            "Epoch: 8 Batch Number: 209 Loss: 1.2880704402923584 Time taken: 0.45813870429992676\n",
            "Epoch: 8 Batch Number: 210 Loss: 1.3389892578125 Time taken: 0.4564836025238037\n",
            "Epoch: 8 Batch Number: 211 Loss: 1.3181689977645874 Time taken: 0.46010661125183105\n",
            "Epoch: 8 Batch Number: 212 Loss: 1.4148385524749756 Time taken: 0.46691036224365234\n",
            "Epoch: 8 Batch Number: 213 Loss: 1.2638400793075562 Time taken: 0.46875691413879395\n",
            "Epoch: 8 Batch Number: 214 Loss: 1.3287022113800049 Time taken: 0.4706454277038574\n",
            "Epoch: 8 Batch Number: 215 Loss: 1.2782407999038696 Time taken: 0.46750402450561523\n",
            "Epoch: 8 Batch Number: 216 Loss: 1.3039549589157104 Time taken: 0.46679162979125977\n",
            "Epoch: 8 Batch Number: 217 Loss: 1.3760756254196167 Time taken: 0.4635343551635742\n",
            "Epoch: 8 Batch Number: 218 Loss: 1.3956584930419922 Time taken: 0.4634711742401123\n",
            "Epoch: 8 Batch Number: 219 Loss: 1.3545887470245361 Time taken: 0.46538639068603516\n",
            "Epoch: 8 Batch Number: 220 Loss: 1.2871066331863403 Time taken: 0.48180723190307617\n",
            "Epoch: 8 Batch Number: 221 Loss: 1.2659344673156738 Time taken: 0.45673251152038574\n",
            "Epoch: 8 Batch Number: 222 Loss: 1.249709129333496 Time taken: 0.46152353286743164\n",
            "Epoch: 8 Batch Number: 223 Loss: 1.282369613647461 Time taken: 0.45171570777893066\n",
            "Epoch: 8 Batch Number: 224 Loss: 1.2344868183135986 Time taken: 0.45582127571105957\n",
            "Epoch: 8 Batch Number: 225 Loss: 1.1904858350753784 Time taken: 0.4472534656524658\n",
            "Epoch: 8 Batch Number: 226 Loss: 1.2312723398208618 Time taken: 0.4612870216369629\n",
            "Epoch: 8 Batch Number: 227 Loss: 1.3956559896469116 Time taken: 0.46242189407348633\n",
            "Epoch: 8 Batch Number: 228 Loss: 1.3597662448883057 Time taken: 0.4513413906097412\n",
            "Epoch: 8 Batch Number: 229 Loss: 1.3665441274642944 Time taken: 0.4657764434814453\n",
            "==========================================================================================\n",
            "Start of epoch 9\n",
            "Epoch: 9 Batch Number: 1 Loss: 1.259392499923706 Time taken: 0.4569582939147949\n",
            "Epoch: 9 Batch Number: 2 Loss: 1.3307569026947021 Time taken: 0.4559805393218994\n",
            "Epoch: 9 Batch Number: 3 Loss: 1.2671905755996704 Time taken: 0.4536457061767578\n",
            "Epoch: 9 Batch Number: 4 Loss: 1.2725237607955933 Time taken: 0.4623706340789795\n",
            "Epoch: 9 Batch Number: 5 Loss: 1.2444686889648438 Time taken: 0.4496610164642334\n",
            "Epoch: 9 Batch Number: 6 Loss: 1.1737180948257446 Time taken: 0.45630383491516113\n",
            "Epoch: 9 Batch Number: 7 Loss: 1.2100956439971924 Time taken: 0.4462394714355469\n",
            "Epoch: 9 Batch Number: 8 Loss: 1.2094666957855225 Time taken: 0.4700772762298584\n",
            "Epoch: 9 Batch Number: 9 Loss: 1.212653636932373 Time taken: 0.4527902603149414\n",
            "Epoch: 9 Batch Number: 10 Loss: 1.2560291290283203 Time taken: 0.44742369651794434\n",
            "Epoch: 9 Batch Number: 11 Loss: 1.2072935104370117 Time taken: 0.44977283477783203\n",
            "Epoch: 9 Batch Number: 12 Loss: 1.163452386856079 Time taken: 0.46316003799438477\n",
            "Epoch: 9 Batch Number: 13 Loss: 1.2064085006713867 Time taken: 0.460587739944458\n",
            "Epoch: 9 Batch Number: 14 Loss: 1.2184388637542725 Time taken: 0.4564549922943115\n",
            "Epoch: 9 Batch Number: 15 Loss: 1.1451271772384644 Time taken: 0.46621108055114746\n",
            "Epoch: 9 Batch Number: 16 Loss: 1.2100499868392944 Time taken: 0.46051740646362305\n",
            "Epoch: 9 Batch Number: 17 Loss: 1.3440155982971191 Time taken: 0.4652364253997803\n",
            "Epoch: 9 Batch Number: 18 Loss: 1.3186392784118652 Time taken: 0.45934581756591797\n",
            "Epoch: 9 Batch Number: 19 Loss: 1.3079662322998047 Time taken: 0.4600236415863037\n",
            "Epoch: 9 Batch Number: 20 Loss: 1.1686384677886963 Time taken: 0.4515104293823242\n",
            "Epoch: 9 Batch Number: 21 Loss: 1.391514539718628 Time taken: 0.4571404457092285\n",
            "Epoch: 9 Batch Number: 22 Loss: 1.3530688285827637 Time taken: 0.4645094871520996\n",
            "Epoch: 9 Batch Number: 23 Loss: 1.3633254766464233 Time taken: 0.45891261100769043\n",
            "Epoch: 9 Batch Number: 24 Loss: 1.324930191040039 Time taken: 0.4673881530761719\n",
            "Epoch: 9 Batch Number: 25 Loss: 1.3084936141967773 Time taken: 0.4868042469024658\n",
            "Epoch: 9 Batch Number: 26 Loss: 1.3227410316467285 Time taken: 0.46054983139038086\n",
            "Epoch: 9 Batch Number: 27 Loss: 1.2545958757400513 Time taken: 0.46550965309143066\n",
            "Epoch: 9 Batch Number: 28 Loss: 1.2682372331619263 Time taken: 0.46298933029174805\n",
            "Epoch: 9 Batch Number: 29 Loss: 1.297285795211792 Time taken: 0.4620521068572998\n",
            "Epoch: 9 Batch Number: 30 Loss: 1.0916836261749268 Time taken: 0.47048282623291016\n",
            "Epoch: 9 Batch Number: 31 Loss: 1.2399579286575317 Time taken: 0.46669840812683105\n",
            "Epoch: 9 Batch Number: 32 Loss: 1.2473442554473877 Time taken: 0.4788346290588379\n",
            "Epoch: 9 Batch Number: 33 Loss: 1.2714074850082397 Time taken: 0.44767117500305176\n",
            "Epoch: 9 Batch Number: 34 Loss: 1.293763279914856 Time taken: 0.45033764839172363\n",
            "Epoch: 9 Batch Number: 35 Loss: 1.3427987098693848 Time taken: 0.45912742614746094\n",
            "Epoch: 9 Batch Number: 36 Loss: 1.4071059226989746 Time taken: 0.46658992767333984\n",
            "Epoch: 9 Batch Number: 37 Loss: 1.2516188621520996 Time taken: 0.45847558975219727\n",
            "Epoch: 9 Batch Number: 38 Loss: 1.2917715311050415 Time taken: 0.4602503776550293\n",
            "Epoch: 9 Batch Number: 39 Loss: 1.261671781539917 Time taken: 0.4539322853088379\n",
            "Epoch: 9 Batch Number: 40 Loss: 1.2633233070373535 Time taken: 0.4753549098968506\n",
            "Epoch: 9 Batch Number: 41 Loss: 1.2079671621322632 Time taken: 0.46706080436706543\n",
            "Epoch: 9 Batch Number: 42 Loss: 1.218867301940918 Time taken: 0.4519162178039551\n",
            "Epoch: 9 Batch Number: 43 Loss: 1.2168223857879639 Time taken: 0.47256898880004883\n",
            "Epoch: 9 Batch Number: 44 Loss: 1.1521300077438354 Time taken: 0.4695467948913574\n",
            "Epoch: 9 Batch Number: 45 Loss: 1.204803466796875 Time taken: 0.4653956890106201\n",
            "Epoch: 9 Batch Number: 46 Loss: 1.3688355684280396 Time taken: 0.47046422958374023\n",
            "Epoch: 9 Batch Number: 47 Loss: 1.2601771354675293 Time taken: 0.4578588008880615\n",
            "Epoch: 9 Batch Number: 48 Loss: 1.3274705410003662 Time taken: 0.45462775230407715\n",
            "Epoch: 9 Batch Number: 49 Loss: 1.3378303050994873 Time taken: 0.4546961784362793\n",
            "Epoch: 9 Batch Number: 50 Loss: 1.2061558961868286 Time taken: 0.4589967727661133\n",
            "Epoch: 9 Batch Number: 51 Loss: 1.2194485664367676 Time taken: 0.4695563316345215\n",
            "Epoch: 9 Batch Number: 52 Loss: 1.3437271118164062 Time taken: 0.46268177032470703\n",
            "Epoch: 9 Batch Number: 53 Loss: 1.3892360925674438 Time taken: 0.46151089668273926\n",
            "Epoch: 9 Batch Number: 54 Loss: 1.3497543334960938 Time taken: 0.46404027938842773\n",
            "Epoch: 9 Batch Number: 55 Loss: 1.3040194511413574 Time taken: 0.46613597869873047\n",
            "Epoch: 9 Batch Number: 56 Loss: 1.391492247581482 Time taken: 0.47435998916625977\n",
            "Epoch: 9 Batch Number: 57 Loss: 1.2837889194488525 Time taken: 0.46684741973876953\n",
            "Epoch: 9 Batch Number: 58 Loss: 1.230440378189087 Time taken: 0.48288846015930176\n",
            "Epoch: 9 Batch Number: 59 Loss: 1.2511590719223022 Time taken: 0.45511507987976074\n",
            "Epoch: 9 Batch Number: 60 Loss: 1.2533912658691406 Time taken: 0.4572288990020752\n",
            "Epoch: 9 Batch Number: 61 Loss: 1.3130590915679932 Time taken: 0.4712660312652588\n",
            "Epoch: 9 Batch Number: 62 Loss: 1.2168352603912354 Time taken: 0.4501662254333496\n",
            "Epoch: 9 Batch Number: 63 Loss: 1.209410548210144 Time taken: 0.4591789245605469\n",
            "Epoch: 9 Batch Number: 64 Loss: 1.1621612310409546 Time taken: 0.45730113983154297\n",
            "Epoch: 9 Batch Number: 65 Loss: 1.2037863731384277 Time taken: 0.46561598777770996\n",
            "Epoch: 9 Batch Number: 66 Loss: 1.2276663780212402 Time taken: 0.47220277786254883\n",
            "Epoch: 9 Batch Number: 67 Loss: 1.2147204875946045 Time taken: 0.4580543041229248\n",
            "Epoch: 9 Batch Number: 68 Loss: 1.2768068313598633 Time taken: 0.45838141441345215\n",
            "Epoch: 9 Batch Number: 69 Loss: 1.249306559562683 Time taken: 0.470412015914917\n",
            "Epoch: 9 Batch Number: 70 Loss: 1.252298355102539 Time taken: 0.44983673095703125\n",
            "Epoch: 9 Batch Number: 71 Loss: 1.2419769763946533 Time taken: 0.47890400886535645\n",
            "Epoch: 9 Batch Number: 72 Loss: 1.267905592918396 Time taken: 0.4520576000213623\n",
            "Epoch: 9 Batch Number: 73 Loss: 1.194969654083252 Time taken: 0.4538726806640625\n",
            "Epoch: 9 Batch Number: 74 Loss: 1.311824083328247 Time taken: 0.45037412643432617\n",
            "Epoch: 9 Batch Number: 75 Loss: 1.1455650329589844 Time taken: 0.46746134757995605\n",
            "Epoch: 9 Batch Number: 76 Loss: 1.1960748434066772 Time taken: 0.453326940536499\n",
            "Epoch: 9 Batch Number: 77 Loss: 1.288323163986206 Time taken: 0.45938920974731445\n",
            "Epoch: 9 Batch Number: 78 Loss: 1.1685961484909058 Time taken: 0.4727456569671631\n",
            "Epoch: 9 Batch Number: 79 Loss: 1.2543106079101562 Time taken: 0.4543721675872803\n",
            "Epoch: 9 Batch Number: 80 Loss: 1.2429262399673462 Time taken: 0.456681489944458\n",
            "Epoch: 9 Batch Number: 81 Loss: 1.2368992567062378 Time taken: 0.4528696537017822\n",
            "Epoch: 9 Batch Number: 82 Loss: 1.2681982517242432 Time taken: 0.47554612159729004\n",
            "Epoch: 9 Batch Number: 83 Loss: 1.3307292461395264 Time taken: 0.47460126876831055\n",
            "Epoch: 9 Batch Number: 84 Loss: 1.2756198644638062 Time taken: 0.4533240795135498\n",
            "Epoch: 9 Batch Number: 85 Loss: 1.239148497581482 Time taken: 0.4678158760070801\n",
            "Epoch: 9 Batch Number: 86 Loss: 1.261451005935669 Time taken: 0.4733092784881592\n",
            "Epoch: 9 Batch Number: 87 Loss: 1.30464506149292 Time taken: 0.4540989398956299\n",
            "Epoch: 9 Batch Number: 88 Loss: 1.2214207649230957 Time taken: 0.45528316497802734\n",
            "Epoch: 9 Batch Number: 89 Loss: 1.194237470626831 Time taken: 0.4466063976287842\n",
            "Epoch: 9 Batch Number: 90 Loss: 1.2392460107803345 Time taken: 0.45815443992614746\n",
            "Epoch: 9 Batch Number: 91 Loss: 1.229522466659546 Time taken: 0.4552891254425049\n",
            "Epoch: 9 Batch Number: 92 Loss: 1.1820518970489502 Time taken: 0.4516606330871582\n",
            "Epoch: 9 Batch Number: 93 Loss: 1.1952781677246094 Time taken: 0.45650601387023926\n",
            "Epoch: 9 Batch Number: 94 Loss: 1.331322193145752 Time taken: 0.4672372341156006\n",
            "Epoch: 9 Batch Number: 95 Loss: 1.3081949949264526 Time taken: 0.45117688179016113\n",
            "Epoch: 9 Batch Number: 96 Loss: 1.3421754837036133 Time taken: 0.44882726669311523\n",
            "Epoch: 9 Batch Number: 97 Loss: 1.2843537330627441 Time taken: 0.4499797821044922\n",
            "Epoch: 9 Batch Number: 98 Loss: 1.3395206928253174 Time taken: 0.46570253372192383\n",
            "Epoch: 9 Batch Number: 99 Loss: 1.370751142501831 Time taken: 0.4751300811767578\n",
            "Epoch: 9 Batch Number: 100 Loss: 1.2835886478424072 Time taken: 0.45217418670654297\n",
            "Epoch: 9 Batch Number: 101 Loss: 1.2826974391937256 Time taken: 0.45000410079956055\n",
            "Epoch: 9 Batch Number: 102 Loss: 1.326670527458191 Time taken: 0.4504120349884033\n",
            "Epoch: 9 Batch Number: 103 Loss: 1.4105467796325684 Time taken: 0.4488403797149658\n",
            "Epoch: 9 Batch Number: 104 Loss: 1.3073253631591797 Time taken: 0.45293188095092773\n",
            "Epoch: 9 Batch Number: 105 Loss: 1.3731637001037598 Time taken: 0.44352173805236816\n",
            "Epoch: 9 Batch Number: 106 Loss: 1.3246649503707886 Time taken: 0.4454045295715332\n",
            "Epoch: 9 Batch Number: 107 Loss: 1.325402855873108 Time taken: 0.44681549072265625\n",
            "Epoch: 9 Batch Number: 108 Loss: 1.3010382652282715 Time taken: 0.4512505531311035\n",
            "Epoch: 9 Batch Number: 109 Loss: 1.3131349086761475 Time taken: 0.4426121711730957\n",
            "Epoch: 9 Batch Number: 110 Loss: 1.1732300519943237 Time taken: 0.447784423828125\n",
            "Epoch: 9 Batch Number: 111 Loss: 1.2116057872772217 Time taken: 0.4564964771270752\n",
            "Epoch: 9 Batch Number: 112 Loss: 1.2303593158721924 Time taken: 0.4670143127441406\n",
            "Epoch: 9 Batch Number: 113 Loss: 1.2696316242218018 Time taken: 0.46160411834716797\n",
            "Epoch: 9 Batch Number: 114 Loss: 1.3004114627838135 Time taken: 0.45269346237182617\n",
            "Epoch: 9 Batch Number: 115 Loss: 1.2900636196136475 Time taken: 0.4662771224975586\n",
            "Epoch: 9 Batch Number: 116 Loss: 1.151513695716858 Time taken: 0.45705270767211914\n",
            "Epoch: 9 Batch Number: 117 Loss: 1.2711844444274902 Time taken: 0.45444393157958984\n",
            "Epoch: 9 Batch Number: 118 Loss: 1.2082128524780273 Time taken: 0.46048474311828613\n",
            "Epoch: 9 Batch Number: 119 Loss: 1.2462365627288818 Time taken: 0.4546661376953125\n",
            "Epoch: 9 Batch Number: 120 Loss: 1.3004841804504395 Time taken: 0.4590034484863281\n",
            "Epoch: 9 Batch Number: 121 Loss: 1.2354378700256348 Time taken: 0.47895359992980957\n",
            "Epoch: 9 Batch Number: 122 Loss: 1.2309679985046387 Time taken: 0.4805562496185303\n",
            "Epoch: 9 Batch Number: 123 Loss: 1.2517733573913574 Time taken: 0.4537534713745117\n",
            "Epoch: 9 Batch Number: 124 Loss: 1.2626460790634155 Time taken: 0.4458944797515869\n",
            "Epoch: 9 Batch Number: 125 Loss: 1.2694405317306519 Time taken: 0.46294689178466797\n",
            "Epoch: 9 Batch Number: 126 Loss: 1.280914068222046 Time taken: 0.4561004638671875\n",
            "Epoch: 9 Batch Number: 127 Loss: 1.3082443475723267 Time taken: 0.44779157638549805\n",
            "Epoch: 9 Batch Number: 128 Loss: 1.235123634338379 Time taken: 0.4508366584777832\n",
            "Epoch: 9 Batch Number: 129 Loss: 1.2283191680908203 Time taken: 0.4696497917175293\n",
            "Epoch: 9 Batch Number: 130 Loss: 1.195813536643982 Time taken: 0.45581841468811035\n",
            "Epoch: 9 Batch Number: 131 Loss: 1.3059794902801514 Time taken: 0.4524674415588379\n",
            "Epoch: 9 Batch Number: 132 Loss: 1.150213360786438 Time taken: 0.44252586364746094\n",
            "Epoch: 9 Batch Number: 133 Loss: 1.3046855926513672 Time taken: 0.4644432067871094\n",
            "Epoch: 9 Batch Number: 134 Loss: 1.324824333190918 Time taken: 0.4620532989501953\n",
            "Epoch: 9 Batch Number: 135 Loss: 1.323306918144226 Time taken: 0.44942378997802734\n",
            "Epoch: 9 Batch Number: 136 Loss: 1.3095262050628662 Time taken: 0.4442884922027588\n",
            "Epoch: 9 Batch Number: 137 Loss: 1.271522879600525 Time taken: 0.4627833366394043\n",
            "Epoch: 9 Batch Number: 138 Loss: 1.290203332901001 Time taken: 0.45846080780029297\n",
            "Epoch: 9 Batch Number: 139 Loss: 1.2953482866287231 Time taken: 0.4451408386230469\n",
            "Epoch: 9 Batch Number: 140 Loss: 1.244621753692627 Time taken: 0.45554304122924805\n",
            "Epoch: 9 Batch Number: 141 Loss: 1.2940832376480103 Time taken: 0.4750354290008545\n",
            "Epoch: 9 Batch Number: 142 Loss: 1.347770094871521 Time taken: 0.4626960754394531\n",
            "Epoch: 9 Batch Number: 143 Loss: 1.428849458694458 Time taken: 0.4555542469024658\n",
            "Epoch: 9 Batch Number: 144 Loss: 1.4365205764770508 Time taken: 0.4744405746459961\n",
            "Epoch: 9 Batch Number: 145 Loss: 1.3896458148956299 Time taken: 0.4543142318725586\n",
            "Epoch: 9 Batch Number: 146 Loss: 1.402832269668579 Time taken: 0.4547865390777588\n",
            "Epoch: 9 Batch Number: 147 Loss: 1.2468411922454834 Time taken: 0.45897674560546875\n",
            "Epoch: 9 Batch Number: 148 Loss: 1.2183797359466553 Time taken: 0.45528125762939453\n",
            "Epoch: 9 Batch Number: 149 Loss: 1.3035356998443604 Time taken: 0.4512510299682617\n",
            "Epoch: 9 Batch Number: 150 Loss: 1.4035959243774414 Time taken: 0.4519307613372803\n",
            "Epoch: 9 Batch Number: 151 Loss: 1.3851406574249268 Time taken: 0.45809507369995117\n",
            "Epoch: 9 Batch Number: 152 Loss: 1.3100160360336304 Time taken: 0.4522123336791992\n",
            "Epoch: 9 Batch Number: 153 Loss: 1.2520694732666016 Time taken: 0.4538536071777344\n",
            "Epoch: 9 Batch Number: 154 Loss: 1.2528884410858154 Time taken: 0.4496955871582031\n",
            "Epoch: 9 Batch Number: 155 Loss: 1.2439789772033691 Time taken: 0.461594820022583\n",
            "Epoch: 9 Batch Number: 156 Loss: 1.2754130363464355 Time taken: 0.46689748764038086\n",
            "Epoch: 9 Batch Number: 157 Loss: 1.161699891090393 Time taken: 0.4721965789794922\n",
            "Epoch: 9 Batch Number: 158 Loss: 1.2095354795455933 Time taken: 0.45144152641296387\n",
            "Epoch: 9 Batch Number: 159 Loss: 1.1645020246505737 Time taken: 0.4621453285217285\n",
            "Epoch: 9 Batch Number: 160 Loss: 1.0694156885147095 Time taken: 0.4583594799041748\n",
            "Epoch: 9 Batch Number: 161 Loss: 1.1432052850723267 Time taken: 0.4745197296142578\n",
            "Epoch: 9 Batch Number: 162 Loss: 1.1213874816894531 Time taken: 0.46245527267456055\n",
            "Epoch: 9 Batch Number: 163 Loss: 1.2208963632583618 Time taken: 0.4603850841522217\n",
            "Epoch: 9 Batch Number: 164 Loss: 1.325565218925476 Time taken: 0.4790966510772705\n",
            "Epoch: 9 Batch Number: 165 Loss: 1.2489596605300903 Time taken: 0.4683351516723633\n",
            "Epoch: 9 Batch Number: 166 Loss: 1.281837821006775 Time taken: 0.46921873092651367\n",
            "Epoch: 9 Batch Number: 167 Loss: 1.2671594619750977 Time taken: 0.4605257511138916\n",
            "Epoch: 9 Batch Number: 168 Loss: 1.280110239982605 Time taken: 0.4795670509338379\n",
            "Epoch: 9 Batch Number: 169 Loss: 1.2507271766662598 Time taken: 0.4645833969116211\n",
            "Epoch: 9 Batch Number: 170 Loss: 1.1774944067001343 Time taken: 0.4761526584625244\n",
            "Epoch: 9 Batch Number: 171 Loss: 1.1220312118530273 Time taken: 0.4564974308013916\n",
            "Epoch: 9 Batch Number: 172 Loss: 1.1733486652374268 Time taken: 0.4654574394226074\n",
            "Epoch: 9 Batch Number: 173 Loss: 1.1637206077575684 Time taken: 0.45973944664001465\n",
            "Epoch: 9 Batch Number: 174 Loss: 1.1128506660461426 Time taken: 0.4655451774597168\n",
            "Epoch: 9 Batch Number: 175 Loss: 1.22934889793396 Time taken: 0.4664442539215088\n",
            "Epoch: 9 Batch Number: 176 Loss: 1.2315500974655151 Time taken: 0.46422481536865234\n",
            "Epoch: 9 Batch Number: 177 Loss: 1.2448744773864746 Time taken: 0.46636152267456055\n",
            "Epoch: 9 Batch Number: 178 Loss: 1.2968409061431885 Time taken: 0.461897611618042\n",
            "Epoch: 9 Batch Number: 179 Loss: 1.2810413837432861 Time taken: 0.46525025367736816\n",
            "Epoch: 9 Batch Number: 180 Loss: 1.249157428741455 Time taken: 0.46326136589050293\n",
            "Epoch: 9 Batch Number: 181 Loss: 1.2064988613128662 Time taken: 0.46476006507873535\n",
            "Epoch: 9 Batch Number: 182 Loss: 1.1421122550964355 Time taken: 0.46298694610595703\n",
            "Epoch: 9 Batch Number: 183 Loss: 1.2027798891067505 Time taken: 0.44831037521362305\n",
            "Epoch: 9 Batch Number: 184 Loss: 1.2619524002075195 Time taken: 0.44962310791015625\n",
            "Epoch: 9 Batch Number: 185 Loss: 1.2912390232086182 Time taken: 0.46732187271118164\n",
            "Epoch: 9 Batch Number: 186 Loss: 1.0682482719421387 Time taken: 0.45514774322509766\n",
            "Epoch: 9 Batch Number: 187 Loss: 1.1638092994689941 Time taken: 0.454883337020874\n",
            "Epoch: 9 Batch Number: 188 Loss: 1.1727874279022217 Time taken: 0.44996118545532227\n",
            "Epoch: 9 Batch Number: 189 Loss: 1.2250404357910156 Time taken: 0.4430389404296875\n",
            "Epoch: 9 Batch Number: 190 Loss: 1.4055562019348145 Time taken: 0.4520449638366699\n",
            "Epoch: 9 Batch Number: 191 Loss: 1.6363346576690674 Time taken: 0.4533965587615967\n",
            "Epoch: 9 Batch Number: 192 Loss: 1.3078727722167969 Time taken: 0.45141148567199707\n",
            "Epoch: 9 Batch Number: 193 Loss: 1.4284261465072632 Time taken: 0.45117735862731934\n",
            "Epoch: 9 Batch Number: 194 Loss: 1.318091869354248 Time taken: 0.46506214141845703\n",
            "Epoch: 9 Batch Number: 195 Loss: 1.2439703941345215 Time taken: 0.4455101490020752\n",
            "Epoch: 9 Batch Number: 196 Loss: 1.1916024684906006 Time taken: 0.45191407203674316\n",
            "Epoch: 9 Batch Number: 197 Loss: 1.286818265914917 Time taken: 0.4515814781188965\n",
            "Epoch: 9 Batch Number: 198 Loss: 1.2023018598556519 Time taken: 0.4468719959259033\n",
            "Epoch: 9 Batch Number: 199 Loss: 1.248523235321045 Time taken: 0.4713175296783447\n",
            "Epoch: 9 Batch Number: 200 Loss: 1.183422327041626 Time taken: 0.44614577293395996\n",
            "Epoch: 9 Batch Number: 201 Loss: 1.24271821975708 Time taken: 0.47029829025268555\n",
            "Epoch: 9 Batch Number: 202 Loss: 1.22269606590271 Time taken: 0.4463491439819336\n",
            "Epoch: 9 Batch Number: 203 Loss: 1.218530535697937 Time taken: 0.44762134552001953\n",
            "Epoch: 9 Batch Number: 204 Loss: 1.2539820671081543 Time taken: 0.4471001625061035\n",
            "Epoch: 9 Batch Number: 205 Loss: 1.1721622943878174 Time taken: 0.4526991844177246\n",
            "Epoch: 9 Batch Number: 206 Loss: 1.2351658344268799 Time taken: 0.44620394706726074\n",
            "Epoch: 9 Batch Number: 207 Loss: 1.2269303798675537 Time taken: 0.4668138027191162\n",
            "Epoch: 9 Batch Number: 208 Loss: 1.2051725387573242 Time taken: 0.4577791690826416\n",
            "Epoch: 9 Batch Number: 209 Loss: 1.259608268737793 Time taken: 0.4428985118865967\n",
            "Epoch: 9 Batch Number: 210 Loss: 1.306262493133545 Time taken: 0.46742916107177734\n",
            "Epoch: 9 Batch Number: 211 Loss: 1.2810170650482178 Time taken: 0.4591214656829834\n",
            "Epoch: 9 Batch Number: 212 Loss: 1.3859752416610718 Time taken: 0.46984386444091797\n",
            "Epoch: 9 Batch Number: 213 Loss: 1.235066294670105 Time taken: 0.45033812522888184\n",
            "Epoch: 9 Batch Number: 214 Loss: 1.2952390909194946 Time taken: 0.4654242992401123\n",
            "Epoch: 9 Batch Number: 215 Loss: 1.245509147644043 Time taken: 0.4772012233734131\n",
            "Epoch: 9 Batch Number: 216 Loss: 1.2759424448013306 Time taken: 0.4751169681549072\n",
            "Epoch: 9 Batch Number: 217 Loss: 1.342885971069336 Time taken: 0.4454953670501709\n",
            "Epoch: 9 Batch Number: 218 Loss: 1.357398271560669 Time taken: 0.4498476982116699\n",
            "Epoch: 9 Batch Number: 219 Loss: 1.3269824981689453 Time taken: 0.45063233375549316\n",
            "Epoch: 9 Batch Number: 220 Loss: 1.255527377128601 Time taken: 0.4546244144439697\n",
            "Epoch: 9 Batch Number: 221 Loss: 1.2414119243621826 Time taken: 0.46854114532470703\n",
            "Epoch: 9 Batch Number: 222 Loss: 1.2172234058380127 Time taken: 0.46416521072387695\n",
            "Epoch: 9 Batch Number: 223 Loss: 1.2553415298461914 Time taken: 0.45067787170410156\n",
            "Epoch: 9 Batch Number: 224 Loss: 1.2046481370925903 Time taken: 0.45929598808288574\n",
            "Epoch: 9 Batch Number: 225 Loss: 1.165931224822998 Time taken: 0.45969176292419434\n",
            "Epoch: 9 Batch Number: 226 Loss: 1.205069899559021 Time taken: 0.4585540294647217\n",
            "Epoch: 9 Batch Number: 227 Loss: 1.3546524047851562 Time taken: 0.4583303928375244\n",
            "Epoch: 9 Batch Number: 228 Loss: 1.3211309909820557 Time taken: 0.45397329330444336\n",
            "Epoch: 9 Batch Number: 229 Loss: 1.3262112140655518 Time taken: 0.4570131301879883\n",
            "==========================================================================================\n",
            "Start of epoch 10\n",
            "Epoch: 10 Batch Number: 1 Loss: 1.238350510597229 Time taken: 0.4451322555541992\n",
            "Epoch: 10 Batch Number: 2 Loss: 1.2991673946380615 Time taken: 0.46910858154296875\n",
            "Epoch: 10 Batch Number: 3 Loss: 1.2448382377624512 Time taken: 0.4536325931549072\n",
            "Epoch: 10 Batch Number: 4 Loss: 1.2440266609191895 Time taken: 0.45412445068359375\n",
            "Epoch: 10 Batch Number: 5 Loss: 1.2160842418670654 Time taken: 0.4508521556854248\n",
            "Epoch: 10 Batch Number: 6 Loss: 1.1507033109664917 Time taken: 0.445692777633667\n",
            "Epoch: 10 Batch Number: 7 Loss: 1.1873139142990112 Time taken: 0.44769883155822754\n",
            "Epoch: 10 Batch Number: 8 Loss: 1.1905393600463867 Time taken: 0.4491126537322998\n",
            "Epoch: 10 Batch Number: 9 Loss: 1.1953494548797607 Time taken: 0.45778465270996094\n",
            "Epoch: 10 Batch Number: 10 Loss: 1.2294199466705322 Time taken: 0.4689671993255615\n",
            "Epoch: 10 Batch Number: 11 Loss: 1.1840742826461792 Time taken: 0.46625423431396484\n",
            "Epoch: 10 Batch Number: 12 Loss: 1.1403437852859497 Time taken: 0.45851945877075195\n",
            "Epoch: 10 Batch Number: 13 Loss: 1.181683897972107 Time taken: 0.46228456497192383\n",
            "Epoch: 10 Batch Number: 14 Loss: 1.2053011655807495 Time taken: 0.45190930366516113\n",
            "Epoch: 10 Batch Number: 15 Loss: 1.1658720970153809 Time taken: 0.46091365814208984\n",
            "Epoch: 10 Batch Number: 16 Loss: 1.2017091512680054 Time taken: 0.45583510398864746\n",
            "Epoch: 10 Batch Number: 17 Loss: 1.3168647289276123 Time taken: 0.4499526023864746\n",
            "Epoch: 10 Batch Number: 18 Loss: 1.2905023097991943 Time taken: 0.450671911239624\n",
            "Epoch: 10 Batch Number: 19 Loss: 1.2835633754730225 Time taken: 0.4549117088317871\n",
            "Epoch: 10 Batch Number: 20 Loss: 1.1511871814727783 Time taken: 0.4569671154022217\n",
            "Epoch: 10 Batch Number: 21 Loss: 1.353555679321289 Time taken: 0.46251368522644043\n",
            "Epoch: 10 Batch Number: 22 Loss: 1.3203942775726318 Time taken: 0.47728991508483887\n",
            "Epoch: 10 Batch Number: 23 Loss: 1.3284711837768555 Time taken: 0.45993947982788086\n",
            "Epoch: 10 Batch Number: 24 Loss: 1.2936500310897827 Time taken: 0.457866907119751\n",
            "Epoch: 10 Batch Number: 25 Loss: 1.2741546630859375 Time taken: 0.4511144161224365\n",
            "Epoch: 10 Batch Number: 26 Loss: 1.295606255531311 Time taken: 0.4524385929107666\n",
            "Epoch: 10 Batch Number: 27 Loss: 1.2256829738616943 Time taken: 0.44751453399658203\n",
            "Epoch: 10 Batch Number: 28 Loss: 1.243530035018921 Time taken: 0.455585241317749\n",
            "Epoch: 10 Batch Number: 29 Loss: 1.2771170139312744 Time taken: 0.46200108528137207\n",
            "Epoch: 10 Batch Number: 30 Loss: 1.0564138889312744 Time taken: 0.4674067497253418\n",
            "Epoch: 10 Batch Number: 31 Loss: 1.2117459774017334 Time taken: 0.4613349437713623\n",
            "Epoch: 10 Batch Number: 32 Loss: 1.220137596130371 Time taken: 0.4430656433105469\n",
            "Epoch: 10 Batch Number: 33 Loss: 1.2504725456237793 Time taken: 0.4702792167663574\n",
            "Epoch: 10 Batch Number: 34 Loss: 1.2772939205169678 Time taken: 0.4550302028656006\n",
            "Epoch: 10 Batch Number: 35 Loss: 1.3265624046325684 Time taken: 0.44716382026672363\n",
            "Epoch: 10 Batch Number: 36 Loss: 1.3745567798614502 Time taken: 0.4592313766479492\n",
            "Epoch: 10 Batch Number: 37 Loss: 1.2295619249343872 Time taken: 0.4623417854309082\n",
            "Epoch: 10 Batch Number: 38 Loss: 1.27107572555542 Time taken: 0.4656233787536621\n",
            "Epoch: 10 Batch Number: 39 Loss: 1.2356014251708984 Time taken: 0.4637484550476074\n",
            "Epoch: 10 Batch Number: 40 Loss: 1.235514760017395 Time taken: 0.4496023654937744\n",
            "Epoch: 10 Batch Number: 41 Loss: 1.1832735538482666 Time taken: 0.4356393814086914\n",
            "Epoch: 10 Batch Number: 42 Loss: 1.1891576051712036 Time taken: 0.45854878425598145\n",
            "Epoch: 10 Batch Number: 43 Loss: 1.1862494945526123 Time taken: 0.4477376937866211\n",
            "Epoch: 10 Batch Number: 44 Loss: 1.1287816762924194 Time taken: 0.45777344703674316\n",
            "Epoch: 10 Batch Number: 45 Loss: 1.183934211730957 Time taken: 0.4469733238220215\n",
            "Epoch: 10 Batch Number: 46 Loss: 1.345711588859558 Time taken: 0.44632863998413086\n",
            "Epoch: 10 Batch Number: 47 Loss: 1.2357062101364136 Time taken: 0.4488210678100586\n",
            "Epoch: 10 Batch Number: 48 Loss: 1.3003331422805786 Time taken: 0.4518449306488037\n",
            "Epoch: 10 Batch Number: 49 Loss: 1.3041610717773438 Time taken: 0.4445366859436035\n",
            "Epoch: 10 Batch Number: 50 Loss: 1.1809637546539307 Time taken: 0.44231534004211426\n",
            "Epoch: 10 Batch Number: 51 Loss: 1.1884620189666748 Time taken: 0.44510459899902344\n",
            "Epoch: 10 Batch Number: 52 Loss: 1.3099184036254883 Time taken: 0.44170475006103516\n",
            "Epoch: 10 Batch Number: 53 Loss: 1.3545974493026733 Time taken: 0.45450925827026367\n",
            "Epoch: 10 Batch Number: 54 Loss: 1.3236145973205566 Time taken: 0.450023889541626\n",
            "Epoch: 10 Batch Number: 55 Loss: 1.278836727142334 Time taken: 0.4504084587097168\n",
            "Epoch: 10 Batch Number: 56 Loss: 1.360504388809204 Time taken: 0.4502449035644531\n",
            "Epoch: 10 Batch Number: 57 Loss: 1.2593022584915161 Time taken: 0.44693851470947266\n",
            "Epoch: 10 Batch Number: 58 Loss: 1.2100980281829834 Time taken: 0.44823169708251953\n",
            "Epoch: 10 Batch Number: 59 Loss: 1.2335141897201538 Time taken: 0.4443087577819824\n",
            "Epoch: 10 Batch Number: 60 Loss: 1.2313857078552246 Time taken: 0.4578874111175537\n",
            "Epoch: 10 Batch Number: 61 Loss: 1.2798819541931152 Time taken: 0.4741339683532715\n",
            "Epoch: 10 Batch Number: 62 Loss: 1.1856133937835693 Time taken: 0.4593498706817627\n",
            "Epoch: 10 Batch Number: 63 Loss: 1.190169334411621 Time taken: 0.45540857315063477\n",
            "Epoch: 10 Batch Number: 64 Loss: 1.139475703239441 Time taken: 0.46344614028930664\n",
            "Epoch: 10 Batch Number: 65 Loss: 1.1813112497329712 Time taken: 0.4690117835998535\n",
            "Epoch: 10 Batch Number: 66 Loss: 1.2031100988388062 Time taken: 0.45743274688720703\n",
            "Epoch: 10 Batch Number: 67 Loss: 1.1918131113052368 Time taken: 0.46665096282958984\n",
            "Epoch: 10 Batch Number: 68 Loss: 1.256279468536377 Time taken: 0.45013976097106934\n",
            "Epoch: 10 Batch Number: 69 Loss: 1.2298288345336914 Time taken: 0.44576478004455566\n",
            "Epoch: 10 Batch Number: 70 Loss: 1.233218789100647 Time taken: 0.45359325408935547\n",
            "Epoch: 10 Batch Number: 71 Loss: 1.217049241065979 Time taken: 0.45685362815856934\n",
            "Epoch: 10 Batch Number: 72 Loss: 1.2401046752929688 Time taken: 0.46143054962158203\n",
            "Epoch: 10 Batch Number: 73 Loss: 1.16732919216156 Time taken: 0.46011948585510254\n",
            "Epoch: 10 Batch Number: 74 Loss: 1.2819430828094482 Time taken: 0.4533364772796631\n",
            "Epoch: 10 Batch Number: 75 Loss: 1.1259469985961914 Time taken: 0.4525260925292969\n",
            "Epoch: 10 Batch Number: 76 Loss: 1.1738709211349487 Time taken: 0.45223045349121094\n",
            "Epoch: 10 Batch Number: 77 Loss: 1.2626005411148071 Time taken: 0.4676496982574463\n",
            "Epoch: 10 Batch Number: 78 Loss: 1.1451737880706787 Time taken: 0.44742703437805176\n",
            "Epoch: 10 Batch Number: 79 Loss: 1.2283860445022583 Time taken: 0.4491305351257324\n",
            "Epoch: 10 Batch Number: 80 Loss: 1.2137354612350464 Time taken: 0.4501655101776123\n",
            "Epoch: 10 Batch Number: 81 Loss: 1.2140867710113525 Time taken: 0.460360050201416\n",
            "Epoch: 10 Batch Number: 82 Loss: 1.2396643161773682 Time taken: 0.4604063034057617\n",
            "Epoch: 10 Batch Number: 83 Loss: 1.293694257736206 Time taken: 0.468982458114624\n",
            "Epoch: 10 Batch Number: 84 Loss: 1.249466896057129 Time taken: 0.46215248107910156\n",
            "Epoch: 10 Batch Number: 85 Loss: 1.215620517730713 Time taken: 0.44685959815979004\n",
            "Epoch: 10 Batch Number: 86 Loss: 1.2294976711273193 Time taken: 0.4475119113922119\n",
            "Epoch: 10 Batch Number: 87 Loss: 1.2848424911499023 Time taken: 0.44783997535705566\n",
            "Epoch: 10 Batch Number: 88 Loss: 1.1844340562820435 Time taken: 0.45334482192993164\n",
            "Epoch: 10 Batch Number: 89 Loss: 1.1708097457885742 Time taken: 0.458606481552124\n",
            "Epoch: 10 Batch Number: 90 Loss: 1.2195782661437988 Time taken: 0.46369147300720215\n",
            "Epoch: 10 Batch Number: 91 Loss: 1.2008249759674072 Time taken: 0.44785642623901367\n",
            "Epoch: 10 Batch Number: 92 Loss: 1.1556838750839233 Time taken: 0.4704878330230713\n",
            "Epoch: 10 Batch Number: 93 Loss: 1.1679377555847168 Time taken: 0.4625437259674072\n",
            "Epoch: 10 Batch Number: 94 Loss: 1.3098702430725098 Time taken: 0.4647338390350342\n",
            "Epoch: 10 Batch Number: 95 Loss: 1.275010347366333 Time taken: 0.44745969772338867\n",
            "Epoch: 10 Batch Number: 96 Loss: 1.3165669441223145 Time taken: 0.46347999572753906\n",
            "Epoch: 10 Batch Number: 97 Loss: 1.2553420066833496 Time taken: 0.4714653491973877\n",
            "Epoch: 10 Batch Number: 98 Loss: 1.3104287385940552 Time taken: 0.46079397201538086\n",
            "Epoch: 10 Batch Number: 99 Loss: 1.3406059741973877 Time taken: 0.4428272247314453\n",
            "Epoch: 10 Batch Number: 100 Loss: 1.256440281867981 Time taken: 0.4499778747558594\n",
            "Epoch: 10 Batch Number: 101 Loss: 1.2571512460708618 Time taken: 0.47947049140930176\n",
            "Epoch: 10 Batch Number: 102 Loss: 1.296988606452942 Time taken: 0.45887303352355957\n",
            "Epoch: 10 Batch Number: 103 Loss: 1.3700625896453857 Time taken: 0.4642653465270996\n",
            "Epoch: 10 Batch Number: 104 Loss: 1.2729852199554443 Time taken: 0.47054028511047363\n",
            "Epoch: 10 Batch Number: 105 Loss: 1.3375341892242432 Time taken: 0.45293164253234863\n",
            "Epoch: 10 Batch Number: 106 Loss: 1.2946248054504395 Time taken: 0.4444446563720703\n",
            "Epoch: 10 Batch Number: 107 Loss: 1.2982673645019531 Time taken: 0.4463214874267578\n",
            "Epoch: 10 Batch Number: 108 Loss: 1.2688857316970825 Time taken: 0.46071386337280273\n",
            "Epoch: 10 Batch Number: 109 Loss: 1.2804726362228394 Time taken: 0.4400005340576172\n",
            "Epoch: 10 Batch Number: 110 Loss: 1.1409703493118286 Time taken: 0.45209455490112305\n",
            "Epoch: 10 Batch Number: 111 Loss: 1.1874128580093384 Time taken: 0.44557976722717285\n",
            "Epoch: 10 Batch Number: 112 Loss: 1.2025306224822998 Time taken: 0.46687746047973633\n",
            "Epoch: 10 Batch Number: 113 Loss: 1.2352392673492432 Time taken: 0.43993592262268066\n",
            "Epoch: 10 Batch Number: 114 Loss: 1.2650701999664307 Time taken: 0.46429920196533203\n",
            "Epoch: 10 Batch Number: 115 Loss: 1.2599756717681885 Time taken: 0.4671487808227539\n",
            "Epoch: 10 Batch Number: 116 Loss: 1.1264708042144775 Time taken: 0.47914958000183105\n",
            "Epoch: 10 Batch Number: 117 Loss: 1.2487287521362305 Time taken: 0.4410421848297119\n",
            "Epoch: 10 Batch Number: 118 Loss: 1.18768310546875 Time taken: 0.4534881114959717\n",
            "Epoch: 10 Batch Number: 119 Loss: 1.217044711112976 Time taken: 0.4532029628753662\n",
            "Epoch: 10 Batch Number: 120 Loss: 1.2706046104431152 Time taken: 0.45918893814086914\n",
            "Epoch: 10 Batch Number: 121 Loss: 1.2096800804138184 Time taken: 0.4513375759124756\n",
            "Epoch: 10 Batch Number: 122 Loss: 1.2013050317764282 Time taken: 0.4504399299621582\n",
            "Epoch: 10 Batch Number: 123 Loss: 1.2287217378616333 Time taken: 0.44965100288391113\n",
            "Epoch: 10 Batch Number: 124 Loss: 1.2258086204528809 Time taken: 0.4447965621948242\n",
            "Epoch: 10 Batch Number: 125 Loss: 1.2393358945846558 Time taken: 0.44943857192993164\n",
            "Epoch: 10 Batch Number: 126 Loss: 1.2592802047729492 Time taken: 0.4458732604980469\n",
            "Epoch: 10 Batch Number: 127 Loss: 1.2722631692886353 Time taken: 0.4631350040435791\n",
            "Epoch: 10 Batch Number: 128 Loss: 1.2153205871582031 Time taken: 0.4554157257080078\n",
            "Epoch: 10 Batch Number: 129 Loss: 1.2053731679916382 Time taken: 0.4467885494232178\n",
            "Epoch: 10 Batch Number: 130 Loss: 1.170953392982483 Time taken: 0.44878458976745605\n",
            "Epoch: 10 Batch Number: 131 Loss: 1.2756565809249878 Time taken: 0.4479334354400635\n",
            "Epoch: 10 Batch Number: 132 Loss: 1.1186168193817139 Time taken: 0.4501163959503174\n",
            "Epoch: 10 Batch Number: 133 Loss: 1.2771492004394531 Time taken: 0.44416141510009766\n",
            "Epoch: 10 Batch Number: 134 Loss: 1.3008019924163818 Time taken: 0.4494907855987549\n",
            "Epoch: 10 Batch Number: 135 Loss: 1.2965548038482666 Time taken: 0.44820356369018555\n",
            "Epoch: 10 Batch Number: 136 Loss: 1.2835133075714111 Time taken: 0.4435083866119385\n",
            "Epoch: 10 Batch Number: 137 Loss: 1.2442572116851807 Time taken: 0.4536254405975342\n",
            "Epoch: 10 Batch Number: 138 Loss: 1.2654452323913574 Time taken: 0.455371618270874\n",
            "Epoch: 10 Batch Number: 139 Loss: 1.2573341131210327 Time taken: 0.4560995101928711\n",
            "Epoch: 10 Batch Number: 140 Loss: 1.2184733152389526 Time taken: 0.4462757110595703\n",
            "Epoch: 10 Batch Number: 141 Loss: 1.2748264074325562 Time taken: 0.4435994625091553\n",
            "Epoch: 10 Batch Number: 142 Loss: 1.3193109035491943 Time taken: 0.44097375869750977\n",
            "Epoch: 10 Batch Number: 143 Loss: 1.3947240114212036 Time taken: 0.4436051845550537\n",
            "Epoch: 10 Batch Number: 144 Loss: 1.400261640548706 Time taken: 0.4438018798828125\n",
            "Epoch: 10 Batch Number: 145 Loss: 1.3604469299316406 Time taken: 0.4483039379119873\n",
            "Epoch: 10 Batch Number: 146 Loss: 1.3704712390899658 Time taken: 0.4512791633605957\n",
            "Epoch: 10 Batch Number: 147 Loss: 1.2206099033355713 Time taken: 0.4445509910583496\n",
            "Epoch: 10 Batch Number: 148 Loss: 1.1892850399017334 Time taken: 0.45850491523742676\n",
            "Epoch: 10 Batch Number: 149 Loss: 1.2807170152664185 Time taken: 0.45532822608947754\n",
            "Epoch: 10 Batch Number: 150 Loss: 1.3697072267532349 Time taken: 0.44252467155456543\n",
            "Epoch: 10 Batch Number: 151 Loss: 1.3630361557006836 Time taken: 0.44400596618652344\n",
            "Epoch: 10 Batch Number: 152 Loss: 1.2897403240203857 Time taken: 0.446488618850708\n",
            "Epoch: 10 Batch Number: 153 Loss: 1.225064754486084 Time taken: 0.46045756340026855\n",
            "Epoch: 10 Batch Number: 154 Loss: 1.2230772972106934 Time taken: 0.44690680503845215\n",
            "Epoch: 10 Batch Number: 155 Loss: 1.2141592502593994 Time taken: 0.4462149143218994\n",
            "Epoch: 10 Batch Number: 156 Loss: 1.2544536590576172 Time taken: 0.45343804359436035\n",
            "Epoch: 10 Batch Number: 157 Loss: 1.1404551267623901 Time taken: 0.45762133598327637\n",
            "Epoch: 10 Batch Number: 158 Loss: 1.189622163772583 Time taken: 0.4479258060455322\n",
            "Epoch: 10 Batch Number: 159 Loss: 1.1408782005310059 Time taken: 0.4442474842071533\n",
            "Epoch: 10 Batch Number: 160 Loss: 1.045919418334961 Time taken: 0.44881653785705566\n",
            "Epoch: 10 Batch Number: 161 Loss: 1.1204137802124023 Time taken: 0.44530725479125977\n",
            "Epoch: 10 Batch Number: 162 Loss: 1.0978015661239624 Time taken: 0.4454197883605957\n",
            "Epoch: 10 Batch Number: 163 Loss: 1.1978546380996704 Time taken: 0.4410543441772461\n",
            "Epoch: 10 Batch Number: 164 Loss: 1.300018072128296 Time taken: 0.4485774040222168\n",
            "Epoch: 10 Batch Number: 165 Loss: 1.2201108932495117 Time taken: 0.45224857330322266\n",
            "Epoch: 10 Batch Number: 166 Loss: 1.2603247165679932 Time taken: 0.4577326774597168\n",
            "Epoch: 10 Batch Number: 167 Loss: 1.242732286453247 Time taken: 0.44431400299072266\n",
            "Epoch: 10 Batch Number: 168 Loss: 1.2460769414901733 Time taken: 0.4457991123199463\n",
            "Epoch: 10 Batch Number: 169 Loss: 1.2204262018203735 Time taken: 0.4406261444091797\n",
            "Epoch: 10 Batch Number: 170 Loss: 1.1507601737976074 Time taken: 0.44576382637023926\n",
            "Epoch: 10 Batch Number: 171 Loss: 1.1016565561294556 Time taken: 0.4570486545562744\n",
            "Epoch: 10 Batch Number: 172 Loss: 1.1536264419555664 Time taken: 0.4448208808898926\n",
            "Epoch: 10 Batch Number: 173 Loss: 1.1407251358032227 Time taken: 0.45295000076293945\n",
            "Epoch: 10 Batch Number: 174 Loss: 1.0874048471450806 Time taken: 0.45122838020324707\n",
            "Epoch: 10 Batch Number: 175 Loss: 1.206028938293457 Time taken: 0.4542069435119629\n",
            "Epoch: 10 Batch Number: 176 Loss: 1.1996324062347412 Time taken: 0.4497530460357666\n",
            "Epoch: 10 Batch Number: 177 Loss: 1.2165100574493408 Time taken: 0.44359898567199707\n",
            "Epoch: 10 Batch Number: 178 Loss: 1.2672395706176758 Time taken: 0.45083022117614746\n",
            "Epoch: 10 Batch Number: 179 Loss: 1.2528164386749268 Time taken: 0.44063448905944824\n",
            "Epoch: 10 Batch Number: 180 Loss: 1.2227756977081299 Time taken: 0.4463186264038086\n",
            "Epoch: 10 Batch Number: 181 Loss: 1.1837520599365234 Time taken: 0.4428732395172119\n",
            "Epoch: 10 Batch Number: 182 Loss: 1.1196949481964111 Time taken: 0.4577670097351074\n",
            "Epoch: 10 Batch Number: 183 Loss: 1.1780407428741455 Time taken: 0.44677734375\n",
            "Epoch: 10 Batch Number: 184 Loss: 1.2395023107528687 Time taken: 0.45200657844543457\n",
            "Epoch: 10 Batch Number: 185 Loss: 1.2723469734191895 Time taken: 0.450594425201416\n",
            "Epoch: 10 Batch Number: 186 Loss: 1.0519061088562012 Time taken: 0.4494485855102539\n",
            "Epoch: 10 Batch Number: 187 Loss: 1.1476309299468994 Time taken: 0.44655847549438477\n",
            "Epoch: 10 Batch Number: 188 Loss: 1.1553080081939697 Time taken: 0.4508547782897949\n",
            "Epoch: 10 Batch Number: 189 Loss: 1.2012701034545898 Time taken: 0.4512510299682617\n",
            "Epoch: 10 Batch Number: 190 Loss: 1.381730318069458 Time taken: 0.4357621669769287\n",
            "Epoch: 10 Batch Number: 191 Loss: 1.5951982736587524 Time taken: 0.44319653511047363\n",
            "Epoch: 10 Batch Number: 192 Loss: 1.2755553722381592 Time taken: 0.4439690113067627\n",
            "Epoch: 10 Batch Number: 193 Loss: 1.393364667892456 Time taken: 0.4467432498931885\n",
            "Epoch: 10 Batch Number: 194 Loss: 1.2910542488098145 Time taken: 0.4483466148376465\n",
            "Epoch: 10 Batch Number: 195 Loss: 1.2246204614639282 Time taken: 0.43944334983825684\n",
            "Epoch: 10 Batch Number: 196 Loss: 1.174913763999939 Time taken: 0.4593989849090576\n",
            "Epoch: 10 Batch Number: 197 Loss: 1.261951208114624 Time taken: 0.4442017078399658\n",
            "Epoch: 10 Batch Number: 198 Loss: 1.1778208017349243 Time taken: 0.43961334228515625\n",
            "Epoch: 10 Batch Number: 199 Loss: 1.2217484712600708 Time taken: 0.44574952125549316\n",
            "Epoch: 10 Batch Number: 200 Loss: 1.1668694019317627 Time taken: 0.45273780822753906\n",
            "Epoch: 10 Batch Number: 201 Loss: 1.222440481185913 Time taken: 0.4551072120666504\n",
            "Epoch: 10 Batch Number: 202 Loss: 1.2040154933929443 Time taken: 0.4562342166900635\n",
            "Epoch: 10 Batch Number: 203 Loss: 1.198883056640625 Time taken: 0.45812177658081055\n",
            "Epoch: 10 Batch Number: 204 Loss: 1.2278063297271729 Time taken: 0.4550807476043701\n",
            "Epoch: 10 Batch Number: 205 Loss: 1.1523692607879639 Time taken: 0.4588773250579834\n",
            "Epoch: 10 Batch Number: 206 Loss: 1.2145147323608398 Time taken: 0.4616832733154297\n",
            "Epoch: 10 Batch Number: 207 Loss: 1.2102550268173218 Time taken: 0.4569528102874756\n",
            "Epoch: 10 Batch Number: 208 Loss: 1.1860960721969604 Time taken: 0.4449918270111084\n",
            "Epoch: 10 Batch Number: 209 Loss: 1.2360588312149048 Time taken: 0.45842599868774414\n",
            "Epoch: 10 Batch Number: 210 Loss: 1.2823596000671387 Time taken: 0.463240385055542\n",
            "Epoch: 10 Batch Number: 211 Loss: 1.251078724861145 Time taken: 0.44120144844055176\n",
            "Epoch: 10 Batch Number: 212 Loss: 1.3638055324554443 Time taken: 0.4425313472747803\n",
            "Epoch: 10 Batch Number: 213 Loss: 1.2105746269226074 Time taken: 0.45073366165161133\n",
            "Epoch: 10 Batch Number: 214 Loss: 1.2672157287597656 Time taken: 0.44594502449035645\n",
            "Epoch: 10 Batch Number: 215 Loss: 1.2160052061080933 Time taken: 0.44704341888427734\n",
            "Epoch: 10 Batch Number: 216 Loss: 1.2484657764434814 Time taken: 0.46963024139404297\n",
            "Epoch: 10 Batch Number: 217 Loss: 1.31818687915802 Time taken: 0.4484384059906006\n",
            "Epoch: 10 Batch Number: 218 Loss: 1.3207664489746094 Time taken: 0.44167184829711914\n",
            "Epoch: 10 Batch Number: 219 Loss: 1.3033870458602905 Time taken: 0.45052337646484375\n",
            "Epoch: 10 Batch Number: 220 Loss: 1.2299154996871948 Time taken: 0.45793700218200684\n",
            "Epoch: 10 Batch Number: 221 Loss: 1.223825216293335 Time taken: 0.45450901985168457\n",
            "Epoch: 10 Batch Number: 222 Loss: 1.1944506168365479 Time taken: 0.4499497413635254\n",
            "Epoch: 10 Batch Number: 223 Loss: 1.2340540885925293 Time taken: 0.45453953742980957\n",
            "Epoch: 10 Batch Number: 224 Loss: 1.1808016300201416 Time taken: 0.4640512466430664\n",
            "Epoch: 10 Batch Number: 225 Loss: 1.1465131044387817 Time taken: 0.44556260108947754\n",
            "Epoch: 10 Batch Number: 226 Loss: 1.1822611093521118 Time taken: 0.45375919342041016\n",
            "Epoch: 10 Batch Number: 227 Loss: 1.3311567306518555 Time taken: 0.45698022842407227\n",
            "Epoch: 10 Batch Number: 228 Loss: 1.298410177230835 Time taken: 0.4531824588775635\n",
            "Epoch: 10 Batch Number: 229 Loss: 1.3009693622589111 Time taken: 0.44280195236206055\n",
            "==========================================================================================\n",
            "Start of epoch 11\n",
            "Epoch: 11 Batch Number: 1 Loss: 1.2150483131408691 Time taken: 0.45597290992736816\n",
            "Epoch: 11 Batch Number: 2 Loss: 1.2756545543670654 Time taken: 0.45482850074768066\n",
            "Epoch: 11 Batch Number: 3 Loss: 1.2250639200210571 Time taken: 0.44251275062561035\n",
            "Epoch: 11 Batch Number: 4 Loss: 1.217789888381958 Time taken: 0.4682440757751465\n",
            "Epoch: 11 Batch Number: 5 Loss: 1.1911617517471313 Time taken: 0.4489614963531494\n",
            "Epoch: 11 Batch Number: 6 Loss: 1.1285970211029053 Time taken: 0.44521212577819824\n",
            "Epoch: 11 Batch Number: 7 Loss: 1.1642677783966064 Time taken: 0.44951796531677246\n",
            "Epoch: 11 Batch Number: 8 Loss: 1.1733003854751587 Time taken: 0.4577162265777588\n",
            "Epoch: 11 Batch Number: 9 Loss: 1.170621633529663 Time taken: 0.45157957077026367\n",
            "Epoch: 11 Batch Number: 10 Loss: 1.203948974609375 Time taken: 0.45427751541137695\n",
            "Epoch: 11 Batch Number: 11 Loss: 1.158853530883789 Time taken: 0.441601037979126\n",
            "Epoch: 11 Batch Number: 12 Loss: 1.1167094707489014 Time taken: 0.4410412311553955\n",
            "Epoch: 11 Batch Number: 13 Loss: 1.1611201763153076 Time taken: 0.4640014171600342\n",
            "Epoch: 11 Batch Number: 14 Loss: 1.1701900959014893 Time taken: 0.44745731353759766\n",
            "Epoch: 11 Batch Number: 15 Loss: 1.1026169061660767 Time taken: 0.469712495803833\n",
            "Epoch: 11 Batch Number: 16 Loss: 1.153756022453308 Time taken: 0.46204376220703125\n",
            "Epoch: 11 Batch Number: 17 Loss: 1.2812116146087646 Time taken: 0.45926523208618164\n",
            "Epoch: 11 Batch Number: 18 Loss: 1.2605984210968018 Time taken: 0.45603251457214355\n",
            "Epoch: 11 Batch Number: 19 Loss: 1.2558039426803589 Time taken: 0.44521212577819824\n",
            "Epoch: 11 Batch Number: 20 Loss: 1.1277012825012207 Time taken: 0.45789122581481934\n",
            "Epoch: 11 Batch Number: 21 Loss: 1.3229382038116455 Time taken: 0.45316362380981445\n",
            "Epoch: 11 Batch Number: 22 Loss: 1.2944316864013672 Time taken: 0.4545705318450928\n",
            "Epoch: 11 Batch Number: 23 Loss: 1.298583984375 Time taken: 0.4428861141204834\n",
            "Epoch: 11 Batch Number: 24 Loss: 1.2679965496063232 Time taken: 0.43765854835510254\n",
            "Epoch: 11 Batch Number: 25 Loss: 1.2502200603485107 Time taken: 0.44371747970581055\n",
            "Epoch: 11 Batch Number: 26 Loss: 1.2679935693740845 Time taken: 0.47409582138061523\n",
            "Epoch: 11 Batch Number: 27 Loss: 1.2058405876159668 Time taken: 0.4607553482055664\n",
            "Epoch: 11 Batch Number: 28 Loss: 1.2252827882766724 Time taken: 0.45461058616638184\n",
            "Epoch: 11 Batch Number: 29 Loss: 1.25791335105896 Time taken: 0.4667038917541504\n",
            "Epoch: 11 Batch Number: 30 Loss: 1.0305272340774536 Time taken: 0.4634525775909424\n",
            "Epoch: 11 Batch Number: 31 Loss: 1.1929049491882324 Time taken: 0.45612168312072754\n",
            "Epoch: 11 Batch Number: 32 Loss: 1.201511025428772 Time taken: 0.45420265197753906\n",
            "Epoch: 11 Batch Number: 33 Loss: 1.2311105728149414 Time taken: 0.46879148483276367\n",
            "Epoch: 11 Batch Number: 34 Loss: 1.2579429149627686 Time taken: 0.4451007843017578\n",
            "Epoch: 11 Batch Number: 35 Loss: 1.3072166442871094 Time taken: 0.44621849060058594\n",
            "Epoch: 11 Batch Number: 36 Loss: 1.348257064819336 Time taken: 0.45709228515625\n",
            "Epoch: 11 Batch Number: 37 Loss: 1.2107583284378052 Time taken: 0.46056556701660156\n",
            "Epoch: 11 Batch Number: 38 Loss: 1.253799319267273 Time taken: 0.45385241508483887\n",
            "Epoch: 11 Batch Number: 39 Loss: 1.207648754119873 Time taken: 0.45016908645629883\n",
            "Epoch: 11 Batch Number: 40 Loss: 1.2114479541778564 Time taken: 0.44614243507385254\n",
            "Epoch: 11 Batch Number: 41 Loss: 1.1649665832519531 Time taken: 0.44948577880859375\n",
            "Epoch: 11 Batch Number: 42 Loss: 1.1709086894989014 Time taken: 0.45610976219177246\n",
            "Epoch: 11 Batch Number: 43 Loss: 1.1632750034332275 Time taken: 0.4632396697998047\n",
            "Epoch: 11 Batch Number: 44 Loss: 1.111175298690796 Time taken: 0.4523050785064697\n",
            "Epoch: 11 Batch Number: 45 Loss: 1.1683851480484009 Time taken: 0.44693803787231445\n",
            "Epoch: 11 Batch Number: 46 Loss: 1.3247913122177124 Time taken: 0.4600200653076172\n",
            "Epoch: 11 Batch Number: 47 Loss: 1.2063753604888916 Time taken: 0.4602341651916504\n",
            "Epoch: 11 Batch Number: 48 Loss: 1.2718298435211182 Time taken: 0.4554018974304199\n",
            "Epoch: 11 Batch Number: 49 Loss: 1.2720084190368652 Time taken: 0.4463355541229248\n",
            "Epoch: 11 Batch Number: 50 Loss: 1.161028504371643 Time taken: 0.45064687728881836\n",
            "Epoch: 11 Batch Number: 51 Loss: 1.1635401248931885 Time taken: 0.44747114181518555\n",
            "Epoch: 11 Batch Number: 52 Loss: 1.284698247909546 Time taken: 0.45486974716186523\n",
            "Epoch: 11 Batch Number: 53 Loss: 1.3305244445800781 Time taken: 0.4523196220397949\n",
            "Epoch: 11 Batch Number: 54 Loss: 1.3058347702026367 Time taken: 0.43937230110168457\n",
            "Epoch: 11 Batch Number: 55 Loss: 1.2578134536743164 Time taken: 0.45103025436401367\n",
            "Epoch: 11 Batch Number: 56 Loss: 1.3398791551589966 Time taken: 0.44401073455810547\n",
            "Epoch: 11 Batch Number: 57 Loss: 1.2372561693191528 Time taken: 0.450777530670166\n",
            "Epoch: 11 Batch Number: 58 Loss: 1.1875240802764893 Time taken: 0.4476308822631836\n",
            "Epoch: 11 Batch Number: 59 Loss: 1.2165216207504272 Time taken: 0.4529149532318115\n",
            "Epoch: 11 Batch Number: 60 Loss: 1.2094093561172485 Time taken: 0.4407477378845215\n",
            "Epoch: 11 Batch Number: 61 Loss: 1.252392292022705 Time taken: 0.45910024642944336\n",
            "Epoch: 11 Batch Number: 62 Loss: 1.15932035446167 Time taken: 0.4539175033569336\n",
            "Epoch: 11 Batch Number: 63 Loss: 1.1735072135925293 Time taken: 0.4513247013092041\n",
            "Epoch: 11 Batch Number: 64 Loss: 1.1184700727462769 Time taken: 0.4542405605316162\n",
            "Epoch: 11 Batch Number: 65 Loss: 1.1580047607421875 Time taken: 0.4577951431274414\n",
            "Epoch: 11 Batch Number: 66 Loss: 1.1793489456176758 Time taken: 0.45673060417175293\n",
            "Epoch: 11 Batch Number: 67 Loss: 1.1706464290618896 Time taken: 0.4461326599121094\n",
            "Epoch: 11 Batch Number: 68 Loss: 1.2361326217651367 Time taken: 0.4507639408111572\n",
            "Epoch: 11 Batch Number: 69 Loss: 1.208327054977417 Time taken: 0.46257996559143066\n",
            "Epoch: 11 Batch Number: 70 Loss: 1.2135841846466064 Time taken: 0.44921302795410156\n",
            "Epoch: 11 Batch Number: 71 Loss: 1.1941173076629639 Time taken: 0.44785237312316895\n",
            "Epoch: 11 Batch Number: 72 Loss: 1.2169995307922363 Time taken: 0.4497396945953369\n",
            "Epoch: 11 Batch Number: 73 Loss: 1.1462483406066895 Time taken: 0.45519185066223145\n",
            "Epoch: 11 Batch Number: 74 Loss: 1.2577810287475586 Time taken: 0.4373013973236084\n",
            "Epoch: 11 Batch Number: 75 Loss: 1.1032943725585938 Time taken: 0.44777750968933105\n",
            "Epoch: 11 Batch Number: 76 Loss: 1.1481246948242188 Time taken: 0.4597306251525879\n",
            "Epoch: 11 Batch Number: 77 Loss: 1.2396464347839355 Time taken: 0.45976805686950684\n",
            "Epoch: 11 Batch Number: 78 Loss: 1.1243077516555786 Time taken: 0.4608447551727295\n",
            "Epoch: 11 Batch Number: 79 Loss: 1.2063406705856323 Time taken: 0.457411527633667\n",
            "Epoch: 11 Batch Number: 80 Loss: 1.1884958744049072 Time taken: 0.44873857498168945\n",
            "Epoch: 11 Batch Number: 81 Loss: 1.19791579246521 Time taken: 0.4441680908203125\n",
            "Epoch: 11 Batch Number: 82 Loss: 1.214861512184143 Time taken: 0.4459874629974365\n",
            "Epoch: 11 Batch Number: 83 Loss: 1.273864984512329 Time taken: 0.440554141998291\n",
            "Epoch: 11 Batch Number: 84 Loss: 1.2287241220474243 Time taken: 0.4491908550262451\n",
            "Epoch: 11 Batch Number: 85 Loss: 1.1965389251708984 Time taken: 0.4391214847564697\n",
            "Epoch: 11 Batch Number: 86 Loss: 1.208738088607788 Time taken: 0.44262075424194336\n",
            "Epoch: 11 Batch Number: 87 Loss: 1.2740942239761353 Time taken: 0.44310474395751953\n",
            "Epoch: 11 Batch Number: 88 Loss: 1.160609483718872 Time taken: 0.4469764232635498\n",
            "Epoch: 11 Batch Number: 89 Loss: 1.1539702415466309 Time taken: 0.4482710361480713\n",
            "Epoch: 11 Batch Number: 90 Loss: 1.2089364528656006 Time taken: 0.44510316848754883\n",
            "Epoch: 11 Batch Number: 91 Loss: 1.1826517581939697 Time taken: 0.45380306243896484\n",
            "Epoch: 11 Batch Number: 92 Loss: 1.1365225315093994 Time taken: 0.4427042007446289\n",
            "Epoch: 11 Batch Number: 93 Loss: 1.1502350568771362 Time taken: 0.4481828212738037\n",
            "Epoch: 11 Batch Number: 94 Loss: 1.2923532724380493 Time taken: 0.44111204147338867\n",
            "Epoch: 11 Batch Number: 95 Loss: 1.250330924987793 Time taken: 0.45912718772888184\n",
            "Epoch: 11 Batch Number: 96 Loss: 1.289546012878418 Time taken: 0.44573330879211426\n",
            "Epoch: 11 Batch Number: 97 Loss: 1.2362990379333496 Time taken: 0.4475851058959961\n",
            "Epoch: 11 Batch Number: 98 Loss: 1.2876187562942505 Time taken: 0.4525468349456787\n",
            "Epoch: 11 Batch Number: 99 Loss: 1.3153400421142578 Time taken: 0.4452998638153076\n",
            "Epoch: 11 Batch Number: 100 Loss: 1.237662672996521 Time taken: 0.4632143974304199\n",
            "Epoch: 11 Batch Number: 101 Loss: 1.2357561588287354 Time taken: 0.4607722759246826\n",
            "Epoch: 11 Batch Number: 102 Loss: 1.2721599340438843 Time taken: 0.46054816246032715\n",
            "Epoch: 11 Batch Number: 103 Loss: 1.3382670879364014 Time taken: 0.4583587646484375\n",
            "Epoch: 11 Batch Number: 104 Loss: 1.2348649501800537 Time taken: 0.466998815536499\n",
            "Epoch: 11 Batch Number: 105 Loss: 1.3091803789138794 Time taken: 0.43848228454589844\n",
            "Epoch: 11 Batch Number: 106 Loss: 1.2704548835754395 Time taken: 0.44579029083251953\n",
            "Epoch: 11 Batch Number: 107 Loss: 1.2728080749511719 Time taken: 0.4452555179595947\n",
            "Epoch: 11 Batch Number: 108 Loss: 1.243638038635254 Time taken: 0.46652793884277344\n",
            "Epoch: 11 Batch Number: 109 Loss: 1.2536814212799072 Time taken: 0.4392056465148926\n",
            "Epoch: 11 Batch Number: 110 Loss: 1.1103460788726807 Time taken: 0.44370245933532715\n",
            "Epoch: 11 Batch Number: 111 Loss: 1.1653245687484741 Time taken: 0.44328975677490234\n",
            "Epoch: 11 Batch Number: 112 Loss: 1.1798166036605835 Time taken: 0.4419398307800293\n",
            "Epoch: 11 Batch Number: 113 Loss: 1.211456060409546 Time taken: 0.4526524543762207\n",
            "Epoch: 11 Batch Number: 114 Loss: 1.2443515062332153 Time taken: 0.45751190185546875\n",
            "Epoch: 11 Batch Number: 115 Loss: 1.243643879890442 Time taken: 0.44262146949768066\n",
            "Epoch: 11 Batch Number: 116 Loss: 1.1072944402694702 Time taken: 0.45244264602661133\n",
            "Epoch: 11 Batch Number: 117 Loss: 1.2263078689575195 Time taken: 0.4653632640838623\n",
            "Epoch: 11 Batch Number: 118 Loss: 1.1700574159622192 Time taken: 0.4570200443267822\n",
            "Epoch: 11 Batch Number: 119 Loss: 1.1988434791564941 Time taken: 0.450162410736084\n",
            "Epoch: 11 Batch Number: 120 Loss: 1.2532939910888672 Time taken: 0.45372819900512695\n",
            "Epoch: 11 Batch Number: 121 Loss: 1.191239356994629 Time taken: 0.4511244297027588\n",
            "Epoch: 11 Batch Number: 122 Loss: 1.1792685985565186 Time taken: 0.4502682685852051\n",
            "Epoch: 11 Batch Number: 123 Loss: 1.2061419486999512 Time taken: 0.4394416809082031\n",
            "Epoch: 11 Batch Number: 124 Loss: 1.1904908418655396 Time taken: 0.47120213508605957\n",
            "Epoch: 11 Batch Number: 125 Loss: 1.21584153175354 Time taken: 0.4548344612121582\n",
            "Epoch: 11 Batch Number: 126 Loss: 1.236684799194336 Time taken: 0.4406306743621826\n",
            "Epoch: 11 Batch Number: 127 Loss: 1.2426581382751465 Time taken: 0.43897271156311035\n",
            "Epoch: 11 Batch Number: 128 Loss: 1.201385498046875 Time taken: 0.46123290061950684\n",
            "Epoch: 11 Batch Number: 129 Loss: 1.1875569820404053 Time taken: 0.44757652282714844\n",
            "Epoch: 11 Batch Number: 130 Loss: 1.1539360284805298 Time taken: 0.44880127906799316\n",
            "Epoch: 11 Batch Number: 131 Loss: 1.2495429515838623 Time taken: 0.43562865257263184\n",
            "Epoch: 11 Batch Number: 132 Loss: 1.098306655883789 Time taken: 0.44727563858032227\n",
            "Epoch: 11 Batch Number: 133 Loss: 1.251318335533142 Time taken: 0.45294189453125\n",
            "Epoch: 11 Batch Number: 134 Loss: 1.2691926956176758 Time taken: 0.4367697238922119\n",
            "Epoch: 11 Batch Number: 135 Loss: 1.2697288990020752 Time taken: 0.45502495765686035\n",
            "Epoch: 11 Batch Number: 136 Loss: 1.2603603601455688 Time taken: 0.43921709060668945\n",
            "Epoch: 11 Batch Number: 137 Loss: 1.220482349395752 Time taken: 0.4528329372406006\n",
            "Epoch: 11 Batch Number: 138 Loss: 1.2456110715866089 Time taken: 0.44228434562683105\n",
            "Epoch: 11 Batch Number: 139 Loss: 1.2340471744537354 Time taken: 0.44869256019592285\n",
            "Epoch: 11 Batch Number: 140 Loss: 1.1984825134277344 Time taken: 0.4624598026275635\n",
            "Epoch: 11 Batch Number: 141 Loss: 1.255260944366455 Time taken: 0.4594111442565918\n",
            "Epoch: 11 Batch Number: 142 Loss: 1.2994025945663452 Time taken: 0.4486582279205322\n",
            "Epoch: 11 Batch Number: 143 Loss: 1.3681926727294922 Time taken: 0.44310998916625977\n",
            "Epoch: 11 Batch Number: 144 Loss: 1.3694729804992676 Time taken: 0.4434516429901123\n",
            "Epoch: 11 Batch Number: 145 Loss: 1.3351175785064697 Time taken: 0.4365823268890381\n",
            "Epoch: 11 Batch Number: 146 Loss: 1.3377792835235596 Time taken: 0.4422914981842041\n",
            "Epoch: 11 Batch Number: 147 Loss: 1.1946650743484497 Time taken: 0.4565143585205078\n",
            "Epoch: 11 Batch Number: 148 Loss: 1.1668027639389038 Time taken: 0.4785635471343994\n",
            "Epoch: 11 Batch Number: 149 Loss: 1.2540853023529053 Time taken: 0.4554159641265869\n",
            "Epoch: 11 Batch Number: 150 Loss: 1.3430975675582886 Time taken: 0.456010103225708\n",
            "Epoch: 11 Batch Number: 151 Loss: 1.3408236503601074 Time taken: 0.4430043697357178\n",
            "Epoch: 11 Batch Number: 152 Loss: 1.2661924362182617 Time taken: 0.4518105983734131\n",
            "Epoch: 11 Batch Number: 153 Loss: 1.1985374689102173 Time taken: 0.4457221031188965\n",
            "Epoch: 11 Batch Number: 154 Loss: 1.1937880516052246 Time taken: 0.4525001049041748\n",
            "Epoch: 11 Batch Number: 155 Loss: 1.186503529548645 Time taken: 0.44739747047424316\n",
            "Epoch: 11 Batch Number: 156 Loss: 1.2330292463302612 Time taken: 0.43757057189941406\n",
            "Epoch: 11 Batch Number: 157 Loss: 1.120781421661377 Time taken: 0.45395469665527344\n",
            "Epoch: 11 Batch Number: 158 Loss: 1.1681894063949585 Time taken: 0.446793794631958\n",
            "Epoch: 11 Batch Number: 159 Loss: 1.1181552410125732 Time taken: 0.45223188400268555\n",
            "Epoch: 11 Batch Number: 160 Loss: 1.0235726833343506 Time taken: 0.44168615341186523\n",
            "Epoch: 11 Batch Number: 161 Loss: 1.0999219417572021 Time taken: 0.44249606132507324\n",
            "Epoch: 11 Batch Number: 162 Loss: 1.0749802589416504 Time taken: 0.4443650245666504\n",
            "Epoch: 11 Batch Number: 163 Loss: 1.1886506080627441 Time taken: 0.4572768211364746\n",
            "Epoch: 11 Batch Number: 164 Loss: 1.284415364265442 Time taken: 0.4630911350250244\n",
            "Epoch: 11 Batch Number: 165 Loss: 1.2051467895507812 Time taken: 0.4516611099243164\n",
            "Epoch: 11 Batch Number: 166 Loss: 1.2376220226287842 Time taken: 0.4437985420227051\n",
            "Epoch: 11 Batch Number: 167 Loss: 1.215632438659668 Time taken: 0.4442026615142822\n",
            "Epoch: 11 Batch Number: 168 Loss: 1.2151999473571777 Time taken: 0.47258591651916504\n",
            "Epoch: 11 Batch Number: 169 Loss: 1.1950935125350952 Time taken: 0.45676302909851074\n",
            "Epoch: 11 Batch Number: 170 Loss: 1.1252126693725586 Time taken: 0.4582972526550293\n",
            "Epoch: 11 Batch Number: 171 Loss: 1.0786006450653076 Time taken: 0.44836997985839844\n",
            "Epoch: 11 Batch Number: 172 Loss: 1.1334521770477295 Time taken: 0.4530155658721924\n",
            "Epoch: 11 Batch Number: 173 Loss: 1.1191444396972656 Time taken: 0.4581608772277832\n",
            "Epoch: 11 Batch Number: 174 Loss: 1.0669353008270264 Time taken: 0.4599428176879883\n",
            "Epoch: 11 Batch Number: 175 Loss: 1.1896790266036987 Time taken: 0.45069217681884766\n",
            "Epoch: 11 Batch Number: 176 Loss: 1.1718391180038452 Time taken: 0.4480116367340088\n",
            "Epoch: 11 Batch Number: 177 Loss: 1.1931729316711426 Time taken: 0.4522829055786133\n",
            "Epoch: 11 Batch Number: 178 Loss: 1.2440829277038574 Time taken: 0.4379007816314697\n",
            "Epoch: 11 Batch Number: 179 Loss: 1.2382111549377441 Time taken: 0.446535587310791\n",
            "Epoch: 11 Batch Number: 180 Loss: 1.2043497562408447 Time taken: 0.45291686058044434\n",
            "Epoch: 11 Batch Number: 181 Loss: 1.1622294187545776 Time taken: 0.45891666412353516\n",
            "Epoch: 11 Batch Number: 182 Loss: 1.104355812072754 Time taken: 0.44429755210876465\n",
            "Epoch: 11 Batch Number: 183 Loss: 1.1575993299484253 Time taken: 0.45336246490478516\n",
            "Epoch: 11 Batch Number: 184 Loss: 1.2228152751922607 Time taken: 0.4599123001098633\n",
            "Epoch: 11 Batch Number: 185 Loss: 1.2524330615997314 Time taken: 0.45132017135620117\n",
            "Epoch: 11 Batch Number: 186 Loss: 1.0394089221954346 Time taken: 0.43955349922180176\n",
            "Epoch: 11 Batch Number: 187 Loss: 1.1297296285629272 Time taken: 0.45998191833496094\n",
            "Epoch: 11 Batch Number: 188 Loss: 1.1383168697357178 Time taken: 0.4466552734375\n",
            "Epoch: 11 Batch Number: 189 Loss: 1.175225853919983 Time taken: 0.4445011615753174\n",
            "Epoch: 11 Batch Number: 190 Loss: 1.363410472869873 Time taken: 0.45142340660095215\n",
            "Epoch: 11 Batch Number: 191 Loss: 1.559225082397461 Time taken: 0.46192049980163574\n",
            "Epoch: 11 Batch Number: 192 Loss: 1.2545411586761475 Time taken: 0.44020986557006836\n",
            "Epoch: 11 Batch Number: 193 Loss: 1.3715693950653076 Time taken: 0.4427790641784668\n",
            "Epoch: 11 Batch Number: 194 Loss: 1.276170015335083 Time taken: 0.4619624614715576\n",
            "Epoch: 11 Batch Number: 195 Loss: 1.2126083374023438 Time taken: 0.44501686096191406\n",
            "Epoch: 11 Batch Number: 196 Loss: 1.1601669788360596 Time taken: 0.4452221393585205\n",
            "Epoch: 11 Batch Number: 197 Loss: 1.2370070219039917 Time taken: 0.4481682777404785\n",
            "Epoch: 11 Batch Number: 198 Loss: 1.1566168069839478 Time taken: 0.441608190536499\n",
            "Epoch: 11 Batch Number: 199 Loss: 1.2003222703933716 Time taken: 0.4465489387512207\n",
            "Epoch: 11 Batch Number: 200 Loss: 1.147791862487793 Time taken: 0.4460012912750244\n",
            "Epoch: 11 Batch Number: 201 Loss: 1.2005189657211304 Time taken: 0.43998003005981445\n",
            "Epoch: 11 Batch Number: 202 Loss: 1.1855469942092896 Time taken: 0.44507646560668945\n",
            "Epoch: 11 Batch Number: 203 Loss: 1.1818302869796753 Time taken: 0.44062161445617676\n",
            "Epoch: 11 Batch Number: 204 Loss: 1.2111823558807373 Time taken: 0.43850183486938477\n",
            "Epoch: 11 Batch Number: 205 Loss: 1.1374937295913696 Time taken: 0.4586966037750244\n",
            "Epoch: 11 Batch Number: 206 Loss: 1.1928000450134277 Time taken: 0.4462435245513916\n",
            "Epoch: 11 Batch Number: 207 Loss: 1.196781039237976 Time taken: 0.4577615261077881\n",
            "Epoch: 11 Batch Number: 208 Loss: 1.1685577630996704 Time taken: 0.4453706741333008\n",
            "Epoch: 11 Batch Number: 209 Loss: 1.2157092094421387 Time taken: 0.4509913921356201\n",
            "Epoch: 11 Batch Number: 210 Loss: 1.264493703842163 Time taken: 0.4473254680633545\n",
            "Epoch: 11 Batch Number: 211 Loss: 1.233397126197815 Time taken: 0.45202016830444336\n",
            "Epoch: 11 Batch Number: 212 Loss: 1.3497331142425537 Time taken: 0.4466679096221924\n",
            "Epoch: 11 Batch Number: 213 Loss: 1.1928719282150269 Time taken: 0.45038604736328125\n",
            "Epoch: 11 Batch Number: 214 Loss: 1.2502120733261108 Time taken: 0.46160888671875\n",
            "Epoch: 11 Batch Number: 215 Loss: 1.2052849531173706 Time taken: 0.45270776748657227\n",
            "Epoch: 11 Batch Number: 216 Loss: 1.2329084873199463 Time taken: 0.4460890293121338\n",
            "Epoch: 11 Batch Number: 217 Loss: 1.3012269735336304 Time taken: 0.4507918357849121\n",
            "Epoch: 11 Batch Number: 218 Loss: 1.2917145490646362 Time taken: 0.4492530822753906\n",
            "Epoch: 11 Batch Number: 219 Loss: 1.2854841947555542 Time taken: 0.4526524543762207\n",
            "Epoch: 11 Batch Number: 220 Loss: 1.2119650840759277 Time taken: 0.4517171382904053\n",
            "Epoch: 11 Batch Number: 221 Loss: 1.2045762538909912 Time taken: 0.4666423797607422\n",
            "Epoch: 11 Batch Number: 222 Loss: 1.1743320226669312 Time taken: 0.4517190456390381\n",
            "Epoch: 11 Batch Number: 223 Loss: 1.2139112949371338 Time taken: 0.44194841384887695\n",
            "Epoch: 11 Batch Number: 224 Loss: 1.159365177154541 Time taken: 0.45279741287231445\n",
            "Epoch: 11 Batch Number: 225 Loss: 1.1299512386322021 Time taken: 0.47693514823913574\n",
            "Epoch: 11 Batch Number: 226 Loss: 1.1659984588623047 Time taken: 0.4746584892272949\n",
            "Epoch: 11 Batch Number: 227 Loss: 1.30125093460083 Time taken: 0.44782185554504395\n",
            "Epoch: 11 Batch Number: 228 Loss: 1.2712994813919067 Time taken: 0.4461185932159424\n",
            "Epoch: 11 Batch Number: 229 Loss: 1.277396559715271 Time taken: 0.44513607025146484\n",
            "==========================================================================================\n",
            "Start of epoch 12\n",
            "Epoch: 12 Batch Number: 1 Loss: 1.1952465772628784 Time taken: 0.4497795104980469\n",
            "Epoch: 12 Batch Number: 2 Loss: 1.2577826976776123 Time taken: 0.4391930103302002\n",
            "Epoch: 12 Batch Number: 3 Loss: 1.2117866277694702 Time taken: 0.45394229888916016\n",
            "Epoch: 12 Batch Number: 4 Loss: 1.1991724967956543 Time taken: 0.4450805187225342\n",
            "Epoch: 12 Batch Number: 5 Loss: 1.1745517253875732 Time taken: 0.45871639251708984\n",
            "Epoch: 12 Batch Number: 6 Loss: 1.1163667440414429 Time taken: 0.4588761329650879\n",
            "Epoch: 12 Batch Number: 7 Loss: 1.1450695991516113 Time taken: 0.4592597484588623\n",
            "Epoch: 12 Batch Number: 8 Loss: 1.1582865715026855 Time taken: 0.46301770210266113\n",
            "Epoch: 12 Batch Number: 9 Loss: 1.152008295059204 Time taken: 0.4523015022277832\n",
            "Epoch: 12 Batch Number: 10 Loss: 1.1886534690856934 Time taken: 0.44330668449401855\n",
            "Epoch: 12 Batch Number: 11 Loss: 1.1439270973205566 Time taken: 0.43634605407714844\n",
            "Epoch: 12 Batch Number: 12 Loss: 1.1026852130889893 Time taken: 0.443845272064209\n",
            "Epoch: 12 Batch Number: 13 Loss: 1.1433730125427246 Time taken: 0.44745874404907227\n",
            "Epoch: 12 Batch Number: 14 Loss: 1.157598614692688 Time taken: 0.4426393508911133\n",
            "Epoch: 12 Batch Number: 15 Loss: 1.101130485534668 Time taken: 0.45092034339904785\n",
            "Epoch: 12 Batch Number: 16 Loss: 1.1347075700759888 Time taken: 0.4419560432434082\n",
            "Epoch: 12 Batch Number: 17 Loss: 1.2597475051879883 Time taken: 0.44342756271362305\n",
            "Epoch: 12 Batch Number: 18 Loss: 1.2436165809631348 Time taken: 0.4585456848144531\n",
            "Epoch: 12 Batch Number: 19 Loss: 1.237691879272461 Time taken: 0.44582295417785645\n",
            "Epoch: 12 Batch Number: 20 Loss: 1.1148712635040283 Time taken: 0.4434964656829834\n",
            "Epoch: 12 Batch Number: 21 Loss: 1.3084492683410645 Time taken: 0.44206953048706055\n",
            "Epoch: 12 Batch Number: 22 Loss: 1.2779042720794678 Time taken: 0.46140074729919434\n",
            "Epoch: 12 Batch Number: 23 Loss: 1.2803901433944702 Time taken: 0.44890475273132324\n",
            "Epoch: 12 Batch Number: 24 Loss: 1.252618432044983 Time taken: 0.44392919540405273\n",
            "Epoch: 12 Batch Number: 25 Loss: 1.2356274127960205 Time taken: 0.43962931632995605\n",
            "Epoch: 12 Batch Number: 26 Loss: 1.250948429107666 Time taken: 0.44779348373413086\n",
            "Epoch: 12 Batch Number: 27 Loss: 1.1954717636108398 Time taken: 0.44836926460266113\n",
            "Epoch: 12 Batch Number: 28 Loss: 1.2123569250106812 Time taken: 0.43735313415527344\n",
            "Epoch: 12 Batch Number: 29 Loss: 1.245776891708374 Time taken: 0.44306492805480957\n",
            "Epoch: 12 Batch Number: 30 Loss: 1.0176297426223755 Time taken: 0.4412980079650879\n",
            "Epoch: 12 Batch Number: 31 Loss: 1.177159070968628 Time taken: 0.4455728530883789\n",
            "Epoch: 12 Batch Number: 32 Loss: 1.1846755743026733 Time taken: 0.4478116035461426\n",
            "Epoch: 12 Batch Number: 33 Loss: 1.211592435836792 Time taken: 0.4586148262023926\n",
            "Epoch: 12 Batch Number: 34 Loss: 1.2398619651794434 Time taken: 0.44281530380249023\n",
            "Epoch: 12 Batch Number: 35 Loss: 1.2887159585952759 Time taken: 0.45291733741760254\n",
            "Epoch: 12 Batch Number: 36 Loss: 1.3290209770202637 Time taken: 0.4529879093170166\n",
            "Epoch: 12 Batch Number: 37 Loss: 1.1955124139785767 Time taken: 0.43698954582214355\n",
            "Epoch: 12 Batch Number: 38 Loss: 1.243416428565979 Time taken: 0.44136881828308105\n",
            "Epoch: 12 Batch Number: 39 Loss: 1.1864445209503174 Time taken: 0.44433116912841797\n",
            "Epoch: 12 Batch Number: 40 Loss: 1.194423794746399 Time taken: 0.43993306159973145\n",
            "Epoch: 12 Batch Number: 41 Loss: 1.1558325290679932 Time taken: 0.43915414810180664\n",
            "Epoch: 12 Batch Number: 42 Loss: 1.155695915222168 Time taken: 0.4452798366546631\n",
            "Epoch: 12 Batch Number: 43 Loss: 1.1453940868377686 Time taken: 0.44594764709472656\n",
            "Epoch: 12 Batch Number: 44 Loss: 1.095160961151123 Time taken: 0.4378633499145508\n",
            "Epoch: 12 Batch Number: 45 Loss: 1.1544560194015503 Time taken: 0.4534142017364502\n",
            "Epoch: 12 Batch Number: 46 Loss: 1.3115794658660889 Time taken: 0.4372978210449219\n",
            "Epoch: 12 Batch Number: 47 Loss: 1.183343529701233 Time taken: 0.43975162506103516\n",
            "Epoch: 12 Batch Number: 48 Loss: 1.2494516372680664 Time taken: 0.4571042060852051\n",
            "Epoch: 12 Batch Number: 49 Loss: 1.248013973236084 Time taken: 0.45435309410095215\n",
            "Epoch: 12 Batch Number: 50 Loss: 1.1441211700439453 Time taken: 0.4511287212371826\n",
            "Epoch: 12 Batch Number: 51 Loss: 1.1443407535552979 Time taken: 0.43558621406555176\n",
            "Epoch: 12 Batch Number: 52 Loss: 1.2628889083862305 Time taken: 0.4517989158630371\n",
            "Epoch: 12 Batch Number: 53 Loss: 1.304015874862671 Time taken: 0.46062493324279785\n",
            "Epoch: 12 Batch Number: 54 Loss: 1.2757041454315186 Time taken: 0.4494204521179199\n",
            "Epoch: 12 Batch Number: 55 Loss: 1.2366044521331787 Time taken: 0.46236681938171387\n",
            "Epoch: 12 Batch Number: 56 Loss: 1.3259433507919312 Time taken: 0.46933865547180176\n",
            "Epoch: 12 Batch Number: 57 Loss: 1.212887167930603 Time taken: 0.44927263259887695\n",
            "Epoch: 12 Batch Number: 58 Loss: 1.1700160503387451 Time taken: 0.4476771354675293\n",
            "Epoch: 12 Batch Number: 59 Loss: 1.1978622674942017 Time taken: 0.44715023040771484\n",
            "Epoch: 12 Batch Number: 60 Loss: 1.1887798309326172 Time taken: 0.44144725799560547\n",
            "Epoch: 12 Batch Number: 61 Loss: 1.2280471324920654 Time taken: 0.44568562507629395\n",
            "Epoch: 12 Batch Number: 62 Loss: 1.135087251663208 Time taken: 0.44315218925476074\n",
            "Epoch: 12 Batch Number: 63 Loss: 1.1543176174163818 Time taken: 0.45886778831481934\n",
            "Epoch: 12 Batch Number: 64 Loss: 1.1009151935577393 Time taken: 0.4449191093444824\n",
            "Epoch: 12 Batch Number: 65 Loss: 1.1425862312316895 Time taken: 0.43997836112976074\n",
            "Epoch: 12 Batch Number: 66 Loss: 1.1652826070785522 Time taken: 0.4464428424835205\n",
            "Epoch: 12 Batch Number: 67 Loss: 1.1510432958602905 Time taken: 0.4514026641845703\n",
            "Epoch: 12 Batch Number: 68 Loss: 1.2186343669891357 Time taken: 0.44146060943603516\n",
            "Epoch: 12 Batch Number: 69 Loss: 1.1944350004196167 Time taken: 0.44151759147644043\n",
            "Epoch: 12 Batch Number: 70 Loss: 1.196978211402893 Time taken: 0.45797181129455566\n",
            "Epoch: 12 Batch Number: 71 Loss: 1.1759788990020752 Time taken: 0.4416022300720215\n",
            "Epoch: 12 Batch Number: 72 Loss: 1.1974557638168335 Time taken: 0.4526786804199219\n",
            "Epoch: 12 Batch Number: 73 Loss: 1.1305203437805176 Time taken: 0.4425387382507324\n",
            "Epoch: 12 Batch Number: 74 Loss: 1.2399394512176514 Time taken: 0.44651341438293457\n",
            "Epoch: 12 Batch Number: 75 Loss: 1.0862536430358887 Time taken: 0.44313836097717285\n",
            "Epoch: 12 Batch Number: 76 Loss: 1.128372073173523 Time taken: 0.4415547847747803\n",
            "Epoch: 12 Batch Number: 77 Loss: 1.2190659046173096 Time taken: 0.46035075187683105\n",
            "Epoch: 12 Batch Number: 78 Loss: 1.1055692434310913 Time taken: 0.45980310440063477\n",
            "Epoch: 12 Batch Number: 79 Loss: 1.1891478300094604 Time taken: 0.4655029773712158\n",
            "Epoch: 12 Batch Number: 80 Loss: 1.1658072471618652 Time taken: 0.44289255142211914\n",
            "Epoch: 12 Batch Number: 81 Loss: 1.182054042816162 Time taken: 0.45577096939086914\n",
            "Epoch: 12 Batch Number: 82 Loss: 1.1934168338775635 Time taken: 0.4422130584716797\n",
            "Epoch: 12 Batch Number: 83 Loss: 1.2537438869476318 Time taken: 0.437772274017334\n",
            "Epoch: 12 Batch Number: 84 Loss: 1.208266019821167 Time taken: 0.4502298831939697\n",
            "Epoch: 12 Batch Number: 85 Loss: 1.1764798164367676 Time taken: 0.45514345169067383\n",
            "Epoch: 12 Batch Number: 86 Loss: 1.1811059713363647 Time taken: 0.4518439769744873\n",
            "Epoch: 12 Batch Number: 87 Loss: 1.258163332939148 Time taken: 0.44122886657714844\n",
            "Epoch: 12 Batch Number: 88 Loss: 1.1565477848052979 Time taken: 0.448352575302124\n",
            "Epoch: 12 Batch Number: 89 Loss: 1.1297541856765747 Time taken: 0.44624996185302734\n",
            "Epoch: 12 Batch Number: 90 Loss: 1.1843626499176025 Time taken: 0.4719078540802002\n",
            "Epoch: 12 Batch Number: 91 Loss: 1.1526169776916504 Time taken: 0.45566654205322266\n",
            "Epoch: 12 Batch Number: 92 Loss: 1.1169092655181885 Time taken: 0.4828040599822998\n",
            "Epoch: 12 Batch Number: 93 Loss: 1.1288139820098877 Time taken: 0.45929598808288574\n",
            "Epoch: 12 Batch Number: 94 Loss: 1.269359827041626 Time taken: 0.4522426128387451\n",
            "Epoch: 12 Batch Number: 95 Loss: 1.2286827564239502 Time taken: 0.46374034881591797\n",
            "Epoch: 12 Batch Number: 96 Loss: 1.2674765586853027 Time taken: 0.4540233612060547\n",
            "Epoch: 12 Batch Number: 97 Loss: 1.2169135808944702 Time taken: 0.44026708602905273\n",
            "Epoch: 12 Batch Number: 98 Loss: 1.2688199281692505 Time taken: 0.4581174850463867\n",
            "Epoch: 12 Batch Number: 99 Loss: 1.295633316040039 Time taken: 0.47586679458618164\n",
            "Epoch: 12 Batch Number: 100 Loss: 1.2165474891662598 Time taken: 0.4468872547149658\n",
            "Epoch: 12 Batch Number: 101 Loss: 1.216422438621521 Time taken: 0.4432094097137451\n",
            "Epoch: 12 Batch Number: 102 Loss: 1.252565622329712 Time taken: 0.4432802200317383\n",
            "Epoch: 12 Batch Number: 103 Loss: 1.317299246788025 Time taken: 0.4492671489715576\n",
            "Epoch: 12 Batch Number: 104 Loss: 1.2065415382385254 Time taken: 0.44191884994506836\n",
            "Epoch: 12 Batch Number: 105 Loss: 1.286049485206604 Time taken: 0.46361398696899414\n",
            "Epoch: 12 Batch Number: 106 Loss: 1.2512304782867432 Time taken: 0.4573476314544678\n",
            "Epoch: 12 Batch Number: 107 Loss: 1.2524621486663818 Time taken: 0.4480016231536865\n",
            "Epoch: 12 Batch Number: 108 Loss: 1.2215369939804077 Time taken: 0.44234561920166016\n",
            "Epoch: 12 Batch Number: 109 Loss: 1.2283560037612915 Time taken: 0.44607090950012207\n",
            "Epoch: 12 Batch Number: 110 Loss: 1.084543228149414 Time taken: 0.43459177017211914\n",
            "Epoch: 12 Batch Number: 111 Loss: 1.1428208351135254 Time taken: 0.4422779083251953\n",
            "Epoch: 12 Batch Number: 112 Loss: 1.156629204750061 Time taken: 0.44454360008239746\n",
            "Epoch: 12 Batch Number: 113 Loss: 1.1924207210540771 Time taken: 0.4435286521911621\n",
            "Epoch: 12 Batch Number: 114 Loss: 1.2322505712509155 Time taken: 0.45954442024230957\n",
            "Epoch: 12 Batch Number: 115 Loss: 1.227359414100647 Time taken: 0.44852137565612793\n",
            "Epoch: 12 Batch Number: 116 Loss: 1.0922224521636963 Time taken: 0.44620752334594727\n",
            "Epoch: 12 Batch Number: 117 Loss: 1.2046411037445068 Time taken: 0.43799829483032227\n",
            "Epoch: 12 Batch Number: 118 Loss: 1.1545944213867188 Time taken: 0.43872666358947754\n",
            "Epoch: 12 Batch Number: 119 Loss: 1.184919834136963 Time taken: 0.4439809322357178\n",
            "Epoch: 12 Batch Number: 120 Loss: 1.2274973392486572 Time taken: 0.44306182861328125\n",
            "Epoch: 12 Batch Number: 121 Loss: 1.1672320365905762 Time taken: 0.44678449630737305\n",
            "Epoch: 12 Batch Number: 122 Loss: 1.1577892303466797 Time taken: 0.456662654876709\n",
            "Epoch: 12 Batch Number: 123 Loss: 1.1910016536712646 Time taken: 0.44579577445983887\n",
            "Epoch: 12 Batch Number: 124 Loss: 1.1665561199188232 Time taken: 0.44293999671936035\n",
            "Epoch: 12 Batch Number: 125 Loss: 1.1996876001358032 Time taken: 0.4422321319580078\n",
            "Epoch: 12 Batch Number: 126 Loss: 1.222929835319519 Time taken: 0.44635915756225586\n",
            "Epoch: 12 Batch Number: 127 Loss: 1.2229220867156982 Time taken: 0.43929386138916016\n",
            "Epoch: 12 Batch Number: 128 Loss: 1.1881948709487915 Time taken: 0.44692063331604004\n",
            "Epoch: 12 Batch Number: 129 Loss: 1.1744292974472046 Time taken: 0.4412522315979004\n",
            "Epoch: 12 Batch Number: 130 Loss: 1.1374578475952148 Time taken: 0.44316720962524414\n",
            "Epoch: 12 Batch Number: 131 Loss: 1.2286661863327026 Time taken: 0.4423103332519531\n",
            "Epoch: 12 Batch Number: 132 Loss: 1.0782535076141357 Time taken: 0.4451303482055664\n",
            "Epoch: 12 Batch Number: 133 Loss: 1.234083652496338 Time taken: 0.44033193588256836\n",
            "Epoch: 12 Batch Number: 134 Loss: 1.2464255094528198 Time taken: 0.44686102867126465\n",
            "Epoch: 12 Batch Number: 135 Loss: 1.249985694885254 Time taken: 0.44420409202575684\n",
            "Epoch: 12 Batch Number: 136 Loss: 1.2405235767364502 Time taken: 0.44525671005249023\n",
            "Epoch: 12 Batch Number: 137 Loss: 1.2010303735733032 Time taken: 0.4509119987487793\n",
            "Epoch: 12 Batch Number: 138 Loss: 1.2267024517059326 Time taken: 0.4456486701965332\n",
            "Epoch: 12 Batch Number: 139 Loss: 1.2070621252059937 Time taken: 0.44957995414733887\n",
            "Epoch: 12 Batch Number: 140 Loss: 1.1789960861206055 Time taken: 0.44071054458618164\n",
            "Epoch: 12 Batch Number: 141 Loss: 1.2353845834732056 Time taken: 0.447406530380249\n",
            "Epoch: 12 Batch Number: 142 Loss: 1.2758126258850098 Time taken: 0.4440898895263672\n",
            "Epoch: 12 Batch Number: 143 Loss: 1.3470313549041748 Time taken: 0.45846033096313477\n",
            "Epoch: 12 Batch Number: 144 Loss: 1.3508353233337402 Time taken: 0.4482436180114746\n",
            "Epoch: 12 Batch Number: 145 Loss: 1.3211703300476074 Time taken: 0.4582490921020508\n",
            "Epoch: 12 Batch Number: 146 Loss: 1.3119263648986816 Time taken: 0.4826045036315918\n",
            "Epoch: 12 Batch Number: 147 Loss: 1.1742439270019531 Time taken: 0.4432809352874756\n",
            "Epoch: 12 Batch Number: 148 Loss: 1.1424740552902222 Time taken: 0.4506654739379883\n",
            "Epoch: 12 Batch Number: 149 Loss: 1.227329134941101 Time taken: 0.4430069923400879\n",
            "Epoch: 12 Batch Number: 150 Loss: 1.3183722496032715 Time taken: 0.45797204971313477\n",
            "Epoch: 12 Batch Number: 151 Loss: 1.3206943273544312 Time taken: 0.459303617477417\n",
            "Epoch: 12 Batch Number: 152 Loss: 1.2458484172821045 Time taken: 0.45752382278442383\n",
            "Epoch: 12 Batch Number: 153 Loss: 1.1775226593017578 Time taken: 0.46395111083984375\n",
            "Epoch: 12 Batch Number: 154 Loss: 1.1709109544754028 Time taken: 0.4584035873413086\n",
            "Epoch: 12 Batch Number: 155 Loss: 1.166316270828247 Time taken: 0.45241379737854004\n",
            "Epoch: 12 Batch Number: 156 Loss: 1.2176250219345093 Time taken: 0.45139551162719727\n",
            "Epoch: 12 Batch Number: 157 Loss: 1.1065657138824463 Time taken: 0.45715832710266113\n",
            "Epoch: 12 Batch Number: 158 Loss: 1.158466100692749 Time taken: 0.4675428867340088\n",
            "Epoch: 12 Batch Number: 159 Loss: 1.1042985916137695 Time taken: 0.4605526924133301\n",
            "Epoch: 12 Batch Number: 160 Loss: 1.008586049079895 Time taken: 0.46677088737487793\n",
            "Epoch: 12 Batch Number: 161 Loss: 1.0819129943847656 Time taken: 0.4755270481109619\n",
            "Epoch: 12 Batch Number: 162 Loss: 1.0576421022415161 Time taken: 0.46288299560546875\n",
            "Epoch: 12 Batch Number: 163 Loss: 1.1641592979431152 Time taken: 0.4597163200378418\n",
            "Epoch: 12 Batch Number: 164 Loss: 1.2580511569976807 Time taken: 0.45340728759765625\n",
            "Epoch: 12 Batch Number: 165 Loss: 1.1744341850280762 Time taken: 0.46106791496276855\n",
            "Epoch: 12 Batch Number: 166 Loss: 1.2142665386199951 Time taken: 0.46879148483276367\n",
            "Epoch: 12 Batch Number: 167 Loss: 1.194483995437622 Time taken: 0.45800137519836426\n",
            "Epoch: 12 Batch Number: 168 Loss: 1.1943219900131226 Time taken: 0.464796781539917\n",
            "Epoch: 12 Batch Number: 169 Loss: 1.1735903024673462 Time taken: 0.45789670944213867\n",
            "Epoch: 12 Batch Number: 170 Loss: 1.1023569107055664 Time taken: 0.45708346366882324\n",
            "Epoch: 12 Batch Number: 171 Loss: 1.0614840984344482 Time taken: 0.4604964256286621\n",
            "Epoch: 12 Batch Number: 172 Loss: 1.1180033683776855 Time taken: 0.44368672370910645\n",
            "Epoch: 12 Batch Number: 173 Loss: 1.1005637645721436 Time taken: 0.43985414505004883\n",
            "Epoch: 12 Batch Number: 174 Loss: 1.0482070446014404 Time taken: 0.4386715888977051\n",
            "Epoch: 12 Batch Number: 175 Loss: 1.1697925329208374 Time taken: 0.4483211040496826\n",
            "Epoch: 12 Batch Number: 176 Loss: 1.1503899097442627 Time taken: 0.4396355152130127\n",
            "Epoch: 12 Batch Number: 177 Loss: 1.1665072441101074 Time taken: 0.4598228931427002\n",
            "Epoch: 12 Batch Number: 178 Loss: 1.2219367027282715 Time taken: 0.46085333824157715\n",
            "Epoch: 12 Batch Number: 179 Loss: 1.2119255065917969 Time taken: 0.4448425769805908\n",
            "Epoch: 12 Batch Number: 180 Loss: 1.1791878938674927 Time taken: 0.43872690200805664\n",
            "Epoch: 12 Batch Number: 181 Loss: 1.142467737197876 Time taken: 0.4586153030395508\n",
            "Epoch: 12 Batch Number: 182 Loss: 1.088597297668457 Time taken: 0.4606969356536865\n",
            "Epoch: 12 Batch Number: 183 Loss: 1.1404051780700684 Time taken: 0.4531691074371338\n",
            "Epoch: 12 Batch Number: 184 Loss: 1.2065390348434448 Time taken: 0.43707752227783203\n",
            "Epoch: 12 Batch Number: 185 Loss: 1.2331035137176514 Time taken: 0.4502866268157959\n",
            "Epoch: 12 Batch Number: 186 Loss: 1.0267131328582764 Time taken: 0.4488484859466553\n",
            "Epoch: 12 Batch Number: 187 Loss: 1.1137490272521973 Time taken: 0.44103384017944336\n",
            "Epoch: 12 Batch Number: 188 Loss: 1.1243295669555664 Time taken: 0.4552781581878662\n",
            "Epoch: 12 Batch Number: 189 Loss: 1.158703088760376 Time taken: 0.4646439552307129\n",
            "Epoch: 12 Batch Number: 190 Loss: 1.3434759378433228 Time taken: 0.44847679138183594\n",
            "Epoch: 12 Batch Number: 191 Loss: 1.5226738452911377 Time taken: 0.4390833377838135\n",
            "Epoch: 12 Batch Number: 192 Loss: 1.229546308517456 Time taken: 0.45105934143066406\n",
            "Epoch: 12 Batch Number: 193 Loss: 1.3480130434036255 Time taken: 0.44306159019470215\n",
            "Epoch: 12 Batch Number: 194 Loss: 1.2610359191894531 Time taken: 0.4569880962371826\n",
            "Epoch: 12 Batch Number: 195 Loss: 1.196396827697754 Time taken: 0.45192742347717285\n",
            "Epoch: 12 Batch Number: 196 Loss: 1.1478683948516846 Time taken: 0.44411802291870117\n",
            "Epoch: 12 Batch Number: 197 Loss: 1.2244116067886353 Time taken: 0.446211576461792\n",
            "Epoch: 12 Batch Number: 198 Loss: 1.1430965662002563 Time taken: 0.4394350051879883\n",
            "Epoch: 12 Batch Number: 199 Loss: 1.1793900728225708 Time taken: 0.4501349925994873\n",
            "Epoch: 12 Batch Number: 200 Loss: 1.1315425634384155 Time taken: 0.43883180618286133\n",
            "Epoch: 12 Batch Number: 201 Loss: 1.1825518608093262 Time taken: 0.461702823638916\n",
            "Epoch: 12 Batch Number: 202 Loss: 1.1638433933258057 Time taken: 0.45284390449523926\n",
            "Epoch: 12 Batch Number: 203 Loss: 1.1646233797073364 Time taken: 0.449399471282959\n",
            "Epoch: 12 Batch Number: 204 Loss: 1.187276840209961 Time taken: 0.44713425636291504\n",
            "Epoch: 12 Batch Number: 205 Loss: 1.1178951263427734 Time taken: 0.4373135566711426\n",
            "Epoch: 12 Batch Number: 206 Loss: 1.1736644506454468 Time taken: 0.44086623191833496\n",
            "Epoch: 12 Batch Number: 207 Loss: 1.182187795639038 Time taken: 0.45305562019348145\n",
            "Epoch: 12 Batch Number: 208 Loss: 1.1502668857574463 Time taken: 0.45269322395324707\n",
            "Epoch: 12 Batch Number: 209 Loss: 1.1936300992965698 Time taken: 0.43666577339172363\n",
            "Epoch: 12 Batch Number: 210 Loss: 1.2493964433670044 Time taken: 0.4425334930419922\n",
            "Epoch: 12 Batch Number: 211 Loss: 1.2166991233825684 Time taken: 0.4410700798034668\n",
            "Epoch: 12 Batch Number: 212 Loss: 1.3384277820587158 Time taken: 0.4456055164337158\n",
            "Epoch: 12 Batch Number: 213 Loss: 1.1762583255767822 Time taken: 0.44449329376220703\n",
            "Epoch: 12 Batch Number: 214 Loss: 1.2300450801849365 Time taken: 0.4478731155395508\n",
            "Epoch: 12 Batch Number: 215 Loss: 1.179100751876831 Time taken: 0.43933653831481934\n",
            "Epoch: 12 Batch Number: 216 Loss: 1.216761589050293 Time taken: 0.44391584396362305\n",
            "Epoch: 12 Batch Number: 217 Loss: 1.2841500043869019 Time taken: 0.45169878005981445\n",
            "Epoch: 12 Batch Number: 218 Loss: 1.273294448852539 Time taken: 0.43816566467285156\n",
            "Epoch: 12 Batch Number: 219 Loss: 1.2683268785476685 Time taken: 0.45543575286865234\n",
            "Epoch: 12 Batch Number: 220 Loss: 1.196465253829956 Time taken: 0.4432523250579834\n",
            "Epoch: 12 Batch Number: 221 Loss: 1.1886162757873535 Time taken: 0.4603090286254883\n",
            "Epoch: 12 Batch Number: 222 Loss: 1.1634750366210938 Time taken: 0.4558291435241699\n",
            "Epoch: 12 Batch Number: 223 Loss: 1.1989822387695312 Time taken: 0.4566977024078369\n",
            "Epoch: 12 Batch Number: 224 Loss: 1.1457685232162476 Time taken: 0.46438121795654297\n",
            "Epoch: 12 Batch Number: 225 Loss: 1.1161643266677856 Time taken: 0.45853590965270996\n",
            "Epoch: 12 Batch Number: 226 Loss: 1.1537665128707886 Time taken: 0.45700764656066895\n",
            "Epoch: 12 Batch Number: 227 Loss: 1.2820249795913696 Time taken: 0.46475982666015625\n",
            "Epoch: 12 Batch Number: 228 Loss: 1.2441853284835815 Time taken: 0.45510268211364746\n",
            "Epoch: 12 Batch Number: 229 Loss: 1.2588361501693726 Time taken: 0.4531726837158203\n",
            "==========================================================================================\n",
            "Start of epoch 13\n",
            "Epoch: 13 Batch Number: 1 Loss: 1.1809852123260498 Time taken: 0.4641871452331543\n",
            "Epoch: 13 Batch Number: 2 Loss: 1.242037296295166 Time taken: 0.4447824954986572\n",
            "Epoch: 13 Batch Number: 3 Loss: 1.1964436769485474 Time taken: 0.44769978523254395\n",
            "Epoch: 13 Batch Number: 4 Loss: 1.1776691675186157 Time taken: 0.44318389892578125\n",
            "Epoch: 13 Batch Number: 5 Loss: 1.1564568281173706 Time taken: 0.44261789321899414\n",
            "Epoch: 13 Batch Number: 6 Loss: 1.102477788925171 Time taken: 0.44087696075439453\n",
            "Epoch: 13 Batch Number: 7 Loss: 1.128312587738037 Time taken: 0.4400315284729004\n",
            "Epoch: 13 Batch Number: 8 Loss: 1.1447679996490479 Time taken: 0.44922447204589844\n",
            "Epoch: 13 Batch Number: 9 Loss: 1.134230613708496 Time taken: 0.45612430572509766\n",
            "Epoch: 13 Batch Number: 10 Loss: 1.168534278869629 Time taken: 0.45629239082336426\n",
            "Epoch: 13 Batch Number: 11 Loss: 1.1277029514312744 Time taken: 0.4443373680114746\n",
            "Epoch: 13 Batch Number: 12 Loss: 1.0869081020355225 Time taken: 0.4486656188964844\n",
            "Epoch: 13 Batch Number: 13 Loss: 1.125273585319519 Time taken: 0.44152283668518066\n",
            "Epoch: 13 Batch Number: 14 Loss: 1.133242130279541 Time taken: 0.4361248016357422\n",
            "Epoch: 13 Batch Number: 15 Loss: 1.072893500328064 Time taken: 0.4430093765258789\n",
            "Epoch: 13 Batch Number: 16 Loss: 1.1201872825622559 Time taken: 0.447986364364624\n",
            "Epoch: 13 Batch Number: 17 Loss: 1.237169623374939 Time taken: 0.4517397880554199\n",
            "Epoch: 13 Batch Number: 18 Loss: 1.2176196575164795 Time taken: 0.4443857669830322\n",
            "Epoch: 13 Batch Number: 19 Loss: 1.2154884338378906 Time taken: 0.4400489330291748\n",
            "Epoch: 13 Batch Number: 20 Loss: 1.0916699171066284 Time taken: 0.443178653717041\n",
            "Epoch: 13 Batch Number: 21 Loss: 1.2891781330108643 Time taken: 0.44794154167175293\n",
            "Epoch: 13 Batch Number: 22 Loss: 1.2714128494262695 Time taken: 0.463944673538208\n",
            "Epoch: 13 Batch Number: 23 Loss: 1.273917555809021 Time taken: 0.46078038215637207\n",
            "Epoch: 13 Batch Number: 24 Loss: 1.2420032024383545 Time taken: 0.4482405185699463\n",
            "Epoch: 13 Batch Number: 25 Loss: 1.2286041975021362 Time taken: 0.4546658992767334\n",
            "Epoch: 13 Batch Number: 26 Loss: 1.234587550163269 Time taken: 0.4475715160369873\n",
            "Epoch: 13 Batch Number: 27 Loss: 1.1756232976913452 Time taken: 0.45354628562927246\n",
            "Epoch: 13 Batch Number: 28 Loss: 1.1944022178649902 Time taken: 0.45842695236206055\n",
            "Epoch: 13 Batch Number: 29 Loss: 1.2297239303588867 Time taken: 0.44351863861083984\n",
            "Epoch: 13 Batch Number: 30 Loss: 0.996731162071228 Time taken: 0.45540285110473633\n",
            "Epoch: 13 Batch Number: 31 Loss: 1.1614108085632324 Time taken: 0.44545984268188477\n",
            "Epoch: 13 Batch Number: 32 Loss: 1.1678543090820312 Time taken: 0.44805264472961426\n",
            "Epoch: 13 Batch Number: 33 Loss: 1.195756196975708 Time taken: 0.46361732482910156\n",
            "Epoch: 13 Batch Number: 34 Loss: 1.2212660312652588 Time taken: 0.4448518753051758\n",
            "Epoch: 13 Batch Number: 35 Loss: 1.2690399885177612 Time taken: 0.4369699954986572\n",
            "Epoch: 13 Batch Number: 36 Loss: 1.3111788034439087 Time taken: 0.44339942932128906\n",
            "Epoch: 13 Batch Number: 37 Loss: 1.1802592277526855 Time taken: 0.4512600898742676\n",
            "Epoch: 13 Batch Number: 38 Loss: 1.2305477857589722 Time taken: 0.44619226455688477\n",
            "Epoch: 13 Batch Number: 39 Loss: 1.17528235912323 Time taken: 0.46328091621398926\n",
            "Epoch: 13 Batch Number: 40 Loss: 1.1854948997497559 Time taken: 0.4476006031036377\n",
            "Epoch: 13 Batch Number: 41 Loss: 1.15141761302948 Time taken: 0.44309353828430176\n",
            "Epoch: 13 Batch Number: 42 Loss: 1.1433682441711426 Time taken: 0.44459986686706543\n",
            "Epoch: 13 Batch Number: 43 Loss: 1.133876085281372 Time taken: 0.4460725784301758\n",
            "Epoch: 13 Batch Number: 44 Loss: 1.0851342678070068 Time taken: 0.44762158393859863\n",
            "Epoch: 13 Batch Number: 45 Loss: 1.14385187625885 Time taken: 0.44597578048706055\n",
            "Epoch: 13 Batch Number: 46 Loss: 1.303788185119629 Time taken: 0.45062828063964844\n",
            "Epoch: 13 Batch Number: 47 Loss: 1.168331265449524 Time taken: 0.4438302516937256\n",
            "Epoch: 13 Batch Number: 48 Loss: 1.2367393970489502 Time taken: 0.4408700466156006\n",
            "Epoch: 13 Batch Number: 49 Loss: 1.2339783906936646 Time taken: 0.4424419403076172\n",
            "Epoch: 13 Batch Number: 50 Loss: 1.1363072395324707 Time taken: 0.4391469955444336\n",
            "Epoch: 13 Batch Number: 51 Loss: 1.1329340934753418 Time taken: 0.45316553115844727\n",
            "Epoch: 13 Batch Number: 52 Loss: 1.2468562126159668 Time taken: 0.43788647651672363\n",
            "Epoch: 13 Batch Number: 53 Loss: 1.2887367010116577 Time taken: 0.46798276901245117\n",
            "Epoch: 13 Batch Number: 54 Loss: 1.261534333229065 Time taken: 0.44539332389831543\n",
            "Epoch: 13 Batch Number: 55 Loss: 1.2197399139404297 Time taken: 0.45059704780578613\n",
            "Epoch: 13 Batch Number: 56 Loss: 1.3119221925735474 Time taken: 0.43610358238220215\n",
            "Epoch: 13 Batch Number: 57 Loss: 1.1956281661987305 Time taken: 0.4530186653137207\n",
            "Epoch: 13 Batch Number: 58 Loss: 1.1555900573730469 Time taken: 0.45368409156799316\n",
            "Epoch: 13 Batch Number: 59 Loss: 1.1836851835250854 Time taken: 0.44870471954345703\n",
            "Epoch: 13 Batch Number: 60 Loss: 1.1714468002319336 Time taken: 0.4524524211883545\n",
            "Epoch: 13 Batch Number: 61 Loss: 1.2120884656906128 Time taken: 0.4517219066619873\n",
            "Epoch: 13 Batch Number: 62 Loss: 1.117527961730957 Time taken: 0.45648670196533203\n",
            "Epoch: 13 Batch Number: 63 Loss: 1.1408637762069702 Time taken: 0.43454885482788086\n",
            "Epoch: 13 Batch Number: 64 Loss: 1.0921900272369385 Time taken: 0.4469907283782959\n",
            "Epoch: 13 Batch Number: 65 Loss: 1.1297824382781982 Time taken: 0.43964433670043945\n",
            "Epoch: 13 Batch Number: 66 Loss: 1.1510961055755615 Time taken: 0.4396216869354248\n",
            "Epoch: 13 Batch Number: 67 Loss: 1.1361581087112427 Time taken: 0.44581103324890137\n",
            "Epoch: 13 Batch Number: 68 Loss: 1.2023811340332031 Time taken: 0.4391365051269531\n",
            "Epoch: 13 Batch Number: 69 Loss: 1.1757612228393555 Time taken: 0.4524848461151123\n",
            "Epoch: 13 Batch Number: 70 Loss: 1.1836802959442139 Time taken: 0.4435873031616211\n",
            "Epoch: 13 Batch Number: 71 Loss: 1.158083200454712 Time taken: 0.45288872718811035\n",
            "Epoch: 13 Batch Number: 72 Loss: 1.1811306476593018 Time taken: 0.4561910629272461\n",
            "Epoch: 13 Batch Number: 73 Loss: 1.1156560182571411 Time taken: 0.4453864097595215\n",
            "Epoch: 13 Batch Number: 74 Loss: 1.2239880561828613 Time taken: 0.44103264808654785\n",
            "Epoch: 13 Batch Number: 75 Loss: 1.065612554550171 Time taken: 0.4478118419647217\n",
            "Epoch: 13 Batch Number: 76 Loss: 1.1065032482147217 Time taken: 0.44984889030456543\n",
            "Epoch: 13 Batch Number: 77 Loss: 1.2009071111679077 Time taken: 0.4510972499847412\n",
            "Epoch: 13 Batch Number: 78 Loss: 1.0903211832046509 Time taken: 0.44527721405029297\n",
            "Epoch: 13 Batch Number: 79 Loss: 1.1703728437423706 Time taken: 0.4463231563568115\n",
            "Epoch: 13 Batch Number: 80 Loss: 1.1461143493652344 Time taken: 0.44644880294799805\n",
            "Epoch: 13 Batch Number: 81 Loss: 1.1641197204589844 Time taken: 0.43842458724975586\n",
            "Epoch: 13 Batch Number: 82 Loss: 1.1654858589172363 Time taken: 0.44638562202453613\n",
            "Epoch: 13 Batch Number: 83 Loss: 1.229672908782959 Time taken: 0.4568326473236084\n",
            "Epoch: 13 Batch Number: 84 Loss: 1.1885554790496826 Time taken: 0.43993091583251953\n",
            "Epoch: 13 Batch Number: 85 Loss: 1.156449317932129 Time taken: 0.4449493885040283\n",
            "Epoch: 13 Batch Number: 86 Loss: 1.1583404541015625 Time taken: 0.44312238693237305\n",
            "Epoch: 13 Batch Number: 87 Loss: 1.2402186393737793 Time taken: 0.44447803497314453\n",
            "Epoch: 13 Batch Number: 88 Loss: 1.201579213142395 Time taken: 0.4433131217956543\n",
            "Epoch: 13 Batch Number: 89 Loss: 1.1140493154525757 Time taken: 0.4635336399078369\n",
            "Epoch: 13 Batch Number: 90 Loss: 1.17238450050354 Time taken: 0.44011402130126953\n",
            "Epoch: 13 Batch Number: 91 Loss: 1.1302024126052856 Time taken: 0.46117734909057617\n",
            "Epoch: 13 Batch Number: 92 Loss: 1.1044875383377075 Time taken: 0.44075751304626465\n",
            "Epoch: 13 Batch Number: 93 Loss: 1.1117727756500244 Time taken: 0.4491844177246094\n",
            "Epoch: 13 Batch Number: 94 Loss: 1.2478349208831787 Time taken: 0.44264698028564453\n",
            "Epoch: 13 Batch Number: 95 Loss: 1.2095578908920288 Time taken: 0.4622335433959961\n",
            "Epoch: 13 Batch Number: 96 Loss: 1.2493101358413696 Time taken: 0.45909547805786133\n",
            "Epoch: 13 Batch Number: 97 Loss: 1.2022705078125 Time taken: 0.4600059986114502\n",
            "Epoch: 13 Batch Number: 98 Loss: 1.2533965110778809 Time taken: 0.44016242027282715\n",
            "Epoch: 13 Batch Number: 99 Loss: 1.2814030647277832 Time taken: 0.4573202133178711\n",
            "Epoch: 13 Batch Number: 100 Loss: 1.2016100883483887 Time taken: 0.4674386978149414\n",
            "Epoch: 13 Batch Number: 101 Loss: 1.200557827949524 Time taken: 0.45972514152526855\n",
            "Epoch: 13 Batch Number: 102 Loss: 1.2378814220428467 Time taken: 0.4608266353607178\n",
            "Epoch: 13 Batch Number: 103 Loss: 1.3010694980621338 Time taken: 0.4486234188079834\n",
            "Epoch: 13 Batch Number: 104 Loss: 1.189476490020752 Time taken: 0.44299840927124023\n",
            "Epoch: 13 Batch Number: 105 Loss: 1.2719265222549438 Time taken: 0.44252943992614746\n",
            "Epoch: 13 Batch Number: 106 Loss: 1.2343478202819824 Time taken: 0.45877718925476074\n",
            "Epoch: 13 Batch Number: 107 Loss: 1.2367515563964844 Time taken: 0.46303677558898926\n",
            "Epoch: 13 Batch Number: 108 Loss: 1.2028461694717407 Time taken: 0.45907092094421387\n",
            "Epoch: 13 Batch Number: 109 Loss: 1.2084343433380127 Time taken: 0.44379258155822754\n",
            "Epoch: 13 Batch Number: 110 Loss: 1.0593383312225342 Time taken: 0.44468259811401367\n",
            "Epoch: 13 Batch Number: 111 Loss: 1.1271562576293945 Time taken: 0.45540571212768555\n",
            "Epoch: 13 Batch Number: 112 Loss: 1.1380536556243896 Time taken: 0.4434354305267334\n",
            "Epoch: 13 Batch Number: 113 Loss: 1.1744768619537354 Time taken: 0.4484744071960449\n",
            "Epoch: 13 Batch Number: 114 Loss: 1.218313455581665 Time taken: 0.4395148754119873\n",
            "Epoch: 13 Batch Number: 115 Loss: 1.214242696762085 Time taken: 0.45376062393188477\n",
            "Epoch: 13 Batch Number: 116 Loss: 1.079215168952942 Time taken: 0.4524109363555908\n",
            "Epoch: 13 Batch Number: 117 Loss: 1.1865057945251465 Time taken: 0.4531552791595459\n",
            "Epoch: 13 Batch Number: 118 Loss: 1.1403459310531616 Time taken: 0.44524049758911133\n",
            "Epoch: 13 Batch Number: 119 Loss: 1.1723358631134033 Time taken: 0.44165563583374023\n",
            "Epoch: 13 Batch Number: 120 Loss: 1.2119762897491455 Time taken: 0.4570772647857666\n",
            "Epoch: 13 Batch Number: 121 Loss: 1.1491456031799316 Time taken: 0.44118762016296387\n",
            "Epoch: 13 Batch Number: 122 Loss: 1.1375641822814941 Time taken: 0.46813392639160156\n",
            "Epoch: 13 Batch Number: 123 Loss: 1.1730659008026123 Time taken: 0.45190000534057617\n",
            "Epoch: 13 Batch Number: 124 Loss: 1.1388347148895264 Time taken: 0.442202091217041\n",
            "Epoch: 13 Batch Number: 125 Loss: 1.1831504106521606 Time taken: 0.45223212242126465\n",
            "Epoch: 13 Batch Number: 126 Loss: 1.20798659324646 Time taken: 0.4478466510772705\n",
            "Epoch: 13 Batch Number: 127 Loss: 1.2060155868530273 Time taken: 0.4625585079193115\n",
            "Epoch: 13 Batch Number: 128 Loss: 1.1630980968475342 Time taken: 0.4475064277648926\n",
            "Epoch: 13 Batch Number: 129 Loss: 1.1558756828308105 Time taken: 0.4415414333343506\n",
            "Epoch: 13 Batch Number: 130 Loss: 1.1238967180252075 Time taken: 0.4469141960144043\n",
            "Epoch: 13 Batch Number: 131 Loss: 1.2100650072097778 Time taken: 0.44086313247680664\n",
            "Epoch: 13 Batch Number: 132 Loss: 1.0599801540374756 Time taken: 0.4571559429168701\n",
            "Epoch: 13 Batch Number: 133 Loss: 1.2147812843322754 Time taken: 0.4379749298095703\n",
            "Epoch: 13 Batch Number: 134 Loss: 1.2253843545913696 Time taken: 0.4481360912322998\n",
            "Epoch: 13 Batch Number: 135 Loss: 1.2302021980285645 Time taken: 0.444199800491333\n",
            "Epoch: 13 Batch Number: 136 Loss: 1.2215516567230225 Time taken: 0.44622373580932617\n",
            "Epoch: 13 Batch Number: 137 Loss: 1.1811202764511108 Time taken: 0.4382438659667969\n",
            "Epoch: 13 Batch Number: 138 Loss: 1.2091889381408691 Time taken: 0.45124340057373047\n",
            "Epoch: 13 Batch Number: 139 Loss: 1.2151131629943848 Time taken: 0.43724489212036133\n",
            "Epoch: 13 Batch Number: 140 Loss: 1.1637697219848633 Time taken: 0.4429779052734375\n",
            "Epoch: 13 Batch Number: 141 Loss: 1.218552589416504 Time taken: 0.4443812370300293\n",
            "Epoch: 13 Batch Number: 142 Loss: 1.2648171186447144 Time taken: 0.44291234016418457\n",
            "Epoch: 13 Batch Number: 143 Loss: 1.3281521797180176 Time taken: 0.46222829818725586\n",
            "Epoch: 13 Batch Number: 144 Loss: 1.3328015804290771 Time taken: 0.4473252296447754\n",
            "Epoch: 13 Batch Number: 145 Loss: 1.3110705614089966 Time taken: 0.4585764408111572\n",
            "Epoch: 13 Batch Number: 146 Loss: 1.2894694805145264 Time taken: 0.4692108631134033\n",
            "Epoch: 13 Batch Number: 147 Loss: 1.1607348918914795 Time taken: 0.4514884948730469\n",
            "Epoch: 13 Batch Number: 148 Loss: 1.1253390312194824 Time taken: 0.4440724849700928\n",
            "Epoch: 13 Batch Number: 149 Loss: 1.212411880493164 Time taken: 0.4482393264770508\n",
            "Epoch: 13 Batch Number: 150 Loss: 1.3021979331970215 Time taken: 0.4546053409576416\n",
            "Epoch: 13 Batch Number: 151 Loss: 1.302878499031067 Time taken: 0.4483635425567627\n",
            "Epoch: 13 Batch Number: 152 Loss: 1.230883002281189 Time taken: 0.44500207901000977\n",
            "Epoch: 13 Batch Number: 153 Loss: 1.1608281135559082 Time taken: 0.4437413215637207\n",
            "Epoch: 13 Batch Number: 154 Loss: 1.150439739227295 Time taken: 0.46285343170166016\n",
            "Epoch: 13 Batch Number: 155 Loss: 1.1509628295898438 Time taken: 0.44562721252441406\n",
            "Epoch: 13 Batch Number: 156 Loss: 1.2056803703308105 Time taken: 0.4529001712799072\n",
            "Epoch: 13 Batch Number: 157 Loss: 1.094346284866333 Time taken: 0.43722987174987793\n",
            "Epoch: 13 Batch Number: 158 Loss: 1.1521607637405396 Time taken: 0.4575996398925781\n",
            "Epoch: 13 Batch Number: 159 Loss: 1.0928921699523926 Time taken: 0.4465339183807373\n",
            "Epoch: 13 Batch Number: 160 Loss: 1.0020105838775635 Time taken: 0.4428997039794922\n",
            "Epoch: 13 Batch Number: 161 Loss: 1.0726405382156372 Time taken: 0.45316267013549805\n",
            "Epoch: 13 Batch Number: 162 Loss: 1.0478951930999756 Time taken: 0.4710829257965088\n",
            "Epoch: 13 Batch Number: 163 Loss: 1.1522389650344849 Time taken: 0.47367095947265625\n",
            "Epoch: 13 Batch Number: 164 Loss: 1.2509734630584717 Time taken: 0.4490058422088623\n",
            "Epoch: 13 Batch Number: 165 Loss: 1.1618919372558594 Time taken: 0.44034314155578613\n",
            "Epoch: 13 Batch Number: 166 Loss: 1.19840407371521 Time taken: 0.44382667541503906\n",
            "Epoch: 13 Batch Number: 167 Loss: 1.1809771060943604 Time taken: 0.45348358154296875\n",
            "Epoch: 13 Batch Number: 168 Loss: 1.1802401542663574 Time taken: 0.44466209411621094\n",
            "Epoch: 13 Batch Number: 169 Loss: 1.1687977313995361 Time taken: 0.44650864601135254\n",
            "Epoch: 13 Batch Number: 170 Loss: 1.1054874658584595 Time taken: 0.45433926582336426\n",
            "Epoch: 13 Batch Number: 171 Loss: 1.0579214096069336 Time taken: 0.46560168266296387\n",
            "Epoch: 13 Batch Number: 172 Loss: 1.1174906492233276 Time taken: 0.44271349906921387\n",
            "Epoch: 13 Batch Number: 173 Loss: 1.1004408597946167 Time taken: 0.44774699211120605\n",
            "Epoch: 13 Batch Number: 174 Loss: 1.0434144735336304 Time taken: 0.4672689437866211\n",
            "Epoch: 13 Batch Number: 175 Loss: 1.1643528938293457 Time taken: 0.44403553009033203\n",
            "Epoch: 13 Batch Number: 176 Loss: 1.1395666599273682 Time taken: 0.44403815269470215\n",
            "Epoch: 13 Batch Number: 177 Loss: 1.1543647050857544 Time taken: 0.45550060272216797\n",
            "Epoch: 13 Batch Number: 178 Loss: 1.2131407260894775 Time taken: 0.4496040344238281\n",
            "Epoch: 13 Batch Number: 179 Loss: 1.212425947189331 Time taken: 0.44319939613342285\n",
            "Epoch: 13 Batch Number: 180 Loss: 1.163865327835083 Time taken: 0.44242238998413086\n",
            "Epoch: 13 Batch Number: 181 Loss: 1.1309733390808105 Time taken: 0.44802427291870117\n",
            "Epoch: 13 Batch Number: 182 Loss: 1.0764200687408447 Time taken: 0.4598989486694336\n",
            "Epoch: 13 Batch Number: 183 Loss: 1.1316808462142944 Time taken: 0.4544084072113037\n",
            "Epoch: 13 Batch Number: 184 Loss: 1.1952002048492432 Time taken: 0.4590725898742676\n",
            "Epoch: 13 Batch Number: 185 Loss: 1.2170087099075317 Time taken: 0.4444754123687744\n",
            "Epoch: 13 Batch Number: 186 Loss: 0.9973742961883545 Time taken: 0.4412970542907715\n",
            "Epoch: 13 Batch Number: 187 Loss: 1.099237322807312 Time taken: 0.4468662738800049\n",
            "Epoch: 13 Batch Number: 188 Loss: 1.1045985221862793 Time taken: 0.44806694984436035\n",
            "Epoch: 13 Batch Number: 189 Loss: 1.1377042531967163 Time taken: 0.44278931617736816\n",
            "Epoch: 13 Batch Number: 190 Loss: 1.32664155960083 Time taken: 0.44286322593688965\n",
            "Epoch: 13 Batch Number: 191 Loss: 1.490858793258667 Time taken: 0.44591283798217773\n",
            "Epoch: 13 Batch Number: 192 Loss: 1.209433913230896 Time taken: 0.45568108558654785\n",
            "Epoch: 13 Batch Number: 193 Loss: 1.3249431848526 Time taken: 0.46227598190307617\n",
            "Epoch: 13 Batch Number: 194 Loss: 1.2389589548110962 Time taken: 0.4477243423461914\n",
            "Epoch: 13 Batch Number: 195 Loss: 1.1806963682174683 Time taken: 0.44429922103881836\n",
            "Epoch: 13 Batch Number: 196 Loss: 1.1342754364013672 Time taken: 0.445110559463501\n",
            "Epoch: 13 Batch Number: 197 Loss: 1.21091890335083 Time taken: 0.44422316551208496\n",
            "Epoch: 13 Batch Number: 198 Loss: 1.12827730178833 Time taken: 0.45847535133361816\n",
            "Epoch: 13 Batch Number: 199 Loss: 1.1625279188156128 Time taken: 0.44568920135498047\n",
            "Epoch: 13 Batch Number: 200 Loss: 1.1108304262161255 Time taken: 0.4495728015899658\n",
            "Epoch: 13 Batch Number: 201 Loss: 1.16811203956604 Time taken: 0.44084596633911133\n",
            "Epoch: 13 Batch Number: 202 Loss: 1.1435072422027588 Time taken: 0.4463787078857422\n",
            "Epoch: 13 Batch Number: 203 Loss: 1.14625084400177 Time taken: 0.43938279151916504\n",
            "Epoch: 13 Batch Number: 204 Loss: 1.1717160940170288 Time taken: 0.45429491996765137\n",
            "Epoch: 13 Batch Number: 205 Loss: 1.0985233783721924 Time taken: 0.4409339427947998\n",
            "Epoch: 13 Batch Number: 206 Loss: 1.1516547203063965 Time taken: 0.440798282623291\n",
            "Epoch: 13 Batch Number: 207 Loss: 1.1723564863204956 Time taken: 0.4433267116546631\n",
            "Epoch: 13 Batch Number: 208 Loss: 1.1397342681884766 Time taken: 0.43657398223876953\n",
            "Epoch: 13 Batch Number: 209 Loss: 1.1707231998443604 Time taken: 0.4516439437866211\n",
            "Epoch: 13 Batch Number: 210 Loss: 1.2286932468414307 Time taken: 0.4397149085998535\n",
            "Epoch: 13 Batch Number: 211 Loss: 1.1906991004943848 Time taken: 0.4429621696472168\n",
            "Epoch: 13 Batch Number: 212 Loss: 1.3147053718566895 Time taken: 0.44288158416748047\n",
            "Epoch: 13 Batch Number: 213 Loss: 1.1631481647491455 Time taken: 0.4476912021636963\n",
            "Epoch: 13 Batch Number: 214 Loss: 1.2082667350769043 Time taken: 0.4584362506866455\n",
            "Epoch: 13 Batch Number: 215 Loss: 1.1572422981262207 Time taken: 0.44702863693237305\n",
            "Epoch: 13 Batch Number: 216 Loss: 1.1974310874938965 Time taken: 0.4559459686279297\n",
            "Epoch: 13 Batch Number: 217 Loss: 1.2660304307937622 Time taken: 0.4459662437438965\n",
            "Epoch: 13 Batch Number: 218 Loss: 1.2617621421813965 Time taken: 0.44376659393310547\n",
            "Epoch: 13 Batch Number: 219 Loss: 1.2494672536849976 Time taken: 0.4522826671600342\n",
            "Epoch: 13 Batch Number: 220 Loss: 1.18037748336792 Time taken: 0.4562525749206543\n",
            "Epoch: 13 Batch Number: 221 Loss: 1.174330234527588 Time taken: 0.45769596099853516\n",
            "Epoch: 13 Batch Number: 222 Loss: 1.15390944480896 Time taken: 0.4445607662200928\n",
            "Epoch: 13 Batch Number: 223 Loss: 1.1866830587387085 Time taken: 0.4484071731567383\n",
            "Epoch: 13 Batch Number: 224 Loss: 1.1381878852844238 Time taken: 0.43833422660827637\n",
            "Epoch: 13 Batch Number: 225 Loss: 1.110551357269287 Time taken: 0.4390239715576172\n",
            "Epoch: 13 Batch Number: 226 Loss: 1.1463063955307007 Time taken: 0.44214773178100586\n",
            "Epoch: 13 Batch Number: 227 Loss: 1.25735342502594 Time taken: 0.4398510456085205\n",
            "Epoch: 13 Batch Number: 228 Loss: 1.225227952003479 Time taken: 0.45545077323913574\n",
            "Epoch: 13 Batch Number: 229 Loss: 1.2459732294082642 Time taken: 0.4492788314819336\n",
            "==========================================================================================\n",
            "Start of epoch 14\n",
            "Epoch: 14 Batch Number: 1 Loss: 1.1679739952087402 Time taken: 0.4436051845550537\n",
            "Epoch: 14 Batch Number: 2 Loss: 1.2248677015304565 Time taken: 0.4491276741027832\n",
            "Epoch: 14 Batch Number: 3 Loss: 1.184404969215393 Time taken: 0.45035600662231445\n",
            "Epoch: 14 Batch Number: 4 Loss: 1.1624112129211426 Time taken: 0.4375443458557129\n",
            "Epoch: 14 Batch Number: 5 Loss: 1.1463841199874878 Time taken: 0.44263386726379395\n",
            "Epoch: 14 Batch Number: 6 Loss: 1.0876436233520508 Time taken: 0.44248485565185547\n",
            "Epoch: 14 Batch Number: 7 Loss: 1.1108726263046265 Time taken: 0.44481873512268066\n",
            "Epoch: 14 Batch Number: 8 Loss: 1.1324292421340942 Time taken: 0.4459657669067383\n",
            "Epoch: 14 Batch Number: 9 Loss: 1.1270372867584229 Time taken: 0.45094871520996094\n",
            "Epoch: 14 Batch Number: 10 Loss: 1.157750129699707 Time taken: 0.4616422653198242\n",
            "Epoch: 14 Batch Number: 11 Loss: 1.1118167638778687 Time taken: 0.458099365234375\n",
            "Epoch: 14 Batch Number: 12 Loss: 1.078932762145996 Time taken: 0.466306209564209\n",
            "Epoch: 14 Batch Number: 13 Loss: 1.1100451946258545 Time taken: 0.4486081600189209\n",
            "Epoch: 14 Batch Number: 14 Loss: 1.1232004165649414 Time taken: 0.44037890434265137\n",
            "Epoch: 14 Batch Number: 15 Loss: 1.069532036781311 Time taken: 0.4377477169036865\n",
            "Epoch: 14 Batch Number: 16 Loss: 1.1082780361175537 Time taken: 0.4418148994445801\n",
            "Epoch: 14 Batch Number: 17 Loss: 1.2234359979629517 Time taken: 0.44040656089782715\n",
            "Epoch: 14 Batch Number: 18 Loss: 1.2065637111663818 Time taken: 0.43741774559020996\n",
            "Epoch: 14 Batch Number: 19 Loss: 1.2054543495178223 Time taken: 0.449723482131958\n",
            "Epoch: 14 Batch Number: 20 Loss: 1.0852012634277344 Time taken: 0.4494609832763672\n",
            "Epoch: 14 Batch Number: 21 Loss: 1.2696439027786255 Time taken: 0.4490780830383301\n",
            "Epoch: 14 Batch Number: 22 Loss: 1.2608263492584229 Time taken: 0.4535491466522217\n",
            "Epoch: 14 Batch Number: 23 Loss: 1.2653212547302246 Time taken: 0.4456026554107666\n",
            "Epoch: 14 Batch Number: 24 Loss: 1.2318189144134521 Time taken: 0.4376850128173828\n",
            "Epoch: 14 Batch Number: 25 Loss: 1.2262696027755737 Time taken: 0.44137096405029297\n",
            "Epoch: 14 Batch Number: 26 Loss: 1.2224960327148438 Time taken: 0.45832252502441406\n",
            "Epoch: 14 Batch Number: 27 Loss: 1.1631114482879639 Time taken: 0.4609036445617676\n",
            "Epoch: 14 Batch Number: 28 Loss: 1.1794283390045166 Time taken: 0.45004820823669434\n",
            "Epoch: 14 Batch Number: 29 Loss: 1.2170330286026 Time taken: 0.45216989517211914\n",
            "Epoch: 14 Batch Number: 30 Loss: 0.9871081113815308 Time taken: 0.44185900688171387\n",
            "Epoch: 14 Batch Number: 31 Loss: 1.1491304636001587 Time taken: 0.43886470794677734\n",
            "Epoch: 14 Batch Number: 32 Loss: 1.1531240940093994 Time taken: 0.44120001792907715\n",
            "Epoch: 14 Batch Number: 33 Loss: 1.1843953132629395 Time taken: 0.45665407180786133\n",
            "Epoch: 14 Batch Number: 34 Loss: 1.2065825462341309 Time taken: 0.4411172866821289\n",
            "Epoch: 14 Batch Number: 35 Loss: 1.2516005039215088 Time taken: 0.4385082721710205\n",
            "Epoch: 14 Batch Number: 36 Loss: 1.2946633100509644 Time taken: 0.45136380195617676\n",
            "Epoch: 14 Batch Number: 37 Loss: 1.1677886247634888 Time taken: 0.4773855209350586\n",
            "Epoch: 14 Batch Number: 38 Loss: 1.217489242553711 Time taken: 0.4622964859008789\n",
            "Epoch: 14 Batch Number: 39 Loss: 1.1603825092315674 Time taken: 0.45545506477355957\n",
            "Epoch: 14 Batch Number: 40 Loss: 1.1680707931518555 Time taken: 0.43909454345703125\n",
            "Epoch: 14 Batch Number: 41 Loss: 1.1383907794952393 Time taken: 0.4471256732940674\n",
            "Epoch: 14 Batch Number: 42 Loss: 1.1341230869293213 Time taken: 0.4471280574798584\n",
            "Epoch: 14 Batch Number: 43 Loss: 1.1227660179138184 Time taken: 0.4446723461151123\n",
            "Epoch: 14 Batch Number: 44 Loss: 1.0732673406600952 Time taken: 0.4655635356903076\n",
            "Epoch: 14 Batch Number: 45 Loss: 1.1354306936264038 Time taken: 0.45365023612976074\n",
            "Epoch: 14 Batch Number: 46 Loss: 1.2946027517318726 Time taken: 0.4537239074707031\n",
            "Epoch: 14 Batch Number: 47 Loss: 1.1530823707580566 Time taken: 0.4537830352783203\n",
            "Epoch: 14 Batch Number: 48 Loss: 1.2213815450668335 Time taken: 0.4528203010559082\n",
            "Epoch: 14 Batch Number: 49 Loss: 1.2225337028503418 Time taken: 0.4611647129058838\n",
            "Epoch: 14 Batch Number: 50 Loss: 1.1262069940567017 Time taken: 0.45827794075012207\n",
            "Epoch: 14 Batch Number: 51 Loss: 1.1202192306518555 Time taken: 0.4546048641204834\n",
            "Epoch: 14 Batch Number: 52 Loss: 1.232521414756775 Time taken: 0.4507308006286621\n",
            "Epoch: 14 Batch Number: 53 Loss: 1.267879605293274 Time taken: 0.46131110191345215\n",
            "Epoch: 14 Batch Number: 54 Loss: 1.2463696002960205 Time taken: 0.4513969421386719\n",
            "Epoch: 14 Batch Number: 55 Loss: 1.2088910341262817 Time taken: 0.45607566833496094\n",
            "Epoch: 14 Batch Number: 56 Loss: 1.2949968576431274 Time taken: 0.44359493255615234\n",
            "Epoch: 14 Batch Number: 57 Loss: 1.1818246841430664 Time taken: 0.44788575172424316\n",
            "Epoch: 14 Batch Number: 58 Loss: 1.145552396774292 Time taken: 0.45874452590942383\n",
            "Epoch: 14 Batch Number: 59 Loss: 1.1724348068237305 Time taken: 0.4541890621185303\n",
            "Epoch: 14 Batch Number: 60 Loss: 1.1597627401351929 Time taken: 0.44942450523376465\n",
            "Epoch: 14 Batch Number: 61 Loss: 1.1961033344268799 Time taken: 0.4481801986694336\n",
            "Epoch: 14 Batch Number: 62 Loss: 1.1022605895996094 Time taken: 0.4426705837249756\n",
            "Epoch: 14 Batch Number: 63 Loss: 1.1250088214874268 Time taken: 0.43946123123168945\n",
            "Epoch: 14 Batch Number: 64 Loss: 1.0759806632995605 Time taken: 0.44983410835266113\n",
            "Epoch: 14 Batch Number: 65 Loss: 1.122696876525879 Time taken: 0.46646881103515625\n",
            "Epoch: 14 Batch Number: 66 Loss: 1.1396660804748535 Time taken: 0.4618237018585205\n",
            "Epoch: 14 Batch Number: 67 Loss: 1.124193787574768 Time taken: 0.4515407085418701\n",
            "Epoch: 14 Batch Number: 68 Loss: 1.1898446083068848 Time taken: 0.4646308422088623\n",
            "Epoch: 14 Batch Number: 69 Loss: 1.1658003330230713 Time taken: 0.45043301582336426\n",
            "Epoch: 14 Batch Number: 70 Loss: 1.1722825765609741 Time taken: 0.4481089115142822\n",
            "Epoch: 14 Batch Number: 71 Loss: 1.141339898109436 Time taken: 0.4380760192871094\n",
            "Epoch: 14 Batch Number: 72 Loss: 1.1632990837097168 Time taken: 0.44480395317077637\n",
            "Epoch: 14 Batch Number: 73 Loss: 1.1019234657287598 Time taken: 0.45389771461486816\n",
            "Epoch: 14 Batch Number: 74 Loss: 1.209200382232666 Time taken: 0.45833468437194824\n",
            "Epoch: 14 Batch Number: 75 Loss: 1.0510234832763672 Time taken: 0.46140527725219727\n",
            "Epoch: 14 Batch Number: 76 Loss: 1.0903465747833252 Time taken: 0.44428396224975586\n",
            "Epoch: 14 Batch Number: 77 Loss: 1.187175989151001 Time taken: 0.45365142822265625\n",
            "Epoch: 14 Batch Number: 78 Loss: 1.0763492584228516 Time taken: 0.4462282657623291\n",
            "Epoch: 14 Batch Number: 79 Loss: 1.1527901887893677 Time taken: 0.43572378158569336\n",
            "Epoch: 14 Batch Number: 80 Loss: 1.1294209957122803 Time taken: 0.44034290313720703\n",
            "Epoch: 14 Batch Number: 81 Loss: 1.1508547067642212 Time taken: 0.4382016658782959\n",
            "Epoch: 14 Batch Number: 82 Loss: 1.147574782371521 Time taken: 0.44779491424560547\n",
            "Epoch: 14 Batch Number: 83 Loss: 1.2103394269943237 Time taken: 0.4443089962005615\n",
            "Epoch: 14 Batch Number: 84 Loss: 1.172243356704712 Time taken: 0.4413607120513916\n",
            "Epoch: 14 Batch Number: 85 Loss: 1.1410621404647827 Time taken: 0.4382059574127197\n",
            "Epoch: 14 Batch Number: 86 Loss: 1.1422338485717773 Time taken: 0.4465761184692383\n",
            "Epoch: 14 Batch Number: 87 Loss: 1.2236788272857666 Time taken: 0.45296549797058105\n",
            "Epoch: 14 Batch Number: 88 Loss: 1.1495307683944702 Time taken: 0.4467043876647949\n",
            "Epoch: 14 Batch Number: 89 Loss: 1.1048858165740967 Time taken: 0.4460287094116211\n",
            "Epoch: 14 Batch Number: 90 Loss: 1.1532726287841797 Time taken: 0.4426734447479248\n",
            "Epoch: 14 Batch Number: 91 Loss: 1.1136915683746338 Time taken: 0.443112850189209\n",
            "Epoch: 14 Batch Number: 92 Loss: 1.0878952741622925 Time taken: 0.4391310214996338\n",
            "Epoch: 14 Batch Number: 93 Loss: 1.1009249687194824 Time taken: 0.45060133934020996\n",
            "Epoch: 14 Batch Number: 94 Loss: 1.2373003959655762 Time taken: 0.447354793548584\n",
            "Epoch: 14 Batch Number: 95 Loss: 1.1925816535949707 Time taken: 0.4553403854370117\n",
            "Epoch: 14 Batch Number: 96 Loss: 1.2392868995666504 Time taken: 0.44574999809265137\n",
            "Epoch: 14 Batch Number: 97 Loss: 1.1873904466629028 Time taken: 0.45514678955078125\n",
            "Epoch: 14 Batch Number: 98 Loss: 1.2383750677108765 Time taken: 0.44326043128967285\n",
            "Epoch: 14 Batch Number: 99 Loss: 1.2675416469573975 Time taken: 0.44748377799987793\n",
            "Epoch: 14 Batch Number: 100 Loss: 1.1914795637130737 Time taken: 0.43932366371154785\n",
            "Epoch: 14 Batch Number: 101 Loss: 1.1863298416137695 Time taken: 0.44106459617614746\n",
            "Epoch: 14 Batch Number: 102 Loss: 1.2233564853668213 Time taken: 0.43989133834838867\n",
            "Epoch: 14 Batch Number: 103 Loss: 1.287542462348938 Time taken: 0.44211602210998535\n",
            "Epoch: 14 Batch Number: 104 Loss: 1.1750998497009277 Time taken: 0.4466369152069092\n",
            "Epoch: 14 Batch Number: 105 Loss: 1.2526466846466064 Time taken: 0.4464738368988037\n",
            "Epoch: 14 Batch Number: 106 Loss: 1.2208741903305054 Time taken: 0.4528694152832031\n",
            "Epoch: 14 Batch Number: 107 Loss: 1.2232667207717896 Time taken: 0.4370570182800293\n",
            "Epoch: 14 Batch Number: 108 Loss: 1.1901912689208984 Time taken: 0.4419691562652588\n",
            "Epoch: 14 Batch Number: 109 Loss: 1.1907048225402832 Time taken: 0.4463346004486084\n",
            "Epoch: 14 Batch Number: 110 Loss: 1.044335961341858 Time taken: 0.44483113288879395\n",
            "Epoch: 14 Batch Number: 111 Loss: 1.116491675376892 Time taken: 0.4528543949127197\n",
            "Epoch: 14 Batch Number: 112 Loss: 1.1269289255142212 Time taken: 0.4385673999786377\n",
            "Epoch: 14 Batch Number: 113 Loss: 1.165755033493042 Time taken: 0.44774842262268066\n",
            "Epoch: 14 Batch Number: 114 Loss: 1.2094122171401978 Time taken: 0.4606022834777832\n",
            "Epoch: 14 Batch Number: 115 Loss: 1.2064939737319946 Time taken: 0.4424781799316406\n",
            "Epoch: 14 Batch Number: 116 Loss: 1.0722362995147705 Time taken: 0.4471311569213867\n",
            "Epoch: 14 Batch Number: 117 Loss: 1.1721819639205933 Time taken: 0.4522268772125244\n",
            "Epoch: 14 Batch Number: 118 Loss: 1.1263710260391235 Time taken: 0.44230151176452637\n",
            "Epoch: 14 Batch Number: 119 Loss: 1.1611138582229614 Time taken: 0.4447345733642578\n",
            "Epoch: 14 Batch Number: 120 Loss: 1.201021432876587 Time taken: 0.4419991970062256\n",
            "Epoch: 14 Batch Number: 121 Loss: 1.1372766494750977 Time taken: 0.44923830032348633\n",
            "Epoch: 14 Batch Number: 122 Loss: 1.1255707740783691 Time taken: 0.43628787994384766\n",
            "Epoch: 14 Batch Number: 123 Loss: 1.1593108177185059 Time taken: 0.441617488861084\n",
            "Epoch: 14 Batch Number: 124 Loss: 1.127112865447998 Time taken: 0.4657142162322998\n",
            "Epoch: 14 Batch Number: 125 Loss: 1.1669092178344727 Time taken: 0.44059085845947266\n",
            "Epoch: 14 Batch Number: 126 Loss: 1.1860153675079346 Time taken: 0.44684576988220215\n",
            "Epoch: 14 Batch Number: 127 Loss: 1.185723066329956 Time taken: 0.4462125301361084\n",
            "Epoch: 14 Batch Number: 128 Loss: 1.143302083015442 Time taken: 0.44228339195251465\n",
            "Epoch: 14 Batch Number: 129 Loss: 1.1408840417861938 Time taken: 0.4402933120727539\n",
            "Epoch: 14 Batch Number: 130 Loss: 1.114547848701477 Time taken: 0.4447329044342041\n",
            "Epoch: 14 Batch Number: 131 Loss: 1.1974464654922485 Time taken: 0.4510068893432617\n",
            "Epoch: 14 Batch Number: 132 Loss: 1.0459896326065063 Time taken: 0.43856000900268555\n",
            "Epoch: 14 Batch Number: 133 Loss: 1.2023320198059082 Time taken: 0.45618247985839844\n",
            "Epoch: 14 Batch Number: 134 Loss: 1.2117066383361816 Time taken: 0.4604768753051758\n",
            "Epoch: 14 Batch Number: 135 Loss: 1.2138783931732178 Time taken: 0.45574450492858887\n",
            "Epoch: 14 Batch Number: 136 Loss: 1.2067241668701172 Time taken: 0.4412875175476074\n",
            "Epoch: 14 Batch Number: 137 Loss: 1.16444993019104 Time taken: 0.44602084159851074\n",
            "Epoch: 14 Batch Number: 138 Loss: 1.1919381618499756 Time taken: 0.440321683883667\n",
            "Epoch: 14 Batch Number: 139 Loss: 1.182493805885315 Time taken: 0.43864917755126953\n",
            "Epoch: 14 Batch Number: 140 Loss: 1.1498637199401855 Time taken: 0.4402956962585449\n",
            "Epoch: 14 Batch Number: 141 Loss: 1.203253149986267 Time taken: 0.4446837902069092\n",
            "Epoch: 14 Batch Number: 142 Loss: 1.242180585861206 Time taken: 0.44409990310668945\n",
            "Epoch: 14 Batch Number: 143 Loss: 1.3073382377624512 Time taken: 0.45859837532043457\n",
            "Epoch: 14 Batch Number: 144 Loss: 1.312196969985962 Time taken: 0.4608628749847412\n",
            "Epoch: 14 Batch Number: 145 Loss: 1.29628586769104 Time taken: 0.44512081146240234\n",
            "Epoch: 14 Batch Number: 146 Loss: 1.2750860452651978 Time taken: 0.4470696449279785\n",
            "Epoch: 14 Batch Number: 147 Loss: 1.1387550830841064 Time taken: 0.44701528549194336\n",
            "Epoch: 14 Batch Number: 148 Loss: 1.1001605987548828 Time taken: 0.45350193977355957\n",
            "Epoch: 14 Batch Number: 149 Loss: 1.1889774799346924 Time taken: 0.46083760261535645\n",
            "Epoch: 14 Batch Number: 150 Loss: 1.2786762714385986 Time taken: 0.4544808864593506\n",
            "Epoch: 14 Batch Number: 151 Loss: 1.2815022468566895 Time taken: 0.4588346481323242\n",
            "Epoch: 14 Batch Number: 152 Loss: 1.2100701332092285 Time taken: 0.4429285526275635\n",
            "Epoch: 14 Batch Number: 153 Loss: 1.1415311098098755 Time taken: 0.43972158432006836\n",
            "Epoch: 14 Batch Number: 154 Loss: 1.132117509841919 Time taken: 0.4416947364807129\n",
            "Epoch: 14 Batch Number: 155 Loss: 1.1288695335388184 Time taken: 0.4452850818634033\n",
            "Epoch: 14 Batch Number: 156 Loss: 1.189807653427124 Time taken: 0.44012451171875\n",
            "Epoch: 14 Batch Number: 157 Loss: 1.0746846199035645 Time taken: 0.44008374214172363\n",
            "Epoch: 14 Batch Number: 158 Loss: 1.134108304977417 Time taken: 0.443131685256958\n",
            "Epoch: 14 Batch Number: 159 Loss: 1.0742838382720947 Time taken: 0.4362151622772217\n",
            "Epoch: 14 Batch Number: 160 Loss: 0.981799304485321 Time taken: 0.45118260383605957\n",
            "Epoch: 14 Batch Number: 161 Loss: 1.0546491146087646 Time taken: 0.4627809524536133\n",
            "Epoch: 14 Batch Number: 162 Loss: 1.0265718698501587 Time taken: 0.4466538429260254\n",
            "Epoch: 14 Batch Number: 163 Loss: 1.1381864547729492 Time taken: 0.44483518600463867\n",
            "Epoch: 14 Batch Number: 164 Loss: 1.2349932193756104 Time taken: 0.43785762786865234\n",
            "Epoch: 14 Batch Number: 165 Loss: 1.1538718938827515 Time taken: 0.4632132053375244\n",
            "Epoch: 14 Batch Number: 166 Loss: 1.1846468448638916 Time taken: 0.442202091217041\n",
            "Epoch: 14 Batch Number: 167 Loss: 1.1634008884429932 Time taken: 0.44359803199768066\n",
            "Epoch: 14 Batch Number: 168 Loss: 1.165407657623291 Time taken: 0.44897937774658203\n",
            "Epoch: 14 Batch Number: 169 Loss: 1.1604199409484863 Time taken: 0.45706987380981445\n",
            "Epoch: 14 Batch Number: 170 Loss: 1.0988057851791382 Time taken: 0.4349365234375\n",
            "Epoch: 14 Batch Number: 171 Loss: 1.051149606704712 Time taken: 0.44333839416503906\n",
            "Epoch: 14 Batch Number: 172 Loss: 1.1048264503479004 Time taken: 0.447404146194458\n",
            "Epoch: 14 Batch Number: 173 Loss: 1.0867019891738892 Time taken: 0.4454672336578369\n",
            "Epoch: 14 Batch Number: 174 Loss: 1.0304338932037354 Time taken: 0.4484713077545166\n",
            "Epoch: 14 Batch Number: 175 Loss: 1.1505510807037354 Time taken: 0.45977139472961426\n",
            "Epoch: 14 Batch Number: 176 Loss: 1.126457929611206 Time taken: 0.4647650718688965\n",
            "Epoch: 14 Batch Number: 177 Loss: 1.1384596824645996 Time taken: 0.4463236331939697\n",
            "Epoch: 14 Batch Number: 178 Loss: 1.1986162662506104 Time taken: 0.4447658061981201\n",
            "Epoch: 14 Batch Number: 179 Loss: 1.2068885564804077 Time taken: 0.4526660442352295\n",
            "Epoch: 14 Batch Number: 180 Loss: 1.1482508182525635 Time taken: 0.4409041404724121\n",
            "Epoch: 14 Batch Number: 181 Loss: 1.1197700500488281 Time taken: 0.43639683723449707\n",
            "Epoch: 14 Batch Number: 182 Loss: 1.062283992767334 Time taken: 0.44680023193359375\n",
            "Epoch: 14 Batch Number: 183 Loss: 1.1178710460662842 Time taken: 0.4436831474304199\n",
            "Epoch: 14 Batch Number: 184 Loss: 1.1822271347045898 Time taken: 0.4429340362548828\n",
            "Epoch: 14 Batch Number: 185 Loss: 1.2052403688430786 Time taken: 0.44170379638671875\n",
            "Epoch: 14 Batch Number: 186 Loss: 1.0137709379196167 Time taken: 0.45471763610839844\n",
            "Epoch: 14 Batch Number: 187 Loss: 1.0895500183105469 Time taken: 0.4525747299194336\n",
            "Epoch: 14 Batch Number: 188 Loss: 1.0975754261016846 Time taken: 0.4435093402862549\n",
            "Epoch: 14 Batch Number: 189 Loss: 1.1333465576171875 Time taken: 0.4445619583129883\n",
            "Epoch: 14 Batch Number: 190 Loss: 1.31302809715271 Time taken: 0.44440674781799316\n",
            "Epoch: 14 Batch Number: 191 Loss: 1.4724390506744385 Time taken: 0.44193029403686523\n",
            "Epoch: 14 Batch Number: 192 Loss: 1.1992428302764893 Time taken: 0.4403204917907715\n",
            "Epoch: 14 Batch Number: 193 Loss: 1.309643268585205 Time taken: 0.4575619697570801\n",
            "Epoch: 14 Batch Number: 194 Loss: 1.223314642906189 Time taken: 0.44790101051330566\n",
            "Epoch: 14 Batch Number: 195 Loss: 1.1664328575134277 Time taken: 0.4611508846282959\n",
            "Epoch: 14 Batch Number: 196 Loss: 1.120009183883667 Time taken: 0.47712206840515137\n",
            "Epoch: 14 Batch Number: 197 Loss: 1.1921086311340332 Time taken: 0.4407920837402344\n",
            "Epoch: 14 Batch Number: 198 Loss: 1.1143100261688232 Time taken: 0.4525489807128906\n",
            "Epoch: 14 Batch Number: 199 Loss: 1.1524264812469482 Time taken: 0.4521372318267822\n",
            "Epoch: 14 Batch Number: 200 Loss: 1.0995999574661255 Time taken: 0.45134568214416504\n",
            "Epoch: 14 Batch Number: 201 Loss: 1.1559562683105469 Time taken: 0.4421687126159668\n",
            "Epoch: 14 Batch Number: 202 Loss: 1.1355031728744507 Time taken: 0.44074368476867676\n",
            "Epoch: 14 Batch Number: 203 Loss: 1.1311111450195312 Time taken: 0.44786667823791504\n",
            "Epoch: 14 Batch Number: 204 Loss: 1.1622498035430908 Time taken: 0.44318723678588867\n",
            "Epoch: 14 Batch Number: 205 Loss: 1.0840668678283691 Time taken: 0.45798468589782715\n",
            "Epoch: 14 Batch Number: 206 Loss: 1.1374589204788208 Time taken: 0.4338092803955078\n",
            "Epoch: 14 Batch Number: 207 Loss: 1.1614673137664795 Time taken: 0.45647096633911133\n",
            "Epoch: 14 Batch Number: 208 Loss: 1.1362395286560059 Time taken: 0.4459068775177002\n",
            "Epoch: 14 Batch Number: 209 Loss: 1.1681716442108154 Time taken: 0.44061994552612305\n",
            "Epoch: 14 Batch Number: 210 Loss: 1.2199904918670654 Time taken: 0.4564094543457031\n",
            "Epoch: 14 Batch Number: 211 Loss: 1.1751906871795654 Time taken: 0.45226573944091797\n",
            "Epoch: 14 Batch Number: 212 Loss: 1.3038018941879272 Time taken: 0.4429740905761719\n",
            "Epoch: 14 Batch Number: 213 Loss: 1.1529881954193115 Time taken: 0.43810582160949707\n",
            "Epoch: 14 Batch Number: 214 Loss: 1.1952099800109863 Time taken: 0.451765775680542\n",
            "Epoch: 14 Batch Number: 215 Loss: 1.1424732208251953 Time taken: 0.44049525260925293\n",
            "Epoch: 14 Batch Number: 216 Loss: 1.1840204000473022 Time taken: 0.4461171627044678\n",
            "Epoch: 14 Batch Number: 217 Loss: 1.2511355876922607 Time taken: 0.4566061496734619\n",
            "Epoch: 14 Batch Number: 218 Loss: 1.2458183765411377 Time taken: 0.45455455780029297\n",
            "Epoch: 14 Batch Number: 219 Loss: 1.2357258796691895 Time taken: 0.4560966491699219\n",
            "Epoch: 14 Batch Number: 220 Loss: 1.1653764247894287 Time taken: 0.4441702365875244\n",
            "Epoch: 14 Batch Number: 221 Loss: 1.1646113395690918 Time taken: 0.45710086822509766\n",
            "Epoch: 14 Batch Number: 222 Loss: 1.1414260864257812 Time taken: 0.4668431282043457\n",
            "Epoch: 14 Batch Number: 223 Loss: 1.1684906482696533 Time taken: 0.4581332206726074\n",
            "Epoch: 14 Batch Number: 224 Loss: 1.1208646297454834 Time taken: 0.44683837890625\n",
            "Epoch: 14 Batch Number: 225 Loss: 1.0971968173980713 Time taken: 0.46412158012390137\n",
            "Epoch: 14 Batch Number: 226 Loss: 1.1334335803985596 Time taken: 0.45945262908935547\n",
            "Epoch: 14 Batch Number: 227 Loss: 1.2485065460205078 Time taken: 0.4765009880065918\n",
            "Epoch: 14 Batch Number: 228 Loss: 1.207349419593811 Time taken: 0.4558143615722656\n",
            "Epoch: 14 Batch Number: 229 Loss: 1.229453682899475 Time taken: 0.45836901664733887\n",
            "==========================================================================================\n",
            "Start of epoch 15\n",
            "Epoch: 15 Batch Number: 1 Loss: 1.1550483703613281 Time taken: 0.4448821544647217\n",
            "Epoch: 15 Batch Number: 2 Loss: 1.2112345695495605 Time taken: 0.4550185203552246\n",
            "Epoch: 15 Batch Number: 3 Loss: 1.1739672422409058 Time taken: 0.4511597156524658\n",
            "Epoch: 15 Batch Number: 4 Loss: 1.1507261991500854 Time taken: 0.44707298278808594\n",
            "Epoch: 15 Batch Number: 5 Loss: 1.1399476528167725 Time taken: 0.46281886100769043\n",
            "Epoch: 15 Batch Number: 6 Loss: 1.0762207508087158 Time taken: 0.4567229747772217\n",
            "Epoch: 15 Batch Number: 7 Loss: 1.0988233089447021 Time taken: 0.447476863861084\n",
            "Epoch: 15 Batch Number: 8 Loss: 1.1244618892669678 Time taken: 0.46493101119995117\n",
            "Epoch: 15 Batch Number: 9 Loss: 1.1147011518478394 Time taken: 0.47381019592285156\n",
            "Epoch: 15 Batch Number: 10 Loss: 1.1427371501922607 Time taken: 0.4481501579284668\n",
            "Epoch: 15 Batch Number: 11 Loss: 1.0989129543304443 Time taken: 0.44416046142578125\n",
            "Epoch: 15 Batch Number: 12 Loss: 1.0659945011138916 Time taken: 0.4489710330963135\n",
            "Epoch: 15 Batch Number: 13 Loss: 1.0970056056976318 Time taken: 0.4416239261627197\n",
            "Epoch: 15 Batch Number: 14 Loss: 1.105135440826416 Time taken: 0.45444822311401367\n",
            "Epoch: 15 Batch Number: 15 Loss: 1.0418058633804321 Time taken: 0.4694650173187256\n",
            "Epoch: 15 Batch Number: 16 Loss: 1.0830295085906982 Time taken: 0.44141578674316406\n",
            "Epoch: 15 Batch Number: 17 Loss: 1.2116541862487793 Time taken: 0.44576478004455566\n",
            "Epoch: 15 Batch Number: 18 Loss: 1.189267873764038 Time taken: 0.46283674240112305\n",
            "Epoch: 15 Batch Number: 19 Loss: 1.1922062635421753 Time taken: 0.45199155807495117\n",
            "Epoch: 15 Batch Number: 20 Loss: 1.0745182037353516 Time taken: 0.44830965995788574\n",
            "Epoch: 15 Batch Number: 21 Loss: 1.2473599910736084 Time taken: 0.43949294090270996\n",
            "Epoch: 15 Batch Number: 22 Loss: 1.2444586753845215 Time taken: 0.4421720504760742\n",
            "Epoch: 15 Batch Number: 23 Loss: 1.2459039688110352 Time taken: 0.4418973922729492\n",
            "Epoch: 15 Batch Number: 24 Loss: 1.215970754623413 Time taken: 0.4426426887512207\n",
            "Epoch: 15 Batch Number: 25 Loss: 1.2117023468017578 Time taken: 0.4418983459472656\n",
            "Epoch: 15 Batch Number: 26 Loss: 1.208726406097412 Time taken: 0.45263218879699707\n",
            "Epoch: 15 Batch Number: 27 Loss: 1.146200180053711 Time taken: 0.452040433883667\n",
            "Epoch: 15 Batch Number: 28 Loss: 1.1613513231277466 Time taken: 0.4421985149383545\n",
            "Epoch: 15 Batch Number: 29 Loss: 1.2041501998901367 Time taken: 0.4398832321166992\n",
            "Epoch: 15 Batch Number: 30 Loss: 0.9764158129692078 Time taken: 0.4433426856994629\n",
            "Epoch: 15 Batch Number: 31 Loss: 1.1355886459350586 Time taken: 0.4633331298828125\n",
            "Epoch: 15 Batch Number: 32 Loss: 1.1398770809173584 Time taken: 0.4467942714691162\n",
            "Epoch: 15 Batch Number: 33 Loss: 1.1709339618682861 Time taken: 0.4478788375854492\n",
            "Epoch: 15 Batch Number: 34 Loss: 1.1930503845214844 Time taken: 0.4461238384246826\n",
            "Epoch: 15 Batch Number: 35 Loss: 1.2372946739196777 Time taken: 0.4412107467651367\n",
            "Epoch: 15 Batch Number: 36 Loss: 1.2777326107025146 Time taken: 0.44931793212890625\n",
            "Epoch: 15 Batch Number: 37 Loss: 1.1581342220306396 Time taken: 0.44465184211730957\n",
            "Epoch: 15 Batch Number: 38 Loss: 1.206871747970581 Time taken: 0.44715213775634766\n",
            "Epoch: 15 Batch Number: 39 Loss: 1.1451823711395264 Time taken: 0.4528181552886963\n",
            "Epoch: 15 Batch Number: 40 Loss: 1.151031255722046 Time taken: 0.44382476806640625\n",
            "Epoch: 15 Batch Number: 41 Loss: 1.1220170259475708 Time taken: 0.44391536712646484\n",
            "Epoch: 15 Batch Number: 42 Loss: 1.1172751188278198 Time taken: 0.4449465274810791\n",
            "Epoch: 15 Batch Number: 43 Loss: 1.1050183773040771 Time taken: 0.4436478614807129\n",
            "Epoch: 15 Batch Number: 44 Loss: 1.062058925628662 Time taken: 0.4427943229675293\n",
            "Epoch: 15 Batch Number: 45 Loss: 1.1242213249206543 Time taken: 0.45453500747680664\n",
            "Epoch: 15 Batch Number: 46 Loss: 1.2840436697006226 Time taken: 0.4395108222961426\n",
            "Epoch: 15 Batch Number: 47 Loss: 1.1396474838256836 Time taken: 0.4627535343170166\n",
            "Epoch: 15 Batch Number: 48 Loss: 1.209128499031067 Time taken: 0.44225287437438965\n",
            "Epoch: 15 Batch Number: 49 Loss: 1.2063379287719727 Time taken: 0.45985960960388184\n",
            "Epoch: 15 Batch Number: 50 Loss: 1.113940715789795 Time taken: 0.4549224376678467\n",
            "Epoch: 15 Batch Number: 51 Loss: 1.1042360067367554 Time taken: 0.4453270435333252\n",
            "Epoch: 15 Batch Number: 52 Loss: 1.2194368839263916 Time taken: 0.4507100582122803\n",
            "Epoch: 15 Batch Number: 53 Loss: 1.2531100511550903 Time taken: 0.4387838840484619\n",
            "Epoch: 15 Batch Number: 54 Loss: 1.2331337928771973 Time taken: 0.4559142589569092\n",
            "Epoch: 15 Batch Number: 55 Loss: 1.200236439704895 Time taken: 0.44925761222839355\n",
            "Epoch: 15 Batch Number: 56 Loss: 1.2836209535598755 Time taken: 0.4447324275970459\n",
            "Epoch: 15 Batch Number: 57 Loss: 1.171522855758667 Time taken: 0.447551965713501\n",
            "Epoch: 15 Batch Number: 58 Loss: 1.1354191303253174 Time taken: 0.43852877616882324\n",
            "Epoch: 15 Batch Number: 59 Loss: 1.165467381477356 Time taken: 0.44124603271484375\n",
            "Epoch: 15 Batch Number: 60 Loss: 1.15125572681427 Time taken: 0.4396059513092041\n",
            "Epoch: 15 Batch Number: 61 Loss: 1.1797724962234497 Time taken: 0.44393157958984375\n",
            "Epoch: 15 Batch Number: 62 Loss: 1.0865333080291748 Time taken: 0.4561014175415039\n",
            "Epoch: 15 Batch Number: 63 Loss: 1.114900827407837 Time taken: 0.452347993850708\n",
            "Epoch: 15 Batch Number: 64 Loss: 1.0663139820098877 Time taken: 0.44317173957824707\n",
            "Epoch: 15 Batch Number: 65 Loss: 1.111741542816162 Time taken: 0.4456446170806885\n",
            "Epoch: 15 Batch Number: 66 Loss: 1.1273634433746338 Time taken: 0.4391520023345947\n",
            "Epoch: 15 Batch Number: 67 Loss: 1.1128695011138916 Time taken: 0.4449143409729004\n",
            "Epoch: 15 Batch Number: 68 Loss: 1.1767568588256836 Time taken: 0.4570882320404053\n",
            "Epoch: 15 Batch Number: 69 Loss: 1.1551454067230225 Time taken: 0.4427223205566406\n",
            "Epoch: 15 Batch Number: 70 Loss: 1.1600232124328613 Time taken: 0.4570789337158203\n",
            "Epoch: 15 Batch Number: 71 Loss: 1.1291954517364502 Time taken: 0.4464237689971924\n",
            "Epoch: 15 Batch Number: 72 Loss: 1.1491003036499023 Time taken: 0.4473094940185547\n",
            "Epoch: 15 Batch Number: 73 Loss: 1.0878911018371582 Time taken: 0.4451158046722412\n",
            "Epoch: 15 Batch Number: 74 Loss: 1.1902501583099365 Time taken: 0.4419581890106201\n",
            "Epoch: 15 Batch Number: 75 Loss: 1.0370043516159058 Time taken: 0.4494452476501465\n",
            "Epoch: 15 Batch Number: 76 Loss: 1.0781537294387817 Time taken: 0.44057130813598633\n",
            "Epoch: 15 Batch Number: 77 Loss: 1.1735719442367554 Time taken: 0.451282262802124\n",
            "Epoch: 15 Batch Number: 78 Loss: 1.0639945268630981 Time taken: 0.44270849227905273\n",
            "Epoch: 15 Batch Number: 79 Loss: 1.1391656398773193 Time taken: 0.43956995010375977\n",
            "Epoch: 15 Batch Number: 80 Loss: 1.1163748502731323 Time taken: 0.4456779956817627\n",
            "Epoch: 15 Batch Number: 81 Loss: 1.1402488946914673 Time taken: 0.44652271270751953\n",
            "Epoch: 15 Batch Number: 82 Loss: 1.135633945465088 Time taken: 0.46303677558898926\n",
            "Epoch: 15 Batch Number: 83 Loss: 1.1985492706298828 Time taken: 0.43702173233032227\n",
            "Epoch: 15 Batch Number: 84 Loss: 1.1574456691741943 Time taken: 0.4624450206756592\n",
            "Epoch: 15 Batch Number: 85 Loss: 1.131443738937378 Time taken: 0.4438045024871826\n",
            "Epoch: 15 Batch Number: 86 Loss: 1.1300368309020996 Time taken: 0.43909573554992676\n",
            "Epoch: 15 Batch Number: 87 Loss: 1.2155308723449707 Time taken: 0.4507303237915039\n",
            "Epoch: 15 Batch Number: 88 Loss: 1.1124005317687988 Time taken: 0.4499218463897705\n",
            "Epoch: 15 Batch Number: 89 Loss: 1.0976736545562744 Time taken: 0.4367995262145996\n",
            "Epoch: 15 Batch Number: 90 Loss: 1.1441535949707031 Time taken: 0.44591474533081055\n",
            "Epoch: 15 Batch Number: 91 Loss: 1.1049466133117676 Time taken: 0.4529707431793213\n",
            "Epoch: 15 Batch Number: 92 Loss: 1.0688762664794922 Time taken: 0.45025157928466797\n",
            "Epoch: 15 Batch Number: 93 Loss: 1.0876758098602295 Time taken: 0.4555532932281494\n",
            "Epoch: 15 Batch Number: 94 Loss: 1.233855128288269 Time taken: 0.44197988510131836\n",
            "Epoch: 15 Batch Number: 95 Loss: 1.1816678047180176 Time taken: 0.44829297065734863\n",
            "Epoch: 15 Batch Number: 96 Loss: 1.2272653579711914 Time taken: 0.4418184757232666\n",
            "Epoch: 15 Batch Number: 97 Loss: 1.1701337099075317 Time taken: 0.4482555389404297\n",
            "Epoch: 15 Batch Number: 98 Loss: 1.2258169651031494 Time taken: 0.448742151260376\n",
            "Epoch: 15 Batch Number: 99 Loss: 1.2590091228485107 Time taken: 0.44995570182800293\n",
            "Epoch: 15 Batch Number: 100 Loss: 1.1832268238067627 Time taken: 0.44127655029296875\n",
            "Epoch: 15 Batch Number: 101 Loss: 1.1753991842269897 Time taken: 0.44508934020996094\n",
            "Epoch: 15 Batch Number: 102 Loss: 1.2108466625213623 Time taken: 0.44266343116760254\n",
            "Epoch: 15 Batch Number: 103 Loss: 1.2740201950073242 Time taken: 0.44560861587524414\n",
            "Epoch: 15 Batch Number: 104 Loss: 1.1626570224761963 Time taken: 0.4646036624908447\n",
            "Epoch: 15 Batch Number: 105 Loss: 1.2404141426086426 Time taken: 0.44977807998657227\n",
            "Epoch: 15 Batch Number: 106 Loss: 1.2060431241989136 Time taken: 0.44989991188049316\n",
            "Epoch: 15 Batch Number: 107 Loss: 1.2119300365447998 Time taken: 0.45253443717956543\n",
            "Epoch: 15 Batch Number: 108 Loss: 1.1879115104675293 Time taken: 0.4496631622314453\n",
            "Epoch: 15 Batch Number: 109 Loss: 1.176184892654419 Time taken: 0.44477295875549316\n",
            "Epoch: 15 Batch Number: 110 Loss: 1.0314795970916748 Time taken: 0.4396665096282959\n",
            "Epoch: 15 Batch Number: 111 Loss: 1.1074790954589844 Time taken: 0.43854594230651855\n",
            "Epoch: 15 Batch Number: 112 Loss: 1.1204755306243896 Time taken: 0.4514636993408203\n",
            "Epoch: 15 Batch Number: 113 Loss: 1.151536464691162 Time taken: 0.4428536891937256\n",
            "Epoch: 15 Batch Number: 114 Loss: 1.1962946653366089 Time taken: 0.44432902336120605\n",
            "Epoch: 15 Batch Number: 115 Loss: 1.1921955347061157 Time taken: 0.45491957664489746\n",
            "Epoch: 15 Batch Number: 116 Loss: 1.0627177953720093 Time taken: 0.4376194477081299\n",
            "Epoch: 15 Batch Number: 117 Loss: 1.1588257551193237 Time taken: 0.4532787799835205\n",
            "Epoch: 15 Batch Number: 118 Loss: 1.1128249168395996 Time taken: 0.4394247531890869\n",
            "Epoch: 15 Batch Number: 119 Loss: 1.1516143083572388 Time taken: 0.44704413414001465\n",
            "Epoch: 15 Batch Number: 120 Loss: 1.1893386840820312 Time taken: 0.44592738151550293\n",
            "Epoch: 15 Batch Number: 121 Loss: 1.1275994777679443 Time taken: 0.44389891624450684\n",
            "Epoch: 15 Batch Number: 122 Loss: 1.117246389389038 Time taken: 0.46261048316955566\n",
            "Epoch: 15 Batch Number: 123 Loss: 1.1493349075317383 Time taken: 0.45426225662231445\n",
            "Epoch: 15 Batch Number: 124 Loss: 1.1133005619049072 Time taken: 0.4671483039855957\n",
            "Epoch: 15 Batch Number: 125 Loss: 1.1550986766815186 Time taken: 0.4549064636230469\n",
            "Epoch: 15 Batch Number: 126 Loss: 1.1743685007095337 Time taken: 0.4421877861022949\n",
            "Epoch: 15 Batch Number: 127 Loss: 1.1738848686218262 Time taken: 0.44335412979125977\n",
            "Epoch: 15 Batch Number: 128 Loss: 1.1352249383926392 Time taken: 0.43663811683654785\n",
            "Epoch: 15 Batch Number: 129 Loss: 1.130183219909668 Time taken: 0.4405021667480469\n",
            "Epoch: 15 Batch Number: 130 Loss: 1.1067860126495361 Time taken: 0.4427368640899658\n",
            "Epoch: 15 Batch Number: 131 Loss: 1.1895804405212402 Time taken: 0.4440734386444092\n",
            "Epoch: 15 Batch Number: 132 Loss: 1.0390583276748657 Time taken: 0.45976734161376953\n",
            "Epoch: 15 Batch Number: 133 Loss: 1.1951372623443604 Time taken: 0.4613320827484131\n",
            "Epoch: 15 Batch Number: 134 Loss: 1.1974031925201416 Time taken: 0.44445347785949707\n",
            "Epoch: 15 Batch Number: 135 Loss: 1.2023885250091553 Time taken: 0.4474449157714844\n",
            "Epoch: 15 Batch Number: 136 Loss: 1.1947872638702393 Time taken: 0.4594101905822754\n",
            "Epoch: 15 Batch Number: 137 Loss: 1.1514511108398438 Time taken: 0.4466886520385742\n",
            "Epoch: 15 Batch Number: 138 Loss: 1.181459665298462 Time taken: 0.44358158111572266\n",
            "Epoch: 15 Batch Number: 139 Loss: 1.226845622062683 Time taken: 0.4385242462158203\n",
            "Epoch: 15 Batch Number: 140 Loss: 1.1386325359344482 Time taken: 0.4530665874481201\n",
            "Epoch: 15 Batch Number: 141 Loss: 1.182800054550171 Time taken: 0.4581582546234131\n",
            "Epoch: 15 Batch Number: 142 Loss: 1.230495810508728 Time taken: 0.4566645622253418\n",
            "Epoch: 15 Batch Number: 143 Loss: 1.2918274402618408 Time taken: 0.4519011974334717\n",
            "Epoch: 15 Batch Number: 144 Loss: 1.293165683746338 Time taken: 0.4560277462005615\n",
            "Epoch: 15 Batch Number: 145 Loss: 1.2809669971466064 Time taken: 0.45008015632629395\n",
            "Epoch: 15 Batch Number: 146 Loss: 1.2594490051269531 Time taken: 0.4630897045135498\n",
            "Epoch: 15 Batch Number: 147 Loss: 1.129452109336853 Time taken: 0.4573485851287842\n",
            "Epoch: 15 Batch Number: 148 Loss: 1.0903079509735107 Time taken: 0.46114468574523926\n",
            "Epoch: 15 Batch Number: 149 Loss: 1.175473928451538 Time taken: 0.4559929370880127\n",
            "Epoch: 15 Batch Number: 150 Loss: 1.2641034126281738 Time taken: 0.4737541675567627\n",
            "Epoch: 15 Batch Number: 151 Loss: 1.269558072090149 Time taken: 0.4689791202545166\n",
            "Epoch: 15 Batch Number: 152 Loss: 1.1993515491485596 Time taken: 0.45442819595336914\n",
            "Epoch: 15 Batch Number: 153 Loss: 1.1299484968185425 Time taken: 0.4670064449310303\n",
            "Epoch: 15 Batch Number: 154 Loss: 1.1220428943634033 Time taken: 0.45601677894592285\n",
            "Epoch: 15 Batch Number: 155 Loss: 1.1178780794143677 Time taken: 0.46168041229248047\n",
            "Epoch: 15 Batch Number: 156 Loss: 1.1936326026916504 Time taken: 0.4578392505645752\n",
            "Epoch: 15 Batch Number: 157 Loss: 1.0681052207946777 Time taken: 0.4645044803619385\n",
            "Epoch: 15 Batch Number: 158 Loss: 1.1287286281585693 Time taken: 0.45568227767944336\n",
            "Epoch: 15 Batch Number: 159 Loss: 1.064394235610962 Time taken: 0.4880506992340088\n",
            "Epoch: 15 Batch Number: 160 Loss: 0.9777292013168335 Time taken: 0.464385986328125\n",
            "Epoch: 15 Batch Number: 161 Loss: 1.0499240159988403 Time taken: 0.46775126457214355\n",
            "Epoch: 15 Batch Number: 162 Loss: 1.0159999132156372 Time taken: 0.46961331367492676\n",
            "Epoch: 15 Batch Number: 163 Loss: 1.1286826133728027 Time taken: 0.4572885036468506\n",
            "Epoch: 15 Batch Number: 164 Loss: 1.2222182750701904 Time taken: 0.45798659324645996\n",
            "Epoch: 15 Batch Number: 165 Loss: 1.1319749355316162 Time taken: 0.4545319080352783\n",
            "Epoch: 15 Batch Number: 166 Loss: 1.1826865673065186 Time taken: 0.4503288269042969\n",
            "Epoch: 15 Batch Number: 167 Loss: 1.1516249179840088 Time taken: 0.44427919387817383\n",
            "Epoch: 15 Batch Number: 168 Loss: 1.1479603052139282 Time taken: 0.44159722328186035\n",
            "Epoch: 15 Batch Number: 169 Loss: 1.1459406614303589 Time taken: 0.46318507194519043\n",
            "Epoch: 15 Batch Number: 170 Loss: 1.0783571004867554 Time taken: 0.47185540199279785\n",
            "Epoch: 15 Batch Number: 171 Loss: 1.0319806337356567 Time taken: 0.44801878929138184\n",
            "Epoch: 15 Batch Number: 172 Loss: 1.0906922817230225 Time taken: 0.43622279167175293\n",
            "Epoch: 15 Batch Number: 173 Loss: 1.074444055557251 Time taken: 0.4484100341796875\n",
            "Epoch: 15 Batch Number: 174 Loss: 1.020960807800293 Time taken: 0.45076656341552734\n",
            "Epoch: 15 Batch Number: 175 Loss: 1.1295031309127808 Time taken: 0.46769189834594727\n",
            "Epoch: 15 Batch Number: 176 Loss: 1.1065707206726074 Time taken: 0.4476132392883301\n",
            "Epoch: 15 Batch Number: 177 Loss: 1.1263679265975952 Time taken: 0.44765448570251465\n",
            "Epoch: 15 Batch Number: 178 Loss: 1.1892445087432861 Time taken: 0.44875025749206543\n",
            "Epoch: 15 Batch Number: 179 Loss: 1.1878632307052612 Time taken: 0.46150732040405273\n",
            "Epoch: 15 Batch Number: 180 Loss: 1.1389003992080688 Time taken: 0.45749449729919434\n",
            "Epoch: 15 Batch Number: 181 Loss: 1.10540771484375 Time taken: 0.45418858528137207\n",
            "Epoch: 15 Batch Number: 182 Loss: 1.050596833229065 Time taken: 0.4554893970489502\n",
            "Epoch: 15 Batch Number: 183 Loss: 1.1054186820983887 Time taken: 0.44693470001220703\n",
            "Epoch: 15 Batch Number: 184 Loss: 1.170090913772583 Time taken: 0.4478273391723633\n",
            "Epoch: 15 Batch Number: 185 Loss: 1.197322964668274 Time taken: 0.4583728313446045\n",
            "Epoch: 15 Batch Number: 186 Loss: 0.9801221489906311 Time taken: 0.4500894546508789\n",
            "Epoch: 15 Batch Number: 187 Loss: 1.0825181007385254 Time taken: 0.4453589916229248\n",
            "Epoch: 15 Batch Number: 188 Loss: 1.0843218564987183 Time taken: 0.4489712715148926\n",
            "Epoch: 15 Batch Number: 189 Loss: 1.119100570678711 Time taken: 0.4430515766143799\n",
            "Epoch: 15 Batch Number: 190 Loss: 1.298060655593872 Time taken: 0.4496173858642578\n",
            "Epoch: 15 Batch Number: 191 Loss: 1.4501559734344482 Time taken: 0.43965768814086914\n",
            "Epoch: 15 Batch Number: 192 Loss: 1.1807634830474854 Time taken: 0.44504427909851074\n",
            "Epoch: 15 Batch Number: 193 Loss: 1.292179822921753 Time taken: 0.4399714469909668\n",
            "Epoch: 15 Batch Number: 194 Loss: 1.2089269161224365 Time taken: 0.45525360107421875\n",
            "Epoch: 15 Batch Number: 195 Loss: 1.1534886360168457 Time taken: 0.46019840240478516\n",
            "Epoch: 15 Batch Number: 196 Loss: 1.1079936027526855 Time taken: 0.4589390754699707\n",
            "Epoch: 15 Batch Number: 197 Loss: 1.1788532733917236 Time taken: 0.45493149757385254\n",
            "Epoch: 15 Batch Number: 198 Loss: 1.100885033607483 Time taken: 0.43663716316223145\n",
            "Epoch: 15 Batch Number: 199 Loss: 1.1386007070541382 Time taken: 0.45150065422058105\n",
            "Epoch: 15 Batch Number: 200 Loss: 1.0909031629562378 Time taken: 0.4542560577392578\n",
            "Epoch: 15 Batch Number: 201 Loss: 1.1407592296600342 Time taken: 0.44594907760620117\n",
            "Epoch: 15 Batch Number: 202 Loss: 1.1188603639602661 Time taken: 0.4548346996307373\n",
            "Epoch: 15 Batch Number: 203 Loss: 1.1176918745040894 Time taken: 0.4600062370300293\n",
            "Epoch: 15 Batch Number: 204 Loss: 1.1426031589508057 Time taken: 0.43715929985046387\n",
            "Epoch: 15 Batch Number: 205 Loss: 1.0738234519958496 Time taken: 0.43691325187683105\n",
            "Epoch: 15 Batch Number: 206 Loss: 1.1242377758026123 Time taken: 0.4453623294830322\n",
            "Epoch: 15 Batch Number: 207 Loss: 1.1526837348937988 Time taken: 0.43794798851013184\n",
            "Epoch: 15 Batch Number: 208 Loss: 1.1301310062408447 Time taken: 0.4471414089202881\n",
            "Epoch: 15 Batch Number: 209 Loss: 1.1647124290466309 Time taken: 0.4559974670410156\n",
            "Epoch: 15 Batch Number: 210 Loss: 1.2155277729034424 Time taken: 0.4459078311920166\n",
            "Epoch: 15 Batch Number: 211 Loss: 1.1653902530670166 Time taken: 0.43987464904785156\n",
            "Epoch: 15 Batch Number: 212 Loss: 1.2908878326416016 Time taken: 0.4426579475402832\n",
            "Epoch: 15 Batch Number: 213 Loss: 1.1387720108032227 Time taken: 0.4596066474914551\n",
            "Epoch: 15 Batch Number: 214 Loss: 1.188870906829834 Time taken: 0.4380948543548584\n",
            "Epoch: 15 Batch Number: 215 Loss: 1.1327416896820068 Time taken: 0.4518134593963623\n",
            "Epoch: 15 Batch Number: 216 Loss: 1.176302194595337 Time taken: 0.4568321704864502\n",
            "Epoch: 15 Batch Number: 217 Loss: 1.244551658630371 Time taken: 0.44832515716552734\n",
            "Epoch: 15 Batch Number: 218 Loss: 1.2315709590911865 Time taken: 0.4451773166656494\n",
            "Epoch: 15 Batch Number: 219 Loss: 1.2239384651184082 Time taken: 0.4478573799133301\n",
            "Epoch: 15 Batch Number: 220 Loss: 1.1556888818740845 Time taken: 0.44405317306518555\n",
            "Epoch: 15 Batch Number: 221 Loss: 1.1518250703811646 Time taken: 0.4446895122528076\n",
            "Epoch: 15 Batch Number: 222 Loss: 1.1303858757019043 Time taken: 0.4621419906616211\n",
            "Epoch: 15 Batch Number: 223 Loss: 1.1560609340667725 Time taken: 0.45971226692199707\n",
            "Epoch: 15 Batch Number: 224 Loss: 1.1102900505065918 Time taken: 0.44727325439453125\n",
            "Epoch: 15 Batch Number: 225 Loss: 1.0857183933258057 Time taken: 0.45253586769104004\n",
            "Epoch: 15 Batch Number: 226 Loss: 1.1217466592788696 Time taken: 0.45856618881225586\n",
            "Epoch: 15 Batch Number: 227 Loss: 1.2296199798583984 Time taken: 0.4589567184448242\n",
            "Epoch: 15 Batch Number: 228 Loss: 1.188571572303772 Time taken: 0.45209360122680664\n",
            "Epoch: 15 Batch Number: 229 Loss: 1.2182066440582275 Time taken: 0.4486274719238281\n",
            "==========================================================================================\n",
            "Start of epoch 16\n",
            "Epoch: 16 Batch Number: 1 Loss: 1.143247127532959 Time taken: 0.4671022891998291\n",
            "Epoch: 16 Batch Number: 2 Loss: 1.2012361288070679 Time taken: 0.44368457794189453\n",
            "Epoch: 16 Batch Number: 3 Loss: 1.1653282642364502 Time taken: 0.4448380470275879\n",
            "Epoch: 16 Batch Number: 4 Loss: 1.1392109394073486 Time taken: 0.44059133529663086\n",
            "Epoch: 16 Batch Number: 5 Loss: 1.133655309677124 Time taken: 0.4551999568939209\n",
            "Epoch: 16 Batch Number: 6 Loss: 1.0680387020111084 Time taken: 0.44021153450012207\n",
            "Epoch: 16 Batch Number: 7 Loss: 1.0847325325012207 Time taken: 0.4600372314453125\n",
            "Epoch: 16 Batch Number: 8 Loss: 1.116952657699585 Time taken: 0.45295190811157227\n",
            "Epoch: 16 Batch Number: 9 Loss: 1.1036125421524048 Time taken: 0.4613525867462158\n",
            "Epoch: 16 Batch Number: 10 Loss: 1.1317999362945557 Time taken: 0.46127963066101074\n",
            "Epoch: 16 Batch Number: 11 Loss: 1.0887356996536255 Time taken: 0.45771169662475586\n",
            "Epoch: 16 Batch Number: 12 Loss: 1.0544215440750122 Time taken: 0.44443440437316895\n",
            "Epoch: 16 Batch Number: 13 Loss: 1.0877712965011597 Time taken: 0.44021129608154297\n",
            "Epoch: 16 Batch Number: 14 Loss: 1.0997384786605835 Time taken: 0.43884921073913574\n",
            "Epoch: 16 Batch Number: 15 Loss: 1.0505549907684326 Time taken: 0.4426548480987549\n",
            "Epoch: 16 Batch Number: 16 Loss: 1.075901985168457 Time taken: 0.4471461772918701\n",
            "Epoch: 16 Batch Number: 17 Loss: 1.1959651708602905 Time taken: 0.44896912574768066\n",
            "Epoch: 16 Batch Number: 18 Loss: 1.176114559173584 Time taken: 0.44394564628601074\n",
            "Epoch: 16 Batch Number: 19 Loss: 1.1807701587677002 Time taken: 0.44826269149780273\n",
            "Epoch: 16 Batch Number: 20 Loss: 1.062734842300415 Time taken: 0.44544339179992676\n",
            "Epoch: 16 Batch Number: 21 Loss: 1.2342469692230225 Time taken: 0.4525148868560791\n",
            "Epoch: 16 Batch Number: 22 Loss: 1.226015329360962 Time taken: 0.4458630084991455\n",
            "Epoch: 16 Batch Number: 23 Loss: 1.2273160219192505 Time taken: 0.46480226516723633\n",
            "Epoch: 16 Batch Number: 24 Loss: 1.201871633529663 Time taken: 0.47777223587036133\n",
            "Epoch: 16 Batch Number: 25 Loss: 1.1958879232406616 Time taken: 0.4657557010650635\n",
            "Epoch: 16 Batch Number: 26 Loss: 1.1985197067260742 Time taken: 0.4413304328918457\n",
            "Epoch: 16 Batch Number: 27 Loss: 1.1338050365447998 Time taken: 0.4413423538208008\n",
            "Epoch: 16 Batch Number: 28 Loss: 1.1431084871292114 Time taken: 0.44699645042419434\n",
            "Epoch: 16 Batch Number: 29 Loss: 1.1906919479370117 Time taken: 0.43848681449890137\n",
            "Epoch: 16 Batch Number: 30 Loss: 0.9555909633636475 Time taken: 0.44663548469543457\n",
            "Epoch: 16 Batch Number: 31 Loss: 1.1219271421432495 Time taken: 0.4417991638183594\n",
            "Epoch: 16 Batch Number: 32 Loss: 1.1258718967437744 Time taken: 0.4383382797241211\n",
            "Epoch: 16 Batch Number: 33 Loss: 1.159353494644165 Time taken: 0.45723390579223633\n",
            "Epoch: 16 Batch Number: 34 Loss: 1.1854989528656006 Time taken: 0.4439733028411865\n",
            "Epoch: 16 Batch Number: 35 Loss: 1.2262147665023804 Time taken: 0.44515085220336914\n",
            "Epoch: 16 Batch Number: 36 Loss: 1.264864444732666 Time taken: 0.4564676284790039\n",
            "Epoch: 16 Batch Number: 37 Loss: 1.1490519046783447 Time taken: 0.44773316383361816\n",
            "Epoch: 16 Batch Number: 38 Loss: 1.1958820819854736 Time taken: 0.456240177154541\n",
            "Epoch: 16 Batch Number: 39 Loss: 1.1278398036956787 Time taken: 0.46248722076416016\n",
            "Epoch: 16 Batch Number: 40 Loss: 1.1418685913085938 Time taken: 0.44996213912963867\n",
            "Epoch: 16 Batch Number: 41 Loss: 1.1102993488311768 Time taken: 0.4458436965942383\n",
            "Epoch: 16 Batch Number: 42 Loss: 1.106785774230957 Time taken: 0.44600987434387207\n",
            "Epoch: 16 Batch Number: 43 Loss: 1.089437484741211 Time taken: 0.44325757026672363\n",
            "Epoch: 16 Batch Number: 44 Loss: 1.0497344732284546 Time taken: 0.4479203224182129\n",
            "Epoch: 16 Batch Number: 45 Loss: 1.1125482320785522 Time taken: 0.4570190906524658\n",
            "Epoch: 16 Batch Number: 46 Loss: 1.2728691101074219 Time taken: 0.4481792449951172\n",
            "Epoch: 16 Batch Number: 47 Loss: 1.1332775354385376 Time taken: 0.4621164798736572\n",
            "Epoch: 16 Batch Number: 48 Loss: 1.194509744644165 Time taken: 0.44371914863586426\n",
            "Epoch: 16 Batch Number: 49 Loss: 1.189139723777771 Time taken: 0.4360201358795166\n",
            "Epoch: 16 Batch Number: 50 Loss: 1.100889801979065 Time taken: 0.4408690929412842\n",
            "Epoch: 16 Batch Number: 51 Loss: 1.088875651359558 Time taken: 0.4514474868774414\n",
            "Epoch: 16 Batch Number: 52 Loss: 1.2074432373046875 Time taken: 0.46065425872802734\n",
            "Epoch: 16 Batch Number: 53 Loss: 1.2444789409637451 Time taken: 0.45987629890441895\n",
            "Epoch: 16 Batch Number: 54 Loss: 1.2181038856506348 Time taken: 0.4532947540283203\n",
            "Epoch: 16 Batch Number: 55 Loss: 1.1925163269042969 Time taken: 0.4402780532836914\n",
            "Epoch: 16 Batch Number: 56 Loss: 1.282616376876831 Time taken: 0.44051098823547363\n",
            "Epoch: 16 Batch Number: 57 Loss: 1.159470558166504 Time taken: 0.4393184185028076\n",
            "Epoch: 16 Batch Number: 58 Loss: 1.118162751197815 Time taken: 0.4548161029815674\n",
            "Epoch: 16 Batch Number: 59 Loss: 1.1482422351837158 Time taken: 0.4427928924560547\n",
            "Epoch: 16 Batch Number: 60 Loss: 1.1347906589508057 Time taken: 0.439852237701416\n",
            "Epoch: 16 Batch Number: 61 Loss: 1.1630266904830933 Time taken: 0.4620933532714844\n",
            "Epoch: 16 Batch Number: 62 Loss: 1.0707730054855347 Time taken: 0.45238208770751953\n",
            "Epoch: 16 Batch Number: 63 Loss: 1.0992002487182617 Time taken: 0.447446346282959\n",
            "Epoch: 16 Batch Number: 64 Loss: 1.0533634424209595 Time taken: 0.448397159576416\n",
            "Epoch: 16 Batch Number: 65 Loss: 1.1011903285980225 Time taken: 0.45180511474609375\n",
            "Epoch: 16 Batch Number: 66 Loss: 1.116173267364502 Time taken: 0.4402890205383301\n",
            "Epoch: 16 Batch Number: 67 Loss: 1.099582552909851 Time taken: 0.44419360160827637\n",
            "Epoch: 16 Batch Number: 68 Loss: 1.1631593704223633 Time taken: 0.4586496353149414\n",
            "Epoch: 16 Batch Number: 69 Loss: 1.1424392461776733 Time taken: 0.4632096290588379\n",
            "Epoch: 16 Batch Number: 70 Loss: 1.1475896835327148 Time taken: 0.46602296829223633\n",
            "Epoch: 16 Batch Number: 71 Loss: 1.117178201675415 Time taken: 0.4665224552154541\n",
            "Epoch: 16 Batch Number: 72 Loss: 1.1352778673171997 Time taken: 0.46314096450805664\n",
            "Epoch: 16 Batch Number: 73 Loss: 1.0806334018707275 Time taken: 0.44055867195129395\n",
            "Epoch: 16 Batch Number: 74 Loss: 1.1835253238677979 Time taken: 0.4403719902038574\n",
            "Epoch: 16 Batch Number: 75 Loss: 1.025716781616211 Time taken: 0.45026516914367676\n",
            "Epoch: 16 Batch Number: 76 Loss: 1.0654340982437134 Time taken: 0.4396815299987793\n",
            "Epoch: 16 Batch Number: 77 Loss: 1.1580320596694946 Time taken: 0.44875621795654297\n",
            "Epoch: 16 Batch Number: 78 Loss: 1.053516149520874 Time taken: 0.4525017738342285\n",
            "Epoch: 16 Batch Number: 79 Loss: 1.1305737495422363 Time taken: 0.4468858242034912\n",
            "Epoch: 16 Batch Number: 80 Loss: 1.1051158905029297 Time taken: 0.4407532215118408\n",
            "Epoch: 16 Batch Number: 81 Loss: 1.128493309020996 Time taken: 0.445009708404541\n",
            "Epoch: 16 Batch Number: 82 Loss: 1.126592755317688 Time taken: 0.4611937999725342\n",
            "Epoch: 16 Batch Number: 83 Loss: 1.186170220375061 Time taken: 0.4376037120819092\n",
            "Epoch: 16 Batch Number: 84 Loss: 1.1459108591079712 Time taken: 0.4489784240722656\n",
            "Epoch: 16 Batch Number: 85 Loss: 1.1197490692138672 Time taken: 0.44719839096069336\n",
            "Epoch: 16 Batch Number: 86 Loss: 1.1140327453613281 Time taken: 0.4440634250640869\n",
            "Epoch: 16 Batch Number: 87 Loss: 1.1932406425476074 Time taken: 0.4533205032348633\n",
            "Epoch: 16 Batch Number: 88 Loss: 1.0803037881851196 Time taken: 0.46490025520324707\n",
            "Epoch: 16 Batch Number: 89 Loss: 1.0808111429214478 Time taken: 0.4576585292816162\n",
            "Epoch: 16 Batch Number: 90 Loss: 1.126515507698059 Time taken: 0.45160961151123047\n",
            "Epoch: 16 Batch Number: 91 Loss: 1.0870624780654907 Time taken: 0.46732163429260254\n",
            "Epoch: 16 Batch Number: 92 Loss: 1.0535392761230469 Time taken: 0.43844056129455566\n",
            "Epoch: 16 Batch Number: 93 Loss: 1.0723685026168823 Time taken: 0.444446325302124\n",
            "Epoch: 16 Batch Number: 94 Loss: 1.2214868068695068 Time taken: 0.4409172534942627\n",
            "Epoch: 16 Batch Number: 95 Loss: 1.1678601503372192 Time taken: 0.4349031448364258\n",
            "Epoch: 16 Batch Number: 96 Loss: 1.214472770690918 Time taken: 0.4649815559387207\n",
            "Epoch: 16 Batch Number: 97 Loss: 1.1575489044189453 Time taken: 0.45671892166137695\n",
            "Epoch: 16 Batch Number: 98 Loss: 1.2155849933624268 Time taken: 0.44356489181518555\n",
            "Epoch: 16 Batch Number: 99 Loss: 1.2468260526657104 Time taken: 0.43823862075805664\n",
            "Epoch: 16 Batch Number: 100 Loss: 1.1744961738586426 Time taken: 0.450908899307251\n",
            "Epoch: 16 Batch Number: 101 Loss: 1.1618732213974 Time taken: 0.43741893768310547\n",
            "Epoch: 16 Batch Number: 102 Loss: 1.1971663236618042 Time taken: 0.4425480365753174\n",
            "Epoch: 16 Batch Number: 103 Loss: 1.260111927986145 Time taken: 0.44504332542419434\n",
            "Epoch: 16 Batch Number: 104 Loss: 1.1541792154312134 Time taken: 0.43903636932373047\n",
            "Epoch: 16 Batch Number: 105 Loss: 1.2299495935440063 Time taken: 0.45715880393981934\n",
            "Epoch: 16 Batch Number: 106 Loss: 1.1961324214935303 Time taken: 0.4445931911468506\n",
            "Epoch: 16 Batch Number: 107 Loss: 1.2001854181289673 Time taken: 0.4473397731781006\n",
            "Epoch: 16 Batch Number: 108 Loss: 1.1759955883026123 Time taken: 0.45965099334716797\n",
            "Epoch: 16 Batch Number: 109 Loss: 1.1625797748565674 Time taken: 0.44681549072265625\n",
            "Epoch: 16 Batch Number: 110 Loss: 1.0237748622894287 Time taken: 0.4454803466796875\n",
            "Epoch: 16 Batch Number: 111 Loss: 1.0981011390686035 Time taken: 0.44706153869628906\n",
            "Epoch: 16 Batch Number: 112 Loss: 1.1142334938049316 Time taken: 0.4623730182647705\n",
            "Epoch: 16 Batch Number: 113 Loss: 1.1396842002868652 Time taken: 0.4556915760040283\n",
            "Epoch: 16 Batch Number: 114 Loss: 1.1847862005233765 Time taken: 0.46199584007263184\n",
            "Epoch: 16 Batch Number: 115 Loss: 1.1762551069259644 Time taken: 0.4446220397949219\n",
            "Epoch: 16 Batch Number: 116 Loss: 1.0513789653778076 Time taken: 0.45108795166015625\n",
            "Epoch: 16 Batch Number: 117 Loss: 1.1447501182556152 Time taken: 0.46146273612976074\n",
            "Epoch: 16 Batch Number: 118 Loss: 1.1014444828033447 Time taken: 0.4504354000091553\n",
            "Epoch: 16 Batch Number: 119 Loss: 1.142298698425293 Time taken: 0.443251371383667\n",
            "Epoch: 16 Batch Number: 120 Loss: 1.1776882410049438 Time taken: 0.45004773139953613\n",
            "Epoch: 16 Batch Number: 121 Loss: 1.1196073293685913 Time taken: 0.45754313468933105\n",
            "Epoch: 16 Batch Number: 122 Loss: 1.1073611974716187 Time taken: 0.45563673973083496\n",
            "Epoch: 16 Batch Number: 123 Loss: 1.1371362209320068 Time taken: 0.4537315368652344\n",
            "Epoch: 16 Batch Number: 124 Loss: 1.1068843603134155 Time taken: 0.44671630859375\n",
            "Epoch: 16 Batch Number: 125 Loss: 1.1413931846618652 Time taken: 0.44716644287109375\n",
            "Epoch: 16 Batch Number: 126 Loss: 1.1596055030822754 Time taken: 0.4644322395324707\n",
            "Epoch: 16 Batch Number: 127 Loss: 1.1694045066833496 Time taken: 0.4487617015838623\n",
            "Epoch: 16 Batch Number: 128 Loss: 1.1205775737762451 Time taken: 0.45625782012939453\n",
            "Epoch: 16 Batch Number: 129 Loss: 1.1186213493347168 Time taken: 0.44843268394470215\n",
            "Epoch: 16 Batch Number: 130 Loss: 1.0958187580108643 Time taken: 0.46063899993896484\n",
            "Epoch: 16 Batch Number: 131 Loss: 1.1816115379333496 Time taken: 0.4490211009979248\n",
            "Epoch: 16 Batch Number: 132 Loss: 1.0278406143188477 Time taken: 0.44611310958862305\n",
            "Epoch: 16 Batch Number: 133 Loss: 1.1872084140777588 Time taken: 0.45148348808288574\n",
            "Epoch: 16 Batch Number: 134 Loss: 1.1854097843170166 Time taken: 0.45954275131225586\n",
            "Epoch: 16 Batch Number: 135 Loss: 1.1896880865097046 Time taken: 0.4438343048095703\n",
            "Epoch: 16 Batch Number: 136 Loss: 1.1820847988128662 Time taken: 0.44080662727355957\n",
            "Epoch: 16 Batch Number: 137 Loss: 1.134027361869812 Time taken: 0.45813632011413574\n",
            "Epoch: 16 Batch Number: 138 Loss: 1.1641864776611328 Time taken: 0.45498037338256836\n",
            "Epoch: 16 Batch Number: 139 Loss: 1.1711094379425049 Time taken: 0.4429616928100586\n",
            "Epoch: 16 Batch Number: 140 Loss: 1.13277006149292 Time taken: 0.45714259147644043\n",
            "Epoch: 16 Batch Number: 141 Loss: 1.1847875118255615 Time taken: 0.45082998275756836\n",
            "Epoch: 16 Batch Number: 142 Loss: 1.2228704690933228 Time taken: 0.4483206272125244\n",
            "Epoch: 16 Batch Number: 143 Loss: 1.282538890838623 Time taken: 0.45410990715026855\n",
            "Epoch: 16 Batch Number: 144 Loss: 1.2811646461486816 Time taken: 0.4492678642272949\n",
            "Epoch: 16 Batch Number: 145 Loss: 1.2706220149993896 Time taken: 0.4555668830871582\n",
            "Epoch: 16 Batch Number: 146 Loss: 1.2466238737106323 Time taken: 0.4442620277404785\n",
            "Epoch: 16 Batch Number: 147 Loss: 1.115532636642456 Time taken: 0.4443047046661377\n",
            "Epoch: 16 Batch Number: 148 Loss: 1.0813777446746826 Time taken: 0.44004273414611816\n",
            "Epoch: 16 Batch Number: 149 Loss: 1.1654865741729736 Time taken: 0.44028782844543457\n",
            "Epoch: 16 Batch Number: 150 Loss: 1.2507764101028442 Time taken: 0.4546494483947754\n",
            "Epoch: 16 Batch Number: 151 Loss: 1.2553589344024658 Time taken: 0.4598069190979004\n",
            "Epoch: 16 Batch Number: 152 Loss: 1.1889114379882812 Time taken: 0.44156908988952637\n",
            "Epoch: 16 Batch Number: 153 Loss: 1.1177473068237305 Time taken: 0.4362916946411133\n",
            "Epoch: 16 Batch Number: 154 Loss: 1.1084883213043213 Time taken: 0.43741345405578613\n",
            "Epoch: 16 Batch Number: 155 Loss: 1.0977942943572998 Time taken: 0.4519467353820801\n",
            "Epoch: 16 Batch Number: 156 Loss: 1.1684021949768066 Time taken: 0.44346046447753906\n",
            "Epoch: 16 Batch Number: 157 Loss: 1.0515161752700806 Time taken: 0.44524049758911133\n",
            "Epoch: 16 Batch Number: 158 Loss: 1.1177406311035156 Time taken: 0.4419538974761963\n",
            "Epoch: 16 Batch Number: 159 Loss: 1.0558456182479858 Time taken: 0.4440121650695801\n",
            "Epoch: 16 Batch Number: 160 Loss: 0.9664047956466675 Time taken: 0.44289708137512207\n",
            "Epoch: 16 Batch Number: 161 Loss: 1.0362650156021118 Time taken: 0.4406301975250244\n",
            "Epoch: 16 Batch Number: 162 Loss: 1.004256248474121 Time taken: 0.4524226188659668\n",
            "Epoch: 16 Batch Number: 163 Loss: 1.1203210353851318 Time taken: 0.4467294216156006\n",
            "Epoch: 16 Batch Number: 164 Loss: 1.2100181579589844 Time taken: 0.4437246322631836\n",
            "Epoch: 16 Batch Number: 165 Loss: 1.1253242492675781 Time taken: 0.4450411796569824\n",
            "Epoch: 16 Batch Number: 166 Loss: 1.1782481670379639 Time taken: 0.445723295211792\n",
            "Epoch: 16 Batch Number: 167 Loss: 1.1426584720611572 Time taken: 0.46679210662841797\n",
            "Epoch: 16 Batch Number: 168 Loss: 1.1385899782180786 Time taken: 0.44562792778015137\n",
            "Epoch: 16 Batch Number: 169 Loss: 1.1416345834732056 Time taken: 0.465146541595459\n",
            "Epoch: 16 Batch Number: 170 Loss: 1.070196509361267 Time taken: 0.43874311447143555\n",
            "Epoch: 16 Batch Number: 171 Loss: 1.0139119625091553 Time taken: 0.44737744331359863\n",
            "Epoch: 16 Batch Number: 172 Loss: 1.0819764137268066 Time taken: 0.444732666015625\n",
            "Epoch: 16 Batch Number: 173 Loss: 1.068221926689148 Time taken: 0.44738006591796875\n",
            "Epoch: 16 Batch Number: 174 Loss: 1.009071946144104 Time taken: 0.45732951164245605\n",
            "Epoch: 16 Batch Number: 175 Loss: 1.1209440231323242 Time taken: 0.44523167610168457\n",
            "Epoch: 16 Batch Number: 176 Loss: 1.0950216054916382 Time taken: 0.4553554058074951\n",
            "Epoch: 16 Batch Number: 177 Loss: 1.1180243492126465 Time taken: 0.43877553939819336\n",
            "Epoch: 16 Batch Number: 178 Loss: 1.1817777156829834 Time taken: 0.45066070556640625\n",
            "Epoch: 16 Batch Number: 179 Loss: 1.1766104698181152 Time taken: 0.44062209129333496\n",
            "Epoch: 16 Batch Number: 180 Loss: 1.1263985633850098 Time taken: 0.45821452140808105\n",
            "Epoch: 16 Batch Number: 181 Loss: 1.0954355001449585 Time taken: 0.43958091735839844\n",
            "Epoch: 16 Batch Number: 182 Loss: 1.0415098667144775 Time taken: 0.4438610076904297\n",
            "Epoch: 16 Batch Number: 183 Loss: 1.0949816703796387 Time taken: 0.4452812671661377\n",
            "Epoch: 16 Batch Number: 184 Loss: 1.1596137285232544 Time taken: 0.4656968116760254\n",
            "Epoch: 16 Batch Number: 185 Loss: 1.1867156028747559 Time taken: 0.46138501167297363\n",
            "Epoch: 16 Batch Number: 186 Loss: 0.9997290968894958 Time taken: 0.4498410224914551\n",
            "Epoch: 16 Batch Number: 187 Loss: 1.0756957530975342 Time taken: 0.469463586807251\n",
            "Epoch: 16 Batch Number: 188 Loss: 1.0797958374023438 Time taken: 0.44042468070983887\n",
            "Epoch: 16 Batch Number: 189 Loss: 1.1163203716278076 Time taken: 0.4355201721191406\n",
            "Epoch: 16 Batch Number: 190 Loss: 1.288055419921875 Time taken: 0.43523192405700684\n",
            "Epoch: 16 Batch Number: 191 Loss: 1.424987554550171 Time taken: 0.4410538673400879\n",
            "Epoch: 16 Batch Number: 192 Loss: 1.175318717956543 Time taken: 0.44185709953308105\n",
            "Epoch: 16 Batch Number: 193 Loss: 1.2796956300735474 Time taken: 0.4542262554168701\n",
            "Epoch: 16 Batch Number: 194 Loss: 1.196283221244812 Time taken: 0.4486689567565918\n",
            "Epoch: 16 Batch Number: 195 Loss: 1.1425180435180664 Time taken: 0.45379209518432617\n",
            "Epoch: 16 Batch Number: 196 Loss: 1.0965887308120728 Time taken: 0.468289852142334\n",
            "Epoch: 16 Batch Number: 197 Loss: 1.1674492359161377 Time taken: 0.4402947425842285\n",
            "Epoch: 16 Batch Number: 198 Loss: 1.085442066192627 Time taken: 0.44942378997802734\n",
            "Epoch: 16 Batch Number: 199 Loss: 1.1279652118682861 Time taken: 0.44500136375427246\n",
            "Epoch: 16 Batch Number: 200 Loss: 1.0792404413223267 Time taken: 0.4485912322998047\n",
            "Epoch: 16 Batch Number: 201 Loss: 1.1280750036239624 Time taken: 0.4423031806945801\n",
            "Epoch: 16 Batch Number: 202 Loss: 1.107794165611267 Time taken: 0.4441821575164795\n",
            "Epoch: 16 Batch Number: 203 Loss: 1.1047207117080688 Time taken: 0.4484059810638428\n",
            "Epoch: 16 Batch Number: 204 Loss: 1.1293432712554932 Time taken: 0.4442014694213867\n",
            "Epoch: 16 Batch Number: 205 Loss: 1.0628604888916016 Time taken: 0.4500861167907715\n",
            "Epoch: 16 Batch Number: 206 Loss: 1.1122722625732422 Time taken: 0.441237211227417\n",
            "Epoch: 16 Batch Number: 207 Loss: 1.144069790840149 Time taken: 0.4577486515045166\n",
            "Epoch: 16 Batch Number: 208 Loss: 1.1198513507843018 Time taken: 0.45929408073425293\n",
            "Epoch: 16 Batch Number: 209 Loss: 1.1563246250152588 Time taken: 0.4476900100708008\n",
            "Epoch: 16 Batch Number: 210 Loss: 1.2086257934570312 Time taken: 0.45933961868286133\n",
            "Epoch: 16 Batch Number: 211 Loss: 1.1723270416259766 Time taken: 0.4468994140625\n",
            "Epoch: 16 Batch Number: 212 Loss: 1.2898318767547607 Time taken: 0.4563889503479004\n",
            "Epoch: 16 Batch Number: 213 Loss: 1.1313867568969727 Time taken: 0.44652628898620605\n",
            "Epoch: 16 Batch Number: 214 Loss: 1.1739482879638672 Time taken: 0.46503639221191406\n",
            "Epoch: 16 Batch Number: 215 Loss: 1.1096398830413818 Time taken: 0.4488406181335449\n",
            "Epoch: 16 Batch Number: 216 Loss: 1.167158603668213 Time taken: 0.44687366485595703\n",
            "Epoch: 16 Batch Number: 217 Loss: 1.2308576107025146 Time taken: 0.4429478645324707\n",
            "Epoch: 16 Batch Number: 218 Loss: 1.208865761756897 Time taken: 0.44434332847595215\n",
            "Epoch: 16 Batch Number: 219 Loss: 1.213644027709961 Time taken: 0.4429466724395752\n",
            "Epoch: 16 Batch Number: 220 Loss: 1.1462047100067139 Time taken: 0.4361910820007324\n",
            "Epoch: 16 Batch Number: 221 Loss: 1.1408700942993164 Time taken: 0.45561933517456055\n",
            "Epoch: 16 Batch Number: 222 Loss: 1.116714358329773 Time taken: 0.4460299015045166\n",
            "Epoch: 16 Batch Number: 223 Loss: 1.1401692628860474 Time taken: 0.4422492980957031\n",
            "Epoch: 16 Batch Number: 224 Loss: 1.0909647941589355 Time taken: 0.43919825553894043\n",
            "Epoch: 16 Batch Number: 225 Loss: 1.0716025829315186 Time taken: 0.4395618438720703\n",
            "Epoch: 16 Batch Number: 226 Loss: 1.1034290790557861 Time taken: 0.4443516731262207\n",
            "Epoch: 16 Batch Number: 227 Loss: 1.2282049655914307 Time taken: 0.449002742767334\n",
            "Epoch: 16 Batch Number: 228 Loss: 1.1883299350738525 Time taken: 0.46669554710388184\n",
            "Epoch: 16 Batch Number: 229 Loss: 1.2052748203277588 Time taken: 0.46801137924194336\n",
            "==========================================================================================\n",
            "Start of epoch 17\n",
            "Epoch: 17 Batch Number: 1 Loss: 1.1305673122406006 Time taken: 0.4432988166809082\n",
            "Epoch: 17 Batch Number: 2 Loss: 1.1903939247131348 Time taken: 0.4363667964935303\n",
            "Epoch: 17 Batch Number: 3 Loss: 1.1540189981460571 Time taken: 0.44262123107910156\n",
            "Epoch: 17 Batch Number: 4 Loss: 1.124084234237671 Time taken: 0.4608314037322998\n",
            "Epoch: 17 Batch Number: 5 Loss: 1.1216095685958862 Time taken: 0.44695043563842773\n",
            "Epoch: 17 Batch Number: 6 Loss: 1.0589812994003296 Time taken: 0.43938565254211426\n",
            "Epoch: 17 Batch Number: 7 Loss: 1.0707519054412842 Time taken: 0.45319414138793945\n",
            "Epoch: 17 Batch Number: 8 Loss: 1.1081438064575195 Time taken: 0.44092512130737305\n",
            "Epoch: 17 Batch Number: 9 Loss: 1.08888578414917 Time taken: 0.4476203918457031\n",
            "Epoch: 17 Batch Number: 10 Loss: 1.1229041814804077 Time taken: 0.4384794235229492\n",
            "Epoch: 17 Batch Number: 11 Loss: 1.0765093564987183 Time taken: 0.4356701374053955\n",
            "Epoch: 17 Batch Number: 12 Loss: 1.0470339059829712 Time taken: 0.4385075569152832\n",
            "Epoch: 17 Batch Number: 13 Loss: 1.0813419818878174 Time taken: 0.4448246955871582\n",
            "Epoch: 17 Batch Number: 14 Loss: 1.0841994285583496 Time taken: 0.4448988437652588\n",
            "Epoch: 17 Batch Number: 15 Loss: 1.0203044414520264 Time taken: 0.4462275505065918\n",
            "Epoch: 17 Batch Number: 16 Loss: 1.057866096496582 Time taken: 0.43573474884033203\n",
            "Epoch: 17 Batch Number: 17 Loss: 1.187148094177246 Time taken: 0.455338716506958\n",
            "Epoch: 17 Batch Number: 18 Loss: 1.1626205444335938 Time taken: 0.45383381843566895\n",
            "Epoch: 17 Batch Number: 19 Loss: 1.1671645641326904 Time taken: 0.4601600170135498\n",
            "Epoch: 17 Batch Number: 20 Loss: 1.0500121116638184 Time taken: 0.4658069610595703\n",
            "Epoch: 17 Batch Number: 21 Loss: 1.2227344512939453 Time taken: 0.45659971237182617\n",
            "Epoch: 17 Batch Number: 22 Loss: 1.2135175466537476 Time taken: 0.43897509574890137\n",
            "Epoch: 17 Batch Number: 23 Loss: 1.211760401725769 Time taken: 0.45804286003112793\n",
            "Epoch: 17 Batch Number: 24 Loss: 1.1908024549484253 Time taken: 0.45651674270629883\n",
            "Epoch: 17 Batch Number: 25 Loss: 1.1848869323730469 Time taken: 0.4580495357513428\n",
            "Epoch: 17 Batch Number: 26 Loss: 1.1878023147583008 Time taken: 0.457913875579834\n",
            "Epoch: 17 Batch Number: 27 Loss: 1.1254607439041138 Time taken: 0.455599308013916\n",
            "Epoch: 17 Batch Number: 28 Loss: 1.1322391033172607 Time taken: 0.4721846580505371\n",
            "Epoch: 17 Batch Number: 29 Loss: 1.1805391311645508 Time taken: 0.46318531036376953\n",
            "Epoch: 17 Batch Number: 30 Loss: 0.9428112506866455 Time taken: 0.45748209953308105\n",
            "Epoch: 17 Batch Number: 31 Loss: 1.113424301147461 Time taken: 0.4507176876068115\n",
            "Epoch: 17 Batch Number: 32 Loss: 1.1159727573394775 Time taken: 0.4559357166290283\n",
            "Epoch: 17 Batch Number: 33 Loss: 1.1495637893676758 Time taken: 0.450000524520874\n",
            "Epoch: 17 Batch Number: 34 Loss: 1.1797446012496948 Time taken: 0.4605536460876465\n",
            "Epoch: 17 Batch Number: 35 Loss: 1.2188715934753418 Time taken: 0.4794340133666992\n",
            "Epoch: 17 Batch Number: 36 Loss: 1.2480854988098145 Time taken: 0.476254940032959\n",
            "Epoch: 17 Batch Number: 37 Loss: 1.1403388977050781 Time taken: 0.4564669132232666\n",
            "Epoch: 17 Batch Number: 38 Loss: 1.1896235942840576 Time taken: 0.4411168098449707\n",
            "Epoch: 17 Batch Number: 39 Loss: 1.1178522109985352 Time taken: 0.4406626224517822\n",
            "Epoch: 17 Batch Number: 40 Loss: 1.1310789585113525 Time taken: 0.4449577331542969\n",
            "Epoch: 17 Batch Number: 41 Loss: 1.1023738384246826 Time taken: 0.4571068286895752\n",
            "Epoch: 17 Batch Number: 42 Loss: 1.1027878522872925 Time taken: 0.4627392292022705\n",
            "Epoch: 17 Batch Number: 43 Loss: 1.0804173946380615 Time taken: 0.44282007217407227\n",
            "Epoch: 17 Batch Number: 44 Loss: 1.0421262979507446 Time taken: 0.4442179203033447\n",
            "Epoch: 17 Batch Number: 45 Loss: 1.1038172245025635 Time taken: 0.45475125312805176\n",
            "Epoch: 17 Batch Number: 46 Loss: 1.266022801399231 Time taken: 0.44292712211608887\n",
            "Epoch: 17 Batch Number: 47 Loss: 1.1187057495117188 Time taken: 0.4579470157623291\n",
            "Epoch: 17 Batch Number: 48 Loss: 1.1839661598205566 Time taken: 0.4619600772857666\n",
            "Epoch: 17 Batch Number: 49 Loss: 1.1789815425872803 Time taken: 0.44423604011535645\n",
            "Epoch: 17 Batch Number: 50 Loss: 1.0959830284118652 Time taken: 0.43564748764038086\n",
            "Epoch: 17 Batch Number: 51 Loss: 1.0807504653930664 Time taken: 0.4371933937072754\n",
            "Epoch: 17 Batch Number: 52 Loss: 1.1989843845367432 Time taken: 0.44672155380249023\n",
            "Epoch: 17 Batch Number: 53 Loss: 1.229447603225708 Time taken: 0.4452240467071533\n",
            "Epoch: 17 Batch Number: 54 Loss: 1.2042897939682007 Time taken: 0.44977617263793945\n",
            "Epoch: 17 Batch Number: 55 Loss: 1.1798720359802246 Time taken: 0.44460248947143555\n",
            "Epoch: 17 Batch Number: 56 Loss: 1.2723994255065918 Time taken: 0.44614386558532715\n",
            "Epoch: 17 Batch Number: 57 Loss: 1.1516668796539307 Time taken: 0.44109463691711426\n",
            "Epoch: 17 Batch Number: 58 Loss: 1.1082733869552612 Time taken: 0.45396971702575684\n",
            "Epoch: 17 Batch Number: 59 Loss: 1.1356663703918457 Time taken: 0.44609832763671875\n",
            "Epoch: 17 Batch Number: 60 Loss: 1.1220353841781616 Time taken: 0.43988585472106934\n",
            "Epoch: 17 Batch Number: 61 Loss: 1.1507582664489746 Time taken: 0.43993401527404785\n",
            "Epoch: 17 Batch Number: 62 Loss: 1.0597155094146729 Time taken: 0.44820690155029297\n",
            "Epoch: 17 Batch Number: 63 Loss: 1.0932304859161377 Time taken: 0.4562695026397705\n",
            "Epoch: 17 Batch Number: 64 Loss: 1.040716290473938 Time taken: 0.4417703151702881\n",
            "Epoch: 17 Batch Number: 65 Loss: 1.0892562866210938 Time taken: 0.4618847370147705\n",
            "Epoch: 17 Batch Number: 66 Loss: 1.1031177043914795 Time taken: 0.4480905532836914\n",
            "Epoch: 17 Batch Number: 67 Loss: 1.0868515968322754 Time taken: 0.4450347423553467\n",
            "Epoch: 17 Batch Number: 68 Loss: 1.1507445573806763 Time taken: 0.44401001930236816\n",
            "Epoch: 17 Batch Number: 69 Loss: 1.1309846639633179 Time taken: 0.4526083469390869\n",
            "Epoch: 17 Batch Number: 70 Loss: 1.1360828876495361 Time taken: 0.4503781795501709\n",
            "Epoch: 17 Batch Number: 71 Loss: 1.1054176092147827 Time taken: 0.4531538486480713\n",
            "Epoch: 17 Batch Number: 72 Loss: 1.1263234615325928 Time taken: 0.45264673233032227\n",
            "Epoch: 17 Batch Number: 73 Loss: 1.0676357746124268 Time taken: 0.45655369758605957\n",
            "Epoch: 17 Batch Number: 74 Loss: 1.1609410047531128 Time taken: 0.4533970355987549\n",
            "Epoch: 17 Batch Number: 75 Loss: 1.01608145236969 Time taken: 0.44765353202819824\n",
            "Epoch: 17 Batch Number: 76 Loss: 1.054046392440796 Time taken: 0.4481782913208008\n",
            "Epoch: 17 Batch Number: 77 Loss: 1.1466619968414307 Time taken: 0.4519338607788086\n",
            "Epoch: 17 Batch Number: 78 Loss: 1.0425877571105957 Time taken: 0.441725492477417\n",
            "Epoch: 17 Batch Number: 79 Loss: 1.1171174049377441 Time taken: 0.4527394771575928\n",
            "Epoch: 17 Batch Number: 80 Loss: 1.094576358795166 Time taken: 0.4486696720123291\n",
            "Epoch: 17 Batch Number: 81 Loss: 1.1165125370025635 Time taken: 0.44669604301452637\n",
            "Epoch: 17 Batch Number: 82 Loss: 1.1218063831329346 Time taken: 0.45194387435913086\n",
            "Epoch: 17 Batch Number: 83 Loss: 1.181551218032837 Time taken: 0.46564626693725586\n",
            "Epoch: 17 Batch Number: 84 Loss: 1.139248251914978 Time taken: 0.4488852024078369\n",
            "Epoch: 17 Batch Number: 85 Loss: 1.115706443786621 Time taken: 0.44391584396362305\n",
            "Epoch: 17 Batch Number: 86 Loss: 1.1042791604995728 Time taken: 0.45938730239868164\n",
            "Epoch: 17 Batch Number: 87 Loss: 1.1913549900054932 Time taken: 0.4651329517364502\n",
            "Epoch: 17 Batch Number: 88 Loss: 1.0760284662246704 Time taken: 0.46288132667541504\n",
            "Epoch: 17 Batch Number: 89 Loss: 1.0705208778381348 Time taken: 0.45629000663757324\n",
            "Epoch: 17 Batch Number: 90 Loss: 1.1190927028656006 Time taken: 0.4384267330169678\n",
            "Epoch: 17 Batch Number: 91 Loss: 1.074347734451294 Time taken: 0.4359591007232666\n",
            "Epoch: 17 Batch Number: 92 Loss: 1.04607355594635 Time taken: 0.45017457008361816\n",
            "Epoch: 17 Batch Number: 93 Loss: 1.0636773109436035 Time taken: 0.4470229148864746\n",
            "Epoch: 17 Batch Number: 94 Loss: 1.2122514247894287 Time taken: 0.44516921043395996\n",
            "Epoch: 17 Batch Number: 95 Loss: 1.1587774753570557 Time taken: 0.45514464378356934\n",
            "Epoch: 17 Batch Number: 96 Loss: 1.1924099922180176 Time taken: 0.4443225860595703\n",
            "Epoch: 17 Batch Number: 97 Loss: 1.13673734664917 Time taken: 0.44543886184692383\n",
            "Epoch: 17 Batch Number: 98 Loss: 1.1971372365951538 Time taken: 0.4555845260620117\n",
            "Epoch: 17 Batch Number: 99 Loss: 1.2349445819854736 Time taken: 0.44223833084106445\n",
            "Epoch: 17 Batch Number: 100 Loss: 1.1550108194351196 Time taken: 0.4616091251373291\n",
            "Epoch: 17 Batch Number: 101 Loss: 1.149369478225708 Time taken: 0.45973682403564453\n",
            "Epoch: 17 Batch Number: 102 Loss: 1.1863930225372314 Time taken: 0.4446218013763428\n",
            "Epoch: 17 Batch Number: 103 Loss: 1.249877691268921 Time taken: 0.44280052185058594\n",
            "Epoch: 17 Batch Number: 104 Loss: 1.1400797367095947 Time taken: 0.4548630714416504\n",
            "Epoch: 17 Batch Number: 105 Loss: 1.215788722038269 Time taken: 0.4469113349914551\n",
            "Epoch: 17 Batch Number: 106 Loss: 1.1812552213668823 Time taken: 0.45946741104125977\n",
            "Epoch: 17 Batch Number: 107 Loss: 1.1834440231323242 Time taken: 0.4566643238067627\n",
            "Epoch: 17 Batch Number: 108 Loss: 1.147061824798584 Time taken: 0.46743345260620117\n",
            "Epoch: 17 Batch Number: 109 Loss: 1.144782543182373 Time taken: 0.45630955696105957\n",
            "Epoch: 17 Batch Number: 110 Loss: 1.0073890686035156 Time taken: 0.43430042266845703\n",
            "Epoch: 17 Batch Number: 111 Loss: 1.0817214250564575 Time taken: 0.4578728675842285\n",
            "Epoch: 17 Batch Number: 112 Loss: 1.0957305431365967 Time taken: 0.4446597099304199\n",
            "Epoch: 17 Batch Number: 113 Loss: 1.1182258129119873 Time taken: 0.45781731605529785\n",
            "Epoch: 17 Batch Number: 114 Loss: 1.1701399087905884 Time taken: 0.46452784538269043\n",
            "Epoch: 17 Batch Number: 115 Loss: 1.156792402267456 Time taken: 0.4607675075531006\n",
            "Epoch: 17 Batch Number: 116 Loss: 1.0394995212554932 Time taken: 0.4487173557281494\n",
            "Epoch: 17 Batch Number: 117 Loss: 1.1303842067718506 Time taken: 0.4420008659362793\n",
            "Epoch: 17 Batch Number: 118 Loss: 1.089317798614502 Time taken: 0.4538593292236328\n",
            "Epoch: 17 Batch Number: 119 Loss: 1.1310029029846191 Time taken: 0.4438943862915039\n",
            "Epoch: 17 Batch Number: 120 Loss: 1.1682205200195312 Time taken: 0.440535306930542\n",
            "Epoch: 17 Batch Number: 121 Loss: 1.113853931427002 Time taken: 0.44640111923217773\n",
            "Epoch: 17 Batch Number: 122 Loss: 1.100092887878418 Time taken: 0.46256160736083984\n",
            "Epoch: 17 Batch Number: 123 Loss: 1.1263494491577148 Time taken: 0.4607088565826416\n",
            "Epoch: 17 Batch Number: 124 Loss: 1.0925016403198242 Time taken: 0.4579594135284424\n",
            "Epoch: 17 Batch Number: 125 Loss: 1.1270339488983154 Time taken: 0.4385647773742676\n",
            "Epoch: 17 Batch Number: 126 Loss: 1.1456067562103271 Time taken: 0.45909738540649414\n",
            "Epoch: 17 Batch Number: 127 Loss: 1.162009358406067 Time taken: 0.440793514251709\n",
            "Epoch: 17 Batch Number: 128 Loss: 1.1048983335494995 Time taken: 0.4525611400604248\n",
            "Epoch: 17 Batch Number: 129 Loss: 1.1086781024932861 Time taken: 0.4357471466064453\n",
            "Epoch: 17 Batch Number: 130 Loss: 1.085229516029358 Time taken: 0.4597153663635254\n",
            "Epoch: 17 Batch Number: 131 Loss: 1.1738066673278809 Time taken: 0.43401432037353516\n",
            "Epoch: 17 Batch Number: 132 Loss: 1.0160099267959595 Time taken: 0.43242549896240234\n",
            "Epoch: 17 Batch Number: 133 Loss: 1.1823296546936035 Time taken: 0.4384346008300781\n",
            "Epoch: 17 Batch Number: 134 Loss: 1.1740586757659912 Time taken: 0.44011497497558594\n",
            "Epoch: 17 Batch Number: 135 Loss: 1.1775965690612793 Time taken: 0.44839930534362793\n",
            "Epoch: 17 Batch Number: 136 Loss: 1.170599341392517 Time taken: 0.4605753421783447\n",
            "Epoch: 17 Batch Number: 137 Loss: 1.1214946508407593 Time taken: 0.445326566696167\n",
            "Epoch: 17 Batch Number: 138 Loss: 1.154099464416504 Time taken: 0.4412543773651123\n",
            "Epoch: 17 Batch Number: 139 Loss: 1.1673510074615479 Time taken: 0.4525430202484131\n",
            "Epoch: 17 Batch Number: 140 Loss: 1.1146296262741089 Time taken: 0.45349884033203125\n",
            "Epoch: 17 Batch Number: 141 Loss: 1.1594610214233398 Time taken: 0.4368863105773926\n",
            "Epoch: 17 Batch Number: 142 Loss: 1.1959799528121948 Time taken: 0.44034600257873535\n",
            "Epoch: 17 Batch Number: 143 Loss: 1.2666692733764648 Time taken: 0.44182562828063965\n",
            "Epoch: 17 Batch Number: 144 Loss: 1.2612652778625488 Time taken: 0.45812416076660156\n",
            "Epoch: 17 Batch Number: 145 Loss: 1.260388970375061 Time taken: 0.43984389305114746\n",
            "Epoch: 17 Batch Number: 146 Loss: 1.2306632995605469 Time taken: 0.44565439224243164\n",
            "Epoch: 17 Batch Number: 147 Loss: 1.1063687801361084 Time taken: 0.4485762119293213\n",
            "Epoch: 17 Batch Number: 148 Loss: 1.077813744544983 Time taken: 0.4529082775115967\n",
            "Epoch: 17 Batch Number: 149 Loss: 1.1596845388412476 Time taken: 0.43587231636047363\n",
            "Epoch: 17 Batch Number: 150 Loss: 1.2438163757324219 Time taken: 0.44188714027404785\n",
            "Epoch: 17 Batch Number: 151 Loss: 1.2480225563049316 Time taken: 0.43892788887023926\n",
            "Epoch: 17 Batch Number: 152 Loss: 1.1831656694412231 Time taken: 0.44373011589050293\n",
            "Epoch: 17 Batch Number: 153 Loss: 1.1102447509765625 Time taken: 0.45651698112487793\n",
            "Epoch: 17 Batch Number: 154 Loss: 1.099533200263977 Time taken: 0.43978166580200195\n",
            "Epoch: 17 Batch Number: 155 Loss: 1.0942530632019043 Time taken: 0.43623852729797363\n",
            "Epoch: 17 Batch Number: 156 Loss: 1.151442050933838 Time taken: 0.44162511825561523\n",
            "Epoch: 17 Batch Number: 157 Loss: 1.0317610502243042 Time taken: 0.44997692108154297\n",
            "Epoch: 17 Batch Number: 158 Loss: 1.1028213500976562 Time taken: 0.43924641609191895\n",
            "Epoch: 17 Batch Number: 159 Loss: 1.0380969047546387 Time taken: 0.457763671875\n",
            "Epoch: 17 Batch Number: 160 Loss: 0.9511885643005371 Time taken: 0.45110273361206055\n",
            "Epoch: 17 Batch Number: 161 Loss: 1.0248676538467407 Time taken: 0.44649457931518555\n",
            "Epoch: 17 Batch Number: 162 Loss: 0.9890154600143433 Time taken: 0.4372851848602295\n",
            "Epoch: 17 Batch Number: 163 Loss: 1.099852204322815 Time taken: 0.43900060653686523\n",
            "Epoch: 17 Batch Number: 164 Loss: 1.1913174390792847 Time taken: 0.4436025619506836\n",
            "Epoch: 17 Batch Number: 165 Loss: 1.0815808773040771 Time taken: 0.44232726097106934\n",
            "Epoch: 17 Batch Number: 166 Loss: 1.1496598720550537 Time taken: 0.4730679988861084\n",
            "Epoch: 17 Batch Number: 167 Loss: 1.1301183700561523 Time taken: 0.46087074279785156\n",
            "Epoch: 17 Batch Number: 168 Loss: 1.1226060390472412 Time taken: 0.46428799629211426\n",
            "Epoch: 17 Batch Number: 169 Loss: 1.1181880235671997 Time taken: 0.443087100982666\n",
            "Epoch: 17 Batch Number: 170 Loss: 1.0566428899765015 Time taken: 0.44588136672973633\n",
            "Epoch: 17 Batch Number: 171 Loss: 1.0144517421722412 Time taken: 0.4422035217285156\n",
            "Epoch: 17 Batch Number: 172 Loss: 1.0808312892913818 Time taken: 0.4457728862762451\n",
            "Epoch: 17 Batch Number: 173 Loss: 1.0683172941207886 Time taken: 0.44349074363708496\n",
            "Epoch: 17 Batch Number: 174 Loss: 1.004242181777954 Time taken: 0.44761180877685547\n",
            "Epoch: 17 Batch Number: 175 Loss: 1.1104624271392822 Time taken: 0.4536936283111572\n",
            "Epoch: 17 Batch Number: 176 Loss: 1.0866739749908447 Time taken: 0.4384644031524658\n",
            "Epoch: 17 Batch Number: 177 Loss: 1.1105332374572754 Time taken: 0.4613163471221924\n",
            "Epoch: 17 Batch Number: 178 Loss: 1.1684703826904297 Time taken: 0.4613535404205322\n",
            "Epoch: 17 Batch Number: 179 Loss: 1.1704014539718628 Time taken: 0.46070241928100586\n",
            "Epoch: 17 Batch Number: 180 Loss: 1.1142867803573608 Time taken: 0.4554462432861328\n",
            "Epoch: 17 Batch Number: 181 Loss: 1.0838394165039062 Time taken: 0.45618391036987305\n",
            "Epoch: 17 Batch Number: 182 Loss: 1.0349863767623901 Time taken: 0.46355223655700684\n",
            "Epoch: 17 Batch Number: 183 Loss: 1.08971107006073 Time taken: 0.4460139274597168\n",
            "Epoch: 17 Batch Number: 184 Loss: 1.1532998085021973 Time taken: 0.4513261318206787\n",
            "Epoch: 17 Batch Number: 185 Loss: 1.1753572225570679 Time taken: 0.445753812789917\n",
            "Epoch: 17 Batch Number: 186 Loss: 0.9743432998657227 Time taken: 0.44882702827453613\n",
            "Epoch: 17 Batch Number: 187 Loss: 1.067965030670166 Time taken: 0.44681334495544434\n",
            "Epoch: 17 Batch Number: 188 Loss: 1.0694583654403687 Time taken: 0.45255041122436523\n",
            "Epoch: 17 Batch Number: 189 Loss: 1.1011743545532227 Time taken: 0.4423182010650635\n",
            "Epoch: 17 Batch Number: 190 Loss: 1.278308391571045 Time taken: 0.44291234016418457\n",
            "Epoch: 17 Batch Number: 191 Loss: 1.410945177078247 Time taken: 0.4479639530181885\n",
            "Epoch: 17 Batch Number: 192 Loss: 1.168698787689209 Time taken: 0.4537358283996582\n",
            "Epoch: 17 Batch Number: 193 Loss: 1.2696912288665771 Time taken: 0.4425508975982666\n",
            "Epoch: 17 Batch Number: 194 Loss: 1.186532735824585 Time taken: 0.43836092948913574\n",
            "Epoch: 17 Batch Number: 195 Loss: 1.1314888000488281 Time taken: 0.4500091075897217\n",
            "Epoch: 17 Batch Number: 196 Loss: 1.0826067924499512 Time taken: 0.4411942958831787\n",
            "Epoch: 17 Batch Number: 197 Loss: 1.1555919647216797 Time taken: 0.44148826599121094\n",
            "Epoch: 17 Batch Number: 198 Loss: 1.071472406387329 Time taken: 0.44214558601379395\n",
            "Epoch: 17 Batch Number: 199 Loss: 1.11692214012146 Time taken: 0.4539318084716797\n",
            "Epoch: 17 Batch Number: 200 Loss: 1.0716806650161743 Time taken: 0.44777965545654297\n",
            "Epoch: 17 Batch Number: 201 Loss: 1.1152530908584595 Time taken: 0.4473097324371338\n",
            "Epoch: 17 Batch Number: 202 Loss: 1.1016801595687866 Time taken: 0.43659353256225586\n",
            "Epoch: 17 Batch Number: 203 Loss: 1.1055203676223755 Time taken: 0.4653000831604004\n",
            "Epoch: 17 Batch Number: 204 Loss: 1.1169922351837158 Time taken: 0.465834379196167\n",
            "Epoch: 17 Batch Number: 205 Loss: 1.053371787071228 Time taken: 0.4432234764099121\n",
            "Epoch: 17 Batch Number: 206 Loss: 1.105567455291748 Time taken: 0.4368603229522705\n",
            "Epoch: 17 Batch Number: 207 Loss: 1.139580249786377 Time taken: 0.4426546096801758\n",
            "Epoch: 17 Batch Number: 208 Loss: 1.1151083707809448 Time taken: 0.4570457935333252\n",
            "Epoch: 17 Batch Number: 209 Loss: 1.153874397277832 Time taken: 0.4404764175415039\n",
            "Epoch: 17 Batch Number: 210 Loss: 1.2044302225112915 Time taken: 0.45621681213378906\n",
            "Epoch: 17 Batch Number: 211 Loss: 1.1578313112258911 Time taken: 0.4378986358642578\n",
            "Epoch: 17 Batch Number: 212 Loss: 1.279298186302185 Time taken: 0.4602997303009033\n",
            "Epoch: 17 Batch Number: 213 Loss: 1.1286110877990723 Time taken: 0.4485049247741699\n",
            "Epoch: 17 Batch Number: 214 Loss: 1.169548511505127 Time taken: 0.4703662395477295\n",
            "Epoch: 17 Batch Number: 215 Loss: 1.1008611917495728 Time taken: 0.45540404319763184\n",
            "Epoch: 17 Batch Number: 216 Loss: 1.1620229482650757 Time taken: 0.4452075958251953\n",
            "Epoch: 17 Batch Number: 217 Loss: 1.2196502685546875 Time taken: 0.46068716049194336\n",
            "Epoch: 17 Batch Number: 218 Loss: 1.2008280754089355 Time taken: 0.4527859687805176\n",
            "Epoch: 17 Batch Number: 219 Loss: 1.2049016952514648 Time taken: 0.44401121139526367\n",
            "Epoch: 17 Batch Number: 220 Loss: 1.1444822549819946 Time taken: 0.44741249084472656\n",
            "Epoch: 17 Batch Number: 221 Loss: 1.1350218057632446 Time taken: 0.4338955879211426\n",
            "Epoch: 17 Batch Number: 222 Loss: 1.1073603630065918 Time taken: 0.44345641136169434\n",
            "Epoch: 17 Batch Number: 223 Loss: 1.1298038959503174 Time taken: 0.44116997718811035\n",
            "Epoch: 17 Batch Number: 224 Loss: 1.082979679107666 Time taken: 0.44007372856140137\n",
            "Epoch: 17 Batch Number: 225 Loss: 1.05991530418396 Time taken: 0.4442939758300781\n",
            "Epoch: 17 Batch Number: 226 Loss: 1.0946208238601685 Time taken: 0.4503347873687744\n",
            "Epoch: 17 Batch Number: 227 Loss: 1.2175214290618896 Time taken: 0.4410400390625\n",
            "Epoch: 17 Batch Number: 228 Loss: 1.1777747869491577 Time taken: 0.44606471061706543\n",
            "Epoch: 17 Batch Number: 229 Loss: 1.197566032409668 Time taken: 0.46245455741882324\n",
            "==========================================================================================\n",
            "Start of epoch 18\n",
            "Epoch: 18 Batch Number: 1 Loss: 1.1242327690124512 Time taken: 0.4473583698272705\n",
            "Epoch: 18 Batch Number: 2 Loss: 1.1816860437393188 Time taken: 0.4424128532409668\n",
            "Epoch: 18 Batch Number: 3 Loss: 1.144606590270996 Time taken: 0.44515228271484375\n",
            "Epoch: 18 Batch Number: 4 Loss: 1.109271764755249 Time taken: 0.44605159759521484\n",
            "Epoch: 18 Batch Number: 5 Loss: 1.114748239517212 Time taken: 0.4429185390472412\n",
            "Epoch: 18 Batch Number: 6 Loss: 1.054335594177246 Time taken: 0.45291638374328613\n",
            "Epoch: 18 Batch Number: 7 Loss: 1.0590310096740723 Time taken: 0.4434511661529541\n",
            "Epoch: 18 Batch Number: 8 Loss: 1.098928451538086 Time taken: 0.43666911125183105\n",
            "Epoch: 18 Batch Number: 9 Loss: 1.0756515264511108 Time taken: 0.44213342666625977\n",
            "Epoch: 18 Batch Number: 10 Loss: 1.114980936050415 Time taken: 0.44213366508483887\n",
            "Epoch: 18 Batch Number: 11 Loss: 1.0666553974151611 Time taken: 0.4533729553222656\n",
            "Epoch: 18 Batch Number: 12 Loss: 1.0379976034164429 Time taken: 0.4523587226867676\n",
            "Epoch: 18 Batch Number: 13 Loss: 1.0673339366912842 Time taken: 0.4523742198944092\n",
            "Epoch: 18 Batch Number: 14 Loss: 1.0785958766937256 Time taken: 0.4435310363769531\n",
            "Epoch: 18 Batch Number: 15 Loss: 1.0251160860061646 Time taken: 0.44712138175964355\n",
            "Epoch: 18 Batch Number: 16 Loss: 1.059853434562683 Time taken: 0.44519591331481934\n",
            "Epoch: 18 Batch Number: 17 Loss: 1.183716058731079 Time taken: 0.4442563056945801\n",
            "Epoch: 18 Batch Number: 18 Loss: 1.157987356185913 Time taken: 0.4704124927520752\n",
            "Epoch: 18 Batch Number: 19 Loss: 1.167168140411377 Time taken: 0.4689910411834717\n",
            "Epoch: 18 Batch Number: 20 Loss: 1.0513836145401 Time taken: 0.44649195671081543\n",
            "Epoch: 18 Batch Number: 21 Loss: 1.2184879779815674 Time taken: 0.4513509273529053\n",
            "Epoch: 18 Batch Number: 22 Loss: 1.2058746814727783 Time taken: 0.4712867736816406\n",
            "Epoch: 18 Batch Number: 23 Loss: 1.2062687873840332 Time taken: 0.44965672492980957\n",
            "Epoch: 18 Batch Number: 24 Loss: 1.1844096183776855 Time taken: 0.44231629371643066\n",
            "Epoch: 18 Batch Number: 25 Loss: 1.1783866882324219 Time taken: 0.444133996963501\n",
            "Epoch: 18 Batch Number: 26 Loss: 1.1780235767364502 Time taken: 0.45361995697021484\n",
            "Epoch: 18 Batch Number: 27 Loss: 1.1265815496444702 Time taken: 0.4477391242980957\n",
            "Epoch: 18 Batch Number: 28 Loss: 1.1292942762374878 Time taken: 0.44532060623168945\n",
            "Epoch: 18 Batch Number: 29 Loss: 1.177080512046814 Time taken: 0.45987939834594727\n",
            "Epoch: 18 Batch Number: 30 Loss: 0.9738598465919495 Time taken: 0.44812583923339844\n",
            "Epoch: 18 Batch Number: 31 Loss: 1.1149845123291016 Time taken: 0.46555233001708984\n",
            "Epoch: 18 Batch Number: 32 Loss: 1.1098453998565674 Time taken: 0.46006226539611816\n",
            "Epoch: 18 Batch Number: 33 Loss: 1.1446053981781006 Time taken: 0.4506533145904541\n",
            "Epoch: 18 Batch Number: 34 Loss: 1.1760501861572266 Time taken: 0.4445171356201172\n",
            "Epoch: 18 Batch Number: 35 Loss: 1.2156312465667725 Time taken: 0.4532442092895508\n",
            "Epoch: 18 Batch Number: 36 Loss: 1.236815094947815 Time taken: 0.4388442039489746\n",
            "Epoch: 18 Batch Number: 37 Loss: 1.1291303634643555 Time taken: 0.4427063465118408\n",
            "Epoch: 18 Batch Number: 38 Loss: 1.1825318336486816 Time taken: 0.4408407211303711\n",
            "Epoch: 18 Batch Number: 39 Loss: 1.1032800674438477 Time taken: 0.4415912628173828\n",
            "Epoch: 18 Batch Number: 40 Loss: 1.1193976402282715 Time taken: 0.44853830337524414\n",
            "Epoch: 18 Batch Number: 41 Loss: 1.0896108150482178 Time taken: 0.4423542022705078\n",
            "Epoch: 18 Batch Number: 42 Loss: 1.0845460891723633 Time taken: 0.4413111209869385\n",
            "Epoch: 18 Batch Number: 43 Loss: 1.0700218677520752 Time taken: 0.44604015350341797\n",
            "Epoch: 18 Batch Number: 44 Loss: 1.0355116128921509 Time taken: 0.4517655372619629\n",
            "Epoch: 18 Batch Number: 45 Loss: 1.0956225395202637 Time taken: 0.433377742767334\n",
            "Epoch: 18 Batch Number: 46 Loss: 1.2561231851577759 Time taken: 0.4440939426422119\n",
            "Epoch: 18 Batch Number: 47 Loss: 1.1025153398513794 Time taken: 0.44616222381591797\n",
            "Epoch: 18 Batch Number: 48 Loss: 1.1679714918136597 Time taken: 0.44150614738464355\n",
            "Epoch: 18 Batch Number: 49 Loss: 1.1640963554382324 Time taken: 0.4499189853668213\n",
            "Epoch: 18 Batch Number: 50 Loss: 1.0805691480636597 Time taken: 0.44429707527160645\n",
            "Epoch: 18 Batch Number: 51 Loss: 1.066247820854187 Time taken: 0.4440605640411377\n",
            "Epoch: 18 Batch Number: 52 Loss: 1.188287615776062 Time taken: 0.4378700256347656\n",
            "Epoch: 18 Batch Number: 53 Loss: 1.2212483882904053 Time taken: 0.4657151699066162\n",
            "Epoch: 18 Batch Number: 54 Loss: 1.1908931732177734 Time taken: 0.4609982967376709\n",
            "Epoch: 18 Batch Number: 55 Loss: 1.1722623109817505 Time taken: 0.45116186141967773\n",
            "Epoch: 18 Batch Number: 56 Loss: 1.272028923034668 Time taken: 0.45030879974365234\n",
            "Epoch: 18 Batch Number: 57 Loss: 1.1359894275665283 Time taken: 0.43990659713745117\n",
            "Epoch: 18 Batch Number: 58 Loss: 1.0901002883911133 Time taken: 0.45027661323547363\n",
            "Epoch: 18 Batch Number: 59 Loss: 1.1192896366119385 Time taken: 0.452831506729126\n",
            "Epoch: 18 Batch Number: 60 Loss: 1.1037794351577759 Time taken: 0.4477121829986572\n",
            "Epoch: 18 Batch Number: 61 Loss: 1.1312987804412842 Time taken: 0.4420156478881836\n",
            "Epoch: 18 Batch Number: 62 Loss: 1.0425453186035156 Time taken: 0.4394559860229492\n",
            "Epoch: 18 Batch Number: 63 Loss: 1.081287145614624 Time taken: 0.44774746894836426\n",
            "Epoch: 18 Batch Number: 64 Loss: 1.027972936630249 Time taken: 0.4478721618652344\n",
            "Epoch: 18 Batch Number: 65 Loss: 1.0784671306610107 Time taken: 0.4450845718383789\n",
            "Epoch: 18 Batch Number: 66 Loss: 1.094623327255249 Time taken: 0.4387540817260742\n",
            "Epoch: 18 Batch Number: 67 Loss: 1.074520230293274 Time taken: 0.4501490592956543\n",
            "Epoch: 18 Batch Number: 68 Loss: 1.1385228633880615 Time taken: 0.4429342746734619\n",
            "Epoch: 18 Batch Number: 69 Loss: 1.121212124824524 Time taken: 0.4544827938079834\n",
            "Epoch: 18 Batch Number: 70 Loss: 1.1249045133590698 Time taken: 0.4459998607635498\n",
            "Epoch: 18 Batch Number: 71 Loss: 1.097440481185913 Time taken: 0.4451408386230469\n",
            "Epoch: 18 Batch Number: 72 Loss: 1.1154401302337646 Time taken: 0.45792412757873535\n",
            "Epoch: 18 Batch Number: 73 Loss: 1.057199239730835 Time taken: 0.4573781490325928\n",
            "Epoch: 18 Batch Number: 74 Loss: 1.154219627380371 Time taken: 0.4459035396575928\n",
            "Epoch: 18 Batch Number: 75 Loss: 1.0062521696090698 Time taken: 0.4650397300720215\n",
            "Epoch: 18 Batch Number: 76 Loss: 1.0431766510009766 Time taken: 0.45620203018188477\n",
            "Epoch: 18 Batch Number: 77 Loss: 1.1372506618499756 Time taken: 0.4453582763671875\n",
            "Epoch: 18 Batch Number: 78 Loss: 1.0336772203445435 Time taken: 0.4450860023498535\n",
            "Epoch: 18 Batch Number: 79 Loss: 1.1090505123138428 Time taken: 0.4407839775085449\n",
            "Epoch: 18 Batch Number: 80 Loss: 1.0836670398712158 Time taken: 0.44304680824279785\n",
            "Epoch: 18 Batch Number: 81 Loss: 1.1086766719818115 Time taken: 0.4407663345336914\n",
            "Epoch: 18 Batch Number: 82 Loss: 1.1060819625854492 Time taken: 0.4453279972076416\n",
            "Epoch: 18 Batch Number: 83 Loss: 1.1688153743743896 Time taken: 0.4425184726715088\n",
            "Epoch: 18 Batch Number: 84 Loss: 1.1286511421203613 Time taken: 0.44620561599731445\n",
            "Epoch: 18 Batch Number: 85 Loss: 1.1053074598312378 Time taken: 0.4485771656036377\n",
            "Epoch: 18 Batch Number: 86 Loss: 1.0905003547668457 Time taken: 0.44033098220825195\n",
            "Epoch: 18 Batch Number: 87 Loss: 1.177835464477539 Time taken: 0.4460937976837158\n",
            "Epoch: 18 Batch Number: 88 Loss: 1.071378231048584 Time taken: 0.4451925754547119\n",
            "Epoch: 18 Batch Number: 89 Loss: 1.0569865703582764 Time taken: 0.4400198459625244\n",
            "Epoch: 18 Batch Number: 90 Loss: 1.1057777404785156 Time taken: 0.443206787109375\n",
            "Epoch: 18 Batch Number: 91 Loss: 1.0610504150390625 Time taken: 0.466388463973999\n",
            "Epoch: 18 Batch Number: 92 Loss: 1.0378687381744385 Time taken: 0.44264841079711914\n",
            "Epoch: 18 Batch Number: 93 Loss: 1.0550072193145752 Time taken: 0.43982815742492676\n",
            "Epoch: 18 Batch Number: 94 Loss: 1.213147521018982 Time taken: 0.44665980339050293\n",
            "Epoch: 18 Batch Number: 95 Loss: 1.1533864736557007 Time taken: 0.43983983993530273\n",
            "Epoch: 18 Batch Number: 96 Loss: 1.1895604133605957 Time taken: 0.4559452533721924\n",
            "Epoch: 18 Batch Number: 97 Loss: 1.1253232955932617 Time taken: 0.44376635551452637\n",
            "Epoch: 18 Batch Number: 98 Loss: 1.1859577894210815 Time taken: 0.4468703269958496\n",
            "Epoch: 18 Batch Number: 99 Loss: 1.2254550457000732 Time taken: 0.4421813488006592\n",
            "Epoch: 18 Batch Number: 100 Loss: 1.149234652519226 Time taken: 0.4452700614929199\n",
            "Epoch: 18 Batch Number: 101 Loss: 1.1429296731948853 Time taken: 0.43746423721313477\n",
            "Epoch: 18 Batch Number: 102 Loss: 1.1774535179138184 Time taken: 0.444000244140625\n",
            "Epoch: 18 Batch Number: 103 Loss: 1.2374801635742188 Time taken: 0.4492967128753662\n",
            "Epoch: 18 Batch Number: 104 Loss: 1.130873680114746 Time taken: 0.44014525413513184\n",
            "Epoch: 18 Batch Number: 105 Loss: 1.2086714506149292 Time taken: 0.44502925872802734\n",
            "Epoch: 18 Batch Number: 106 Loss: 1.1734588146209717 Time taken: 0.4419233798980713\n",
            "Epoch: 18 Batch Number: 107 Loss: 1.1752536296844482 Time taken: 0.4524421691894531\n",
            "Epoch: 18 Batch Number: 108 Loss: 1.135985016822815 Time taken: 0.4485795497894287\n",
            "Epoch: 18 Batch Number: 109 Loss: 1.1338814496994019 Time taken: 0.44742465019226074\n",
            "Epoch: 18 Batch Number: 110 Loss: 0.9963187575340271 Time taken: 0.43795251846313477\n",
            "Epoch: 18 Batch Number: 111 Loss: 1.0716094970703125 Time taken: 0.45481276512145996\n",
            "Epoch: 18 Batch Number: 112 Loss: 1.0836366415023804 Time taken: 0.4524548053741455\n",
            "Epoch: 18 Batch Number: 113 Loss: 1.1040185689926147 Time taken: 0.4342966079711914\n",
            "Epoch: 18 Batch Number: 114 Loss: 1.1580750942230225 Time taken: 0.44333648681640625\n",
            "Epoch: 18 Batch Number: 115 Loss: 1.1441895961761475 Time taken: 0.4361255168914795\n",
            "Epoch: 18 Batch Number: 116 Loss: 1.0312954187393188 Time taken: 0.4357771873474121\n",
            "Epoch: 18 Batch Number: 117 Loss: 1.1202964782714844 Time taken: 0.45679259300231934\n",
            "Epoch: 18 Batch Number: 118 Loss: 1.078305959701538 Time taken: 0.4527442455291748\n",
            "Epoch: 18 Batch Number: 119 Loss: 1.1214797496795654 Time taken: 0.4452216625213623\n",
            "Epoch: 18 Batch Number: 120 Loss: 1.1617724895477295 Time taken: 0.45239877700805664\n",
            "Epoch: 18 Batch Number: 121 Loss: 1.1061162948608398 Time taken: 0.45078587532043457\n",
            "Epoch: 18 Batch Number: 122 Loss: 1.089707374572754 Time taken: 0.4527888298034668\n",
            "Epoch: 18 Batch Number: 123 Loss: 1.1166205406188965 Time taken: 0.44609928131103516\n",
            "Epoch: 18 Batch Number: 124 Loss: 1.0796265602111816 Time taken: 0.445692777633667\n",
            "Epoch: 18 Batch Number: 125 Loss: 1.1156786680221558 Time taken: 0.4421219825744629\n",
            "Epoch: 18 Batch Number: 126 Loss: 1.134240746498108 Time taken: 0.4438049793243408\n",
            "Epoch: 18 Batch Number: 127 Loss: 1.1564607620239258 Time taken: 0.44033074378967285\n",
            "Epoch: 18 Batch Number: 128 Loss: 1.0949112176895142 Time taken: 0.4443352222442627\n",
            "Epoch: 18 Batch Number: 129 Loss: 1.099804401397705 Time taken: 0.4384028911590576\n",
            "Epoch: 18 Batch Number: 130 Loss: 1.0776118040084839 Time taken: 0.437575101852417\n",
            "Epoch: 18 Batch Number: 131 Loss: 1.1604645252227783 Time taken: 0.44677090644836426\n",
            "Epoch: 18 Batch Number: 132 Loss: 1.0086874961853027 Time taken: 0.4629225730895996\n",
            "Epoch: 18 Batch Number: 133 Loss: 1.1733152866363525 Time taken: 0.439439058303833\n",
            "Epoch: 18 Batch Number: 134 Loss: 1.167089581489563 Time taken: 0.4410727024078369\n",
            "Epoch: 18 Batch Number: 135 Loss: 1.1673579216003418 Time taken: 0.48453402519226074\n",
            "Epoch: 18 Batch Number: 136 Loss: 1.159700632095337 Time taken: 0.4693715572357178\n",
            "Epoch: 18 Batch Number: 137 Loss: 1.1117782592773438 Time taken: 0.45766401290893555\n",
            "Epoch: 18 Batch Number: 138 Loss: 1.1405233144760132 Time taken: 0.4506347179412842\n",
            "Epoch: 18 Batch Number: 139 Loss: 1.1451828479766846 Time taken: 0.4660525321960449\n",
            "Epoch: 18 Batch Number: 140 Loss: 1.103432297706604 Time taken: 0.45781564712524414\n",
            "Epoch: 18 Batch Number: 141 Loss: 1.1480706930160522 Time taken: 0.45579075813293457\n",
            "Epoch: 18 Batch Number: 142 Loss: 1.1857969760894775 Time taken: 0.4543132781982422\n",
            "Epoch: 18 Batch Number: 143 Loss: 1.2549153566360474 Time taken: 0.45386767387390137\n",
            "Epoch: 18 Batch Number: 144 Loss: 1.2494463920593262 Time taken: 0.4629359245300293\n",
            "Epoch: 18 Batch Number: 145 Loss: 1.2481225728988647 Time taken: 0.46063733100891113\n",
            "Epoch: 18 Batch Number: 146 Loss: 1.21764075756073 Time taken: 0.45255136489868164\n",
            "Epoch: 18 Batch Number: 147 Loss: 1.0929805040359497 Time taken: 0.45130395889282227\n",
            "Epoch: 18 Batch Number: 148 Loss: 1.0583488941192627 Time taken: 0.4575765132904053\n",
            "Epoch: 18 Batch Number: 149 Loss: 1.1466292142868042 Time taken: 0.44916224479675293\n",
            "Epoch: 18 Batch Number: 150 Loss: 1.232623815536499 Time taken: 0.4691433906555176\n",
            "Epoch: 18 Batch Number: 151 Loss: 1.238539695739746 Time taken: 0.4647843837738037\n",
            "Epoch: 18 Batch Number: 152 Loss: 1.164841890335083 Time taken: 0.470257043838501\n",
            "Epoch: 18 Batch Number: 153 Loss: 1.096020221710205 Time taken: 0.4477050304412842\n",
            "Epoch: 18 Batch Number: 154 Loss: 1.0832290649414062 Time taken: 0.4481363296508789\n",
            "Epoch: 18 Batch Number: 155 Loss: 1.0829293727874756 Time taken: 0.4537327289581299\n",
            "Epoch: 18 Batch Number: 156 Loss: 1.1487116813659668 Time taken: 0.4576094150543213\n",
            "Epoch: 18 Batch Number: 157 Loss: 1.0231736898422241 Time taken: 0.4452977180480957\n",
            "Epoch: 18 Batch Number: 158 Loss: 1.0951132774353027 Time taken: 0.45462989807128906\n",
            "Epoch: 18 Batch Number: 159 Loss: 1.0292446613311768 Time taken: 0.43991756439208984\n",
            "Epoch: 18 Batch Number: 160 Loss: 0.9408117532730103 Time taken: 0.4491755962371826\n",
            "Epoch: 18 Batch Number: 161 Loss: 1.0138216018676758 Time taken: 0.46301984786987305\n",
            "Epoch: 18 Batch Number: 162 Loss: 0.9754853248596191 Time taken: 0.4449458122253418\n",
            "Epoch: 18 Batch Number: 163 Loss: 1.0910365581512451 Time taken: 0.43393540382385254\n",
            "Epoch: 18 Batch Number: 164 Loss: 1.1833617687225342 Time taken: 0.46043968200683594\n",
            "Epoch: 18 Batch Number: 165 Loss: 1.0828289985656738 Time taken: 0.44913554191589355\n",
            "Epoch: 18 Batch Number: 166 Loss: 1.1404163837432861 Time taken: 0.4598886966705322\n",
            "Epoch: 18 Batch Number: 167 Loss: 1.12232506275177 Time taken: 0.4545571804046631\n",
            "Epoch: 18 Batch Number: 168 Loss: 1.1151509284973145 Time taken: 0.4635632038116455\n",
            "Epoch: 18 Batch Number: 169 Loss: 1.1167242527008057 Time taken: 0.44823312759399414\n",
            "Epoch: 18 Batch Number: 170 Loss: 1.0420242547988892 Time taken: 0.4496493339538574\n",
            "Epoch: 18 Batch Number: 171 Loss: 0.9927470684051514 Time taken: 0.44863057136535645\n",
            "Epoch: 18 Batch Number: 172 Loss: 1.0679311752319336 Time taken: 0.44228339195251465\n",
            "Epoch: 18 Batch Number: 173 Loss: 1.0551979541778564 Time taken: 0.44231081008911133\n",
            "Epoch: 18 Batch Number: 174 Loss: 1.0026713609695435 Time taken: 0.45009708404541016\n",
            "Epoch: 18 Batch Number: 175 Loss: 1.1000959873199463 Time taken: 0.4489750862121582\n",
            "Epoch: 18 Batch Number: 176 Loss: 1.0770753622055054 Time taken: 0.45841240882873535\n",
            "Epoch: 18 Batch Number: 177 Loss: 1.1012957096099854 Time taken: 0.4756801128387451\n",
            "Epoch: 18 Batch Number: 178 Loss: 1.1649889945983887 Time taken: 0.4331321716308594\n",
            "Epoch: 18 Batch Number: 179 Loss: 1.153839349746704 Time taken: 0.44594335556030273\n",
            "Epoch: 18 Batch Number: 180 Loss: 1.1050994396209717 Time taken: 0.46015048027038574\n",
            "Epoch: 18 Batch Number: 181 Loss: 1.0725816488265991 Time taken: 0.4551115036010742\n",
            "Epoch: 18 Batch Number: 182 Loss: 1.0226634740829468 Time taken: 0.43688035011291504\n",
            "Epoch: 18 Batch Number: 183 Loss: 1.0735695362091064 Time taken: 0.44091129302978516\n",
            "Epoch: 18 Batch Number: 184 Loss: 1.145263910293579 Time taken: 0.4578969478607178\n",
            "Epoch: 18 Batch Number: 185 Loss: 1.1632084846496582 Time taken: 0.43978023529052734\n",
            "Epoch: 18 Batch Number: 186 Loss: 0.9949520826339722 Time taken: 0.4535095691680908\n",
            "Epoch: 18 Batch Number: 187 Loss: 1.0618479251861572 Time taken: 0.44239258766174316\n",
            "Epoch: 18 Batch Number: 188 Loss: 1.0591589212417603 Time taken: 0.4451103210449219\n",
            "Epoch: 18 Batch Number: 189 Loss: 1.0961146354675293 Time taken: 0.439220666885376\n",
            "Epoch: 18 Batch Number: 190 Loss: 1.260365605354309 Time taken: 0.45418405532836914\n",
            "Epoch: 18 Batch Number: 191 Loss: 1.3812904357910156 Time taken: 0.4526188373565674\n",
            "Epoch: 18 Batch Number: 192 Loss: 1.1464807987213135 Time taken: 0.4378182888031006\n",
            "Epoch: 18 Batch Number: 193 Loss: 1.249699592590332 Time taken: 0.4672424793243408\n",
            "Epoch: 18 Batch Number: 194 Loss: 1.1713535785675049 Time taken: 0.44698667526245117\n",
            "Epoch: 18 Batch Number: 195 Loss: 1.11974036693573 Time taken: 0.4431188106536865\n",
            "Epoch: 18 Batch Number: 196 Loss: 1.0710499286651611 Time taken: 0.4416646957397461\n",
            "Epoch: 18 Batch Number: 197 Loss: 1.1454898118972778 Time taken: 0.44852590560913086\n",
            "Epoch: 18 Batch Number: 198 Loss: 1.0584630966186523 Time taken: 0.4413583278656006\n",
            "Epoch: 18 Batch Number: 199 Loss: 1.1034489870071411 Time taken: 0.45106005668640137\n",
            "Epoch: 18 Batch Number: 200 Loss: 1.0582642555236816 Time taken: 0.4472217559814453\n",
            "Epoch: 18 Batch Number: 201 Loss: 1.0959504842758179 Time taken: 0.450486421585083\n",
            "Epoch: 18 Batch Number: 202 Loss: 1.0853439569473267 Time taken: 0.4432942867279053\n",
            "Epoch: 18 Batch Number: 203 Loss: 1.0899324417114258 Time taken: 0.44567227363586426\n",
            "Epoch: 18 Batch Number: 204 Loss: 1.1050479412078857 Time taken: 0.4463043212890625\n",
            "Epoch: 18 Batch Number: 205 Loss: 1.0351920127868652 Time taken: 0.4415445327758789\n",
            "Epoch: 18 Batch Number: 206 Loss: 1.0925227403640747 Time taken: 0.4419572353363037\n",
            "Epoch: 18 Batch Number: 207 Loss: 1.129678726196289 Time taken: 0.4438810348510742\n",
            "Epoch: 18 Batch Number: 208 Loss: 1.111396074295044 Time taken: 0.4550802707672119\n",
            "Epoch: 18 Batch Number: 209 Loss: 1.143734097480774 Time taken: 0.45815134048461914\n",
            "Epoch: 18 Batch Number: 210 Loss: 1.2035608291625977 Time taken: 0.44301486015319824\n",
            "Epoch: 18 Batch Number: 211 Loss: 1.150915503501892 Time taken: 0.45397019386291504\n",
            "Epoch: 18 Batch Number: 212 Loss: 1.273181438446045 Time taken: 0.4513702392578125\n",
            "Epoch: 18 Batch Number: 213 Loss: 1.1212562322616577 Time taken: 0.44153666496276855\n",
            "Epoch: 18 Batch Number: 214 Loss: 1.1574242115020752 Time taken: 0.45525121688842773\n",
            "Epoch: 18 Batch Number: 215 Loss: 1.091203212738037 Time taken: 0.45894503593444824\n",
            "Epoch: 18 Batch Number: 216 Loss: 1.1493425369262695 Time taken: 0.4410402774810791\n",
            "Epoch: 18 Batch Number: 217 Loss: 1.2067947387695312 Time taken: 0.44523072242736816\n",
            "Epoch: 18 Batch Number: 218 Loss: 1.182643175125122 Time taken: 0.4376811981201172\n",
            "Epoch: 18 Batch Number: 219 Loss: 1.1943050622940063 Time taken: 0.44265079498291016\n",
            "Epoch: 18 Batch Number: 220 Loss: 1.1259419918060303 Time taken: 0.4546375274658203\n",
            "Epoch: 18 Batch Number: 221 Loss: 1.1277507543563843 Time taken: 0.44864845275878906\n",
            "Epoch: 18 Batch Number: 222 Loss: 1.09542715549469 Time taken: 0.44457173347473145\n",
            "Epoch: 18 Batch Number: 223 Loss: 1.112957239151001 Time taken: 0.4466235637664795\n",
            "Epoch: 18 Batch Number: 224 Loss: 1.0709491968154907 Time taken: 0.43901872634887695\n",
            "Epoch: 18 Batch Number: 225 Loss: 1.0485470294952393 Time taken: 0.44051074981689453\n",
            "Epoch: 18 Batch Number: 226 Loss: 1.0908842086791992 Time taken: 0.4539010524749756\n",
            "Epoch: 18 Batch Number: 227 Loss: 1.2046597003936768 Time taken: 0.44060850143432617\n",
            "Epoch: 18 Batch Number: 228 Loss: 1.1640691757202148 Time taken: 0.4432511329650879\n",
            "Epoch: 18 Batch Number: 229 Loss: 1.186882734298706 Time taken: 0.45754384994506836\n",
            "==========================================================================================\n",
            "Start of epoch 19\n",
            "Epoch: 19 Batch Number: 1 Loss: 1.112754464149475 Time taken: 0.45760035514831543\n",
            "Epoch: 19 Batch Number: 2 Loss: 1.1725693941116333 Time taken: 0.45684099197387695\n",
            "Epoch: 19 Batch Number: 3 Loss: 1.1328771114349365 Time taken: 0.4489290714263916\n",
            "Epoch: 19 Batch Number: 4 Loss: 1.0946723222732544 Time taken: 0.4627196788787842\n",
            "Epoch: 19 Batch Number: 5 Loss: 1.1034915447235107 Time taken: 0.4581942558288574\n",
            "Epoch: 19 Batch Number: 6 Loss: 1.0412920713424683 Time taken: 0.4540543556213379\n",
            "Epoch: 19 Batch Number: 7 Loss: 1.048973798751831 Time taken: 0.44399046897888184\n",
            "Epoch: 19 Batch Number: 8 Loss: 1.089510440826416 Time taken: 0.4444723129272461\n",
            "Epoch: 19 Batch Number: 9 Loss: 1.071696400642395 Time taken: 0.4421873092651367\n",
            "Epoch: 19 Batch Number: 10 Loss: 1.1117653846740723 Time taken: 0.4429595470428467\n",
            "Epoch: 19 Batch Number: 11 Loss: 1.0584172010421753 Time taken: 0.4430408477783203\n",
            "Epoch: 19 Batch Number: 12 Loss: 1.032557487487793 Time taken: 0.4434943199157715\n",
            "Epoch: 19 Batch Number: 13 Loss: 1.0627093315124512 Time taken: 0.4475564956665039\n",
            "Epoch: 19 Batch Number: 14 Loss: 1.0672450065612793 Time taken: 0.44138360023498535\n",
            "Epoch: 19 Batch Number: 15 Loss: 1.0084452629089355 Time taken: 0.4517090320587158\n",
            "Epoch: 19 Batch Number: 16 Loss: 1.0405696630477905 Time taken: 0.4449458122253418\n",
            "Epoch: 19 Batch Number: 17 Loss: 1.170981526374817 Time taken: 0.4571874141693115\n",
            "Epoch: 19 Batch Number: 18 Loss: 1.1440708637237549 Time taken: 0.4519774913787842\n",
            "Epoch: 19 Batch Number: 19 Loss: 1.1507112979888916 Time taken: 0.4432997703552246\n",
            "Epoch: 19 Batch Number: 20 Loss: 1.0364880561828613 Time taken: 0.44117069244384766\n",
            "Epoch: 19 Batch Number: 21 Loss: 1.1876801252365112 Time taken: 0.44597792625427246\n",
            "Epoch: 19 Batch Number: 22 Loss: 1.186499834060669 Time taken: 0.44351863861083984\n",
            "Epoch: 19 Batch Number: 23 Loss: 1.1841912269592285 Time taken: 0.4629964828491211\n",
            "Epoch: 19 Batch Number: 24 Loss: 1.1688945293426514 Time taken: 0.45774149894714355\n",
            "Epoch: 19 Batch Number: 25 Loss: 1.1667574644088745 Time taken: 0.44562697410583496\n",
            "Epoch: 19 Batch Number: 26 Loss: 1.1644765138626099 Time taken: 0.44475865364074707\n",
            "Epoch: 19 Batch Number: 27 Loss: 1.1161994934082031 Time taken: 0.44699621200561523\n",
            "Epoch: 19 Batch Number: 28 Loss: 1.1168946027755737 Time taken: 0.439694881439209\n",
            "Epoch: 19 Batch Number: 29 Loss: 1.1692203283309937 Time taken: 0.45134854316711426\n",
            "Epoch: 19 Batch Number: 30 Loss: 0.9554532170295715 Time taken: 0.4617750644683838\n",
            "Epoch: 19 Batch Number: 31 Loss: 1.1092604398727417 Time taken: 0.4535098075866699\n",
            "Epoch: 19 Batch Number: 32 Loss: 1.1075472831726074 Time taken: 0.444324254989624\n",
            "Epoch: 19 Batch Number: 33 Loss: 1.1380231380462646 Time taken: 0.4438662528991699\n",
            "Epoch: 19 Batch Number: 34 Loss: 1.1624338626861572 Time taken: 0.43853116035461426\n",
            "Epoch: 19 Batch Number: 35 Loss: 1.201024055480957 Time taken: 0.43715906143188477\n",
            "Epoch: 19 Batch Number: 36 Loss: 1.2268545627593994 Time taken: 0.4403717517852783\n",
            "Epoch: 19 Batch Number: 37 Loss: 1.119565486907959 Time taken: 0.44907045364379883\n",
            "Epoch: 19 Batch Number: 38 Loss: 1.172544240951538 Time taken: 0.4393014907836914\n",
            "Epoch: 19 Batch Number: 39 Loss: 1.0971994400024414 Time taken: 0.45476365089416504\n",
            "Epoch: 19 Batch Number: 40 Loss: 1.1069653034210205 Time taken: 0.4459834098815918\n",
            "Epoch: 19 Batch Number: 41 Loss: 1.0840933322906494 Time taken: 0.44620466232299805\n",
            "Epoch: 19 Batch Number: 42 Loss: 1.0782550573349 Time taken: 0.4497413635253906\n",
            "Epoch: 19 Batch Number: 43 Loss: 1.062608003616333 Time taken: 0.4410374164581299\n",
            "Epoch: 19 Batch Number: 44 Loss: 1.025888204574585 Time taken: 0.44555068016052246\n",
            "Epoch: 19 Batch Number: 45 Loss: 1.0851070880889893 Time taken: 0.44130921363830566\n",
            "Epoch: 19 Batch Number: 46 Loss: 1.2506084442138672 Time taken: 0.4522690773010254\n",
            "Epoch: 19 Batch Number: 47 Loss: 1.0980370044708252 Time taken: 0.4391319751739502\n",
            "Epoch: 19 Batch Number: 48 Loss: 1.1622706651687622 Time taken: 0.4345543384552002\n",
            "Epoch: 19 Batch Number: 49 Loss: 1.1573395729064941 Time taken: 0.4402351379394531\n",
            "Epoch: 19 Batch Number: 50 Loss: 1.068768858909607 Time taken: 0.4439549446105957\n",
            "Epoch: 19 Batch Number: 51 Loss: 1.0565550327301025 Time taken: 0.4458472728729248\n",
            "Epoch: 19 Batch Number: 52 Loss: 1.183526873588562 Time taken: 0.44725680351257324\n",
            "Epoch: 19 Batch Number: 53 Loss: 1.2252681255340576 Time taken: 0.44086432456970215\n",
            "Epoch: 19 Batch Number: 54 Loss: 1.1867305040359497 Time taken: 0.43992114067077637\n",
            "Epoch: 19 Batch Number: 55 Loss: 1.1709392070770264 Time taken: 0.4527456760406494\n",
            "Epoch: 19 Batch Number: 56 Loss: 1.2730438709259033 Time taken: 0.44442129135131836\n",
            "Epoch: 19 Batch Number: 57 Loss: 1.129852056503296 Time taken: 0.4404737949371338\n",
            "Epoch: 19 Batch Number: 58 Loss: 1.081611156463623 Time taken: 0.43756723403930664\n",
            "Epoch: 19 Batch Number: 59 Loss: 1.111086368560791 Time taken: 0.45148158073425293\n",
            "Epoch: 19 Batch Number: 60 Loss: 1.0957965850830078 Time taken: 0.45150327682495117\n",
            "Epoch: 19 Batch Number: 61 Loss: 1.1190465688705444 Time taken: 0.45305681228637695\n",
            "Epoch: 19 Batch Number: 62 Loss: 1.0286176204681396 Time taken: 0.44387245178222656\n",
            "Epoch: 19 Batch Number: 63 Loss: 1.0675705671310425 Time taken: 0.45279669761657715\n",
            "Epoch: 19 Batch Number: 64 Loss: 1.0186630487442017 Time taken: 0.44758033752441406\n",
            "Epoch: 19 Batch Number: 65 Loss: 1.0714788436889648 Time taken: 0.44328832626342773\n",
            "Epoch: 19 Batch Number: 66 Loss: 1.0856153964996338 Time taken: 0.4388580322265625\n",
            "Epoch: 19 Batch Number: 67 Loss: 1.0627954006195068 Time taken: 0.44228291511535645\n",
            "Epoch: 19 Batch Number: 68 Loss: 1.1270095109939575 Time taken: 0.45783543586730957\n",
            "Epoch: 19 Batch Number: 69 Loss: 1.1007332801818848 Time taken: 0.4626481533050537\n",
            "Epoch: 19 Batch Number: 70 Loss: 1.1102261543273926 Time taken: 0.45815110206604004\n",
            "Epoch: 19 Batch Number: 71 Loss: 1.0853291749954224 Time taken: 0.43961668014526367\n",
            "Epoch: 19 Batch Number: 72 Loss: 1.0999433994293213 Time taken: 0.4539332389831543\n",
            "Epoch: 19 Batch Number: 73 Loss: 1.0442864894866943 Time taken: 0.4638216495513916\n",
            "Epoch: 19 Batch Number: 74 Loss: 1.1387951374053955 Time taken: 0.44956088066101074\n",
            "Epoch: 19 Batch Number: 75 Loss: 0.9921371340751648 Time taken: 0.44637250900268555\n",
            "Epoch: 19 Batch Number: 76 Loss: 1.0255813598632812 Time taken: 0.4597158432006836\n",
            "Epoch: 19 Batch Number: 77 Loss: 1.1245427131652832 Time taken: 0.44878411293029785\n",
            "Epoch: 19 Batch Number: 78 Loss: 1.0224111080169678 Time taken: 0.44858527183532715\n",
            "Epoch: 19 Batch Number: 79 Loss: 1.1003320217132568 Time taken: 0.4587852954864502\n",
            "Epoch: 19 Batch Number: 80 Loss: 1.0703243017196655 Time taken: 0.4509706497192383\n",
            "Epoch: 19 Batch Number: 81 Loss: 1.1029810905456543 Time taken: 0.4499328136444092\n",
            "Epoch: 19 Batch Number: 82 Loss: 1.0807915925979614 Time taken: 0.44864368438720703\n",
            "Epoch: 19 Batch Number: 83 Loss: 1.1526377201080322 Time taken: 0.4420015811920166\n",
            "Epoch: 19 Batch Number: 84 Loss: 1.1157118082046509 Time taken: 0.4610707759857178\n",
            "Epoch: 19 Batch Number: 85 Loss: 1.0931878089904785 Time taken: 0.45378589630126953\n",
            "Epoch: 19 Batch Number: 86 Loss: 1.0736541748046875 Time taken: 0.4417297840118408\n",
            "Epoch: 19 Batch Number: 87 Loss: 1.1467534303665161 Time taken: 0.44628190994262695\n",
            "Epoch: 19 Batch Number: 88 Loss: 1.0395084619522095 Time taken: 0.4502248764038086\n",
            "Epoch: 19 Batch Number: 89 Loss: 1.0411370992660522 Time taken: 0.45075201988220215\n",
            "Epoch: 19 Batch Number: 90 Loss: 1.0832778215408325 Time taken: 0.4533274173736572\n",
            "Epoch: 19 Batch Number: 91 Loss: 1.0363905429840088 Time taken: 0.45839476585388184\n",
            "Epoch: 19 Batch Number: 92 Loss: 1.0167925357818604 Time taken: 0.44304919242858887\n",
            "Epoch: 19 Batch Number: 93 Loss: 1.0396649837493896 Time taken: 0.44634461402893066\n",
            "Epoch: 19 Batch Number: 94 Loss: 1.195176362991333 Time taken: 0.4412996768951416\n",
            "Epoch: 19 Batch Number: 95 Loss: 1.1369397640228271 Time taken: 0.44824886322021484\n",
            "Epoch: 19 Batch Number: 96 Loss: 1.1812450885772705 Time taken: 0.43869447708129883\n",
            "Epoch: 19 Batch Number: 97 Loss: 1.1166951656341553 Time taken: 0.4403045177459717\n",
            "Epoch: 19 Batch Number: 98 Loss: 1.1744160652160645 Time taken: 0.45212459564208984\n",
            "Epoch: 19 Batch Number: 99 Loss: 1.2167792320251465 Time taken: 0.4422621726989746\n",
            "Epoch: 19 Batch Number: 100 Loss: 1.1409151554107666 Time taken: 0.4417412281036377\n",
            "Epoch: 19 Batch Number: 101 Loss: 1.1321580410003662 Time taken: 0.4410414695739746\n",
            "Epoch: 19 Batch Number: 102 Loss: 1.170109510421753 Time taken: 0.447559118270874\n",
            "Epoch: 19 Batch Number: 103 Loss: 1.2290089130401611 Time taken: 0.44530320167541504\n",
            "Epoch: 19 Batch Number: 104 Loss: 1.1229124069213867 Time taken: 0.44652605056762695\n",
            "Epoch: 19 Batch Number: 105 Loss: 1.2010136842727661 Time taken: 0.4558067321777344\n",
            "Epoch: 19 Batch Number: 106 Loss: 1.1659470796585083 Time taken: 0.44318246841430664\n",
            "Epoch: 19 Batch Number: 107 Loss: 1.1664429903030396 Time taken: 0.45221805572509766\n",
            "Epoch: 19 Batch Number: 108 Loss: 1.1250500679016113 Time taken: 0.43929052352905273\n",
            "Epoch: 19 Batch Number: 109 Loss: 1.1244796514511108 Time taken: 0.44152379035949707\n",
            "Epoch: 19 Batch Number: 110 Loss: 0.9844934940338135 Time taken: 0.4432361125946045\n",
            "Epoch: 19 Batch Number: 111 Loss: 1.0631341934204102 Time taken: 0.4541018009185791\n",
            "Epoch: 19 Batch Number: 112 Loss: 1.0703163146972656 Time taken: 0.4473733901977539\n",
            "Epoch: 19 Batch Number: 113 Loss: 1.0940134525299072 Time taken: 0.44133853912353516\n",
            "Epoch: 19 Batch Number: 114 Loss: 1.1501638889312744 Time taken: 0.4405550956726074\n",
            "Epoch: 19 Batch Number: 115 Loss: 1.1357988119125366 Time taken: 0.44821596145629883\n",
            "Epoch: 19 Batch Number: 116 Loss: 1.0224659442901611 Time taken: 0.45015835762023926\n",
            "Epoch: 19 Batch Number: 117 Loss: 1.1086158752441406 Time taken: 0.43558311462402344\n",
            "Epoch: 19 Batch Number: 118 Loss: 1.068939447402954 Time taken: 0.44300127029418945\n",
            "Epoch: 19 Batch Number: 119 Loss: 1.114772915840149 Time taken: 0.44596195220947266\n",
            "Epoch: 19 Batch Number: 120 Loss: 1.1506853103637695 Time taken: 0.445040225982666\n",
            "Epoch: 19 Batch Number: 121 Loss: 1.0979303121566772 Time taken: 0.44083237648010254\n",
            "Epoch: 19 Batch Number: 122 Loss: 1.0818369388580322 Time taken: 0.45261621475219727\n",
            "Epoch: 19 Batch Number: 123 Loss: 1.1067736148834229 Time taken: 0.4558734893798828\n",
            "Epoch: 19 Batch Number: 124 Loss: 1.0696152448654175 Time taken: 0.45255184173583984\n",
            "Epoch: 19 Batch Number: 125 Loss: 1.105216383934021 Time taken: 0.44022417068481445\n",
            "Epoch: 19 Batch Number: 126 Loss: 1.12333345413208 Time taken: 0.44733548164367676\n",
            "Epoch: 19 Batch Number: 127 Loss: 1.144741415977478 Time taken: 0.44463396072387695\n",
            "Epoch: 19 Batch Number: 128 Loss: 1.0841467380523682 Time taken: 0.44370317459106445\n",
            "Epoch: 19 Batch Number: 129 Loss: 1.0916270017623901 Time taken: 0.450148344039917\n",
            "Epoch: 19 Batch Number: 130 Loss: 1.0683664083480835 Time taken: 0.44269657135009766\n",
            "Epoch: 19 Batch Number: 131 Loss: 1.146878957748413 Time taken: 0.4356544017791748\n",
            "Epoch: 19 Batch Number: 132 Loss: 0.9977847337722778 Time taken: 0.44881653785705566\n",
            "Epoch: 19 Batch Number: 133 Loss: 1.1635870933532715 Time taken: 0.4437394142150879\n",
            "Epoch: 19 Batch Number: 134 Loss: 1.1577600240707397 Time taken: 0.44831395149230957\n",
            "Epoch: 19 Batch Number: 135 Loss: 1.1555814743041992 Time taken: 0.4502532482147217\n",
            "Epoch: 19 Batch Number: 136 Loss: 1.1474967002868652 Time taken: 0.4514641761779785\n",
            "Epoch: 19 Batch Number: 137 Loss: 1.0980299711227417 Time taken: 0.4486813545227051\n",
            "Epoch: 19 Batch Number: 138 Loss: 1.1282336711883545 Time taken: 0.4324021339416504\n",
            "Epoch: 19 Batch Number: 139 Loss: 1.1598858833312988 Time taken: 0.4423682689666748\n",
            "Epoch: 19 Batch Number: 140 Loss: 1.0922889709472656 Time taken: 0.45000290870666504\n",
            "Epoch: 19 Batch Number: 141 Loss: 1.1386204957962036 Time taken: 0.4484744071960449\n",
            "Epoch: 19 Batch Number: 142 Loss: 1.177113652229309 Time taken: 0.45357465744018555\n",
            "Epoch: 19 Batch Number: 143 Loss: 1.2375890016555786 Time taken: 0.4428582191467285\n",
            "Epoch: 19 Batch Number: 144 Loss: 1.2352421283721924 Time taken: 0.44686460494995117\n",
            "Epoch: 19 Batch Number: 145 Loss: 1.236715316772461 Time taken: 0.45181798934936523\n",
            "Epoch: 19 Batch Number: 146 Loss: 1.205984115600586 Time taken: 0.44318628311157227\n",
            "Epoch: 19 Batch Number: 147 Loss: 1.0787216424942017 Time taken: 0.4466583728790283\n",
            "Epoch: 19 Batch Number: 148 Loss: 1.046495795249939 Time taken: 0.44717907905578613\n",
            "Epoch: 19 Batch Number: 149 Loss: 1.1345099210739136 Time taken: 0.44925546646118164\n",
            "Epoch: 19 Batch Number: 150 Loss: 1.2165088653564453 Time taken: 0.45397448539733887\n",
            "Epoch: 19 Batch Number: 151 Loss: 1.2236799001693726 Time taken: 0.4753453731536865\n",
            "Epoch: 19 Batch Number: 152 Loss: 1.1529196500778198 Time taken: 0.4454929828643799\n",
            "Epoch: 19 Batch Number: 153 Loss: 1.0820233821868896 Time taken: 0.4490475654602051\n",
            "Epoch: 19 Batch Number: 154 Loss: 1.070807695388794 Time taken: 0.44876623153686523\n",
            "Epoch: 19 Batch Number: 155 Loss: 1.075016736984253 Time taken: 0.4609096050262451\n",
            "Epoch: 19 Batch Number: 156 Loss: 1.1407979726791382 Time taken: 0.4417252540588379\n",
            "Epoch: 19 Batch Number: 157 Loss: 1.0138952732086182 Time taken: 0.4511220455169678\n",
            "Epoch: 19 Batch Number: 158 Loss: 1.0835016965866089 Time taken: 0.4390876293182373\n",
            "Epoch: 19 Batch Number: 159 Loss: 1.021073818206787 Time taken: 0.44504737854003906\n",
            "Epoch: 19 Batch Number: 160 Loss: 0.9366194009780884 Time taken: 0.44593381881713867\n",
            "Epoch: 19 Batch Number: 161 Loss: 1.008460283279419 Time taken: 0.4519827365875244\n",
            "Epoch: 19 Batch Number: 162 Loss: 0.9698139429092407 Time taken: 0.44910502433776855\n",
            "Epoch: 19 Batch Number: 163 Loss: 1.086928367614746 Time taken: 0.44022059440612793\n",
            "Epoch: 19 Batch Number: 164 Loss: 1.177862286567688 Time taken: 0.4445462226867676\n",
            "Epoch: 19 Batch Number: 165 Loss: 1.075904369354248 Time taken: 0.4366648197174072\n",
            "Epoch: 19 Batch Number: 166 Loss: 1.1300256252288818 Time taken: 0.4465219974517822\n",
            "Epoch: 19 Batch Number: 167 Loss: 1.117250680923462 Time taken: 0.4516468048095703\n",
            "Epoch: 19 Batch Number: 168 Loss: 1.1012332439422607 Time taken: 0.45942258834838867\n",
            "Epoch: 19 Batch Number: 169 Loss: 1.0994877815246582 Time taken: 0.44728708267211914\n",
            "Epoch: 19 Batch Number: 170 Loss: 1.036050796508789 Time taken: 0.4376239776611328\n",
            "Epoch: 19 Batch Number: 171 Loss: 0.9895036220550537 Time taken: 0.4402902126312256\n",
            "Epoch: 19 Batch Number: 172 Loss: 1.05643892288208 Time taken: 0.45014452934265137\n",
            "Epoch: 19 Batch Number: 173 Loss: 1.0460431575775146 Time taken: 0.43953561782836914\n",
            "Epoch: 19 Batch Number: 174 Loss: 0.992699921131134 Time taken: 0.43680572509765625\n",
            "Epoch: 19 Batch Number: 175 Loss: 1.087239146232605 Time taken: 0.44225645065307617\n",
            "Epoch: 19 Batch Number: 176 Loss: 1.0682406425476074 Time taken: 0.4450368881225586\n",
            "Epoch: 19 Batch Number: 177 Loss: 1.0962753295898438 Time taken: 0.4538266658782959\n",
            "Epoch: 19 Batch Number: 178 Loss: 1.1567785739898682 Time taken: 0.4523193836212158\n",
            "Epoch: 19 Batch Number: 179 Loss: 1.1469385623931885 Time taken: 0.459155797958374\n",
            "Epoch: 19 Batch Number: 180 Loss: 1.0993297100067139 Time taken: 0.4516434669494629\n",
            "Epoch: 19 Batch Number: 181 Loss: 1.0667328834533691 Time taken: 0.4405980110168457\n",
            "Epoch: 19 Batch Number: 182 Loss: 1.012572169303894 Time taken: 0.4659910202026367\n",
            "Epoch: 19 Batch Number: 183 Loss: 1.0621287822723389 Time taken: 0.46169281005859375\n",
            "Epoch: 19 Batch Number: 184 Loss: 1.13892662525177 Time taken: 0.4608421325683594\n",
            "Epoch: 19 Batch Number: 185 Loss: 1.1537420749664307 Time taken: 0.4475276470184326\n",
            "Epoch: 19 Batch Number: 186 Loss: 0.970332145690918 Time taken: 0.4536013603210449\n",
            "Epoch: 19 Batch Number: 187 Loss: 1.0525532960891724 Time taken: 0.4424564838409424\n",
            "Epoch: 19 Batch Number: 188 Loss: 1.0518689155578613 Time taken: 0.4565150737762451\n",
            "Epoch: 19 Batch Number: 189 Loss: 1.0797820091247559 Time taken: 0.46243953704833984\n",
            "Epoch: 19 Batch Number: 190 Loss: 1.2461941242218018 Time taken: 0.44383716583251953\n",
            "Epoch: 19 Batch Number: 191 Loss: 1.361372947692871 Time taken: 0.44843554496765137\n",
            "Epoch: 19 Batch Number: 192 Loss: 1.1189504861831665 Time taken: 0.44714784622192383\n",
            "Epoch: 19 Batch Number: 193 Loss: 1.2283384799957275 Time taken: 0.4609651565551758\n",
            "Epoch: 19 Batch Number: 194 Loss: 1.1636015176773071 Time taken: 0.43308448791503906\n",
            "Epoch: 19 Batch Number: 195 Loss: 1.107057809829712 Time taken: 0.4397439956665039\n",
            "Epoch: 19 Batch Number: 196 Loss: 1.0565718412399292 Time taken: 0.4422786235809326\n",
            "Epoch: 19 Batch Number: 197 Loss: 1.1309127807617188 Time taken: 0.4660196304321289\n",
            "Epoch: 19 Batch Number: 198 Loss: 1.0493700504302979 Time taken: 0.4435408115386963\n",
            "Epoch: 19 Batch Number: 199 Loss: 1.0896892547607422 Time taken: 0.45195889472961426\n",
            "Epoch: 19 Batch Number: 200 Loss: 1.0482680797576904 Time taken: 0.4397885799407959\n",
            "Epoch: 19 Batch Number: 201 Loss: 1.0885202884674072 Time taken: 0.4427146911621094\n",
            "Epoch: 19 Batch Number: 202 Loss: 1.0684453248977661 Time taken: 0.4425625801086426\n",
            "Epoch: 19 Batch Number: 203 Loss: 1.080553650856018 Time taken: 0.4527714252471924\n",
            "Epoch: 19 Batch Number: 204 Loss: 1.0910333395004272 Time taken: 0.44461560249328613\n",
            "Epoch: 19 Batch Number: 205 Loss: 1.0255520343780518 Time taken: 0.442746639251709\n",
            "Epoch: 19 Batch Number: 206 Loss: 1.07830011844635 Time taken: 0.4507148265838623\n",
            "Epoch: 19 Batch Number: 207 Loss: 1.1252222061157227 Time taken: 0.441727876663208\n",
            "Epoch: 19 Batch Number: 208 Loss: 1.106410026550293 Time taken: 0.4369523525238037\n",
            "Epoch: 19 Batch Number: 209 Loss: 1.1334398984909058 Time taken: 0.44508814811706543\n",
            "Epoch: 19 Batch Number: 210 Loss: 1.1885733604431152 Time taken: 0.4465641975402832\n",
            "Epoch: 19 Batch Number: 211 Loss: 1.1476807594299316 Time taken: 0.4435384273529053\n",
            "Epoch: 19 Batch Number: 212 Loss: 1.2657634019851685 Time taken: 0.444286584854126\n",
            "Epoch: 19 Batch Number: 213 Loss: 1.1163781881332397 Time taken: 0.45879077911376953\n",
            "Epoch: 19 Batch Number: 214 Loss: 1.155283808708191 Time taken: 0.4427635669708252\n",
            "Epoch: 19 Batch Number: 215 Loss: 1.082060694694519 Time taken: 0.4453461170196533\n",
            "Epoch: 19 Batch Number: 216 Loss: 1.1470417976379395 Time taken: 0.44382810592651367\n",
            "Epoch: 19 Batch Number: 217 Loss: 1.1969189643859863 Time taken: 0.45598721504211426\n",
            "Epoch: 19 Batch Number: 218 Loss: 1.168360710144043 Time taken: 0.44706034660339355\n",
            "Epoch: 19 Batch Number: 219 Loss: 1.1854690313339233 Time taken: 0.4424476623535156\n",
            "Epoch: 19 Batch Number: 220 Loss: 1.1171380281448364 Time taken: 0.4471752643585205\n",
            "Epoch: 19 Batch Number: 221 Loss: 1.1239581108093262 Time taken: 0.45775294303894043\n",
            "Epoch: 19 Batch Number: 222 Loss: 1.088134527206421 Time taken: 0.4372828006744385\n",
            "Epoch: 19 Batch Number: 223 Loss: 1.101921558380127 Time taken: 0.43892335891723633\n",
            "Epoch: 19 Batch Number: 224 Loss: 1.0653003454208374 Time taken: 0.4676072597503662\n",
            "Epoch: 19 Batch Number: 225 Loss: 1.0458183288574219 Time taken: 0.4458622932434082\n",
            "Epoch: 19 Batch Number: 226 Loss: 1.0823349952697754 Time taken: 0.4442150592803955\n",
            "Epoch: 19 Batch Number: 227 Loss: 1.198428750038147 Time taken: 0.4410054683685303\n",
            "Epoch: 19 Batch Number: 228 Loss: 1.1546688079833984 Time taken: 0.44583725929260254\n",
            "Epoch: 19 Batch Number: 229 Loss: 1.1806470155715942 Time taken: 0.43666863441467285\n",
            "==========================================================================================\n",
            "Start of epoch 20\n",
            "Epoch: 20 Batch Number: 1 Loss: 1.1028633117675781 Time taken: 0.44154834747314453\n",
            "Epoch: 20 Batch Number: 2 Loss: 1.1583359241485596 Time taken: 0.44905781745910645\n",
            "Epoch: 20 Batch Number: 3 Loss: 1.1225557327270508 Time taken: 0.43805742263793945\n",
            "Epoch: 20 Batch Number: 4 Loss: 1.080608606338501 Time taken: 0.45357561111450195\n",
            "Epoch: 20 Batch Number: 5 Loss: 1.0956237316131592 Time taken: 0.44312071800231934\n",
            "Epoch: 20 Batch Number: 6 Loss: 1.029913067817688 Time taken: 0.4594402313232422\n",
            "Epoch: 20 Batch Number: 7 Loss: 1.0387043952941895 Time taken: 0.4386260509490967\n",
            "Epoch: 20 Batch Number: 8 Loss: 1.0807923078536987 Time taken: 0.4741795063018799\n",
            "Epoch: 20 Batch Number: 9 Loss: 1.0541057586669922 Time taken: 0.4737093448638916\n",
            "Epoch: 20 Batch Number: 10 Loss: 1.102325677871704 Time taken: 0.45633506774902344\n",
            "Epoch: 20 Batch Number: 11 Loss: 1.049249529838562 Time taken: 0.4588949680328369\n",
            "Epoch: 20 Batch Number: 12 Loss: 1.0218249559402466 Time taken: 0.4507420063018799\n",
            "Epoch: 20 Batch Number: 13 Loss: 1.047750473022461 Time taken: 0.46978163719177246\n",
            "Epoch: 20 Batch Number: 14 Loss: 1.0611472129821777 Time taken: 0.4566054344177246\n",
            "Epoch: 20 Batch Number: 15 Loss: 1.0147373676300049 Time taken: 0.46043872833251953\n",
            "Epoch: 20 Batch Number: 16 Loss: 1.0367794036865234 Time taken: 0.4502391815185547\n",
            "Epoch: 20 Batch Number: 17 Loss: 1.1609492301940918 Time taken: 0.45081496238708496\n",
            "Epoch: 20 Batch Number: 18 Loss: 1.1342743635177612 Time taken: 0.4528007507324219\n",
            "Epoch: 20 Batch Number: 19 Loss: 1.1506047248840332 Time taken: 0.46596574783325195\n",
            "Epoch: 20 Batch Number: 20 Loss: 1.0324761867523193 Time taken: 0.4632413387298584\n",
            "Epoch: 20 Batch Number: 21 Loss: 1.1674829721450806 Time taken: 0.45183873176574707\n",
            "Epoch: 20 Batch Number: 22 Loss: 1.168515920639038 Time taken: 0.45282411575317383\n",
            "Epoch: 20 Batch Number: 23 Loss: 1.1631015539169312 Time taken: 0.44521021842956543\n",
            "Epoch: 20 Batch Number: 24 Loss: 1.153065800666809 Time taken: 0.458355188369751\n",
            "Epoch: 20 Batch Number: 25 Loss: 1.1549952030181885 Time taken: 0.44133782386779785\n",
            "Epoch: 20 Batch Number: 26 Loss: 1.1543537378311157 Time taken: 0.4402809143066406\n",
            "Epoch: 20 Batch Number: 27 Loss: 1.1057910919189453 Time taken: 0.4500131607055664\n",
            "Epoch: 20 Batch Number: 28 Loss: 1.1019718647003174 Time taken: 0.44118261337280273\n",
            "Epoch: 20 Batch Number: 29 Loss: 1.1583372354507446 Time taken: 0.4502699375152588\n",
            "Epoch: 20 Batch Number: 30 Loss: 0.9407767653465271 Time taken: 0.4463188648223877\n",
            "Epoch: 20 Batch Number: 31 Loss: 1.097428798675537 Time taken: 0.4462769031524658\n",
            "Epoch: 20 Batch Number: 32 Loss: 1.0951564311981201 Time taken: 0.46306848526000977\n",
            "Epoch: 20 Batch Number: 33 Loss: 1.1217985153198242 Time taken: 0.46328091621398926\n",
            "Epoch: 20 Batch Number: 34 Loss: 1.1439998149871826 Time taken: 0.44148755073547363\n",
            "Epoch: 20 Batch Number: 35 Loss: 1.1823832988739014 Time taken: 0.4458327293395996\n",
            "Epoch: 20 Batch Number: 36 Loss: 1.2159929275512695 Time taken: 0.43897342681884766\n",
            "Epoch: 20 Batch Number: 37 Loss: 1.111645221710205 Time taken: 0.4420771598815918\n",
            "Epoch: 20 Batch Number: 38 Loss: 1.1615961790084839 Time taken: 0.43750619888305664\n",
            "Epoch: 20 Batch Number: 39 Loss: 1.0826427936553955 Time taken: 0.45093274116516113\n",
            "Epoch: 20 Batch Number: 40 Loss: 1.0897040367126465 Time taken: 0.449129581451416\n",
            "Epoch: 20 Batch Number: 41 Loss: 1.0707669258117676 Time taken: 0.4437730312347412\n",
            "Epoch: 20 Batch Number: 42 Loss: 1.0679240226745605 Time taken: 0.4487147331237793\n",
            "Epoch: 20 Batch Number: 43 Loss: 1.0512651205062866 Time taken: 0.4560415744781494\n",
            "Epoch: 20 Batch Number: 44 Loss: 1.0155574083328247 Time taken: 0.4437069892883301\n",
            "Epoch: 20 Batch Number: 45 Loss: 1.0748355388641357 Time taken: 0.4641580581665039\n",
            "Epoch: 20 Batch Number: 46 Loss: 1.2360310554504395 Time taken: 0.45650815963745117\n",
            "Epoch: 20 Batch Number: 47 Loss: 1.0890319347381592 Time taken: 0.44925642013549805\n",
            "Epoch: 20 Batch Number: 48 Loss: 1.14668869972229 Time taken: 0.43920111656188965\n",
            "Epoch: 20 Batch Number: 49 Loss: 1.1404165029525757 Time taken: 0.438676118850708\n",
            "Epoch: 20 Batch Number: 50 Loss: 1.0568454265594482 Time taken: 0.44966959953308105\n",
            "Epoch: 20 Batch Number: 51 Loss: 1.0428354740142822 Time taken: 0.452237606048584\n",
            "Epoch: 20 Batch Number: 52 Loss: 1.1646208763122559 Time taken: 0.4378702640533447\n",
            "Epoch: 20 Batch Number: 53 Loss: 1.1908787488937378 Time taken: 0.4484114646911621\n",
            "Epoch: 20 Batch Number: 54 Loss: 1.1677871942520142 Time taken: 0.4543602466583252\n",
            "Epoch: 20 Batch Number: 55 Loss: 1.1513715982437134 Time taken: 0.444049596786499\n",
            "Epoch: 20 Batch Number: 56 Loss: 1.2427775859832764 Time taken: 0.43560338020324707\n",
            "Epoch: 20 Batch Number: 57 Loss: 1.1214213371276855 Time taken: 0.44102907180786133\n",
            "Epoch: 20 Batch Number: 58 Loss: 1.0768802165985107 Time taken: 0.4657301902770996\n",
            "Epoch: 20 Batch Number: 59 Loss: 1.106511116027832 Time taken: 0.44739222526550293\n",
            "Epoch: 20 Batch Number: 60 Loss: 1.0921406745910645 Time taken: 0.4570775032043457\n",
            "Epoch: 20 Batch Number: 61 Loss: 1.1073704957962036 Time taken: 0.4484729766845703\n",
            "Epoch: 20 Batch Number: 62 Loss: 1.0182217359542847 Time taken: 0.44190335273742676\n",
            "Epoch: 20 Batch Number: 63 Loss: 1.059126615524292 Time taken: 0.43656158447265625\n",
            "Epoch: 20 Batch Number: 64 Loss: 1.0116811990737915 Time taken: 0.44748449325561523\n",
            "Epoch: 20 Batch Number: 65 Loss: 1.0618269443511963 Time taken: 0.44506001472473145\n",
            "Epoch: 20 Batch Number: 66 Loss: 1.0789252519607544 Time taken: 0.4398319721221924\n",
            "Epoch: 20 Batch Number: 67 Loss: 1.0609676837921143 Time taken: 0.46435999870300293\n",
            "Epoch: 20 Batch Number: 68 Loss: 1.119356393814087 Time taken: 0.45475029945373535\n",
            "Epoch: 20 Batch Number: 69 Loss: 1.0916705131530762 Time taken: 0.450883150100708\n",
            "Epoch: 20 Batch Number: 70 Loss: 1.1010007858276367 Time taken: 0.4369373321533203\n",
            "Epoch: 20 Batch Number: 71 Loss: 1.0745919942855835 Time taken: 0.45761609077453613\n",
            "Epoch: 20 Batch Number: 72 Loss: 1.0869135856628418 Time taken: 0.45924901962280273\n",
            "Epoch: 20 Batch Number: 73 Loss: 1.0428321361541748 Time taken: 0.44624900817871094\n",
            "Epoch: 20 Batch Number: 74 Loss: 1.13521146774292 Time taken: 0.4581878185272217\n",
            "Epoch: 20 Batch Number: 75 Loss: 0.9877399206161499 Time taken: 0.45256614685058594\n",
            "Epoch: 20 Batch Number: 76 Loss: 1.0225712060928345 Time taken: 0.46157217025756836\n",
            "Epoch: 20 Batch Number: 77 Loss: 1.1143306493759155 Time taken: 0.44823217391967773\n",
            "Epoch: 20 Batch Number: 78 Loss: 1.013106107711792 Time taken: 0.43420982360839844\n",
            "Epoch: 20 Batch Number: 79 Loss: 1.0874673128128052 Time taken: 0.4421508312225342\n",
            "Epoch: 20 Batch Number: 80 Loss: 1.0611393451690674 Time taken: 0.4683694839477539\n",
            "Epoch: 20 Batch Number: 81 Loss: 1.0862383842468262 Time taken: 0.44315385818481445\n",
            "Epoch: 20 Batch Number: 82 Loss: 1.0702836513519287 Time taken: 0.44719743728637695\n",
            "Epoch: 20 Batch Number: 83 Loss: 1.1450755596160889 Time taken: 0.44210243225097656\n",
            "Epoch: 20 Batch Number: 84 Loss: 1.1010241508483887 Time taken: 0.47211384773254395\n",
            "Epoch: 20 Batch Number: 85 Loss: 1.0813937187194824 Time taken: 0.4429326057434082\n",
            "Epoch: 20 Batch Number: 86 Loss: 1.061090111732483 Time taken: 0.4421834945678711\n",
            "Epoch: 20 Batch Number: 87 Loss: 1.1383309364318848 Time taken: 0.4442884922027588\n",
            "Epoch: 20 Batch Number: 88 Loss: 1.0265086889266968 Time taken: 0.4445321559906006\n",
            "Epoch: 20 Batch Number: 89 Loss: 1.0298188924789429 Time taken: 0.4630157947540283\n",
            "Epoch: 20 Batch Number: 90 Loss: 1.0744092464447021 Time taken: 0.4459846019744873\n",
            "Epoch: 20 Batch Number: 91 Loss: 1.0303623676300049 Time taken: 0.46610164642333984\n",
            "Epoch: 20 Batch Number: 92 Loss: 1.0029046535491943 Time taken: 0.4572451114654541\n",
            "Epoch: 20 Batch Number: 93 Loss: 1.0293874740600586 Time taken: 0.4525318145751953\n",
            "Epoch: 20 Batch Number: 94 Loss: 1.1829055547714233 Time taken: 0.4581007957458496\n",
            "Epoch: 20 Batch Number: 95 Loss: 1.12168288230896 Time taken: 0.4572486877441406\n",
            "Epoch: 20 Batch Number: 96 Loss: 1.1698322296142578 Time taken: 0.44727158546447754\n",
            "Epoch: 20 Batch Number: 97 Loss: 1.1041033267974854 Time taken: 0.4438319206237793\n",
            "Epoch: 20 Batch Number: 98 Loss: 1.1636865139007568 Time taken: 0.4644651412963867\n",
            "Epoch: 20 Batch Number: 99 Loss: 1.2060803174972534 Time taken: 0.4539954662322998\n",
            "Epoch: 20 Batch Number: 100 Loss: 1.1314712762832642 Time taken: 0.45122504234313965\n",
            "Epoch: 20 Batch Number: 101 Loss: 1.1213138103485107 Time taken: 0.4602377414703369\n",
            "Epoch: 20 Batch Number: 102 Loss: 1.1614512205123901 Time taken: 0.44534993171691895\n",
            "Epoch: 20 Batch Number: 103 Loss: 1.2183146476745605 Time taken: 0.44604039192199707\n",
            "Epoch: 20 Batch Number: 104 Loss: 1.1172116994857788 Time taken: 0.45240068435668945\n",
            "Epoch: 20 Batch Number: 105 Loss: 1.1895544528961182 Time taken: 0.4420015811920166\n",
            "Epoch: 20 Batch Number: 106 Loss: 1.1547878980636597 Time taken: 0.46257591247558594\n",
            "Epoch: 20 Batch Number: 107 Loss: 1.156496286392212 Time taken: 0.4517982006072998\n",
            "Epoch: 20 Batch Number: 108 Loss: 1.1159782409667969 Time taken: 0.4435853958129883\n",
            "Epoch: 20 Batch Number: 109 Loss: 1.1150418519973755 Time taken: 0.4461333751678467\n",
            "Epoch: 20 Batch Number: 110 Loss: 0.9722902774810791 Time taken: 0.4370718002319336\n",
            "Epoch: 20 Batch Number: 111 Loss: 1.0552765130996704 Time taken: 0.43833494186401367\n",
            "Epoch: 20 Batch Number: 112 Loss: 1.0634379386901855 Time taken: 0.43549394607543945\n",
            "Epoch: 20 Batch Number: 113 Loss: 1.0840058326721191 Time taken: 0.44570422172546387\n",
            "Epoch: 20 Batch Number: 114 Loss: 1.1436762809753418 Time taken: 0.4426281452178955\n",
            "Epoch: 20 Batch Number: 115 Loss: 1.1273181438446045 Time taken: 0.4638988971710205\n",
            "Epoch: 20 Batch Number: 116 Loss: 1.0143502950668335 Time taken: 0.46531152725219727\n",
            "Epoch: 20 Batch Number: 117 Loss: 1.0948400497436523 Time taken: 0.45661091804504395\n",
            "Epoch: 20 Batch Number: 118 Loss: 1.0590307712554932 Time taken: 0.4534480571746826\n",
            "Epoch: 20 Batch Number: 119 Loss: 1.1098146438598633 Time taken: 0.43209123611450195\n",
            "Epoch: 20 Batch Number: 120 Loss: 1.140066385269165 Time taken: 0.4537010192871094\n",
            "Epoch: 20 Batch Number: 121 Loss: 1.0905721187591553 Time taken: 0.44402408599853516\n",
            "Epoch: 20 Batch Number: 122 Loss: 1.0755140781402588 Time taken: 0.4397120475769043\n",
            "Epoch: 20 Batch Number: 123 Loss: 1.0986340045928955 Time taken: 0.44515061378479004\n",
            "Epoch: 20 Batch Number: 124 Loss: 1.060564398765564 Time taken: 0.4500434398651123\n",
            "Epoch: 20 Batch Number: 125 Loss: 1.0976392030715942 Time taken: 0.43849921226501465\n",
            "Epoch: 20 Batch Number: 126 Loss: 1.112673282623291 Time taken: 0.4424755573272705\n",
            "Epoch: 20 Batch Number: 127 Loss: 1.1305853128433228 Time taken: 0.45375585556030273\n",
            "Epoch: 20 Batch Number: 128 Loss: 1.0776503086090088 Time taken: 0.44196200370788574\n",
            "Epoch: 20 Batch Number: 129 Loss: 1.0853497982025146 Time taken: 0.4394111633300781\n",
            "Epoch: 20 Batch Number: 130 Loss: 1.0605794191360474 Time taken: 0.4432413578033447\n",
            "Epoch: 20 Batch Number: 131 Loss: 1.1397322416305542 Time taken: 0.4499680995941162\n",
            "Epoch: 20 Batch Number: 132 Loss: 0.9894053936004639 Time taken: 0.4358673095703125\n",
            "Epoch: 20 Batch Number: 133 Loss: 1.1546006202697754 Time taken: 0.4380764961242676\n",
            "Epoch: 20 Batch Number: 134 Loss: 1.149712324142456 Time taken: 0.45085978507995605\n",
            "Epoch: 20 Batch Number: 135 Loss: 1.1465867757797241 Time taken: 0.4420289993286133\n",
            "Epoch: 20 Batch Number: 136 Loss: 1.1367921829223633 Time taken: 0.4500432014465332\n",
            "Epoch: 20 Batch Number: 137 Loss: 1.0844930410385132 Time taken: 0.45665979385375977\n",
            "Epoch: 20 Batch Number: 138 Loss: 1.1142399311065674 Time taken: 0.4474809169769287\n",
            "Epoch: 20 Batch Number: 139 Loss: 1.1279425621032715 Time taken: 0.44447779655456543\n",
            "Epoch: 20 Batch Number: 140 Loss: 1.0933153629302979 Time taken: 0.442760705947876\n",
            "Epoch: 20 Batch Number: 141 Loss: 1.139586329460144 Time taken: 0.4573681354522705\n",
            "Epoch: 20 Batch Number: 142 Loss: 1.1711064577102661 Time taken: 0.4412555694580078\n",
            "Epoch: 20 Batch Number: 143 Loss: 1.2263870239257812 Time taken: 0.44919729232788086\n",
            "Epoch: 20 Batch Number: 144 Loss: 1.224209189414978 Time taken: 0.44620370864868164\n",
            "Epoch: 20 Batch Number: 145 Loss: 1.2268719673156738 Time taken: 0.4634695053100586\n",
            "Epoch: 20 Batch Number: 146 Loss: 1.1919832229614258 Time taken: 0.452608585357666\n",
            "Epoch: 20 Batch Number: 147 Loss: 1.063136339187622 Time taken: 0.4428136348724365\n",
            "Epoch: 20 Batch Number: 148 Loss: 1.0352935791015625 Time taken: 0.4396648406982422\n",
            "Epoch: 20 Batch Number: 149 Loss: 1.1245636940002441 Time taken: 0.4480476379394531\n",
            "Epoch: 20 Batch Number: 150 Loss: 1.2027150392532349 Time taken: 0.4479813575744629\n",
            "Epoch: 20 Batch Number: 151 Loss: 1.2112761735916138 Time taken: 0.44698596000671387\n",
            "Epoch: 20 Batch Number: 152 Loss: 1.1464738845825195 Time taken: 0.4508368968963623\n",
            "Epoch: 20 Batch Number: 153 Loss: 1.069650650024414 Time taken: 0.4436812400817871\n",
            "Epoch: 20 Batch Number: 154 Loss: 1.0546326637268066 Time taken: 0.4413719177246094\n",
            "Epoch: 20 Batch Number: 155 Loss: 1.060724139213562 Time taken: 0.457852840423584\n",
            "Epoch: 20 Batch Number: 156 Loss: 1.1324282884597778 Time taken: 0.43883776664733887\n",
            "Epoch: 20 Batch Number: 157 Loss: 1.0052752494812012 Time taken: 0.4429337978363037\n",
            "Epoch: 20 Batch Number: 158 Loss: 1.0738439559936523 Time taken: 0.4424095153808594\n",
            "Epoch: 20 Batch Number: 159 Loss: 1.0104386806488037 Time taken: 0.456881046295166\n",
            "Epoch: 20 Batch Number: 160 Loss: 0.9271453619003296 Time taken: 0.4439568519592285\n",
            "Epoch: 20 Batch Number: 161 Loss: 0.9950774908065796 Time taken: 0.43799376487731934\n",
            "Epoch: 20 Batch Number: 162 Loss: 0.9566518664360046 Time taken: 0.4445819854736328\n",
            "Epoch: 20 Batch Number: 163 Loss: 1.0699995756149292 Time taken: 0.45075440406799316\n",
            "Epoch: 20 Batch Number: 164 Loss: 1.1594980955123901 Time taken: 0.44677090644836426\n",
            "Epoch: 20 Batch Number: 165 Loss: 1.049073576927185 Time taken: 0.4416379928588867\n",
            "Epoch: 20 Batch Number: 166 Loss: 1.113255262374878 Time taken: 0.43398451805114746\n",
            "Epoch: 20 Batch Number: 167 Loss: 1.1030200719833374 Time taken: 0.4554870128631592\n",
            "Epoch: 20 Batch Number: 168 Loss: 1.0933619737625122 Time taken: 0.45209193229675293\n",
            "Epoch: 20 Batch Number: 169 Loss: 1.0931599140167236 Time taken: 0.44001007080078125\n",
            "Epoch: 20 Batch Number: 170 Loss: 1.0169435739517212 Time taken: 0.4373033046722412\n",
            "Epoch: 20 Batch Number: 171 Loss: 0.9697103500366211 Time taken: 0.45479345321655273\n",
            "Epoch: 20 Batch Number: 172 Loss: 1.042264699935913 Time taken: 0.4576230049133301\n",
            "Epoch: 20 Batch Number: 173 Loss: 1.033384084701538 Time taken: 0.44045162200927734\n",
            "Epoch: 20 Batch Number: 174 Loss: 0.980059027671814 Time taken: 0.4447629451751709\n",
            "Epoch: 20 Batch Number: 175 Loss: 1.077162265777588 Time taken: 0.43793201446533203\n",
            "Epoch: 20 Batch Number: 176 Loss: 1.0558865070343018 Time taken: 0.4323873519897461\n",
            "Epoch: 20 Batch Number: 177 Loss: 1.0897550582885742 Time taken: 0.4544947147369385\n",
            "Epoch: 20 Batch Number: 178 Loss: 1.1463894844055176 Time taken: 0.4431953430175781\n",
            "Epoch: 20 Batch Number: 179 Loss: 1.1347148418426514 Time taken: 0.4399998188018799\n",
            "Epoch: 20 Batch Number: 180 Loss: 1.0875024795532227 Time taken: 0.4497032165527344\n",
            "Epoch: 20 Batch Number: 181 Loss: 1.0569850206375122 Time taken: 0.4516024589538574\n",
            "Epoch: 20 Batch Number: 182 Loss: 1.0045746564865112 Time taken: 0.4420039653778076\n",
            "Epoch: 20 Batch Number: 183 Loss: 1.0593215227127075 Time taken: 0.443004846572876\n",
            "Epoch: 20 Batch Number: 184 Loss: 1.1306123733520508 Time taken: 0.4523470401763916\n",
            "Epoch: 20 Batch Number: 185 Loss: 1.1439931392669678 Time taken: 0.44124817848205566\n",
            "Epoch: 20 Batch Number: 186 Loss: 0.9733938574790955 Time taken: 0.44579482078552246\n",
            "Epoch: 20 Batch Number: 187 Loss: 1.0467886924743652 Time taken: 0.44355058670043945\n",
            "Epoch: 20 Batch Number: 188 Loss: 1.0432929992675781 Time taken: 0.4490673542022705\n",
            "Epoch: 20 Batch Number: 189 Loss: 1.0748107433319092 Time taken: 0.4562673568725586\n",
            "Epoch: 20 Batch Number: 190 Loss: 1.2394920587539673 Time taken: 0.45108890533447266\n",
            "Epoch: 20 Batch Number: 191 Loss: 1.3500889539718628 Time taken: 0.46159863471984863\n",
            "Epoch: 20 Batch Number: 192 Loss: 1.1118184328079224 Time taken: 0.4422128200531006\n",
            "Epoch: 20 Batch Number: 193 Loss: 1.2184107303619385 Time taken: 0.44040489196777344\n",
            "Epoch: 20 Batch Number: 194 Loss: 1.1497845649719238 Time taken: 0.45032477378845215\n",
            "Epoch: 20 Batch Number: 195 Loss: 1.09471595287323 Time taken: 0.44864344596862793\n",
            "Epoch: 20 Batch Number: 196 Loss: 1.0399818420410156 Time taken: 0.4383809566497803\n",
            "Epoch: 20 Batch Number: 197 Loss: 1.1202075481414795 Time taken: 0.44263672828674316\n",
            "Epoch: 20 Batch Number: 198 Loss: 1.0325464010238647 Time taken: 0.4370708465576172\n",
            "Epoch: 20 Batch Number: 199 Loss: 1.0796724557876587 Time taken: 0.46049046516418457\n",
            "Epoch: 20 Batch Number: 200 Loss: 1.039695382118225 Time taken: 0.4349827766418457\n",
            "Epoch: 20 Batch Number: 201 Loss: 1.0723729133605957 Time taken: 0.44570493698120117\n",
            "Epoch: 20 Batch Number: 202 Loss: 1.0553867816925049 Time taken: 0.4427790641784668\n",
            "Epoch: 20 Batch Number: 203 Loss: 1.070601224899292 Time taken: 0.44373273849487305\n",
            "Epoch: 20 Batch Number: 204 Loss: 1.0721975564956665 Time taken: 0.4398186206817627\n",
            "Epoch: 20 Batch Number: 205 Loss: 1.010506272315979 Time taken: 0.44390273094177246\n",
            "Epoch: 20 Batch Number: 206 Loss: 1.065251350402832 Time taken: 0.45331645011901855\n",
            "Epoch: 20 Batch Number: 207 Loss: 1.1125725507736206 Time taken: 0.4481348991394043\n",
            "Epoch: 20 Batch Number: 208 Loss: 1.1047680377960205 Time taken: 0.47007298469543457\n",
            "Epoch: 20 Batch Number: 209 Loss: 1.1280114650726318 Time taken: 0.44139933586120605\n",
            "Epoch: 20 Batch Number: 210 Loss: 1.19631028175354 Time taken: 0.4496188163757324\n",
            "Epoch: 20 Batch Number: 211 Loss: 1.1311523914337158 Time taken: 0.45717287063598633\n",
            "Epoch: 20 Batch Number: 212 Loss: 1.252278447151184 Time taken: 0.4593691825866699\n",
            "Epoch: 20 Batch Number: 213 Loss: 1.1077440977096558 Time taken: 0.45555543899536133\n",
            "Epoch: 20 Batch Number: 214 Loss: 1.1561036109924316 Time taken: 0.4409916400909424\n",
            "Epoch: 20 Batch Number: 215 Loss: 1.075421690940857 Time taken: 0.4425508975982666\n",
            "Epoch: 20 Batch Number: 216 Loss: 1.1377570629119873 Time taken: 0.45032644271850586\n",
            "Epoch: 20 Batch Number: 217 Loss: 1.1885240077972412 Time taken: 0.44687318801879883\n",
            "Epoch: 20 Batch Number: 218 Loss: 1.1564172506332397 Time taken: 0.4417600631713867\n",
            "Epoch: 20 Batch Number: 219 Loss: 1.179949402809143 Time taken: 0.4387376308441162\n",
            "Epoch: 20 Batch Number: 220 Loss: 1.0998938083648682 Time taken: 0.45111727714538574\n",
            "Epoch: 20 Batch Number: 221 Loss: 1.111739158630371 Time taken: 0.45935750007629395\n",
            "Epoch: 20 Batch Number: 222 Loss: 1.0776206254959106 Time taken: 0.45211100578308105\n",
            "Epoch: 20 Batch Number: 223 Loss: 1.0951777696609497 Time taken: 0.4420592784881592\n",
            "Epoch: 20 Batch Number: 224 Loss: 1.0553503036499023 Time taken: 0.4432229995727539\n",
            "Epoch: 20 Batch Number: 225 Loss: 1.0333515405654907 Time taken: 0.44438886642456055\n",
            "Epoch: 20 Batch Number: 226 Loss: 1.068293809890747 Time taken: 0.4458916187286377\n",
            "Epoch: 20 Batch Number: 227 Loss: 1.1897382736206055 Time taken: 0.4380643367767334\n",
            "Epoch: 20 Batch Number: 228 Loss: 1.1408820152282715 Time taken: 0.444091796875\n",
            "Epoch: 20 Batch Number: 229 Loss: 1.1736937761306763 Time taken: 0.44438767433166504\n",
            "took 2112.7749881744385 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "  print(\"=\"*90)\n",
        "  print('Start of epoch %d' % (epoch+1,))\n",
        "  \n",
        "  batch_nr = 0\n",
        "\n",
        "  for input_batch,target_batch in dataset:\n",
        "      # steps = steps+1\n",
        "      batch_start = time.time()\n",
        "\n",
        "      loss = train_step(input_batch, target_batch)\n",
        "\n",
        "      # if not steps % 30:\n",
        "\n",
        "\n",
        "      batch_nr = batch_nr+1\n",
        "      batch_stop = time.time()\n",
        "  #      rnn_try(batch_data)\n",
        "      # train_acc_metric(target_batch, logits)\n",
        "      # acc = train_acc_metric.result()\n",
        "      print(\"Epoch: {} Batch Number: {} Loss: {} Time taken: {}\".format(epoch+1,batch_nr,loss,batch_stop-batch_start))\n",
        "\n",
        "      # print(\"Loss: {} Accuracy: {}\".format(loss, acc))\n",
        "      # train_acc_metric.reset_states()\n",
        "\n",
        "\n",
        "stop = time.time()\n",
        "print(\"took {} seconds\\n\".format(stop-start))\n",
        "#start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BTKgA_nd0XWA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model_1 = build_model(vocab_size, embedding_dim, rnn_units,1)\n",
        "model_1.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2PmURS8b_56A",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hrrm4NMt0XWC"
      },
      "source": [
        "#### **Printing Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Aha16kv89E0L"
      },
      "source": [
        "**Observations**\n",
        "\n",
        "1. Observed that when using decorator @tf.function they network started giving nans after the first batch even though reset_states were done. \n",
        "\n",
        "2. When generating text it was observed that the text generated had only one character from the play.... When start character is *S*\n",
        "\n",
        "3. When generating text it was observed that the text generated no character from the play.... When start character is */S*\n",
        "\n",
        "4. When generating text it was observed that the text generated no character from the play.... When start character is *PAD*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "colab_type": "code",
        "id": "IhUB-rDA0XWD",
        "outputId": "3fde179b-fb8c-4431-959b-1217e4a50dd2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DUKE OF YORK:\n",
            "Why art thou so?\n",
            "I would this is the door, let's about at love.\n",
            "And I could report thou likest the offen\n",
            "God speak to the Phrirant: thus it is late\n",
            "If this Jove business, make rejoice\n",
            "Behold your grace in your tongue, mounsieur. But yet\n",
            "Temple villain, haste thy dage: pick him\n",
            "rough, thou art of clare for den, are you, sir.\n",
            "Have been this pitched I will provider so\n",
            "uffects that poar: for her I am, the perfecter\n",
            "That speaks with warts; savage meray,\n",
            "Beseech you! What is you?\n",
            "Come, come, by'll spirits in him in money,\n",
            "To have it cloud thine honour, and when the sums he\n",
            "down to the gates, sir, that I have sworn.\n",
            "Be mock'd in the humorous sovereign! Away! away!\n",
            "Our joinate fierce day hath commoner thine engrooted\n",
            "that we do the devil. Lend your token ye\n",
            "o' the word never did respect me light;\n",
            "Which was Egyptian wish'd in me for answer it.\n",
            "Bid him call the Neckoo's folly. Which, madam,\n",
            "And I flee, do not drink two nature. She said\n",
            "Or favours thy kepper of thy wicked Tomach'd.\n",
            "What music for this subject? and words\n",
            "again a monstrous breath that take me, in thy shopt,\n",
            "Troilus is whence and flattery unto this people\n",
            "In case away: good even: cartain, that is any liggless stooles\n",
            "The clothieg's on the basket. Maram, heaven believe\n",
            "him so. But thief, belike; for his kind foul wrong,\n",
            "'Tis too truth, 'tis the Count Clips o'erson'd.\n",
            "Unless drown thain upon you.\n",
            "Thus live as my mine tongues sweet--\n",
            "Bock our catch, sir, to make well done, if thou\n",
            "shalt have you slaughter'd me, but nothing her\n",
            "bears, you shall move you quineth, it is.\n",
            "He through the people, thou like not on a staff's sons.\n",
            "The remedies: and leave me live,\n",
            "'Tis some as gentle ail, and not you he\n",
            "been also dear on my giving, sir.\n",
            "I know nothing, Brutus Roman mother does to speak.\n",
            "And your request would you have,\n",
            "Sirrah, Sir Tapieet, Mistress Pyramus.\n",
            "Do you call you in your fifterchief to young main?\n",
            "But thy first money that thou speak to stead and so\n",
            "All be that or drinking-knight;\n",
            "Or I am a very lark t\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "num_generate = 2000\n",
        "\n",
        "\n",
        "text_generated = []\n",
        "#temp = 1.5\n",
        "#h_t = tf.zeros([1,n_h])\n",
        "start_char = '<S>'\n",
        "choice = []\n",
        "val = vocab[start_char]\n",
        "choice.append(val)\n",
        "\n",
        "model_1.reset_states()\n",
        "for i in range(num_generate):\n",
        "  # x_t = tf.one_hot(choice[-1:],depth = vocab_size)\n",
        "  # a = (tf.matmul(x_t,w_xh))+ (tf.matmul(h_t, w_hh)) + b_h\n",
        "  #print(type(a))\n",
        "  # h_t = tf.nn.tanh(a)\n",
        "  # logits = (tf.matmul(h_t, w_ho)) + b_o\n",
        "  #print(logits)\n",
        "  input_eval = choice[-1:]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  logits = model_1(input_eval)\n",
        "\n",
        "  predictions = tf.nn.softmax(logits)\n",
        "  #predictions = predictions[0,:]\n",
        "  predictions = tf.squeeze(predictions, 0)\n",
        "  #print(\"----------------------\"*3)\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions)\n",
        "  #print(predictions.numpy().shape)\n",
        "  #predictions = predictions / temp\n",
        "\n",
        "  ## The predictions is a tensor \n",
        "  predictions = predictions.numpy()[0]\n",
        "  #predictions = np.array(predictions)\n",
        "  # print(predictions.shape)\n",
        "  #print(predictions)\n",
        "  char_choice = np.random.choice(vocab_size, p = predictions)\n",
        "  #print(char_choice)\n",
        "  choice.append(char_choice)\n",
        "  #print(choice)\n",
        "  #predictions = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "  #print(predictions)\n",
        "  start_char = ind_to_ch[char_choice]\n",
        "  #print(start_char)\n",
        "  #print(input_eval)\n",
        "  text_generated.append(start_char)\n",
        "  \n",
        "print(\"\".join(text_generated))\n",
        "print(\"=\"*90)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yN7nh3Sq0XWF",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "b-A6TAtOMqDu",
        "3wrLAxYhMzDW",
        "XeYTbdk8M767"
      ],
      "name": "IDL_Assignment 6",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
